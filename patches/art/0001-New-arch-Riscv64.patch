From 25088126e5d3435064412f450a59d0fb6b90c1d4 Mon Sep 17 00:00:00 2001
From: xialf <xlf194833@alibaba-inc.com>
Date: Tue, 5 Jan 2021 11:21:06 +0800
Subject: [PATCH] New arch: Riscv64

Implementation of runtime, compiler, dex2oat for riscv64.

Change-Id: If3c7ca4b621c8da61d9f7f87f736c3a1e352f90f
Signed-off-by: Lifang Xia <lifang_xia@linux.alibaba.com>
Signed-off-by: Wendong Wang <wangwd@xcvmbyte.com>
Signed-off-by: Zhengxing Li <lizx@xcvmbyte.com>
---
 build/Android.bp                              |    5 +-
 build/Android.common.mk                       |    2 +-
 build/apex/Android.bp                         |    8 +-
 build/apex/art_apex_test.py                   |    4 +-
 build/art.go                                  |    8 +-
 build/codegen.go                              |    4 +-
 compiler/Android.bp                           |   26 +-
 compiler/common_compiler_test.cc              |    5 +-
 compiler/compiled_method.cc                   |    2 +
 compiler/debug/elf_debug_frame_writer.h       |   25 +
 compiler/debug/elf_debug_line_writer.h        |    1 +
 compiler/debug/elf_debug_loc_writer.h         |    4 +
 compiler/jit/jit_logger.cc                    |    3 +
 compiler/jni/quick/calling_convention.cc      |   16 +
 compiler/jni/quick/jni_compiler.cc            |    3 +-
 .../riscv64/calling_convention_riscv64.cc     |  290 +
 .../riscv64/calling_convention_riscv64.h      |   93 +
 compiler/optimizing/code_generator.cc         |   10 +
 compiler/optimizing/code_generator_riscv64.cc | 7610 +++++++++++++++++
 compiler/optimizing/code_generator_riscv64.h  |  693 ++
 .../code_generator_vector_riscv64.cc          | 1428 ++++
 compiler/optimizing/codegen_test.cc           |    3 +
 compiler/optimizing/codegen_test_utils.h      |   12 +
 compiler/optimizing/intrinsics_riscv64.cc     | 2382 ++++++
 compiler/optimizing/intrinsics_riscv64.h      |   86 +
 compiler/optimizing/loop_optimization.cc      |    5 +
 compiler/optimizing/nodes.h                   |   15 +-
 compiler/optimizing/optimizing_compiler.cc    |   18 +
 compiler/optimizing/register_allocator.cc     |    1 +
 compiler/optimizing/stack_map_stream.cc       |    8 +
 compiler/trampolines/trampoline_compiler.cc   |   40 +
 compiler/utils/assembler_test.h               |  144 +-
 compiler/utils/assembler_test_base.h          |    2 +-
 compiler/utils/jni_macro_assembler.cc         |    7 +
 compiler/utils/label.h                        |    5 +
 compiler/utils/managed_register.h             |    5 +
 compiler/utils/riscv64/assembler_riscv64.cc   | 4874 +++++++++++
 compiler/utils/riscv64/assembler_riscv64.h    | 1928 +++++
 .../utils/riscv64/assembler_riscv64_test.cc   | 4552 ++++++++++
 compiler/utils/riscv64/constants_riscv64.h    |  115 +
 .../riscv64/jni_macro_assembler_riscv64.cc    |  296 +
 .../riscv64/jni_macro_assembler_riscv64.h     |  221 +
 .../utils/riscv64/managed_register_riscv64.cc |   57 +
 .../utils/riscv64/managed_register_riscv64.h  |  163 +
 .../riscv64/managed_register_riscv64_test.cc  |  481 ++
 dex2oat/Android.bp                            |   19 +-
 dex2oat/dex2oat.cc                            |    4 +-
 dex2oat/driver/compiler_driver.cc             |    1 +
 dex2oat/linker/relative_patcher.cc            |    7 +
 .../riscv64/relative_patcher_riscv64.cc       |  102 +
 .../linker/riscv64/relative_patcher_riscv64.h |   54 +
 .../riscv64/relative_patcher_riscv64_base.cc  |  538 ++
 .../riscv64/relative_patcher_riscv64_base.h   |  158 +
 .../riscv64/relative_patcher_riscv64_test.cc  |   97 +
 dexlayout/Android.bp                          |    2 +-
 disassembler/Android.bp                       |   23 +
 disassembler/disassembler.cc                  |    8 +
 disassembler/disassembler_riscv64.cc          |  598 ++
 disassembler/disassembler_riscv64.h           |   53 +
 disassembler/disassembler_riscv64_test.cc     |  713 ++
 libartbase/arch/instruction_set.cc            |    7 +
 libartbase/arch/instruction_set.h             |   30 +-
 libartbase/arch/instruction_set_test.cc       |    4 +
 libartbase/base/mem_map.h                     |    3 +-
 libelffile/dwarf/register.h                   |    2 +
 libelffile/elf/elf_builder.h                  |    5 +
 libelffile/elf/elf_utils.h                    |    2 +
 runtime/Android.bp                            |   31 +-
 runtime/arch/arch_test.cc                     |   20 +-
 runtime/arch/context-inl.h                    |    3 +
 runtime/arch/instruction_set_features.cc      |   19 +
 runtime/arch/instruction_set_features.h       |    4 +
 .../arch/mips64/quick_entrypoints_mips64.S    |    4 +-
 runtime/arch/riscv64/asm_support_riscv64.S    |   94 +
 runtime/arch/riscv64/asm_support_riscv64.h    |   53 +
 .../arch/riscv64/callee_save_frame_riscv64.h  |  133 +
 runtime/arch/riscv64/context_riscv64.cc       |  143 +
 runtime/arch/riscv64/context_riscv64.h        |   99 +
 .../arch/riscv64/entrypoints_init_riscv64.cc  |  212 +
 runtime/arch/riscv64/fault_handler_riscv64.cc |  151 +
 .../instruction_set_features_riscv64.cc       |  105 +
 .../instruction_set_features_riscv64.h        |   90 +
 .../instruction_set_features_riscv64_test.cc  |   50 +
 .../arch/riscv64/jni_entrypoints_riscv64.S    |   79 +
 runtime/arch/riscv64/memcmp16_riscv64.S       |   50 +
 .../arch/riscv64/quick_entrypoints_riscv64.S  | 3351 ++++++++
 runtime/arch/riscv64/registers_riscv64.cc     |   65 +
 runtime/arch/riscv64/registers_riscv64.h      |  161 +
 runtime/arch/riscv64/thread_riscv64.cc        |   36 +
 runtime/arch/stub_test.cc                     |  117 +-
 runtime/art_method.cc                         |    6 +
 runtime/common_runtime_test.cc                |    4 +
 runtime/common_runtime_test.h                 |    6 +
 runtime/compiler_filter.cc                    |    4 +
 runtime/elf_file.cc                           |    2 +
 runtime/entrypoints/quick/callee_save_frame.h |    3 +
 .../quick/quick_trampoline_entrypoints.cc     |  106 +
 runtime/hidden_api_test.cc                    |    8 +-
 runtime/interpreter/mterp/mterp.cc            |   18 +
 runtime/interpreter/mterp/mterp.h             |    5 +
 .../interpreter/mterp/riscv64/arithmetic.S    |  464 +
 runtime/interpreter/mterp/riscv64/array.S     |  241 +
 .../interpreter/mterp/riscv64/control_flow.S  |  223 +
 .../mterp/riscv64/floating_point.S            |  400 +
 runtime/interpreter/mterp/riscv64/invoke.S    |  109 +
 runtime/interpreter/mterp/riscv64/main.S      |  799 ++
 runtime/interpreter/mterp/riscv64/object.S    |  270 +
 runtime/interpreter/mterp/riscv64/other.S     |  368 +
 runtime/jit/jit.cc                            |    2 +
 runtime/mirror/object-readbarrier-inl.h       |    2 +-
 runtime/native_stack_dump.cc                  |    4 +
 runtime/parsed_options_test.cc                |    5 +-
 runtime/runtime.cc                            |    2 +
 runtime/runtime_callbacks_test.cc             |    2 +
 test/Android.bp                               |    3 +
 test/etc/run-test-jar                         |    7 +-
 test/run-test                                 |    2 +-
 tools/checker/common/archs.py                 |    2 +-
 tools/cpp-define-generator/make_header.py     |    2 +-
 119 files changed, 36152 insertions(+), 77 deletions(-)
 create mode 100644 compiler/jni/quick/riscv64/calling_convention_riscv64.cc
 create mode 100644 compiler/jni/quick/riscv64/calling_convention_riscv64.h
 create mode 100644 compiler/optimizing/code_generator_riscv64.cc
 create mode 100644 compiler/optimizing/code_generator_riscv64.h
 create mode 100644 compiler/optimizing/code_generator_vector_riscv64.cc
 create mode 100644 compiler/optimizing/intrinsics_riscv64.cc
 create mode 100644 compiler/optimizing/intrinsics_riscv64.h
 create mode 100644 compiler/utils/riscv64/assembler_riscv64.cc
 create mode 100644 compiler/utils/riscv64/assembler_riscv64.h
 create mode 100644 compiler/utils/riscv64/assembler_riscv64_test.cc
 create mode 100644 compiler/utils/riscv64/constants_riscv64.h
 create mode 100644 compiler/utils/riscv64/jni_macro_assembler_riscv64.cc
 create mode 100644 compiler/utils/riscv64/jni_macro_assembler_riscv64.h
 create mode 100644 compiler/utils/riscv64/managed_register_riscv64.cc
 create mode 100644 compiler/utils/riscv64/managed_register_riscv64.h
 create mode 100644 compiler/utils/riscv64/managed_register_riscv64_test.cc
 create mode 100644 dex2oat/linker/riscv64/relative_patcher_riscv64.cc
 create mode 100644 dex2oat/linker/riscv64/relative_patcher_riscv64.h
 create mode 100644 dex2oat/linker/riscv64/relative_patcher_riscv64_base.cc
 create mode 100644 dex2oat/linker/riscv64/relative_patcher_riscv64_base.h
 create mode 100644 dex2oat/linker/riscv64/relative_patcher_riscv64_test.cc
 create mode 100644 disassembler/disassembler_riscv64.cc
 create mode 100644 disassembler/disassembler_riscv64.h
 create mode 100644 disassembler/disassembler_riscv64_test.cc
 create mode 100644 runtime/arch/riscv64/asm_support_riscv64.S
 create mode 100644 runtime/arch/riscv64/asm_support_riscv64.h
 create mode 100644 runtime/arch/riscv64/callee_save_frame_riscv64.h
 create mode 100644 runtime/arch/riscv64/context_riscv64.cc
 create mode 100644 runtime/arch/riscv64/context_riscv64.h
 create mode 100644 runtime/arch/riscv64/entrypoints_init_riscv64.cc
 create mode 100644 runtime/arch/riscv64/fault_handler_riscv64.cc
 create mode 100644 runtime/arch/riscv64/instruction_set_features_riscv64.cc
 create mode 100644 runtime/arch/riscv64/instruction_set_features_riscv64.h
 create mode 100644 runtime/arch/riscv64/instruction_set_features_riscv64_test.cc
 create mode 100644 runtime/arch/riscv64/jni_entrypoints_riscv64.S
 create mode 100644 runtime/arch/riscv64/memcmp16_riscv64.S
 create mode 100644 runtime/arch/riscv64/quick_entrypoints_riscv64.S
 create mode 100644 runtime/arch/riscv64/registers_riscv64.cc
 create mode 100644 runtime/arch/riscv64/registers_riscv64.h
 create mode 100644 runtime/arch/riscv64/thread_riscv64.cc
 create mode 100644 runtime/interpreter/mterp/riscv64/arithmetic.S
 create mode 100644 runtime/interpreter/mterp/riscv64/array.S
 create mode 100644 runtime/interpreter/mterp/riscv64/control_flow.S
 create mode 100644 runtime/interpreter/mterp/riscv64/floating_point.S
 create mode 100644 runtime/interpreter/mterp/riscv64/invoke.S
 create mode 100644 runtime/interpreter/mterp/riscv64/main.S
 create mode 100644 runtime/interpreter/mterp/riscv64/object.S
 create mode 100644 runtime/interpreter/mterp/riscv64/other.S

diff --git a/build/Android.bp b/build/Android.bp
index 7b807d56b7..b2cb28451f 100644
--- a/build/Android.bp
+++ b/build/Android.bp
@@ -115,7 +115,7 @@ art_global_defaults {
 
                 // To use oprofile_android --callgraph, uncomment this and recompile with
                 //    mmma -j art
-                // "-fno-omit-frame-pointer",
+                "-fno-omit-frame-pointer",
                 // "-marm",
                 // "-mapcs",
             ],
@@ -166,6 +166,9 @@ art_global_defaults {
         mips64: {
             cflags: ["-DART_ENABLE_CODEGEN_mips64"],
         },
+        riscv64: {
+            cflags: ["-DART_ENABLE_CODEGEN_riscv64"],
+        },
         x86: {
             cflags: ["-DART_ENABLE_CODEGEN_x86"],
         },
diff --git a/build/Android.common.mk b/build/Android.common.mk
index e96e3edc34..59b5385c1a 100644
--- a/build/Android.common.mk
+++ b/build/Android.common.mk
@@ -17,7 +17,7 @@
 ifndef ART_ANDROID_COMMON_MK
 ART_ANDROID_COMMON_MK = true
 
-ART_TARGET_SUPPORTED_ARCH := arm arm64 mips mips64 x86 x86_64
+ART_TARGET_SUPPORTED_ARCH := arm arm64 mips mips64 riscv64 x86 x86_64
 ART_HOST_SUPPORTED_ARCH := x86 x86_64
 ART_DEXPREOPT_BOOT_JAR_DIR := system/framework
 
diff --git a/build/apex/Android.bp b/build/apex/Android.bp
index 95aea3cf69..ba1e5dc623 100644
--- a/build/apex/Android.bp
+++ b/build/apex/Android.bp
@@ -164,7 +164,8 @@ libcore_debug_native_shared_libs = [
 libcore_native_device_only_shared_libs = [
     // TODO(b/122876336): Remove libpac.so once it's migrated to Webview.
     // libpac is used by frameworks, not by ART host.
-    "libpac",
+    // FIXME: T-HEAD, disable for riscv64, enable in the future.
+    // "libpac",
 ]
 
 // Temporary library includes for b/123591866 as all libraries are moved into the main art-apex.
@@ -313,6 +314,11 @@ art_apex_test {
             }
         },
     },
+    arch : {
+        riscv64: {
+            enabled: false,
+        }
+    }
 }
 
 python_binary_host {
diff --git a/build/apex/art_apex_test.py b/build/apex/art_apex_test.py
index 1415571469..0f02280f6c 100755
--- a/build/apex/art_apex_test.py
+++ b/build/apex/art_apex_test.py
@@ -26,7 +26,6 @@ import zipfile
 
 logging.basicConfig(format='%(message)s')
 
-
 class FSObject:
   def __init__(self, name, is_dir, is_exec, is_symlink):
     self.name = name
@@ -477,7 +476,8 @@ class ReleaseTargetChecker:
     self._checker.check_native_library('libexpat')
     self._checker.check_native_library('libicui18n')
     self._checker.check_native_library('libicuuc')
-    self._checker.check_native_library('libpac')
+    # FIXME: T-HEAD, disable for riscv64, enable in the future.
+    # self._checker.check_native_library('libpac')
     self._checker.check_native_library('libz')
 
     # TODO(b/124293228): Cuttlefish puts ARM libs in a lib/arm subdirectory.
diff --git a/build/art.go b/build/art.go
index 4db8da28e0..112a432bd3 100644
--- a/build/art.go
+++ b/build/art.go
@@ -25,14 +25,16 @@ import (
 	"github.com/google/blueprint/proptools"
 )
 
-var supportedArches = []string{"arm", "arm64", "mips", "mips64", "x86", "x86_64"}
+var supportedArches = []string{"arm", "arm64", "mips", "mips64", "riscv64", "x86", "x86_64"}
 
 func globalFlags(ctx android.BaseContext) ([]string, []string) {
 	var cflags []string
 	var asflags []string
 
-	opt := envDefault(ctx, "ART_NDEBUG_OPT_FLAG", "-O3")
+	opt := envDefault(ctx, "ART_NDEBUG_OPT_FLAG", "-O2")
 	cflags = append(cflags, opt)
+	cflags = append(cflags, "-g")
+
 
 	tlab := false
 
@@ -89,6 +91,7 @@ func globalFlags(ctx android.BaseContext) ([]string, []string) {
 			"-DART_STACK_OVERFLOW_GAP_arm64=8192",
 			"-DART_STACK_OVERFLOW_GAP_mips=16384",
 			"-DART_STACK_OVERFLOW_GAP_mips64=16384",
+			"-DART_STACK_OVERFLOW_GAP_riscv64=16384",
 			"-DART_STACK_OVERFLOW_GAP_x86=16384",
 			"-DART_STACK_OVERFLOW_GAP_x86_64=20480")
 	} else {
@@ -97,6 +100,7 @@ func globalFlags(ctx android.BaseContext) ([]string, []string) {
 			"-DART_STACK_OVERFLOW_GAP_arm64=8192",
 			"-DART_STACK_OVERFLOW_GAP_mips=16384",
 			"-DART_STACK_OVERFLOW_GAP_mips64=16384",
+			"-DART_STACK_OVERFLOW_GAP_riscv64=16384",
 			"-DART_STACK_OVERFLOW_GAP_x86=8192",
 			"-DART_STACK_OVERFLOW_GAP_x86_64=8192")
 	}
diff --git a/build/codegen.go b/build/codegen.go
index d0db78e571..e63589d0ff 100644
--- a/build/codegen.go
+++ b/build/codegen.go
@@ -66,6 +66,8 @@ func codegen(ctx android.LoadHookContext, c *codegenProperties, library bool) {
 			arch = &c.Codegen.Mips
 		case "mips64":
 			arch = &c.Codegen.Mips64
+		case "riscv64":
+			arch = &c.Codegen.Riscv64
 		case "x86":
 			arch = &c.Codegen.X86
 		case "x86_64":
@@ -128,7 +130,7 @@ type codegenArchProperties struct {
 
 type codegenProperties struct {
 	Codegen struct {
-		Arm, Arm64, Mips, Mips64, X86, X86_64 codegenArchProperties
+		Arm, Arm64, Mips, Mips64, Riscv64, X86, X86_64 codegenArchProperties
 	}
 }
 
diff --git a/compiler/Android.bp b/compiler/Android.bp
index 52bd89fb7d..b575235b1a 100644
--- a/compiler/Android.bp
+++ b/compiler/Android.bp
@@ -170,6 +170,18 @@ art_cc_defaults {
                 "utils/x86_64/managed_register_x86_64.cc",
             ],
         },
+        riscv64: {
+            srcs: [
+                "jni/quick/riscv64/calling_convention_riscv64.cc",
+                "optimizing/code_generator_riscv64.cc",
+                "optimizing/code_generator_vector_riscv64.cc",
+                "optimizing/intrinsics_riscv64.cc",
+                "utils/riscv64/assembler_riscv64.cc",
+                "utils/riscv64/jni_macro_assembler_riscv64.cc",
+                "utils/riscv64/managed_register_riscv64.cc",
+            ],
+        },
+
     },
     generated_sources: ["art_compiler_operator_srcs"],
     shared_libs: [
@@ -254,7 +266,7 @@ art_cc_library {
     target: {
         android: {
             lto: {
-                 thin: true,
+                 thin: false,
             },
         },
     },
@@ -348,6 +360,7 @@ art_cc_test {
     defaults: [
         "art_gtest_defaults",
     ],
+    cflags: ["-Wno-invalid-partial-specialization"],
     srcs: [
         "debug/dwarf/dwarf_test.cc",
         "debug/src_map_elem_test.cc",
@@ -412,6 +425,12 @@ art_cc_test {
                 "utils/mips64/managed_register_mips64_test.cc",
             ],
         },
+        riscv64: {
+            srcs: [
+                "utils/riscv64/managed_register_riscv64_test.cc",
+            ],
+        },
+
         x86: {
             srcs: [
                 "utils/x86/managed_register_x86_test.cc",
@@ -498,6 +517,11 @@ art_cc_test {
                 "utils/x86_64/assembler_x86_64_test.cc",
             ],
         },
+        riscv64: {
+            srcs: [
+                "utils/riscv64/assembler_riscv64_test.cc",
+            ],
+        },
     },
     shared_libs: [
         "libartd-compiler",
diff --git a/compiler/common_compiler_test.cc b/compiler/common_compiler_test.cc
index 18f00e21e4..6a8758ad9a 100644
--- a/compiler/common_compiler_test.cc
+++ b/compiler/common_compiler_test.cc
@@ -209,7 +209,10 @@ void CommonCompilerTest::CompileMethod(ArtMethod* method) {
     TimingLogger::ScopedTiming t2("MakeExecutable", &timings);
     MakeExecutable(method, compiled_method);
   }
-  CompiledMethod::ReleaseSwapAllocatedCompiledMethod(&storage, compiled_method);
+  // FIXME: T-HEAD, workaround, compiled_method would not be null
+  if (compiled_method != nullptr) {
+    CompiledMethod::ReleaseSwapAllocatedCompiledMethod(&storage, compiled_method);
+  }
 }
 
 void CommonCompilerTest::CompileDirectMethod(Handle<mirror::ClassLoader> class_loader,
diff --git a/compiler/compiled_method.cc b/compiler/compiled_method.cc
index 58f7e4f227..950e634528 100644
--- a/compiler/compiled_method.cc
+++ b/compiler/compiled_method.cc
@@ -66,6 +66,7 @@ size_t CompiledCode::CodeDelta(InstructionSet instruction_set) {
     case InstructionSet::kMips64:
     case InstructionSet::kX86:
     case InstructionSet::kX86_64:
+    case InstructionSet::kRiscv64:
       return 0;
     case InstructionSet::kThumb2: {
       // +1 to set the low-order bit so a BLX will switch to Thumb mode
@@ -85,6 +86,7 @@ const void* CompiledCode::CodePointer(const void* code_pointer, InstructionSet i
     case InstructionSet::kMips64:
     case InstructionSet::kX86:
     case InstructionSet::kX86_64:
+    case InstructionSet::kRiscv64:
       return code_pointer;
     case InstructionSet::kThumb2: {
       uintptr_t address = reinterpret_cast<uintptr_t>(code_pointer);
diff --git a/compiler/debug/elf_debug_frame_writer.h b/compiler/debug/elf_debug_frame_writer.h
index 31bfed6d24..0e95c43503 100644
--- a/compiler/debug/elf_debug_frame_writer.h
+++ b/compiler/debug/elf_debug_frame_writer.h
@@ -160,6 +160,31 @@ static void WriteCIE(InstructionSet isa, /*inout*/ std::vector<uint8_t>* buffer)
       WriteCIE(is64bit, return_reg, opcodes, buffer);
       return;
     }
+    case InstructionSet::kRiscv64: {
+      // FIXME: T-HEAD, Need to double check.
+      dwarf::DebugFrameOpCodeWriter<> opcodes;
+      opcodes.DefCFA(Reg::Riscv64Core(2), 0);  // R2(SP).
+      // core registers.
+      for (int reg = 5; reg < 32; reg++) {
+        if (((reg > 9) && (reg < 18)) || reg == 30 || reg == 31) {  // A*, T5/T6 reserved.
+          opcodes.Undefined(Reg::Riscv64Core(reg));
+        } else {
+          opcodes.SameValue(Reg::Riscv64Core(reg));
+        }
+      }
+      // fp registers.
+      for (int reg = 0; reg < 32; reg++) {
+        if (((reg >= 0) && (reg <= 7)) || ((reg >= 10) && (reg <= 17)) || \
+            ((reg >= 28) && (reg <= 31))) {
+          opcodes.Undefined(Reg::Riscv64Fp(reg));
+        } else {
+          opcodes.SameValue(Reg::Riscv64Fp(reg));
+        }
+      }
+      auto return_reg = Reg::Riscv64Core(1);  // R1(RA).
+      WriteCIE(is64bit, return_reg, opcodes, buffer);
+      return;
+    }
     case InstructionSet::kNone:
       break;
   }
diff --git a/compiler/debug/elf_debug_line_writer.h b/compiler/debug/elf_debug_line_writer.h
index 479725be99..d81ffa186b 100644
--- a/compiler/debug/elf_debug_line_writer.h
+++ b/compiler/debug/elf_debug_line_writer.h
@@ -72,6 +72,7 @@ class ElfDebugLineWriter {
       case InstructionSet::kArm64:
       case InstructionSet::kMips:
       case InstructionSet::kMips64:
+      case InstructionSet::kRiscv64:
         code_factor_bits_ = 2;  // 32-bit instructions
         break;
       case InstructionSet::kNone:
diff --git a/compiler/debug/elf_debug_loc_writer.h b/compiler/debug/elf_debug_loc_writer.h
index a5a84bbb10..86d0e6e76c 100644
--- a/compiler/debug/elf_debug_loc_writer.h
+++ b/compiler/debug/elf_debug_loc_writer.h
@@ -46,6 +46,8 @@ static Reg GetDwarfCoreReg(InstructionSet isa, int machine_reg) {
       return Reg::MipsCore(machine_reg);
     case InstructionSet::kMips64:
       return Reg::Mips64Core(machine_reg);
+    case InstructionSet::kRiscv64:
+      return Reg::Riscv64Core(machine_reg);
     case InstructionSet::kNone:
       LOG(FATAL) << "No instruction set";
   }
@@ -67,6 +69,8 @@ static Reg GetDwarfFpReg(InstructionSet isa, int machine_reg) {
       return Reg::MipsFp(machine_reg);
     case InstructionSet::kMips64:
       return Reg::Mips64Fp(machine_reg);
+    case InstructionSet::kRiscv64:
+      return Reg::Riscv64Fp(machine_reg);
     case InstructionSet::kNone:
       LOG(FATAL) << "No instruction set";
   }
diff --git a/compiler/jit/jit_logger.cc b/compiler/jit/jit_logger.cc
index 6b9453f525..7bc6eced96 100644
--- a/compiler/jit/jit_logger.cc
+++ b/compiler/jit/jit_logger.cc
@@ -201,6 +201,9 @@ static uint32_t GetElfMach() {
 #elif defined(__x86_64__)
   static const uint32_t kElfMachX64 = 0x3E;
   return kElfMachX64;
+#elif defined(__riscv)
+  static const uint32_t kElfMachRISCV64 = 0xF3;
+  return kElfMachRISCV64;
 #else
   UNIMPLEMENTED(WARNING) << "Unsupported architecture in JitLogger";
   return 0;
diff --git a/compiler/jni/quick/calling_convention.cc b/compiler/jni/quick/calling_convention.cc
index f031b9be82..f4687924a1 100644
--- a/compiler/jni/quick/calling_convention.cc
+++ b/compiler/jni/quick/calling_convention.cc
@@ -44,6 +44,10 @@
 #include "jni/quick/x86_64/calling_convention_x86_64.h"
 #endif
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+#include "jni/quick/riscv64/calling_convention_riscv64.h"
+#endif
+
 namespace art {
 
 // Managed runtime calling convention
@@ -91,6 +95,12 @@ std::unique_ptr<ManagedRuntimeCallingConvention> ManagedRuntimeCallingConvention
       return std::unique_ptr<ManagedRuntimeCallingConvention>(
           new (allocator) x86_64::X86_64ManagedRuntimeCallingConvention(
               is_static, is_synchronized, shorty));
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64:
+      return std::unique_ptr<ManagedRuntimeCallingConvention>(
+          new (allocator) riscv64::Riscv64ManagedRuntimeCallingConvention(
+              is_static, is_synchronized, shorty));
 #endif
     default:
       LOG(FATAL) << "Unknown InstructionSet: " << instruction_set;
@@ -193,6 +203,12 @@ std::unique_ptr<JniCallingConvention> JniCallingConvention::Create(ArenaAllocato
       return std::unique_ptr<JniCallingConvention>(
           new (allocator) x86_64::X86_64JniCallingConvention(
               is_static, is_synchronized, is_critical_native, shorty));
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64:
+      return std::unique_ptr<JniCallingConvention>(
+          new (allocator) riscv64::Riscv64JniCallingConvention(
+              is_static, is_synchronized, is_critical_native, shorty));
 #endif
     default:
       LOG(FATAL) << "Unknown InstructionSet: " << instruction_set;
diff --git a/compiler/jni/quick/jni_compiler.cc b/compiler/jni/quick/jni_compiler.cc
index 70540783b6..e430688f82 100644
--- a/compiler/jni/quick/jni_compiler.cc
+++ b/compiler/jni/quick/jni_compiler.cc
@@ -313,7 +313,8 @@ static JniCompiledMethod ArtJniCompileMethodInternal(const CompilerOptions& comp
   // Note that we always have outgoing param space available for at least two params.
   if (kUseReadBarrier && is_static && !is_critical_native) {
     const bool kReadBarrierFastPath =
-        (instruction_set != InstructionSet::kMips) && (instruction_set != InstructionSet::kMips64);
+        (instruction_set != InstructionSet::kMips) && (instruction_set != InstructionSet::kMips64)
+        && (instruction_set != InstructionSet::kRiscv64);
     std::unique_ptr<JNIMacroLabel> skip_cold_path_label;
     if (kReadBarrierFastPath) {
       skip_cold_path_label = __ CreateLabel();
diff --git a/compiler/jni/quick/riscv64/calling_convention_riscv64.cc b/compiler/jni/quick/riscv64/calling_convention_riscv64.cc
new file mode 100644
index 0000000000..a5c01b46dc
--- /dev/null
+++ b/compiler/jni/quick/riscv64/calling_convention_riscv64.cc
@@ -0,0 +1,290 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "calling_convention_riscv64.h"
+
+#include <android-base/logging.h>
+
+#include "arch/instruction_set.h"
+#include "handle_scope-inl.h"
+#include "utils/riscv64/managed_register_riscv64.h"
+
+namespace art {
+namespace riscv64 {
+
+// Up to kow many args can be enregistered. The rest of the args must go on the stack.
+constexpr size_t kMaxRegisterArguments = 8u;
+// Up to how many float-like (float, double) args can be enregistered.
+// The rest of the args must go on the stack.
+constexpr size_t kMaxFloatOrDoubleRegisterArguments = 8u;
+// Up to how many integer-like (pointers, objects, longs, int, short, bool, etc) args can be
+// enregistered. The rest of the args must go on the stack.
+constexpr size_t kMaxIntLikeRegisterArguments = 8u;
+
+
+static const GpuRegister kGpuArgumentRegisters[] = {
+  A0, A1, A2, A3, A4, A5, A6, A7
+};
+
+static const FpuRegister kFpuArgumentRegisters[] = {
+  FA0, FA1, FA2, FA3, FA4, FA5, FA6, FA7
+};
+
+static constexpr ManagedRegister kCalleeSaveRegisters[] = {
+    // Core registers.
+    Riscv64ManagedRegister::FromGpuRegister(S2),
+    Riscv64ManagedRegister::FromGpuRegister(S3),
+    Riscv64ManagedRegister::FromGpuRegister(S4),
+    Riscv64ManagedRegister::FromGpuRegister(S5),
+    Riscv64ManagedRegister::FromGpuRegister(S6),
+    Riscv64ManagedRegister::FromGpuRegister(S7),
+    Riscv64ManagedRegister::FromGpuRegister(S8),
+    Riscv64ManagedRegister::FromGpuRegister(S9),
+    Riscv64ManagedRegister::FromGpuRegister(S10),
+    Riscv64ManagedRegister::FromGpuRegister(S0),
+    // No hard float callee saves.
+};
+
+static constexpr uint32_t CalculateCoreCalleeSpillMask() {
+  // RA is a special callee save which is not reported by CalleeSaveRegisters().
+  uint32_t result = 1 << RA;
+  for (auto&& r : kCalleeSaveRegisters) {
+    if (r.AsRiscv64().IsGpuRegister()) {
+      result |= (1 << r.AsRiscv64().AsGpuRegister());
+    }
+  }
+  return result;
+}
+
+static constexpr uint32_t kCoreCalleeSpillMask = CalculateCoreCalleeSpillMask();
+static constexpr uint32_t kFpCalleeSpillMask = 0u;
+
+// Calling convention
+ManagedRegister Riscv64ManagedRuntimeCallingConvention::InterproceduralScratchRegister() {
+  return Riscv64ManagedRegister::FromGpuRegister(T6);
+}
+
+ManagedRegister Riscv64JniCallingConvention::InterproceduralScratchRegister() {
+  return Riscv64ManagedRegister::FromGpuRegister(T6);
+}
+
+static ManagedRegister ReturnRegisterForShorty(const char* shorty) {
+  if (shorty[0] == 'F' || shorty[0] == 'D') {
+    return Riscv64ManagedRegister::FromFpuRegister(FA0);
+  } else if (shorty[0] == 'V') {
+    return Riscv64ManagedRegister::NoRegister();
+  } else {
+    return Riscv64ManagedRegister::FromGpuRegister(A0);
+  }
+}
+
+ManagedRegister Riscv64ManagedRuntimeCallingConvention::ReturnRegister() {
+  return ReturnRegisterForShorty(GetShorty());
+}
+
+ManagedRegister Riscv64JniCallingConvention::ReturnRegister() {
+  return ReturnRegisterForShorty(GetShorty());
+}
+
+ManagedRegister Riscv64JniCallingConvention::IntReturnRegister() {
+  return Riscv64ManagedRegister::FromGpuRegister(A0);
+}
+
+// Managed runtime calling convention
+
+ManagedRegister Riscv64ManagedRuntimeCallingConvention::MethodRegister() {
+  return Riscv64ManagedRegister::FromGpuRegister(A0);
+}
+
+bool Riscv64ManagedRuntimeCallingConvention::IsCurrentParamInRegister() {
+  return false;  // Everything moved to stack on entry.
+}
+
+bool Riscv64ManagedRuntimeCallingConvention::IsCurrentParamOnStack() {
+  return true;
+}
+
+ManagedRegister Riscv64ManagedRuntimeCallingConvention::CurrentParamRegister() {
+  LOG(FATAL) << "Should not reach here";
+  UNREACHABLE();
+}
+
+FrameOffset Riscv64ManagedRuntimeCallingConvention::CurrentParamStackOffset() {
+  CHECK(IsCurrentParamOnStack());
+  FrameOffset result =
+      FrameOffset(displacement_.Int32Value() +  // displacement
+                  kFramePointerSize +  // Method ref
+                  (itr_slots_ * sizeof(uint32_t)));  // offset into in args
+  return result;
+}
+
+const ManagedRegisterEntrySpills& Riscv64ManagedRuntimeCallingConvention::EntrySpills() {
+  if ((entry_spills_.size() == 0) && (NumArgs() > 0)) {
+    int gp_reg_index = 1;   // we start from X1/W1, X0 holds ArtMethod*.
+    int fp_reg_index = 0;   // D0/S0.
+
+    // We need to choose the correct register (D/S or X/W) since the managed
+    // stack uses 32bit stack slots.
+    ResetIterator(FrameOffset(0));
+    while (HasNext()) {
+      if (IsCurrentParamAFloatOrDouble()) {  // FP regs.
+          if (fp_reg_index < 8) {
+            FpuRegister arg = kFpuArgumentRegisters[fp_reg_index];
+            Riscv64ManagedRegister reg = Riscv64ManagedRegister::FromFpuRegister(arg);
+            entry_spills_.push_back(reg, IsCurrentParamADouble() ? 8 : 4);
+            fp_reg_index++;
+          } else {
+            if (!IsCurrentParamADouble()) {
+              entry_spills_.push_back(ManagedRegister::NoRegister(), 4);
+            } else {
+              entry_spills_.push_back(ManagedRegister::NoRegister(), 8);
+            }
+          }
+      } else {  // GP regs.
+          if (gp_reg_index < 8) {
+            GpuRegister arg = kGpuArgumentRegisters[gp_reg_index];
+            Riscv64ManagedRegister reg = Riscv64ManagedRegister::FromGpuRegister(arg);
+            entry_spills_.push_back(reg,
+                                  (IsCurrentParamALong() && (!IsCurrentParamAReference())) ? 8 : 4);
+            gp_reg_index++;
+          } else {
+            if (IsCurrentParamALong() && (!IsCurrentParamAReference())) {
+              entry_spills_.push_back(ManagedRegister::NoRegister(), 8);
+            } else {
+              entry_spills_.push_back(ManagedRegister::NoRegister(), 4);
+            }
+          }
+      }
+      Next();
+    }
+  }
+  return entry_spills_;
+}
+
+// JNI calling convention
+
+Riscv64JniCallingConvention::Riscv64JniCallingConvention(bool is_static,
+                                                       bool is_synchronized,
+                                                       bool is_critical_native,
+                                                       const char* shorty)
+    : JniCallingConvention(is_static,
+                           is_synchronized,
+                           is_critical_native,
+                           shorty,
+                           kRiscv64PointerSize) {
+}
+
+uint32_t Riscv64JniCallingConvention::CoreSpillMask() const {
+  return kCoreCalleeSpillMask;
+}
+
+uint32_t Riscv64JniCallingConvention::FpSpillMask() const {
+  return kFpCalleeSpillMask;
+}
+
+ManagedRegister Riscv64JniCallingConvention::ReturnScratchRegister() const {
+  return Riscv64ManagedRegister::FromGpuRegister(AT);
+}
+
+size_t Riscv64JniCallingConvention::FrameSize() {
+  // ArtMethod*, RA and callee save area size, local reference segment state.
+  size_t method_ptr_size = static_cast<size_t>(kFramePointerSize);
+  size_t ra_and_callee_save_area_size = (CalleeSaveRegisters().size() + 1) * kFramePointerSize;
+
+  size_t frame_data_size = method_ptr_size + ra_and_callee_save_area_size;
+  if (LIKELY(HasLocalReferenceSegmentState())) {                     // Local ref. segment state.
+    // Local reference segment state is sometimes excluded.
+    frame_data_size += sizeof(uint32_t);
+  }
+  // References plus 2 words for HandleScope header.
+  size_t handle_scope_size = HandleScope::SizeOf(kRiscv64PointerSize, ReferenceCount());
+
+  size_t total_size = frame_data_size;
+  if (LIKELY(HasHandleScope())) {
+    // HandleScope is sometimes excluded.
+    total_size += handle_scope_size;                                 // Handle scope size.
+  }
+
+  // Plus return value spill area size.
+  total_size += SizeOfReturnValue();
+
+  return RoundUp(total_size, kStackAlignment);
+}
+
+size_t Riscv64JniCallingConvention::OutArgSize() {
+  return RoundUp(NumberOfOutgoingStackArgs() * kFramePointerSize, kStackAlignment);
+}
+
+ArrayRef<const ManagedRegister> Riscv64JniCallingConvention::CalleeSaveRegisters() const {
+  return ArrayRef<const ManagedRegister>(kCalleeSaveRegisters);
+}
+
+bool Riscv64JniCallingConvention::IsCurrentParamInRegister() {
+  if (IsCurrentParamAFloatOrDouble()) {
+    return (itr_float_and_doubles_ < kMaxFloatOrDoubleRegisterArguments);
+  } else {
+    return ((itr_args_ - itr_float_and_doubles_) < kMaxIntLikeRegisterArguments);
+  }
+  // TODO: Can we just call CurrentParamRegister to figure this out?
+}
+
+bool Riscv64JniCallingConvention::IsCurrentParamOnStack() {
+  return !IsCurrentParamInRegister();
+}
+
+ManagedRegister Riscv64JniCallingConvention::CurrentParamRegister() {
+  CHECK(IsCurrentParamInRegister());
+  if (IsCurrentParamAFloatOrDouble()) {
+    // CHECK_LT(itr_float_and_doubles_, kMaxFloatOrDoubleRegisterArguments);
+    return Riscv64ManagedRegister::FromFpuRegister(kFpuArgumentRegisters[itr_float_and_doubles_]);
+  } else {
+    int gp_reg = itr_args_ - itr_float_and_doubles_;
+    // CHECK_LT(static_cast<unsigned int>(gp_reg), kMaxIntLikeRegisterArguments);
+    return Riscv64ManagedRegister::FromGpuRegister(kGpuArgumentRegisters[gp_reg]);
+  }
+}
+
+FrameOffset Riscv64JniCallingConvention::CurrentParamStackOffset() {
+  CHECK(IsCurrentParamOnStack());
+  size_t args_on_stack = itr_args_
+    - std::min(kMaxFloatOrDoubleRegisterArguments,
+    static_cast<size_t>(itr_float_and_doubles_))
+    - std::min(kMaxIntLikeRegisterArguments,
+    static_cast<size_t>(itr_args_ - itr_float_and_doubles_));
+
+  size_t offset = displacement_.Int32Value() - OutArgSize() + (args_on_stack * kFramePointerSize);
+  CHECK_LT(offset, OutArgSize());
+  return FrameOffset(offset);
+  // TODO: Seems identical to X86_64 code.
+}
+
+size_t Riscv64JniCallingConvention::NumberOfOutgoingStackArgs() {
+  // all arguments including JNI args
+  size_t all_args = NumArgs() + NumberOfExtraArgumentsForJni();
+  DCHECK_GE(all_args, NumFloatOrDoubleArgs());
+
+  size_t all_stack_args = all_args
+    - std::min(kMaxFloatOrDoubleRegisterArguments,
+    static_cast<size_t>(NumFloatOrDoubleArgs()))
+    - std::min(kMaxIntLikeRegisterArguments,
+    static_cast<size_t>((all_args - NumFloatOrDoubleArgs())));
+
+  // TODO: Seems similar to X86_64 code except it doesn't count return pc.
+  return all_stack_args;
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/jni/quick/riscv64/calling_convention_riscv64.h b/compiler/jni/quick/riscv64/calling_convention_riscv64.h
new file mode 100644
index 0000000000..0cc63a3a07
--- /dev/null
+++ b/compiler/jni/quick/riscv64/calling_convention_riscv64.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_JNI_QUICK_RISCV64_CALLING_CONVENTION_RISCV64_H_
+#define ART_COMPILER_JNI_QUICK_RISCV64_CALLING_CONVENTION_RISCV64_H_
+
+#include "base/enums.h"
+#include "jni/quick/calling_convention.h"
+
+namespace art {
+namespace riscv64 {
+
+constexpr size_t kFramePointerSize = 8;
+static_assert(kFramePointerSize == static_cast<size_t>(PointerSize::k64),
+              "Invalid frame pointer size");
+
+class Riscv64ManagedRuntimeCallingConvention final : public ManagedRuntimeCallingConvention {
+ public:
+  Riscv64ManagedRuntimeCallingConvention(bool is_static, bool is_synchronized, const char* shorty)
+      : ManagedRuntimeCallingConvention(is_static,
+                                        is_synchronized,
+                                        shorty,
+                                        PointerSize::k64) {}
+  ~Riscv64ManagedRuntimeCallingConvention() override {}
+  // Calling convention
+  ManagedRegister ReturnRegister() override;
+  ManagedRegister InterproceduralScratchRegister() override;
+  // Managed runtime calling convention
+  ManagedRegister MethodRegister() override;
+  bool IsCurrentParamInRegister() override;
+  bool IsCurrentParamOnStack() override;
+  ManagedRegister CurrentParamRegister() override;
+  FrameOffset CurrentParamStackOffset() override;
+  const ManagedRegisterEntrySpills& EntrySpills() override;
+
+ private:
+  ManagedRegisterEntrySpills entry_spills_;
+
+  DISALLOW_COPY_AND_ASSIGN(Riscv64ManagedRuntimeCallingConvention);
+};
+
+class Riscv64JniCallingConvention final : public JniCallingConvention {
+ public:
+  Riscv64JniCallingConvention(bool is_static,
+                             bool is_synchronized,
+                             bool is_critical_native,
+                             const char* shorty);
+  ~Riscv64JniCallingConvention() override {}
+  // Calling convention
+  ManagedRegister ReturnRegister() override;
+  ManagedRegister IntReturnRegister() override;
+  ManagedRegister InterproceduralScratchRegister() override;
+  // JNI calling convention
+  size_t FrameSize() override;
+  size_t OutArgSize() override;
+  ArrayRef<const ManagedRegister> CalleeSaveRegisters() const override;
+  ManagedRegister ReturnScratchRegister() const override;
+  uint32_t CoreSpillMask() const override;
+  uint32_t FpSpillMask() const override;
+  bool IsCurrentParamInRegister() override;
+  bool IsCurrentParamOnStack() override;
+  ManagedRegister CurrentParamRegister() override;
+  FrameOffset CurrentParamStackOffset() override;
+
+  // Riscv64 does not need to extend small return types.
+  bool RequiresSmallResultTypeExtension() const override {
+    return false;
+  }
+
+ protected:
+  size_t NumberOfOutgoingStackArgs() override;
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(Riscv64JniCallingConvention);
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_COMPILER_JNI_QUICK_RISCV64_CALLING_CONVENTION_RISCV64_H_
diff --git a/compiler/optimizing/code_generator.cc b/compiler/optimizing/code_generator.cc
index 2bbb570c8d..1afc1eaf58 100644
--- a/compiler/optimizing/code_generator.cc
+++ b/compiler/optimizing/code_generator.cc
@@ -40,6 +40,10 @@
 #include "code_generator_mips64.h"
 #endif
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+#include "code_generator_riscv64.h"
+#endif
+
 #include "base/bit_utils.h"
 #include "base/bit_utils_iterator.h"
 #include "base/casts.h"
@@ -920,6 +924,12 @@ std::unique_ptr<CodeGenerator> CodeGenerator::Create(HGraph* graph,
       return std::unique_ptr<CodeGenerator>(
           new (allocator) x86_64::CodeGeneratorX86_64(graph, compiler_options, stats));
     }
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64: {
+      return std::unique_ptr<CodeGenerator>(
+          new (allocator) riscv64::CodeGeneratorRISCV64(graph, compiler_options, stats));
+    }
 #endif
     default:
       return nullptr;
diff --git a/compiler/optimizing/code_generator_riscv64.cc b/compiler/optimizing/code_generator_riscv64.cc
new file mode 100644
index 0000000000..c9dc8872f5
--- /dev/null
+++ b/compiler/optimizing/code_generator_riscv64.cc
@@ -0,0 +1,7610 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "code_generator_riscv64.h"
+
+#include "arch/riscv64/asm_support_riscv64.h"
+#include "art_method.h"
+#include "class_table.h"
+#include "code_generator_utils.h"
+#include "compiled_method.h"
+#include "entrypoints/quick/quick_entrypoints.h"
+#include "entrypoints/quick/quick_entrypoints_enum.h"
+#include "gc/accounting/card_table.h"
+#include "gc/space/image_space.h"
+#include "heap_poisoning.h"
+#include "intrinsics.h"
+#include "intrinsics_riscv64.h"
+#include "linker/linker_patch.h"
+#include "mirror/array-inl.h"
+#include "mirror/class-inl.h"
+#include "offsets.h"
+#include "stack_map_stream.h"
+#include "thread.h"
+#include "utils/assembler.h"
+#include "utils/riscv64/assembler_riscv64.h"
+#include "utils/stack_checks.h"
+
+namespace art {
+namespace riscv64 {
+
+static constexpr int kCurrentMethodStackOffset = 0;
+static constexpr GpuRegister kMethodRegisterArgument = A0;
+
+// Flags controlling the use of thunks for Baker read barriers.
+// FIXME: T-HEAD, disable them for porting. enable them in the future.
+#if 0
+constexpr bool kBakerReadBarrierThunksEnableForFields = true;
+constexpr bool kBakerReadBarrierThunksEnableForArrays = true;
+constexpr bool kBakerReadBarrierThunksEnableForGcRoots = true;
+#else
+constexpr bool kBakerReadBarrierThunksEnableForFields = false;
+constexpr bool kBakerReadBarrierThunksEnableForArrays = false;
+constexpr bool kBakerReadBarrierThunksEnableForGcRoots = false;
+#endif
+
+Location Riscv64ReturnLocation(DataType::Type return_type) {
+  switch (return_type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kUint32:
+    case DataType::Type::kInt32:
+    case DataType::Type::kReference:
+    case DataType::Type::kUint64:
+    case DataType::Type::kInt64:
+      return Location::RegisterLocation(A0);
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      return Location::FpuRegisterLocation(FA0);
+
+    case DataType::Type::kVoid:
+      return Location();
+  }
+  UNREACHABLE();
+}
+
+Location InvokeDexCallingConventionVisitorRISCV64::GetReturnLocation(DataType::Type type) const {
+  return Riscv64ReturnLocation(type);
+}
+
+Location InvokeDexCallingConventionVisitorRISCV64::GetMethodLocation() const {
+  return Location::RegisterLocation(kMethodRegisterArgument);
+}
+
+Location InvokeDexCallingConventionVisitorRISCV64::GetNextLocation(DataType::Type type) {
+  Location next_location;
+  if (type == DataType::Type::kVoid) {
+    LOG(FATAL) << "Unexpected parameter type " << type;
+  }
+
+  if (DataType::IsFloatingPointType(type)) {
+    if (float_index_ < calling_convention.GetNumberOfFpuRegisters()) {
+      next_location = Location::FpuRegisterLocation(
+          calling_convention.GetFpuRegisterAt(float_index_++));
+    } else {
+      // Workaround: Riscv64 will try GPR when FPRs are used out. We don't put the float/double
+      // value in gpr for optimizing compiler code now. But we need keep this placeholder.
+      if (gp_index_ < calling_convention.GetNumberOfRegisters())
+        gp_index_++;
+
+      size_t stack_offset = calling_convention.GetStackOffsetOf(stack_index_);
+      next_location = DataType::Is64BitType(type) ? Location::DoubleStackSlot(stack_offset)
+                                                  : Location::StackSlot(stack_offset);
+    }
+  } else if (!DataType::IsFloatingPointType(type) &&
+             (gp_index_ < calling_convention.GetNumberOfRegisters())) {
+    next_location = Location::RegisterLocation(calling_convention.GetRegisterAt(gp_index_++));
+  } else {
+    size_t stack_offset = calling_convention.GetStackOffsetOf(stack_index_);
+    next_location = DataType::Is64BitType(type) ? Location::DoubleStackSlot(stack_offset)
+                                                : Location::StackSlot(stack_offset);
+  }
+
+  // Space on the stack is reserved for all arguments.
+  stack_index_ += DataType::Is64BitType(type) ? 2 : 1;
+
+  return next_location;
+}
+
+Location InvokeRuntimeCallingConvention::GetReturnLocation(DataType::Type type) {
+  return Riscv64ReturnLocation(type);
+}
+
+static RegisterSet OneRegInReferenceOutSaveEverythingCallerSaves() {
+  InvokeRuntimeCallingConvention calling_convention;
+  RegisterSet caller_saves = RegisterSet::Empty();
+  caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  // The reference is returned in the same register. This differs from the standard return location.
+  return caller_saves;
+}
+
+// NOLINT on __ macro to suppress wrong warning/fix (misc-macro-parentheses) from clang-tidy.
+#define __ down_cast<CodeGeneratorRISCV64*>(codegen)->GetAssembler()->  // NOLINT
+#define QUICK_ENTRY_POINT(x) QUICK_ENTRYPOINT_OFFSET(kRiscv64PointerSize, x).Int32Value()
+
+class BoundsCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit BoundsCheckSlowPathRISCV64(HBoundsCheck* instruction) : SlowPathCodeRISCV64(instruction) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    __ Bind(GetEntryLabel());
+    if (instruction_->CanThrowIntoCatchBlock()) {
+      // Live registers will be restored in the catch block if caught.
+      SaveLiveRegisters(codegen, instruction_->GetLocations());
+    }
+    // We're moving two locations to locations that could overlap, so we need a parallel
+    // move resolver.
+    InvokeRuntimeCallingConvention calling_convention;
+    codegen->EmitParallelMoves(locations->InAt(0),
+                               Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                               DataType::Type::kInt32,
+                               locations->InAt(1),
+                               Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
+                               DataType::Type::kInt32);
+    QuickEntrypointEnum entrypoint = instruction_->AsBoundsCheck()->IsStringCharAt()
+        ? kQuickThrowStringBounds
+        : kQuickThrowArrayBounds;
+    riscv64_codegen->InvokeRuntime(entrypoint, instruction_, instruction_->GetDexPc(), this);
+    CheckEntrypointTypes<kQuickThrowStringBounds, void, int32_t, int32_t>();
+    CheckEntrypointTypes<kQuickThrowArrayBounds, void, int32_t, int32_t>();
+  }
+
+  bool IsFatal() const override { return true; }
+
+  const char* GetDescription() const override { return "BoundsCheckSlowPathRISCV64"; }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(BoundsCheckSlowPathRISCV64);
+};
+
+class DivZeroCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit DivZeroCheckSlowPathRISCV64(HDivZeroCheck* instruction)
+      : SlowPathCodeRISCV64(instruction) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    __ Bind(GetEntryLabel());
+    riscv64_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, instruction_->GetDexPc(), this);
+    CheckEntrypointTypes<kQuickThrowDivZero, void, void>();
+  }
+
+  bool IsFatal() const override { return true; }
+
+  const char* GetDescription() const override { return "DivZeroCheckSlowPathRISCV64"; }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(DivZeroCheckSlowPathRISCV64);
+};
+
+class LoadClassSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  LoadClassSlowPathRISCV64(HLoadClass* cls, HInstruction* at)
+      : SlowPathCodeRISCV64(at), cls_(cls) {
+    DCHECK(at->IsLoadClass() || at->IsClinitCheck());
+    DCHECK_EQ(instruction_->IsLoadClass(), cls_ == instruction_);
+  }
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    Location out = locations->Out();
+    const uint32_t dex_pc = instruction_->GetDexPc();
+    bool must_resolve_type = instruction_->IsLoadClass() && cls_->MustResolveTypeOnSlowPath();
+    bool must_do_clinit = instruction_->IsClinitCheck() || cls_->MustGenerateClinitCheck();
+
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    __ Bind(GetEntryLabel());
+    SaveLiveRegisters(codegen, locations);
+
+    InvokeRuntimeCallingConvention calling_convention;
+    if (must_resolve_type) {
+      DCHECK(IsSameDexFile(cls_->GetDexFile(), riscv64_codegen->GetGraph()->GetDexFile()));
+      dex::TypeIndex type_index = cls_->GetTypeIndex();
+      __ LoadConst32(calling_convention.GetRegisterAt(0), type_index.index_);
+      riscv64_codegen->InvokeRuntime(kQuickResolveType, instruction_, dex_pc, this);
+      CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
+      // If we also must_do_clinit, the resolved type is now in the correct register.
+    } else {
+      DCHECK(must_do_clinit);
+      Location source = instruction_->IsLoadClass() ? out : locations->InAt(0);
+      riscv64_codegen->MoveLocation(Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                                   source,
+                                   cls_->GetType());
+    }
+    if (must_do_clinit) {
+      riscv64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, dex_pc, this);
+      CheckEntrypointTypes<kQuickInitializeStaticStorage, void*, mirror::Class*>();
+    }
+
+    // Move the class to the desired location.
+    if (out.IsValid()) {
+      DCHECK(out.IsRegister() && !locations->GetLiveRegisters()->ContainsCoreRegister(out.reg()));
+      DataType::Type type = instruction_->GetType();
+      riscv64_codegen->MoveLocation(out,
+                                   Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                                   type);
+    }
+    RestoreLiveRegisters(codegen, locations);
+
+    __ Bc(GetExitLabel());
+  }
+
+  const char* GetDescription() const override { return "LoadClassSlowPathRISCV64"; }
+
+ private:
+  // The class this slow path will load.
+  HLoadClass* const cls_;
+
+  DISALLOW_COPY_AND_ASSIGN(LoadClassSlowPathRISCV64);
+};
+
+class LoadStringSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit LoadStringSlowPathRISCV64(HLoadString* instruction)
+      : SlowPathCodeRISCV64(instruction) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    DCHECK(instruction_->IsLoadString());
+    DCHECK_EQ(instruction_->AsLoadString()->GetLoadKind(), HLoadString::LoadKind::kBssEntry);
+    LocationSummary* locations = instruction_->GetLocations();
+    DCHECK(!locations->GetLiveRegisters()->ContainsCoreRegister(locations->Out().reg()));
+    const dex::StringIndex string_index = instruction_->AsLoadString()->GetStringIndex();
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    InvokeRuntimeCallingConvention calling_convention;
+    __ Bind(GetEntryLabel());
+    SaveLiveRegisters(codegen, locations);
+
+    __ LoadConst32(calling_convention.GetRegisterAt(0), string_index.index_);
+    riscv64_codegen->InvokeRuntime(kQuickResolveString,
+                                  instruction_,
+                                  instruction_->GetDexPc(),
+                                  this);
+    CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
+
+    DataType::Type type = instruction_->GetType();
+    riscv64_codegen->MoveLocation(locations->Out(),
+                                 Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                                 type);
+    RestoreLiveRegisters(codegen, locations);
+
+    __ Bc(GetExitLabel());
+  }
+
+  const char* GetDescription() const override { return "LoadStringSlowPathRISCV64"; }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(LoadStringSlowPathRISCV64);
+};
+
+class NullCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit NullCheckSlowPathRISCV64(HNullCheck* instr) : SlowPathCodeRISCV64(instr) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    __ Bind(GetEntryLabel());
+    if (instruction_->CanThrowIntoCatchBlock()) {
+      // Live registers will be restored in the catch block if caught.
+      SaveLiveRegisters(codegen, instruction_->GetLocations());
+    }
+    riscv64_codegen->InvokeRuntime(kQuickThrowNullPointer,
+                                  instruction_,
+                                  instruction_->GetDexPc(),
+                                  this);
+    CheckEntrypointTypes<kQuickThrowNullPointer, void, void>();
+  }
+
+  bool IsFatal() const override { return true; }
+
+  const char* GetDescription() const override { return "NullCheckSlowPathRISCV64"; }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(NullCheckSlowPathRISCV64);
+};
+
+class SuspendCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  SuspendCheckSlowPathRISCV64(HSuspendCheck* instruction, HBasicBlock* successor)
+      : SlowPathCodeRISCV64(instruction), successor_(successor) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    __ Bind(GetEntryLabel());
+    SaveLiveRegisters(codegen, locations);     // Only saves live vector registers for SIMD.
+    riscv64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
+    CheckEntrypointTypes<kQuickTestSuspend, void, void>();
+    RestoreLiveRegisters(codegen, locations);  // Only restores live vector registers for SIMD.
+    if (successor_ == nullptr) {
+      __ Bc(GetReturnLabel());
+    } else {
+      __ Bc(riscv64_codegen->GetLabelOf(successor_));
+    }
+  }
+
+  Riscv64Label* GetReturnLabel() {
+    DCHECK(successor_ == nullptr);
+    return &return_label_;
+  }
+
+  const char* GetDescription() const override { return "SuspendCheckSlowPathRISCV64"; }
+
+  HBasicBlock* GetSuccessor() const {
+    return successor_;
+  }
+
+ private:
+  // If not null, the block to branch to after the suspend check.
+  HBasicBlock* const successor_;
+
+  // If `successor_` is null, the label to branch to after the suspend check.
+  Riscv64Label return_label_;
+
+  DISALLOW_COPY_AND_ASSIGN(SuspendCheckSlowPathRISCV64);
+};
+
+class TypeCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit TypeCheckSlowPathRISCV64(HInstruction* instruction, bool is_fatal)
+      : SlowPathCodeRISCV64(instruction), is_fatal_(is_fatal) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+
+    uint32_t dex_pc = instruction_->GetDexPc();
+    DCHECK(instruction_->IsCheckCast()
+           || !locations->GetLiveRegisters()->ContainsCoreRegister(locations->Out().reg()));
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+
+    __ Bind(GetEntryLabel());
+    if (!is_fatal_ || instruction_->CanThrowIntoCatchBlock()) {
+      SaveLiveRegisters(codegen, locations);
+    }
+
+    // We're moving two locations to locations that could overlap, so we need a parallel
+    // move resolver.
+    InvokeRuntimeCallingConvention calling_convention;
+    codegen->EmitParallelMoves(locations->InAt(0),
+                               Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                               DataType::Type::kReference,
+                               locations->InAt(1),
+                               Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
+                               DataType::Type::kReference);
+    if (instruction_->IsInstanceOf()) {
+      riscv64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, dex_pc, this);
+      CheckEntrypointTypes<kQuickInstanceofNonTrivial, size_t, mirror::Object*, mirror::Class*>();
+      DataType::Type ret_type = instruction_->GetType();
+      Location ret_loc = calling_convention.GetReturnLocation(ret_type);
+      riscv64_codegen->MoveLocation(locations->Out(), ret_loc, ret_type);
+    } else {
+      DCHECK(instruction_->IsCheckCast());
+      riscv64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, dex_pc, this);
+      CheckEntrypointTypes<kQuickCheckInstanceOf, void, mirror::Object*, mirror::Class*>();
+    }
+
+    if (!is_fatal_) {
+      RestoreLiveRegisters(codegen, locations);
+      __ Bc(GetExitLabel());
+    }
+  }
+
+  const char* GetDescription() const override { return "TypeCheckSlowPathRISCV64"; }
+
+  bool IsFatal() const override { return is_fatal_; }
+
+ private:
+  const bool is_fatal_;
+
+  DISALLOW_COPY_AND_ASSIGN(TypeCheckSlowPathRISCV64);
+};
+
+class DeoptimizationSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit DeoptimizationSlowPathRISCV64(HDeoptimize* instruction)
+    : SlowPathCodeRISCV64(instruction) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    __ Bind(GetEntryLabel());
+      LocationSummary* locations = instruction_->GetLocations();
+    SaveLiveRegisters(codegen, locations);
+    InvokeRuntimeCallingConvention calling_convention;
+    __ LoadConst32(calling_convention.GetRegisterAt(0),
+                   static_cast<uint32_t>(instruction_->AsDeoptimize()->GetDeoptimizationKind()));
+    riscv64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, instruction_->GetDexPc(), this);
+    CheckEntrypointTypes<kQuickDeoptimize, void, DeoptimizationKind>();
+  }
+
+  const char* GetDescription() const override { return "DeoptimizationSlowPathRISCV64"; }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(DeoptimizationSlowPathRISCV64);
+};
+
+class ArraySetSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit ArraySetSlowPathRISCV64(HInstruction* instruction) : SlowPathCodeRISCV64(instruction) {}
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    __ Bind(GetEntryLabel());
+    SaveLiveRegisters(codegen, locations);
+
+    InvokeRuntimeCallingConvention calling_convention;
+    HParallelMove parallel_move(codegen->GetGraph()->GetAllocator());
+    parallel_move.AddMove(
+        locations->InAt(0),
+        Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+        DataType::Type::kReference,
+        nullptr);
+    parallel_move.AddMove(
+        locations->InAt(1),
+        Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
+        DataType::Type::kInt32,
+        nullptr);
+    parallel_move.AddMove(
+        locations->InAt(2),
+        Location::RegisterLocation(calling_convention.GetRegisterAt(2)),
+        DataType::Type::kReference,
+        nullptr);
+    codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
+
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    riscv64_codegen->InvokeRuntime(kQuickAputObject, instruction_, instruction_->GetDexPc(), this);
+    CheckEntrypointTypes<kQuickAputObject, void, mirror::Array*, int32_t, mirror::Object*>();
+    RestoreLiveRegisters(codegen, locations);
+    __ Bc(GetExitLabel());
+  }
+
+  const char* GetDescription() const override { return "ArraySetSlowPathRISCV64"; }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(ArraySetSlowPathRISCV64);
+};
+
+// Slow path marking an object reference `ref` during a read
+// barrier. The field `obj.field` in the object `obj` holding this
+// reference does not get updated by this slow path after marking (see
+// ReadBarrierMarkAndUpdateFieldSlowPathRISCV64 below for that).
+//
+// This means that after the execution of this slow path, `ref` will
+// always be up-to-date, but `obj.field` may not; i.e., after the
+// flip, `ref` will be a to-space reference, but `obj.field` will
+// probably still be a from-space reference (unless it gets updated by
+// another thread, or if another thread installed another object
+// reference (different from `ref`) in `obj.field`).
+//
+// If `entrypoint` is a valid location it is assumed to already be
+// holding the entrypoint. The case where the entrypoint is passed in
+// is for the GcRoot read barrier.
+class ReadBarrierMarkSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  ReadBarrierMarkSlowPathRISCV64(HInstruction* instruction,
+                                Location ref,
+                                Location entrypoint = Location::NoLocation())
+      : SlowPathCodeRISCV64(instruction), ref_(ref), entrypoint_(entrypoint) {
+    DCHECK(kEmitCompilerReadBarrier);
+  }
+
+  const char* GetDescription() const override { return "ReadBarrierMarkSlowPathMIPS"; }
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    GpuRegister ref_reg = ref_.AsRegister<GpuRegister>();
+    DCHECK(locations->CanCall());
+    DCHECK(!locations->GetLiveRegisters()->ContainsCoreRegister(ref_reg)) << ref_reg;
+    DCHECK(instruction_->IsInstanceFieldGet() ||
+           instruction_->IsStaticFieldGet() ||
+           instruction_->IsArrayGet() ||
+           instruction_->IsArraySet() ||
+           instruction_->IsLoadClass() ||
+           instruction_->IsLoadString() ||
+           instruction_->IsInstanceOf() ||
+           instruction_->IsCheckCast() ||
+           (instruction_->IsInvokeVirtual() && instruction_->GetLocations()->Intrinsified()) ||
+           (instruction_->IsInvokeStaticOrDirect() && instruction_->GetLocations()->Intrinsified()))
+        << "Unexpected instruction in read barrier marking slow path: "
+        << instruction_->DebugName();
+
+    __ Bind(GetEntryLabel());
+    // No need to save live registers; it's taken care of by the
+    // entrypoint. Also, there is no need to update the stack mask,
+    // as this runtime call will not trigger a garbage collection.
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    DCHECK((T0 <= ref_reg && ref_reg <= S0) ||
+           (A0 <= ref_reg && ref_reg <= S10));
+
+    // "Compact" slow path, saving two moves.
+    //
+    // Instead of using the standard runtime calling convention (input
+    // and output in A0 and V0 respectively):
+    //
+    //   A0 <- ref
+    //   V0 <- ReadBarrierMark(A0)
+    //   ref <- V0
+    //
+    // we just use rX (the register containing `ref`) as input and output
+    // of a dedicated entrypoint:
+    //
+    //   rX <- ReadBarrierMarkRegX(rX)
+    //
+    if (entrypoint_.IsValid()) {
+      riscv64_codegen->ValidateInvokeRuntimeWithoutRecordingPcInfo(instruction_, this);
+      DCHECK_EQ(entrypoint_.AsRegister<GpuRegister>(), T6);
+      __ Jalr(entrypoint_.AsRegister<GpuRegister>());
+      __ Nop();
+    } else {
+      int32_t entry_point_offset =
+          Thread::ReadBarrierMarkEntryPointsOffset<kRiscv64PointerSize>(ref_reg - 1);
+      // This runtime call does not require a stack map.
+      riscv64_codegen->InvokeRuntimeWithoutRecordingPcInfo(entry_point_offset,
+                                                          instruction_,
+                                                          this);
+    }
+    __ Bc(GetExitLabel());
+  }
+
+ private:
+  // The location (register) of the marked object reference.
+  const Location ref_;
+
+  // The location of the entrypoint if already loaded.
+  const Location entrypoint_;
+
+  DISALLOW_COPY_AND_ASSIGN(ReadBarrierMarkSlowPathRISCV64);
+};
+
+// Slow path marking an object reference `ref` during a read barrier,
+// and if needed, atomically updating the field `obj.field` in the
+// object `obj` holding this reference after marking (contrary to
+// ReadBarrierMarkSlowPathRISCV64 above, which never tries to update
+// `obj.field`).
+//
+// This means that after the execution of this slow path, both `ref`
+// and `obj.field` will be up-to-date; i.e., after the flip, both will
+// hold the same to-space reference (unless another thread installed
+// another object reference (different from `ref`) in `obj.field`).
+class ReadBarrierMarkAndUpdateFieldSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  ReadBarrierMarkAndUpdateFieldSlowPathRISCV64(HInstruction* instruction,
+                                              Location ref,
+                                              GpuRegister obj,
+                                              Location field_offset,
+                                              GpuRegister temp1)
+      : SlowPathCodeRISCV64(instruction),
+        ref_(ref),
+        obj_(obj),
+        field_offset_(field_offset),
+        temp1_(temp1) {
+    DCHECK(kEmitCompilerReadBarrier);
+  }
+
+  const char* GetDescription() const override {
+    return "ReadBarrierMarkAndUpdateFieldSlowPathRISCV64";
+  }
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    GpuRegister ref_reg = ref_.AsRegister<GpuRegister>();
+    DCHECK(locations->CanCall());
+    DCHECK(!locations->GetLiveRegisters()->ContainsCoreRegister(ref_reg)) << ref_reg;
+    // This slow path is only used by the UnsafeCASObject intrinsic.
+    DCHECK((instruction_->IsInvokeVirtual() && instruction_->GetLocations()->Intrinsified()))
+        << "Unexpected instruction in read barrier marking and field updating slow path: "
+        << instruction_->DebugName();
+    DCHECK(instruction_->GetLocations()->Intrinsified());
+    DCHECK_EQ(instruction_->AsInvoke()->GetIntrinsic(), Intrinsics::kUnsafeCASObject);
+    DCHECK(field_offset_.IsRegister()) << field_offset_;
+
+    __ Bind(GetEntryLabel());
+
+    // Save the old reference.
+    // Note that we cannot use AT or TMP to save the old reference, as those
+    // are used by the code that follows, but we need the old reference after
+    // the call to the ReadBarrierMarkRegX entry point.
+    DCHECK_NE(temp1_, AT);
+    DCHECK_NE(temp1_, TMP);
+    __ Move(temp1_, ref_reg);
+
+    // No need to save live registers; it's taken care of by the
+    // entrypoint. Also, there is no need to update the stack mask,
+    // as this runtime call will not trigger a garbage collection.
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    DCHECK((T0 <= ref_reg && ref_reg <= S0) ||
+           (A0 <= ref_reg && ref_reg <= S10));
+    // "Compact" slow path, saving two moves.
+    //
+    // Instead of using the standard runtime calling convention (input
+    // and output in A0 and V0 respectively):
+    //
+    //   A0 <- ref
+    //   V0 <- ReadBarrierMark(A0)
+    //   ref <- V0
+    //
+    // we just use rX (the register containing `ref`) as input and output
+    // of a dedicated entrypoint:
+    //
+    //   rX <- ReadBarrierMarkRegX(rX)
+    //
+    int32_t entry_point_offset =
+        Thread::ReadBarrierMarkEntryPointsOffset<kRiscv64PointerSize>(ref_reg - 1);
+    // This runtime call does not require a stack map.
+    riscv64_codegen->InvokeRuntimeWithoutRecordingPcInfo(entry_point_offset,
+                                                        instruction_,
+                                                        this);
+
+    // If the new reference is different from the old reference,
+    // update the field in the holder (`*(obj_ + field_offset_)`).
+    //
+    // Note that this field could also hold a different object, if
+    // another thread had concurrently changed it. In that case, the
+    // the compare-and-set (CAS) loop below would abort, leaving the
+    // field as-is.
+    Riscv64Label done;
+    __ Beqc(temp1_, ref_reg, &done);
+
+    // Update the the holder's field atomically.  This may fail if
+    // mutator updates before us, but it's OK.  This is achieved
+    // using a strong compare-and-set (CAS) operation with relaxed
+    // memory synchronization ordering, where the expected value is
+    // the old reference and the desired value is the new reference.
+
+    // Convenience aliases.
+    GpuRegister base = obj_;
+    GpuRegister offset = field_offset_.AsRegister<GpuRegister>();
+    GpuRegister expected = temp1_;
+    GpuRegister value = ref_reg;
+    GpuRegister tmp_ptr = TMP;      // Pointer to actual memory.
+    GpuRegister tmp = AT;           // Value in memory.
+
+    __ Daddu(tmp_ptr, base, offset);
+
+    if (kPoisonHeapReferences) {
+      __ PoisonHeapReference(expected);
+      // Do not poison `value` if it is the same register as
+      // `expected`, which has just been poisoned.
+      if (value != expected) {
+        __ PoisonHeapReference(value);
+      }
+    }
+
+    // do {
+    //   tmp = [r_ptr] - expected;
+    // } while (tmp == 0 && failure([r_ptr] <- r_new_value));
+
+    Riscv64Label loop_head, exit_loop;
+    __ Bind(&loop_head);
+    __ Ll(tmp, tmp_ptr);
+    // The LL instruction sign-extends the 32-bit value, but
+    // 32-bit references must be zero-extended. Zero-extend `tmp`.
+    __ Dext(tmp, tmp, 0, 32);
+    __ Bnec(tmp, expected, &exit_loop);
+    __ Move(tmp, value);
+    __ Sc(tmp, tmp_ptr);
+    __ Bnezc(tmp, &loop_head);   // return in tmp 0: success, 1 : fail
+    __ Bind(&exit_loop);
+
+    if (kPoisonHeapReferences) {
+      __ UnpoisonHeapReference(expected);
+      // Do not unpoison `value` if it is the same register as
+      // `expected`, which has just been unpoisoned.
+      if (value != expected) {
+        __ UnpoisonHeapReference(value);
+      }
+    }
+
+    __ Bind(&done);
+    __ Bc(GetExitLabel());
+  }
+
+ private:
+  // The location (register) of the marked object reference.
+  const Location ref_;
+  // The register containing the object holding the marked object reference field.
+  const GpuRegister obj_;
+  // The location of the offset of the marked reference field within `obj_`.
+  Location field_offset_;
+
+  const GpuRegister temp1_;
+
+  DISALLOW_COPY_AND_ASSIGN(ReadBarrierMarkAndUpdateFieldSlowPathRISCV64);
+};
+
+// Slow path generating a read barrier for a heap reference.
+class ReadBarrierForHeapReferenceSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  ReadBarrierForHeapReferenceSlowPathRISCV64(HInstruction* instruction,
+                                            Location out,
+                                            Location ref,
+                                            Location obj,
+                                            uint32_t offset,
+                                            Location index)
+      : SlowPathCodeRISCV64(instruction),
+        out_(out),
+        ref_(ref),
+        obj_(obj),
+        offset_(offset),
+        index_(index) {
+    DCHECK(kEmitCompilerReadBarrier);
+    // If `obj` is equal to `out` or `ref`, it means the initial object
+    // has been overwritten by (or after) the heap object reference load
+    // to be instrumented, e.g.:
+    //
+    //   __ LoadFromOffset(kLoadWord, out, out, offset);
+    //   codegen_->GenerateReadBarrierSlow(instruction, out_loc, out_loc, out_loc, offset);
+    //
+    // In that case, we have lost the information about the original
+    // object, and the emitted read barrier cannot work properly.
+    DCHECK(!obj.Equals(out)) << "obj=" << obj << " out=" << out;
+    DCHECK(!obj.Equals(ref)) << "obj=" << obj << " ref=" << ref;
+  }
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    LocationSummary* locations = instruction_->GetLocations();
+    DataType::Type type = DataType::Type::kReference;
+    GpuRegister reg_out = out_.AsRegister<GpuRegister>();
+    DCHECK(locations->CanCall());
+    DCHECK(!locations->GetLiveRegisters()->ContainsCoreRegister(reg_out));
+    DCHECK(instruction_->IsInstanceFieldGet() ||
+           instruction_->IsStaticFieldGet() ||
+           instruction_->IsArrayGet() ||
+           instruction_->IsInstanceOf() ||
+           instruction_->IsCheckCast() ||
+           (instruction_->IsInvokeVirtual() && instruction_->GetLocations()->Intrinsified()))
+        << "Unexpected instruction in read barrier for heap reference slow path: "
+        << instruction_->DebugName();
+
+    __ Bind(GetEntryLabel());
+    SaveLiveRegisters(codegen, locations);
+
+    // We may have to change the index's value, but as `index_` is a
+    // constant member (like other "inputs" of this slow path),
+    // introduce a copy of it, `index`.
+    Location index = index_;
+    if (index_.IsValid()) {
+      // Handle `index_` for HArrayGet and UnsafeGetObject/UnsafeGetObjectVolatile intrinsics.
+      if (instruction_->IsArrayGet()) {
+        // Compute the actual memory offset and store it in `index`.
+        GpuRegister index_reg = index_.AsRegister<GpuRegister>();
+        DCHECK(locations->GetLiveRegisters()->ContainsCoreRegister(index_reg));
+        if (codegen->IsCoreCalleeSaveRegister(index_reg)) {
+          // We are about to change the value of `index_reg` (see the
+          // calls to art::riscv64::Riscv64Assembler::Sll and
+          // art::riscv64::MipsAssembler::Addiu32 below), but it has
+          // not been saved by the previous call to
+          // art::SlowPathCode::SaveLiveRegisters, as it is a
+          // callee-save register --
+          // art::SlowPathCode::SaveLiveRegisters does not consider
+          // callee-save registers, as it has been designed with the
+          // assumption that callee-save registers are supposed to be
+          // handled by the called function.  So, as a callee-save
+          // register, `index_reg` _would_ eventually be saved onto
+          // the stack, but it would be too late: we would have
+          // changed its value earlier.  Therefore, we manually save
+          // it here into another freely available register,
+          // `free_reg`, chosen of course among the caller-save
+          // registers (as a callee-save `free_reg` register would
+          // exhibit the same problem).
+          //
+          // Note we could have requested a temporary register from
+          // the register allocator instead; but we prefer not to, as
+          // this is a slow path, and we know we can find a
+          // caller-save register that is available.
+          GpuRegister free_reg = FindAvailableCallerSaveRegister(codegen);
+          __ Move(free_reg, index_reg);
+          index_reg = free_reg;
+          index = Location::RegisterLocation(index_reg);
+        } else {
+          // The initial register stored in `index_` has already been
+          // saved in the call to art::SlowPathCode::SaveLiveRegisters
+          // (as it is not a callee-save register), so we can freely
+          // use it.
+        }
+        // Shifting the index value contained in `index_reg` by the scale
+        // factor (2) cannot overflow in practice, as the runtime is
+        // unable to allocate object arrays with a size larger than
+        // 2^26 - 1 (that is, 2^28 - 4 bytes).
+        __ Sll(index_reg, index_reg, TIMES_4);
+        static_assert(
+            sizeof(mirror::HeapReference<mirror::Object>) == sizeof(int32_t),
+            "art::mirror::HeapReference<art::mirror::Object> and int32_t have different sizes.");
+        __ Addiu32(index_reg, index_reg, offset_);
+      } else {
+        // In the case of the UnsafeGetObject/UnsafeGetObjectVolatile
+        // intrinsics, `index_` is not shifted by a scale factor of 2
+        // (as in the case of ArrayGet), as it is actually an offset
+        // to an object field within an object.
+        DCHECK(instruction_->IsInvoke()) << instruction_->DebugName();
+        DCHECK(instruction_->GetLocations()->Intrinsified());
+        DCHECK((instruction_->AsInvoke()->GetIntrinsic() == Intrinsics::kUnsafeGetObject) ||
+               (instruction_->AsInvoke()->GetIntrinsic() == Intrinsics::kUnsafeGetObjectVolatile))
+            << instruction_->AsInvoke()->GetIntrinsic();
+        DCHECK_EQ(offset_, 0U);
+        DCHECK(index_.IsRegister());
+      }
+    }
+
+    // We're moving two or three locations to locations that could
+    // overlap, so we need a parallel move resolver.
+    InvokeRuntimeCallingConvention calling_convention;
+    HParallelMove parallel_move(codegen->GetGraph()->GetAllocator());
+    parallel_move.AddMove(ref_,
+                          Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                          DataType::Type::kReference,
+                          nullptr);
+    parallel_move.AddMove(obj_,
+                          Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
+                          DataType::Type::kReference,
+                          nullptr);
+    if (index.IsValid()) {
+      parallel_move.AddMove(index,
+                            Location::RegisterLocation(calling_convention.GetRegisterAt(2)),
+                            DataType::Type::kInt32,
+                            nullptr);
+      codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
+    } else {
+      codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
+      __ LoadConst32(calling_convention.GetRegisterAt(2), offset_);
+    }
+    riscv64_codegen->InvokeRuntime(kQuickReadBarrierSlow,
+                                  instruction_,
+                                  instruction_->GetDexPc(),
+                                  this);
+    CheckEntrypointTypes<
+        kQuickReadBarrierSlow, mirror::Object*, mirror::Object*, mirror::Object*, uint32_t>();
+    riscv64_codegen->MoveLocation(out_, calling_convention.GetReturnLocation(type), type);
+
+    RestoreLiveRegisters(codegen, locations);
+    __ Bc(GetExitLabel());
+  }
+
+  const char* GetDescription() const override {
+    return "ReadBarrierForHeapReferenceSlowPathRISCV64";
+  }
+
+ private:
+  GpuRegister FindAvailableCallerSaveRegister(CodeGenerator* codegen) {
+    size_t ref = static_cast<int>(ref_.AsRegister<GpuRegister>());
+    size_t obj = static_cast<int>(obj_.AsRegister<GpuRegister>());
+    for (size_t i = 0, e = codegen->GetNumberOfCoreRegisters(); i < e; ++i) {
+      if (i != ref &&
+          i != obj &&
+          !codegen->IsCoreCalleeSaveRegister(i) &&
+          !codegen->IsBlockedCoreRegister(i)) {
+        return static_cast<GpuRegister>(i);
+      }
+    }
+    // We shall never fail to find a free caller-save register, as
+    // there are more than two core caller-save registers on RISCV64
+    // (meaning it is possible to find one which is different from
+    // `ref` and `obj`).
+    DCHECK_GT(codegen->GetNumberOfCoreCallerSaveRegisters(), 2u);
+    LOG(FATAL) << "Could not find a free caller-save register";
+    UNREACHABLE();
+  }
+
+  const Location out_;
+  const Location ref_;
+  const Location obj_;
+  const uint32_t offset_;
+  // An additional location containing an index to an array.
+  // Only used for HArrayGet and the UnsafeGetObject &
+  // UnsafeGetObjectVolatile intrinsics.
+  const Location index_;
+
+  DISALLOW_COPY_AND_ASSIGN(ReadBarrierForHeapReferenceSlowPathRISCV64);
+};
+
+// Slow path generating a read barrier for a GC root.
+class ReadBarrierForRootSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  ReadBarrierForRootSlowPathRISCV64(HInstruction* instruction, Location out, Location root)
+      : SlowPathCodeRISCV64(instruction), out_(out), root_(root) {
+    DCHECK(kEmitCompilerReadBarrier);
+  }
+
+  void EmitNativeCode(CodeGenerator* codegen) override {
+    LocationSummary* locations = instruction_->GetLocations();
+    DataType::Type type = DataType::Type::kReference;
+    GpuRegister reg_out = out_.AsRegister<GpuRegister>();
+    DCHECK(locations->CanCall());
+    DCHECK(!locations->GetLiveRegisters()->ContainsCoreRegister(reg_out));
+    DCHECK(instruction_->IsLoadClass() || instruction_->IsLoadString())
+        << "Unexpected instruction in read barrier for GC root slow path: "
+        << instruction_->DebugName();
+
+    __ Bind(GetEntryLabel());
+    SaveLiveRegisters(codegen, locations);
+
+    InvokeRuntimeCallingConvention calling_convention;
+    CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
+    riscv64_codegen->MoveLocation(Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
+                                 root_,
+                                 DataType::Type::kReference);
+    riscv64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow,
+                                  instruction_,
+                                  instruction_->GetDexPc(),
+                                  this);
+    CheckEntrypointTypes<kQuickReadBarrierForRootSlow, mirror::Object*, GcRoot<mirror::Object>*>();
+    riscv64_codegen->MoveLocation(out_, calling_convention.GetReturnLocation(type), type);
+
+    RestoreLiveRegisters(codegen, locations);
+    __ Bc(GetExitLabel());
+  }
+
+  const char* GetDescription() const override { return "ReadBarrierForRootSlowPathRISCV64"; }
+
+ private:
+  const Location out_;
+  const Location root_;
+
+  DISALLOW_COPY_AND_ASSIGN(ReadBarrierForRootSlowPathRISCV64);
+};
+
+CodeGeneratorRISCV64::CodeGeneratorRISCV64(HGraph* graph,
+                                         const CompilerOptions& compiler_options,
+                                         OptimizingCompilerStats* stats)
+    : CodeGenerator(graph,
+                    kNumberOfGpuRegisters,
+                    kNumberOfFpuRegisters,
+                    /* number_of_register_pairs= */ 0,
+                    ComputeRegisterMask(reinterpret_cast<const int*>(kCoreCalleeSaves),
+                                        arraysize(kCoreCalleeSaves)),
+                    ComputeRegisterMask(reinterpret_cast<const int*>(kFpuCalleeSaves),
+                                        arraysize(kFpuCalleeSaves)),
+                    compiler_options,
+                    stats),
+      block_labels_(nullptr),
+      location_builder_(graph, this),
+      instruction_visitor_(graph, this),
+      move_resolver_(graph->GetAllocator(), this),
+      assembler_(graph->GetAllocator(),
+                 compiler_options.GetInstructionSetFeatures()->AsRiscv64InstructionSetFeatures()),
+      uint32_literals_(std::less<uint32_t>(),
+                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      uint64_literals_(std::less<uint64_t>(),
+                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      boot_image_method_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      method_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      boot_image_type_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      type_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      boot_image_string_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      string_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      boot_image_intrinsic_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      jit_string_patches_(StringReferenceValueComparator(),
+                          graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      jit_class_patches_(TypeReferenceValueComparator(),
+                         graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)) {
+  // Save RA (containing the return address) to mimic Quick.
+  AddAllocatedRegister(Location::RegisterLocation(RA));
+}
+
+#undef __
+// NOLINT on __ macro to suppress wrong warning/fix (misc-macro-parentheses) from clang-tidy.
+#define __ down_cast<Riscv64Assembler*>(GetAssembler())->  // NOLINT
+#define QUICK_ENTRY_POINT(x) QUICK_ENTRYPOINT_OFFSET(kRiscv64PointerSize, x).Int32Value()
+
+void CodeGeneratorRISCV64::Finalize(CodeAllocator* allocator) {
+  // Ensure that we fix up branches.
+  __ FinalizeCode();
+
+  // Adjust native pc offsets in stack maps.
+  StackMapStream* stack_map_stream = GetStackMapStream();
+  for (size_t i = 0, num = stack_map_stream->GetNumberOfStackMaps(); i != num; ++i) {
+    uint32_t old_position = stack_map_stream->GetStackMapNativePcOffset(i);
+    uint32_t new_position = __ GetAdjustedPosition(old_position);
+    DCHECK_GE(new_position, old_position);
+    stack_map_stream->SetStackMapNativePcOffset(i, new_position);
+  }
+
+  // Adjust pc offsets for the disassembly information.
+  if (disasm_info_ != nullptr) {
+    GeneratedCodeInterval* frame_entry_interval = disasm_info_->GetFrameEntryInterval();
+    frame_entry_interval->start = __ GetAdjustedPosition(frame_entry_interval->start);
+    frame_entry_interval->end = __ GetAdjustedPosition(frame_entry_interval->end);
+    for (auto& it : *disasm_info_->GetInstructionIntervals()) {
+      it.second.start = __ GetAdjustedPosition(it.second.start);
+      it.second.end = __ GetAdjustedPosition(it.second.end);
+    }
+    for (auto& it : *disasm_info_->GetSlowPathIntervals()) {
+      it.code_interval.start = __ GetAdjustedPosition(it.code_interval.start);
+      it.code_interval.end = __ GetAdjustedPosition(it.code_interval.end);
+    }
+  }
+
+  CodeGenerator::Finalize(allocator);
+}
+
+Riscv64Assembler* ParallelMoveResolverRISCV64::GetAssembler() const {
+  return codegen_->GetAssembler();
+}
+
+void ParallelMoveResolverRISCV64::EmitMove(size_t index) {
+  MoveOperands* move = moves_[index];
+  codegen_->MoveLocation(move->GetDestination(), move->GetSource(), move->GetType());
+}
+
+void ParallelMoveResolverRISCV64::EmitSwap(size_t index) {
+  MoveOperands* move = moves_[index];
+  codegen_->SwapLocations(move->GetDestination(), move->GetSource(), move->GetType());
+}
+
+void ParallelMoveResolverRISCV64::RestoreScratch(int reg) {
+  // Pop reg
+  __ Ld(GpuRegister(reg), SP, 0);
+  __ DecreaseFrameSize(kRiscv64DoublewordSize);
+}
+
+void ParallelMoveResolverRISCV64::SpillScratch(int reg) {
+  // Push reg
+  __ IncreaseFrameSize(kRiscv64DoublewordSize);
+  __ Sd(GpuRegister(reg), SP, 0);
+}
+
+void ParallelMoveResolverRISCV64::Exchange(int index1, int index2, bool double_slot) {
+  LoadOperandType load_type = double_slot ? kLoadDoubleword : kLoadWord;
+  StoreOperandType store_type = double_slot ? kStoreDoubleword : kStoreWord;
+  // Allocate a scratch register other than TMP, if available.
+  // Else, spill V0 (arbitrary choice) and use it as a scratch register (it will be
+  // automatically unspilled when the scratch scope object is destroyed).
+  ScratchRegisterScope ensure_scratch(this, TMP, V0, codegen_->GetNumberOfCoreRegisters());
+  // If V0 spills onto the stack, SP-relative offsets need to be adjusted.
+  int stack_offset = ensure_scratch.IsSpilled() ? kRiscv64DoublewordSize : 0;
+  __ LoadFromOffset(load_type,
+                    GpuRegister(ensure_scratch.GetRegister()),
+                    SP,
+                    index1 + stack_offset);
+  __ LoadFromOffset(load_type,
+                    TMP,
+                    SP,
+                    index2 + stack_offset);
+  __ StoreToOffset(store_type,
+                   GpuRegister(ensure_scratch.GetRegister()),
+                   SP,
+                   index2 + stack_offset);
+  __ StoreToOffset(store_type, TMP, SP, index1 + stack_offset);
+}
+
+void ParallelMoveResolverRISCV64::ExchangeQuadSlots(int index1, int index2) {
+  __ LoadFpuFromOffset(kLoadQuadword, FTMP, SP, index1);
+  __ LoadFpuFromOffset(kLoadQuadword, FTMP2, SP, index2);
+  __ StoreFpuToOffset(kStoreQuadword, FTMP, SP, index2);
+  __ StoreFpuToOffset(kStoreQuadword, FTMP2, SP, index1);
+}
+
+static dwarf::Reg DWARFReg(GpuRegister reg) {
+  return dwarf::Reg::Riscv64Core(static_cast<int>(reg));
+}
+
+static dwarf::Reg DWARFReg(FpuRegister reg) {
+  return dwarf::Reg::Riscv64Fp(static_cast<int>(reg));
+}
+
+void CodeGeneratorRISCV64::GenerateFrameEntry() {
+  __ Bind(&frame_entry_label_);
+
+  if (GetCompilerOptions().CountHotnessInCompiledCode()) {
+    __ Lhu(TMP, kMethodRegisterArgument, ArtMethod::HotnessCountOffset().Int32Value());
+    __ Addiu(TMP, TMP, 1);
+    __ Sh(TMP, kMethodRegisterArgument, ArtMethod::HotnessCountOffset().Int32Value());
+  }
+
+  bool do_overflow_check =
+      FrameNeedsStackCheck(GetFrameSize(), InstructionSet::kRiscv64) || !IsLeafMethod();
+
+  if (do_overflow_check) {
+    __ LoadFromOffset(
+        kLoadWord,
+        ZERO,
+        SP,
+        -static_cast<int32_t>(GetStackOverflowReservedBytes(InstructionSet::kRiscv64)));
+    RecordPcInfo(nullptr, 0);
+  }
+
+  if (HasEmptyFrame()) {
+    return;
+  }
+
+  // Make sure the frame size isn't unreasonably large.
+  if (GetFrameSize() > GetStackOverflowReservedBytes(InstructionSet::kRiscv64)) {
+    LOG(FATAL) << "Stack frame larger than "
+        << GetStackOverflowReservedBytes(InstructionSet::kRiscv64) << " bytes";
+  }
+
+  // Spill callee-saved registers.
+
+  uint32_t ofs = GetFrameSize();
+  __ IncreaseFrameSize(ofs);
+
+  for (int i = arraysize(kCoreCalleeSaves) - 1; i >= 0; --i) {
+    GpuRegister reg = kCoreCalleeSaves[i];
+    if (allocated_registers_.ContainsCoreRegister(reg)) {
+      ofs -= kRiscv64DoublewordSize;
+      __ StoreToOffset(kStoreDoubleword, reg, SP, ofs);
+      __ cfi().RelOffset(DWARFReg(reg), ofs);
+    }
+  }
+
+  for (int i = arraysize(kFpuCalleeSaves) - 1; i >= 0; --i) {
+    FpuRegister reg = kFpuCalleeSaves[i];
+    if (allocated_registers_.ContainsFloatingPointRegister(reg)) {
+      ofs -= kRiscv64DoublewordSize;
+      __ StoreFpuToOffset(kStoreDoubleword, reg, SP, ofs);
+      __ cfi().RelOffset(DWARFReg(reg), ofs);
+    }
+  }
+
+  // Save the current method if we need it. Note that we do not
+  // do this in HCurrentMethod, as the instruction might have been removed
+  // in the SSA graph.
+  if (RequiresCurrentMethod()) {
+    __ StoreToOffset(kStoreDoubleword, kMethodRegisterArgument, SP, kCurrentMethodStackOffset);
+  }
+
+  if (GetGraph()->HasShouldDeoptimizeFlag()) {
+    // Initialize should_deoptimize flag to 0.
+    __ StoreToOffset(kStoreWord, ZERO, SP, GetStackOffsetOfShouldDeoptimizeFlag());
+  }
+}
+
+void CodeGeneratorRISCV64::GenerateFrameExit() {
+  __ cfi().RememberState();
+
+  if (!HasEmptyFrame()) {
+    // Restore callee-saved registers.
+
+    // For better instruction scheduling restore RA before other registers.
+    uint32_t ofs = GetFrameSize();
+    for (int i = arraysize(kCoreCalleeSaves) - 1; i >= 0; --i) {
+      GpuRegister reg = kCoreCalleeSaves[i];
+      if (allocated_registers_.ContainsCoreRegister(reg)) {
+        ofs -= kRiscv64DoublewordSize;
+        __ LoadFromOffset(kLoadDoubleword, reg, SP, ofs);
+        __ cfi().Restore(DWARFReg(reg));
+      }
+    }
+
+    for (int i = arraysize(kFpuCalleeSaves) - 1; i >= 0; --i) {
+      FpuRegister reg = kFpuCalleeSaves[i];
+      if (allocated_registers_.ContainsFloatingPointRegister(reg)) {
+        ofs -= kRiscv64DoublewordSize;
+        __ LoadFpuFromOffset(kLoadDoubleword, reg, SP, ofs);
+        __ cfi().Restore(DWARFReg(reg));
+      }
+    }
+
+    __ DecreaseFrameSize(GetFrameSize());
+  }
+
+  __ Jr(RA);
+
+  __ cfi().RestoreState();
+  __ cfi().DefCFAOffset(GetFrameSize());
+}
+
+void CodeGeneratorRISCV64::Bind(HBasicBlock* block) {
+  __ Bind(GetLabelOf(block));
+}
+
+void CodeGeneratorRISCV64::MoveLocation(Location destination,
+                                       Location source,
+                                       DataType::Type dst_type) {
+  if (source.Equals(destination)) {
+    return;
+  }
+
+  // A valid move can always be inferred from the destination and source
+  // locations. When moving from and to a register, the argument type can be
+  // used to generate 32bit instead of 64bit moves.
+  bool unspecified_type = (dst_type == DataType::Type::kVoid);
+  DCHECK_EQ(unspecified_type, false);
+
+  if (destination.IsRegister() || destination.IsFpuRegister()) {
+    if (unspecified_type) {
+      HConstant* src_cst = source.IsConstant() ? source.GetConstant() : nullptr;
+      if (source.IsStackSlot() ||
+          (src_cst != nullptr && (src_cst->IsIntConstant()
+                                  || src_cst->IsFloatConstant()
+                                  || src_cst->IsNullConstant()))) {
+        // For stack slots and 32bit constants, a 64bit type is appropriate.
+        dst_type = destination.IsRegister() ? DataType::Type::kInt32 : DataType::Type::kFloat32;
+      } else {
+        // If the source is a double stack slot or a 64bit constant, a 64bit
+        // type is appropriate. Else the source is a register, and since the
+        // type has not been specified, we chose a 64bit type to force a 64bit
+        // move.
+        dst_type = destination.IsRegister() ? DataType::Type::kInt64 : DataType::Type::kFloat64;
+      }
+    }
+    DCHECK((destination.IsFpuRegister() && DataType::IsFloatingPointType(dst_type)) ||
+           (destination.IsRegister() && !DataType::IsFloatingPointType(dst_type)));
+    if (source.IsStackSlot() || source.IsDoubleStackSlot()) {
+      // Move to GPR/FPR from stack
+      LoadOperandType load_type = source.IsStackSlot() ? kLoadWord : kLoadDoubleword;
+      if (DataType::IsFloatingPointType(dst_type)) {
+        __ LoadFpuFromOffset(load_type,
+                             destination.AsFpuRegister<FpuRegister>(),
+                             SP,
+                             source.GetStackIndex());
+      } else {
+        // TODO: use load_type = kLoadUnsignedWord when type == DataType::Type::kReference.
+        __ LoadFromOffset(load_type,
+                          destination.AsRegister<GpuRegister>(),
+                          SP,
+                          source.GetStackIndex());
+      }
+    } else if (source.IsSIMDStackSlot()) {
+      __ LoadFpuFromOffset(kLoadQuadword,
+                           destination.AsFpuRegister<FpuRegister>(),
+                           SP,
+                           source.GetStackIndex());
+    } else if (source.IsConstant()) {
+      // Move to GPR/FPR from constant
+      GpuRegister gpr = AT;
+      if (!DataType::IsFloatingPointType(dst_type)) {
+        gpr = destination.AsRegister<GpuRegister>();
+      }
+      if (dst_type == DataType::Type::kInt32 || dst_type == DataType::Type::kFloat32) {
+        int32_t value = GetInt32ValueOf(source.GetConstant()->AsConstant());
+        if (DataType::IsFloatingPointType(dst_type) && value == 0) {
+          gpr = ZERO;
+        } else {
+          __ LoadConst32(gpr, value);
+        }
+      } else {
+        int64_t value = GetInt64ValueOf(source.GetConstant()->AsConstant());
+        if (DataType::IsFloatingPointType(dst_type) && value == 0) {
+          gpr = ZERO;
+        } else {
+          __ LoadConst64(gpr, value);
+        }
+      }
+      if (dst_type == DataType::Type::kFloat32) {
+        __ Mtc1(gpr, destination.AsFpuRegister<FpuRegister>());
+      } else if (dst_type == DataType::Type::kFloat64) {
+        __ Dmtc1(gpr, destination.AsFpuRegister<FpuRegister>());
+      }
+    } else if (source.IsRegister()) {
+      if (destination.IsRegister()) {
+        // Move to GPR from GPR
+        __ Move(destination.AsRegister<GpuRegister>(), source.AsRegister<GpuRegister>());
+      } else {
+        DCHECK(destination.IsFpuRegister());
+        if (DataType::Is64BitType(dst_type)) {
+          __ Dmtc1(source.AsRegister<GpuRegister>(), destination.AsFpuRegister<FpuRegister>());
+        } else {
+          __ Mtc1(source.AsRegister<GpuRegister>(), destination.AsFpuRegister<FpuRegister>());
+        }
+      }
+    } else if (source.IsFpuRegister()) {
+      if (destination.IsFpuRegister()) {
+        if (GetGraph()->HasSIMD()) {
+          __ MoveV(VectorRegisterFrom(destination),
+                   VectorRegisterFrom(source));
+        } else {
+          // Move to FPR from FPR
+          if (dst_type == DataType::Type::kFloat32) {
+            __ MovS(destination.AsFpuRegister<FpuRegister>(), source.AsFpuRegister<FpuRegister>());
+          } else {
+            DCHECK_EQ(dst_type, DataType::Type::kFloat64);
+            __ MovD(destination.AsFpuRegister<FpuRegister>(), source.AsFpuRegister<FpuRegister>());
+          }
+        }
+      } else {
+        DCHECK(destination.IsRegister());
+        if (DataType::Is64BitType(dst_type)) {
+          __ Dmfc1(destination.AsRegister<GpuRegister>(), source.AsFpuRegister<FpuRegister>());
+        } else {
+          __ Mfc1(destination.AsRegister<GpuRegister>(), source.AsFpuRegister<FpuRegister>());
+        }
+      }
+    }
+  } else if (destination.IsSIMDStackSlot()) {
+    if (source.IsFpuRegister()) {
+      __ StoreFpuToOffset(kStoreQuadword,
+                          source.AsFpuRegister<FpuRegister>(),
+                          SP,
+                          destination.GetStackIndex());
+    } else {
+      DCHECK(source.IsSIMDStackSlot());
+      __ LoadFpuFromOffset(kLoadQuadword,
+                           FTMP,
+                           SP,
+                           source.GetStackIndex());
+      __ StoreFpuToOffset(kStoreQuadword,
+                          FTMP,
+                          SP,
+                          destination.GetStackIndex());
+    }
+  } else {  // The destination is not a register. It must be a stack slot.
+    DCHECK(destination.IsStackSlot() || destination.IsDoubleStackSlot());
+    if (source.IsRegister() || source.IsFpuRegister()) {
+      if (unspecified_type) {
+        if (source.IsRegister()) {
+          dst_type = destination.IsStackSlot() ? DataType::Type::kInt32 : DataType::Type::kInt64;
+        } else {
+          dst_type =
+              destination.IsStackSlot() ? DataType::Type::kFloat32 : DataType::Type::kFloat64;
+        }
+      }
+      DCHECK((destination.IsDoubleStackSlot() == DataType::Is64BitType(dst_type)) &&
+             (source.IsFpuRegister() == DataType::IsFloatingPointType(dst_type)));
+      // Move to stack from GPR/FPR
+      StoreOperandType store_type = destination.IsStackSlot() ? kStoreWord : kStoreDoubleword;
+      if (source.IsRegister()) {
+        __ StoreToOffset(store_type,
+                         source.AsRegister<GpuRegister>(),
+                         SP,
+                         destination.GetStackIndex());
+      } else {
+        __ StoreFpuToOffset(store_type,
+                            source.AsFpuRegister<FpuRegister>(),
+                            SP,
+                            destination.GetStackIndex());
+      }
+    } else if (source.IsConstant()) {
+      // Move to stack from constant
+      HConstant* src_cst = source.GetConstant();
+      StoreOperandType store_type = destination.IsStackSlot() ? kStoreWord : kStoreDoubleword;
+      GpuRegister gpr = ZERO;
+      if (destination.IsStackSlot()) {
+        int32_t value = GetInt32ValueOf(src_cst->AsConstant());
+        if (value != 0) {
+          gpr = TMP;
+          __ LoadConst32(gpr, value);
+        }
+      } else {
+        DCHECK(destination.IsDoubleStackSlot());
+        int64_t value = GetInt64ValueOf(src_cst->AsConstant());
+        if (value != 0) {
+          gpr = TMP;
+          __ LoadConst64(gpr, value);
+        }
+      }
+      __ StoreToOffset(store_type, gpr, SP, destination.GetStackIndex());
+    } else {
+      DCHECK(source.IsStackSlot() || source.IsDoubleStackSlot());
+      DCHECK_EQ(source.IsDoubleStackSlot(), destination.IsDoubleStackSlot());
+      // Move to stack from stack
+      if (destination.IsStackSlot()) {
+        __ LoadFromOffset(kLoadWord, TMP, SP, source.GetStackIndex());
+        __ StoreToOffset(kStoreWord, TMP, SP, destination.GetStackIndex());
+      } else {
+        __ LoadFromOffset(kLoadDoubleword, TMP, SP, source.GetStackIndex());
+        __ StoreToOffset(kStoreDoubleword, TMP, SP, destination.GetStackIndex());
+      }
+    }
+  }
+}
+
+void CodeGeneratorRISCV64::SwapLocations(Location loc1, Location loc2, DataType::Type type) {
+  DCHECK(!loc1.IsConstant());
+  DCHECK(!loc2.IsConstant());
+
+  if (loc1.Equals(loc2)) {
+    return;
+  }
+
+  bool is_slot1 = loc1.IsStackSlot() || loc1.IsDoubleStackSlot();
+  bool is_slot2 = loc2.IsStackSlot() || loc2.IsDoubleStackSlot();
+  bool is_simd1 = loc1.IsSIMDStackSlot();
+  bool is_simd2 = loc2.IsSIMDStackSlot();
+  bool is_fp_reg1 = loc1.IsFpuRegister();
+  bool is_fp_reg2 = loc2.IsFpuRegister();
+
+  if (loc2.IsRegister() && loc1.IsRegister()) {
+    // Swap 2 GPRs
+    GpuRegister r1 = loc1.AsRegister<GpuRegister>();
+    GpuRegister r2 = loc2.AsRegister<GpuRegister>();
+    __ Move(TMP, r2);
+    __ Move(r2, r1);
+    __ Move(r1, TMP);
+  } else if (is_fp_reg2 && is_fp_reg1) {
+    // Swap 2 FPRs
+    if (GetGraph()->HasSIMD()) {
+      __ MoveV(static_cast<VectorRegister>(FTMP), VectorRegisterFrom(loc1));
+      __ MoveV(VectorRegisterFrom(loc1), VectorRegisterFrom(loc2));
+      __ MoveV(VectorRegisterFrom(loc2), static_cast<VectorRegister>(FTMP));
+    } else {
+      FpuRegister r1 = loc1.AsFpuRegister<FpuRegister>();
+      FpuRegister r2 = loc2.AsFpuRegister<FpuRegister>();
+      if (type == DataType::Type::kFloat32) {
+        __ MovS(FTMP, r1);
+        __ MovS(r1, r2);
+        __ MovS(r2, FTMP);
+      } else {
+        DCHECK_EQ(type, DataType::Type::kFloat64);
+        __ MovD(FTMP, r1);
+        __ MovD(r1, r2);
+        __ MovD(r2, FTMP);
+      }
+    }
+  } else if (is_slot1 != is_slot2) {
+    // Swap GPR/FPR and stack slot
+    Location reg_loc = is_slot1 ? loc2 : loc1;
+    Location mem_loc = is_slot1 ? loc1 : loc2;
+    LoadOperandType load_type = mem_loc.IsStackSlot() ? kLoadWord : kLoadDoubleword;
+    StoreOperandType store_type = mem_loc.IsStackSlot() ? kStoreWord : kStoreDoubleword;
+    // TODO: use load_type = kLoadUnsignedWord when type == DataType::Type::kReference.
+    __ LoadFromOffset(load_type, TMP, SP, mem_loc.GetStackIndex());
+    if (reg_loc.IsFpuRegister()) {
+      __ StoreFpuToOffset(store_type,
+                          reg_loc.AsFpuRegister<FpuRegister>(),
+                          SP,
+                          mem_loc.GetStackIndex());
+      if (mem_loc.IsStackSlot()) {
+        __ Mtc1(TMP, reg_loc.AsFpuRegister<FpuRegister>());
+      } else {
+        DCHECK(mem_loc.IsDoubleStackSlot());
+        __ Dmtc1(TMP, reg_loc.AsFpuRegister<FpuRegister>());
+      }
+    } else {
+      __ StoreToOffset(store_type, reg_loc.AsRegister<GpuRegister>(), SP, mem_loc.GetStackIndex());
+      __ Move(reg_loc.AsRegister<GpuRegister>(), TMP);
+    }
+  } else if (is_slot1 && is_slot2) {
+    move_resolver_.Exchange(loc1.GetStackIndex(),
+                            loc2.GetStackIndex(),
+                            loc1.IsDoubleStackSlot());
+  } else if (is_simd1 && is_simd2) {
+    move_resolver_.ExchangeQuadSlots(loc1.GetStackIndex(), loc2.GetStackIndex());
+  } else if ((is_fp_reg1 && is_simd2) || (is_fp_reg2 && is_simd1)) {
+    Location fp_reg_loc = is_fp_reg1 ? loc1 : loc2;
+    Location mem_loc = is_fp_reg1 ? loc2 : loc1;
+    __ LoadFpuFromOffset(kLoadQuadword, FTMP, SP, mem_loc.GetStackIndex());
+    __ StoreFpuToOffset(kStoreQuadword,
+                        fp_reg_loc.AsFpuRegister<FpuRegister>(),
+                        SP,
+                        mem_loc.GetStackIndex());
+    __ MoveV(VectorRegisterFrom(fp_reg_loc), static_cast<VectorRegister>(FTMP));
+  } else {
+    LOG(FATAL) << "Unimplemented swap between locations " << loc1 << " and " << loc2;
+  }
+}
+
+void CodeGeneratorRISCV64::MoveConstant(Location location, int32_t value) {
+  DCHECK(location.IsRegister());
+  __ LoadConst32(location.AsRegister<GpuRegister>(), value);
+}
+
+void CodeGeneratorRISCV64::AddLocationAsTemp(Location location, LocationSummary* locations) {
+  if (location.IsRegister()) {
+    locations->AddTemp(location);
+  } else {
+    UNIMPLEMENTED(FATAL) << "AddLocationAsTemp not implemented for location " << location;
+  }
+}
+
+void CodeGeneratorRISCV64::MarkGCCard(GpuRegister object,
+                                     GpuRegister value,
+                                     bool value_can_be_null) {
+  Riscv64Label done;
+  GpuRegister card = AT;
+  GpuRegister temp = TMP;
+  if (value_can_be_null) {
+    __ Beqzc(value, &done);
+  }
+  // Load the address of the card table into `card`.
+  __ LoadFromOffset(kLoadDoubleword,
+                    card,
+                    TR,
+                    Thread::CardTableOffset<kRiscv64PointerSize>().Int32Value());
+  // Calculate the address of the card corresponding to `object`.
+  __ Dsrl(temp, object, gc::accounting::CardTable::kCardShift);
+  __ Daddu(temp, card, temp);
+  // Write the `art::gc::accounting::CardTable::kCardDirty` value into the
+  // `object`'s card.
+  //
+  // Register `card` contains the address of the card table. Note that the card
+  // table's base is biased during its creation so that it always starts at an
+  // address whose least-significant byte is equal to `kCardDirty` (see
+  // art::gc::accounting::CardTable::Create). Therefore the SB instruction
+  // below writes the `kCardDirty` (byte) value into the `object`'s card
+  // (located at `card + object >> kCardShift`).
+  //
+  // This dual use of the value in register `card` (1. to calculate the location
+  // of the card to mark; and 2. to load the `kCardDirty` value) saves a load
+  // (no need to explicitly load `kCardDirty` as an immediate value).
+  __ Sb(card, temp, 0);
+  if (value_can_be_null) {
+    __ Bind(&done);
+  }
+}
+
+template <linker::LinkerPatch (*Factory)(size_t, const DexFile*, uint32_t, uint32_t)>
+inline void CodeGeneratorRISCV64::EmitPcRelativeLinkerPatches(
+    const ArenaDeque<PcRelativePatchInfo>& infos,
+    ArenaVector<linker::LinkerPatch>* linker_patches) {
+  for (const PcRelativePatchInfo& info : infos) {
+    const DexFile* dex_file = info.target_dex_file;
+    size_t offset_or_index = info.offset_or_index;
+    DCHECK(info.label.IsBound());
+    uint32_t literal_offset = __ GetLabelLocation(&info.label);
+    const PcRelativePatchInfo& info_high = info.patch_info_high ? *info.patch_info_high : info;
+    uint32_t pc_rel_offset = __ GetLabelLocation(&info_high.label);
+    linker_patches->push_back(Factory(literal_offset, dex_file, pc_rel_offset, offset_or_index));
+  }
+}
+
+template <linker::LinkerPatch (*Factory)(size_t, uint32_t, uint32_t)>
+linker::LinkerPatch NoDexFileAdapter(size_t literal_offset,
+                                     const DexFile* target_dex_file,
+                                     uint32_t pc_insn_offset,
+                                     uint32_t boot_image_offset) {
+  DCHECK(target_dex_file == nullptr);  // Unused for these patches, should be null.
+  return Factory(literal_offset, pc_insn_offset, boot_image_offset);
+}
+
+void CodeGeneratorRISCV64::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* linker_patches) {
+  DCHECK(linker_patches->empty());
+  size_t size =
+      boot_image_method_patches_.size() +
+      method_bss_entry_patches_.size() +
+      boot_image_type_patches_.size() +
+      type_bss_entry_patches_.size() +
+      boot_image_string_patches_.size() +
+      string_bss_entry_patches_.size() +
+      boot_image_intrinsic_patches_.size();
+  linker_patches->reserve(size);
+  if (GetCompilerOptions().IsBootImage()) {
+    EmitPcRelativeLinkerPatches<linker::LinkerPatch::RelativeMethodPatch>(
+        boot_image_method_patches_, linker_patches);
+    EmitPcRelativeLinkerPatches<linker::LinkerPatch::RelativeTypePatch>(
+        boot_image_type_patches_, linker_patches);
+    EmitPcRelativeLinkerPatches<linker::LinkerPatch::RelativeStringPatch>(
+        boot_image_string_patches_, linker_patches);
+    EmitPcRelativeLinkerPatches<NoDexFileAdapter<linker::LinkerPatch::IntrinsicReferencePatch>>(
+        boot_image_intrinsic_patches_, linker_patches);
+  } else {
+    EmitPcRelativeLinkerPatches<NoDexFileAdapter<linker::LinkerPatch::DataBimgRelRoPatch>>(
+        boot_image_method_patches_, linker_patches);
+    DCHECK(boot_image_type_patches_.empty());
+    DCHECK(boot_image_string_patches_.empty());
+    DCHECK(boot_image_intrinsic_patches_.empty());
+  }
+  EmitPcRelativeLinkerPatches<linker::LinkerPatch::MethodBssEntryPatch>(
+      method_bss_entry_patches_, linker_patches);
+  EmitPcRelativeLinkerPatches<linker::LinkerPatch::TypeBssEntryPatch>(
+      type_bss_entry_patches_, linker_patches);
+  EmitPcRelativeLinkerPatches<linker::LinkerPatch::StringBssEntryPatch>(
+      string_bss_entry_patches_, linker_patches);
+  DCHECK_EQ(size, linker_patches->size());
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewBootImageIntrinsicPatch(
+    uint32_t intrinsic_data,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(
+      /* dex_file= */ nullptr, intrinsic_data, info_high, &boot_image_intrinsic_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewBootImageRelRoPatch(
+    uint32_t boot_image_offset,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(
+      /* dex_file= */ nullptr, boot_image_offset, info_high, &boot_image_method_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewBootImageMethodPatch(
+    MethodReference target_method,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(
+      target_method.dex_file, target_method.index, info_high, &boot_image_method_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewMethodBssEntryPatch(
+    MethodReference target_method,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(
+      target_method.dex_file, target_method.index, info_high, &method_bss_entry_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewBootImageTypePatch(
+    const DexFile& dex_file,
+    dex::TypeIndex type_index,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(&dex_file, type_index.index_, info_high, &boot_image_type_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewTypeBssEntryPatch(
+    const DexFile& dex_file,
+    dex::TypeIndex type_index,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(&dex_file, type_index.index_, info_high, &type_bss_entry_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewBootImageStringPatch(
+    const DexFile& dex_file,
+    dex::StringIndex string_index,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(
+      &dex_file, string_index.index_, info_high, &boot_image_string_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewStringBssEntryPatch(
+    const DexFile& dex_file,
+    dex::StringIndex string_index,
+    const PcRelativePatchInfo* info_high) {
+  return NewPcRelativePatch(&dex_file, string_index.index_, info_high, &string_bss_entry_patches_);
+}
+
+CodeGeneratorRISCV64::PcRelativePatchInfo* CodeGeneratorRISCV64::NewPcRelativePatch(
+    const DexFile* dex_file,
+    uint32_t offset_or_index,
+    const PcRelativePatchInfo* info_high,
+    ArenaDeque<PcRelativePatchInfo>* patches) {
+  patches->emplace_back(dex_file, offset_or_index, info_high);
+  return &patches->back();
+}
+
+Literal* CodeGeneratorRISCV64::DeduplicateUint32Literal(uint32_t value, Uint32ToLiteralMap* map) {
+  return map->GetOrCreate(
+      value,
+      [this, value]() { return __ NewLiteral<uint32_t>(value); });
+}
+
+Literal* CodeGeneratorRISCV64::DeduplicateUint64Literal(uint64_t value) {
+  return uint64_literals_.GetOrCreate(
+      value,
+      [this, value]() { return __ NewLiteral<uint64_t>(value); });
+}
+
+Literal* CodeGeneratorRISCV64::DeduplicateBootImageAddressLiteral(uint64_t address) {
+  return DeduplicateUint32Literal(dchecked_integral_cast<uint32_t>(address), &uint32_literals_);
+}
+
+void CodeGeneratorRISCV64::EmitPcRelativeAddressPlaceholderHigh(PcRelativePatchInfo* info_high,
+                                                               GpuRegister out,
+                                                               PcRelativePatchInfo* info_low) {
+  DCHECK(!info_high->patch_info_high);
+  __ Bind(&info_high->label);
+  // Add the high 20-bit of a 32-bit offset to PC.
+  __ Auipc(out, /* imm20= */ 0x12345);
+  // A following instruction will add the sign-extended low half of the 32-bit
+  // offset to `out` (e.g. ld, jialc, daddiu).
+  if (info_low != nullptr) {
+    DCHECK_EQ(info_low->patch_info_high, info_high);
+    __ Bind(&info_low->label);
+  }
+}
+
+void CodeGeneratorRISCV64::LoadBootImageAddress(GpuRegister reg, uint32_t boot_image_reference) {
+  if (GetCompilerOptions().IsBootImage()) {
+    PcRelativePatchInfo* info_high = NewBootImageIntrinsicPatch(boot_image_reference);
+    PcRelativePatchInfo* info_low = NewBootImageIntrinsicPatch(boot_image_reference, info_high);
+    EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+    __ Addi(reg, AT, /* imm12= */ 0x678);
+  } else if (GetCompilerOptions().GetCompilePic()) {
+    PcRelativePatchInfo* info_high = NewBootImageRelRoPatch(boot_image_reference);
+    PcRelativePatchInfo* info_low = NewBootImageRelRoPatch(boot_image_reference, info_high);
+    EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+    // Note: Boot image is in the low 4GiB and the entry is 32-bit, so emit a 32-bit load.
+    __ Lwu(reg, AT, /* imm12= */ 0x678);
+  } else {
+    DCHECK(Runtime::Current()->UseJitCompilation());
+    gc::Heap* heap = Runtime::Current()->GetHeap();
+    DCHECK(!heap->GetBootImageSpaces().empty());
+    uintptr_t address =
+        reinterpret_cast<uintptr_t>(heap->GetBootImageSpaces()[0]->Begin() + boot_image_reference);
+    __ LoadLiteral(reg, kLoadDoubleword, DeduplicateBootImageAddressLiteral(address));
+  }
+}
+
+void CodeGeneratorRISCV64::AllocateInstanceForIntrinsic(HInvokeStaticOrDirect* invoke,
+                                                       uint32_t boot_image_offset) {
+  DCHECK(invoke->IsStatic());
+  InvokeRuntimeCallingConvention calling_convention;
+  GpuRegister argument = calling_convention.GetRegisterAt(0);
+  if (GetCompilerOptions().IsBootImage()) {
+    DCHECK_EQ(boot_image_offset, IntrinsicVisitor::IntegerValueOfInfo::kInvalidReference);
+    // Load the class the same way as for HLoadClass::LoadKind::kBootImageLinkTimePcRelative.
+    MethodReference target_method = invoke->GetTargetMethod();
+    dex::TypeIndex type_idx = target_method.dex_file->GetMethodId(target_method.index).class_idx_;
+    PcRelativePatchInfo* info_high = NewBootImageTypePatch(*target_method.dex_file, type_idx);
+    PcRelativePatchInfo* info_low =
+        NewBootImageTypePatch(*target_method.dex_file, type_idx, info_high);
+    EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+    __ Addi(argument, AT, /* imm12= */ 0x678);
+  } else {
+    LoadBootImageAddress(argument, boot_image_offset);
+  }
+  InvokeRuntime(kQuickAllocObjectInitialized, invoke, invoke->GetDexPc());
+  CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
+}
+
+Literal* CodeGeneratorRISCV64::DeduplicateJitStringLiteral(const DexFile& dex_file,
+                                                          dex::StringIndex string_index,
+                                                          Handle<mirror::String> handle) {
+  ReserveJitStringRoot(StringReference(&dex_file, string_index), handle);
+  return jit_string_patches_.GetOrCreate(
+      StringReference(&dex_file, string_index),
+      [this]() { return __ NewLiteral<uint32_t>(/* value= */ 0u); });
+}
+
+Literal* CodeGeneratorRISCV64::DeduplicateJitClassLiteral(const DexFile& dex_file,
+                                                         dex::TypeIndex type_index,
+                                                         Handle<mirror::Class> handle) {
+  ReserveJitClassRoot(TypeReference(&dex_file, type_index), handle);
+  return jit_class_patches_.GetOrCreate(
+      TypeReference(&dex_file, type_index),
+      [this]() { return __ NewLiteral<uint32_t>(/* value= */ 0u); });
+}
+
+void CodeGeneratorRISCV64::PatchJitRootUse(uint8_t* code,
+                                          const uint8_t* roots_data,
+                                          const Literal* literal,
+                                          uint64_t index_in_table) const {
+  uint32_t literal_offset = GetAssembler().GetLabelLocation(literal->GetLabel());
+  uintptr_t address =
+      reinterpret_cast<uintptr_t>(roots_data) + index_in_table * sizeof(GcRoot<mirror::Object>);
+  reinterpret_cast<uint32_t*>(code + literal_offset)[0] = dchecked_integral_cast<uint32_t>(address);
+}
+
+void CodeGeneratorRISCV64::EmitJitRootPatches(uint8_t* code, const uint8_t* roots_data) {
+  for (const auto& entry : jit_string_patches_) {
+    const StringReference& string_reference = entry.first;
+    Literal* table_entry_literal = entry.second;
+    uint64_t index_in_table = GetJitStringRootIndex(string_reference);
+    PatchJitRootUse(code, roots_data, table_entry_literal, index_in_table);
+  }
+  for (const auto& entry : jit_class_patches_) {
+    const TypeReference& type_reference = entry.first;
+    Literal* table_entry_literal = entry.second;
+    uint64_t index_in_table = GetJitClassRootIndex(type_reference);
+    PatchJitRootUse(code, roots_data, table_entry_literal, index_in_table);
+  }
+}
+
+void CodeGeneratorRISCV64::SetupBlockedRegisters() const {
+  // ZERO, GP, SP, RA TP are always reserved and can't be allocated.
+  blocked_core_registers_[ZERO] = true;
+  blocked_core_registers_[GP] = true;
+  blocked_core_registers_[SP] = true;
+  blocked_core_registers_[RA] = true;
+  blocked_core_registers_[TP] = true;
+
+  // TMP(T5), TMP2(T4), AT(T3), FTMP(FT11), FTMP2(FT10) are used as temporary/scratch
+  // registers.
+  blocked_core_registers_[TMP] = true;
+  blocked_core_registers_[TMP2] = true;
+  blocked_core_registers_[AT] = true;
+  blocked_fpu_registers_[FTMP] = true;
+  blocked_fpu_registers_[FTMP2] = true;
+
+  // Reserve suspend and self registers.
+  blocked_core_registers_[S11] = true;
+  blocked_core_registers_[S1] = true;
+
+  // Reserve T6 for function calls
+  blocked_core_registers_[T6] = true;
+
+  if (GetGraph()->IsDebuggable()) {
+    // Stubs do not save callee-save floating point registers. If the graph
+    // is debuggable, we need to deal with these registers differently. For
+    // now, just block them.
+    for (size_t i = 0; i < arraysize(kFpuCalleeSaves); ++i) {
+      blocked_fpu_registers_[kFpuCalleeSaves[i]] = true;
+    }
+  }
+}
+
+size_t CodeGeneratorRISCV64::SaveCoreRegister(size_t stack_index, uint32_t reg_id) {
+  __ StoreToOffset(kStoreDoubleword, GpuRegister(reg_id), SP, stack_index);
+  return kRiscv64DoublewordSize;
+}
+
+size_t CodeGeneratorRISCV64::RestoreCoreRegister(size_t stack_index, uint32_t reg_id) {
+  __ LoadFromOffset(kLoadDoubleword, GpuRegister(reg_id), SP, stack_index);
+  return kRiscv64DoublewordSize;
+}
+
+size_t CodeGeneratorRISCV64::SaveFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
+  __ StoreFpuToOffset(GetGraph()->HasSIMD() ? kStoreQuadword : kStoreDoubleword,
+                      FpuRegister(reg_id),
+                      SP,
+                      stack_index);
+  return GetFloatingPointSpillSlotSize();
+}
+
+size_t CodeGeneratorRISCV64::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
+  __ LoadFpuFromOffset(GetGraph()->HasSIMD() ? kLoadQuadword : kLoadDoubleword,
+                       FpuRegister(reg_id),
+                       SP,
+                       stack_index);
+  return GetFloatingPointSpillSlotSize();
+}
+
+void CodeGeneratorRISCV64::DumpCoreRegister(std::ostream& stream, int reg) const {
+  stream << GpuRegister(reg);
+}
+
+void CodeGeneratorRISCV64::DumpFloatingPointRegister(std::ostream& stream, int reg) const {
+  stream << FpuRegister(reg);
+}
+
+const Riscv64InstructionSetFeatures& CodeGeneratorRISCV64::GetInstructionSetFeatures() const {
+  return *GetCompilerOptions().GetInstructionSetFeatures()->AsRiscv64InstructionSetFeatures();
+}
+
+void CodeGeneratorRISCV64::InvokeRuntime(QuickEntrypointEnum entrypoint,
+                                        HInstruction* instruction,
+                                        uint32_t dex_pc,
+                                        SlowPathCode* slow_path) {
+  ValidateInvokeRuntime(entrypoint, instruction, slow_path);
+  GenerateInvokeRuntime(GetThreadOffset<kRiscv64PointerSize>(entrypoint).Int32Value());
+  if (EntrypointRequiresStackMap(entrypoint)) {
+    RecordPcInfo(instruction, dex_pc, slow_path);
+  }
+}
+
+void CodeGeneratorRISCV64::InvokeRuntimeWithoutRecordingPcInfo(int32_t entry_point_offset,
+                                                              HInstruction* instruction,
+                                                              SlowPathCode* slow_path) {
+  ValidateInvokeRuntimeWithoutRecordingPcInfo(instruction, slow_path);
+  GenerateInvokeRuntime(entry_point_offset);
+}
+
+void CodeGeneratorRISCV64::GenerateInvokeRuntime(int32_t entry_point_offset) {
+  __ LoadFromOffset(kLoadDoubleword, T6, TR, entry_point_offset);
+  __ Jalr(T6);
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateClassInitializationCheck(SlowPathCodeRISCV64* slow_path,
+                                                                      GpuRegister class_reg) {
+  constexpr size_t status_lsb_position = SubtypeCheckBits::BitStructSizeOf();
+  const size_t status_byte_offset =
+      mirror::Class::StatusOffset().SizeValue() + (status_lsb_position / kBitsPerByte);
+  constexpr uint32_t shifted_initialized_value =
+      enum_cast<uint32_t>(ClassStatus::kInitialized) << (status_lsb_position % kBitsPerByte);
+
+  __ LoadFromOffset(kLoadUnsignedByte, TMP, class_reg, status_byte_offset);
+  __ Sltiu(TMP, TMP, shifted_initialized_value);
+  __ Bnezc(TMP, slow_path->GetEntryLabel());
+  // Even if the initialized flag is set, we need to ensure consistent memory ordering.
+  __ Sync(0);
+  __ Bind(slow_path->GetExitLabel());
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateBitstringTypeCheckCompare(HTypeCheckInstruction* check,
+                                                                       GpuRegister temp) {
+  uint32_t path_to_root = check->GetBitstringPathToRoot();
+  uint32_t mask = check->GetBitstringMask();
+  DCHECK(IsPowerOfTwo(mask + 1));
+  size_t mask_bits = WhichPowerOf2(mask + 1);
+
+  if (mask_bits == 16u) {
+    // Load only the bitstring part of the status word.
+    __ LoadFromOffset(
+        kLoadUnsignedHalfword, temp, temp, mirror::Class::StatusOffset().Int32Value());
+    // Compare the bitstring bits using XOR.
+    __ Xori(temp, temp, dchecked_integral_cast<uint16_t>(path_to_root));
+  } else {
+    // /* uint32_t */ temp = temp->status_
+    __ LoadFromOffset(kLoadWord, temp, temp, mirror::Class::StatusOffset().Int32Value());
+    // Compare the bitstring bits using XOR.
+    if (IsUint<16>(path_to_root)) {
+      __ Xori(temp, temp, dchecked_integral_cast<uint16_t>(path_to_root));
+    } else {
+      __ LoadConst32(TMP, path_to_root);
+      __ Xor(temp, temp, TMP);
+    }
+    // Shift out bits that do not contribute to the comparison.
+    __ Sll(temp, temp, 32 - mask_bits);
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateMemoryBarrier(MemBarrierKind kind ATTRIBUTE_UNUSED) {
+  __ Sync(0);  // only stype 0 is supported
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateSuspendCheck(HSuspendCheck* instruction,
+                                                          HBasicBlock* successor) {
+  SuspendCheckSlowPathRISCV64* slow_path =
+      down_cast<SuspendCheckSlowPathRISCV64*>(instruction->GetSlowPath());
+
+  if (slow_path == nullptr) {
+    slow_path =
+        new (codegen_->GetScopedAllocator()) SuspendCheckSlowPathRISCV64(instruction, successor);
+    instruction->SetSlowPath(slow_path);
+    codegen_->AddSlowPath(slow_path);
+    if (successor != nullptr) {
+      DCHECK(successor->IsLoopHeader());
+    }
+  } else {
+    DCHECK_EQ(slow_path->GetSuccessor(), successor);
+  }
+
+  __ LoadFromOffset(kLoadUnsignedHalfword,
+                    TMP,
+                    TR,
+                    Thread::ThreadFlagsOffset<kRiscv64PointerSize>().Int32Value());
+  if (successor == nullptr) {
+    __ Bnezc(TMP, slow_path->GetEntryLabel());
+    __ Bind(slow_path->GetReturnLabel());
+  } else {
+    __ Beqzc(TMP, codegen_->GetLabelOf(successor));
+    __ Bc(slow_path->GetEntryLabel());
+    // slow_path will return to GetLabelOf(successor).
+  }
+}
+
+InstructionCodeGeneratorRISCV64::InstructionCodeGeneratorRISCV64(HGraph* graph,
+                                                               CodeGeneratorRISCV64* codegen)
+      : InstructionCodeGenerator(graph, codegen),
+        assembler_(codegen->GetAssembler()),
+        codegen_(codegen) {}
+
+void LocationsBuilderRISCV64::HandleBinaryOp(HBinaryOperation* instruction) {
+  DCHECK_EQ(instruction->InputCount(), 2U);
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  DataType::Type type = instruction->GetResultType();
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      locations->SetInAt(0, Location::RequiresRegister());
+      HInstruction* right = instruction->InputAt(1);
+      bool can_use_imm = false;
+      if (right->IsConstant()) {
+        int64_t imm = CodeGenerator::GetInt64ValueOf(right->AsConstant());
+        if (instruction->IsAnd() || instruction->IsOr() || instruction->IsXor()) {
+          can_use_imm = IsUint<12>(imm);
+        } else {
+          DCHECK(instruction->IsAdd() || instruction->IsSub());
+          bool single_use = right->GetUses().HasExactlyOneElement();
+          if (instruction->IsSub()) {
+            if (!(type == DataType::Type::kInt32 && imm == INT32_MIN)) {
+              imm = -imm;
+            }
+          }
+          if (type == DataType::Type::kInt32) {
+            can_use_imm = IsInt<12>(imm) || (Low16Bits(imm) == 0) || single_use;
+          } else {
+            can_use_imm = IsInt<12>(imm) || (IsInt<32>(imm) && (Low12Bits(imm) == 0)) || single_use;
+          }
+        }
+      }
+      if (can_use_imm)
+        locations->SetInAt(1, Location::ConstantLocation(right->AsConstant()));
+      else
+        locations->SetInAt(1, Location::RequiresRegister());
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      }
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+
+    default:
+      LOG(FATAL) << "Unexpected " << instruction->DebugName() << " type " << type;
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::HandleBinaryOp(HBinaryOperation* instruction) {
+  DataType::Type type = instruction->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+      GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+      Location rhs_location = locations->InAt(1);
+
+      GpuRegister rhs_reg = ZERO;
+      int64_t rhs_imm = 0;
+      bool use_imm = rhs_location.IsConstant();
+      if (use_imm) {
+        rhs_imm = CodeGenerator::GetInt64ValueOf(rhs_location.GetConstant());
+      } else {
+        rhs_reg = rhs_location.AsRegister<GpuRegister>();
+      }
+
+      if (instruction->IsAnd()) {
+        if (use_imm)
+          __ Andi(dst, lhs, rhs_imm);
+        else
+          __ And(dst, lhs, rhs_reg);
+      } else if (instruction->IsOr()) {
+        if (use_imm)
+          __ Ori(dst, lhs, rhs_imm);
+        else
+          __ Or(dst, lhs, rhs_reg);
+      } else if (instruction->IsXor()) {
+        if (use_imm)
+          __ Xori(dst, lhs, rhs_imm);
+        else
+          __ Xor(dst, lhs, rhs_reg);
+      } else if (instruction->IsAdd() || instruction->IsSub()) {
+        if (instruction->IsSub()) {
+          rhs_imm = -rhs_imm;
+        }
+        if (type == DataType::Type::kInt32) {
+          if (use_imm) {
+            if (IsInt<12>(rhs_imm)) {
+              __ Addiw(dst, lhs, rhs_imm);
+            } else {
+              __ LoadConst32(TMP, rhs_imm);
+              __ Addu(dst, lhs, TMP);
+            }
+          } else {
+            if (instruction->IsAdd()) {
+              __ Addu(dst, lhs, rhs_reg);
+            } else {
+              DCHECK(instruction->IsSub());
+              __ Subu(dst, lhs, rhs_reg);
+            }
+          }
+        } else {
+          if (use_imm) {
+            if (IsInt<12>(rhs_imm)) {
+              __ Addi(dst, lhs, rhs_imm);
+            } else if (IsInt<32>(rhs_imm)) {
+              __ LoadConst32(TMP, rhs_imm);
+              __ Daddu(dst, lhs, TMP);
+            } else {
+              __ LoadConst64(TMP, rhs_imm);
+              __ Daddu(dst, lhs, TMP);
+            }
+          } else if (instruction->IsAdd()) {
+            __ Daddu(dst, lhs, rhs_reg);
+          } else {
+            DCHECK(instruction->IsSub());
+            __ Dsubu(dst, lhs, rhs_reg);
+          }
+        }
+      }
+      break;
+    }
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64: {
+      FpuRegister dst = locations->Out().AsFpuRegister<FpuRegister>();
+      FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+      if (instruction->IsAdd()) {
+        if (type == DataType::Type::kFloat32)
+          __ AddS(dst, lhs, rhs);
+        else
+          __ AddD(dst, lhs, rhs);
+      } else if (instruction->IsSub()) {
+        if (type == DataType::Type::kFloat32)
+          __ SubS(dst, lhs, rhs);
+        else
+          __ SubD(dst, lhs, rhs);
+      } else {
+        LOG(FATAL) << "Unexpected floating-point binary operation";
+      }
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected binary operation type " << type;
+  }
+}
+
+void LocationsBuilderRISCV64::HandleShift(HBinaryOperation* instr) {
+  DCHECK(instr->IsShl() || instr->IsShr() || instr->IsUShr() || instr->IsRor());
+
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instr);
+  DataType::Type type = instr->GetResultType();
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RegisterOrConstant(instr->InputAt(1)));
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected shift type " << type;
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::HandleShift(HBinaryOperation* instr) {
+  DCHECK(instr->IsShl() || instr->IsShr() || instr->IsUShr() || instr->IsRor());
+  LocationSummary* locations = instr->GetLocations();
+  DataType::Type type = instr->GetType();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+      GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+      Location rhs_location = locations->InAt(1);
+
+      GpuRegister rhs_reg = ZERO;
+      int64_t rhs_imm = 0;
+      bool use_imm = rhs_location.IsConstant();
+      if (use_imm) {
+        rhs_imm = CodeGenerator::GetInt64ValueOf(rhs_location.GetConstant());
+      } else {
+        rhs_reg = rhs_location.AsRegister<GpuRegister>();
+      }
+
+      if (use_imm) {
+        uint32_t shift_value = rhs_imm &
+            (type == DataType::Type::kInt32 ? kMaxIntShiftDistance : kMaxLongShiftDistance);
+
+        if (shift_value == 0) {
+          if (dst != lhs) {
+            __ Move(dst, lhs);
+          }
+        } else if (type == DataType::Type::kInt32) {
+          if (instr->IsShl()) {
+            __ Sll(dst, lhs, shift_value);
+          } else if (instr->IsShr()) {
+            __ Sra(dst, lhs, shift_value);
+          } else if (instr->IsUShr()) {
+            __ Srl(dst, lhs, shift_value);
+          } else {
+            __ Rotr(dst, lhs, shift_value);
+          }
+        } else {
+          if (shift_value < 32) {
+            if (instr->IsShl()) {
+              __ Dsll(dst, lhs, shift_value);
+            } else if (instr->IsShr()) {
+              __ Dsra(dst, lhs, shift_value);
+            } else if (instr->IsUShr()) {
+              __ Dsrl(dst, lhs, shift_value);
+            } else {
+              __ Drotr(dst, lhs, shift_value);
+            }
+          } else {
+            shift_value -= 32;
+            if (instr->IsShl()) {
+              __ Dsll32(dst, lhs, shift_value);
+            } else if (instr->IsShr()) {
+              __ Dsra32(dst, lhs, shift_value);
+            } else if (instr->IsUShr()) {
+              __ Dsrl32(dst, lhs, shift_value);
+            } else {
+              __ Drotr32(dst, lhs, shift_value);
+            }
+          }
+        }
+      } else {
+        if (type == DataType::Type::kInt32) {
+          if (instr->IsShl()) {
+            __ Sllv(dst, lhs, rhs_reg);
+          } else if (instr->IsShr()) {
+            __ Srav(dst, lhs, rhs_reg);
+          } else if (instr->IsUShr()) {
+            __ Srlv(dst, lhs, rhs_reg);
+          } else {
+            __ Rotrv(dst, lhs, rhs_reg);
+          }
+        } else {
+          if (instr->IsShl()) {
+            __ Dsllv(dst, lhs, rhs_reg);
+          } else if (instr->IsShr()) {
+            __ Dsrav(dst, lhs, rhs_reg);
+          } else if (instr->IsUShr()) {
+            __ Dsrlv(dst, lhs, rhs_reg);
+          } else {
+            __ Drotrv(dst, lhs, rhs_reg);
+          }
+        }
+      }
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected shift operation type " << type;
+  }
+}
+
+void LocationsBuilderRISCV64::VisitAdd(HAdd* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitAdd(HAdd* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitAnd(HAnd* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitAnd(HAnd* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitArrayGet(HArrayGet* instruction) {
+  DataType::Type type = instruction->GetType();
+  bool object_array_get_with_read_barrier =
+      kEmitCompilerReadBarrier && (type == DataType::Type::kReference);
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(instruction,
+                                                       object_array_get_with_read_barrier
+                                                           ? LocationSummary::kCallOnSlowPath
+                                                           : LocationSummary::kNoCall);
+  if (object_array_get_with_read_barrier && kUseBakerReadBarrier) {
+    locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
+  }
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RegisterOrConstant(instruction->InputAt(1)));
+  if (DataType::IsFloatingPointType(type)) {
+    locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+  } else {
+    // The output overlaps in the case of an object array get with
+    // read barriers enabled: we do not want the move to overwrite the
+    // array's location, as we need it to emit the read barrier.
+    locations->SetOut(Location::RequiresRegister(),
+                      object_array_get_with_read_barrier
+                          ? Location::kOutputOverlap
+                          : Location::kNoOutputOverlap);
+  }
+  // We need a temporary register for the read barrier marking slow
+  // path in CodeGeneratorRISCV64::GenerateArrayLoadWithBakerReadBarrier.
+  if (object_array_get_with_read_barrier && kUseBakerReadBarrier) {
+    bool temp_needed = instruction->GetIndex()->IsConstant()
+        ? !kBakerReadBarrierThunksEnableForFields
+        : !kBakerReadBarrierThunksEnableForArrays;
+    if (temp_needed) {
+      locations->AddTemp(Location::RequiresRegister());
+    }
+  }
+}
+
+static auto GetImplicitNullChecker(HInstruction* instruction, CodeGeneratorRISCV64* codegen) {
+  auto null_checker = [codegen, instruction]() {
+    codegen->MaybeRecordImplicitNullCheck(instruction);
+  };
+  return null_checker;
+}
+
+void InstructionCodeGeneratorRISCV64::VisitArrayGet(HArrayGet* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  Location obj_loc = locations->InAt(0);
+  GpuRegister obj = obj_loc.AsRegister<GpuRegister>();
+  Location out_loc = locations->Out();
+  Location index = locations->InAt(1);
+  uint32_t data_offset = CodeGenerator::GetArrayDataOffset(instruction);
+  auto null_checker = GetImplicitNullChecker(instruction, codegen_);
+
+  DataType::Type type = instruction->GetType();
+  const bool maybe_compressed_char_at = mirror::kUseStringCompression &&
+                                        instruction->IsStringCharAt();
+  switch (type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8: {
+      GpuRegister out = out_loc.AsRegister<GpuRegister>();
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_1) + data_offset;
+        __ LoadFromOffset(kLoadUnsignedByte, out, obj, offset, null_checker);
+      } else {
+        __ Daddu(TMP, obj, index.AsRegister<GpuRegister>());
+        __ LoadFromOffset(kLoadUnsignedByte, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kInt8: {
+      GpuRegister out = out_loc.AsRegister<GpuRegister>();
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_1) + data_offset;
+        __ LoadFromOffset(kLoadSignedByte, out, obj, offset, null_checker);
+      } else {
+        __ Daddu(TMP, obj, index.AsRegister<GpuRegister>());
+        __ LoadFromOffset(kLoadSignedByte, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kUint16: {
+      GpuRegister out = out_loc.AsRegister<GpuRegister>();
+      if (maybe_compressed_char_at) {
+        uint32_t count_offset = mirror::String::CountOffset().Uint32Value();
+        __ LoadFromOffset(kLoadWord, TMP, obj, count_offset, null_checker);
+        __ Dext(TMP, TMP, 0, 1);
+        static_assert(static_cast<uint32_t>(mirror::StringCompressionFlag::kCompressed) == 0u,
+                      "Expecting 0=compressed, 1=uncompressed");
+      }
+      if (index.IsConstant()) {
+        int32_t const_index = index.GetConstant()->AsIntConstant()->GetValue();
+        if (maybe_compressed_char_at) {
+          Riscv64Label uncompressed_load, done;
+          __ Bnezc(TMP, &uncompressed_load);
+          __ LoadFromOffset(kLoadUnsignedByte,
+                            out,
+                            obj,
+                            data_offset + (const_index << TIMES_1));
+          __ Bc(&done);
+          __ Bind(&uncompressed_load);
+          __ LoadFromOffset(kLoadUnsignedHalfword,
+                            out,
+                            obj,
+                            data_offset + (const_index << TIMES_2));
+          __ Bind(&done);
+        } else {
+          __ LoadFromOffset(kLoadUnsignedHalfword,
+                            out,
+                            obj,
+                            data_offset + (const_index << TIMES_2),
+                            null_checker);
+        }
+      } else {
+        GpuRegister index_reg = index.AsRegister<GpuRegister>();
+        if (maybe_compressed_char_at) {
+          Riscv64Label uncompressed_load, done;
+          __ Bnezc(TMP, &uncompressed_load);
+          __ Daddu(TMP, obj, index_reg);
+          __ LoadFromOffset(kLoadUnsignedByte, out, TMP, data_offset);
+          __ Bc(&done);
+          __ Bind(&uncompressed_load);
+          __ Dlsa(TMP, index_reg, obj, TIMES_2);
+          __ LoadFromOffset(kLoadUnsignedHalfword, out, TMP, data_offset);
+          __ Bind(&done);
+        } else {
+          __ Dlsa(TMP, index_reg, obj, TIMES_2);
+          __ LoadFromOffset(kLoadUnsignedHalfword, out, TMP, data_offset, null_checker);
+        }
+      }
+      break;
+    }
+
+    case DataType::Type::kInt16: {
+      GpuRegister out = out_loc.AsRegister<GpuRegister>();
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_2) + data_offset;
+        __ LoadFromOffset(kLoadSignedHalfword, out, obj, offset, null_checker);
+      } else {
+        __ Dlsa(TMP, index.AsRegister<GpuRegister>(), obj, TIMES_2);
+        __ LoadFromOffset(kLoadSignedHalfword, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kInt32: {
+      DCHECK_EQ(sizeof(mirror::HeapReference<mirror::Object>), sizeof(int32_t));
+      GpuRegister out = out_loc.AsRegister<GpuRegister>();
+      LoadOperandType load_type =
+          (type == DataType::Type::kReference) ? kLoadUnsignedWord : kLoadWord;
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4) + data_offset;
+        __ LoadFromOffset(load_type, out, obj, offset, null_checker);
+      } else {
+        __ Dlsa(TMP, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+        __ LoadFromOffset(load_type, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kReference: {
+      static_assert(
+          sizeof(mirror::HeapReference<mirror::Object>) == sizeof(int32_t),
+          "art::mirror::HeapReference<art::mirror::Object> and int32_t have different sizes.");
+      // /* HeapReference<Object> */ out =
+      //     *(obj + data_offset + index * sizeof(HeapReference<Object>))
+      if (kEmitCompilerReadBarrier && kUseBakerReadBarrier) {
+        bool temp_needed = index.IsConstant()
+            ? !kBakerReadBarrierThunksEnableForFields
+            : !kBakerReadBarrierThunksEnableForArrays;
+        Location temp = temp_needed ? locations->GetTemp(0) : Location::NoLocation();
+        // Note that a potential implicit null check is handled in this
+        // CodeGeneratorRISCV64::GenerateArrayLoadWithBakerReadBarrier call.
+        DCHECK(!instruction->CanDoImplicitNullCheckOn(instruction->InputAt(0)));
+        if (index.IsConstant()) {
+          // Array load with a constant index can be treated as a field load.
+          size_t offset =
+              (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4) + data_offset;
+          codegen_->GenerateFieldLoadWithBakerReadBarrier(instruction,
+                                                          out_loc,
+                                                          obj,
+                                                          offset,
+                                                          temp,
+                                                          /* needs_null_check= */ false);
+        } else {
+          codegen_->GenerateArrayLoadWithBakerReadBarrier(instruction,
+                                                          out_loc,
+                                                          obj,
+                                                          data_offset,
+                                                          index,
+                                                          temp,
+                                                          /* needs_null_check= */ false);
+        }
+      } else {
+        GpuRegister out = out_loc.AsRegister<GpuRegister>();
+        if (index.IsConstant()) {
+          size_t offset =
+              (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4) + data_offset;
+          __ LoadFromOffset(kLoadUnsignedWord, out, obj, offset, null_checker);
+          // If read barriers are enabled, emit read barriers other than
+          // Baker's using a slow path (and also unpoison the loaded
+          // reference, if heap poisoning is enabled).
+          codegen_->MaybeGenerateReadBarrierSlow(instruction, out_loc, out_loc, obj_loc, offset);
+        } else {
+          __ Dlsa(TMP, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+          __ LoadFromOffset(kLoadUnsignedWord, out, TMP, data_offset, null_checker);
+          // If read barriers are enabled, emit read barriers other than
+          // Baker's using a slow path (and also unpoison the loaded
+          // reference, if heap poisoning is enabled).
+          codegen_->MaybeGenerateReadBarrierSlow(instruction,
+                                                 out_loc,
+                                                 out_loc,
+                                                 obj_loc,
+                                                 data_offset,
+                                                 index);
+        }
+      }
+      break;
+    }
+
+    case DataType::Type::kInt64: {
+      GpuRegister out = out_loc.AsRegister<GpuRegister>();
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_8) + data_offset;
+        __ LoadFromOffset(kLoadDoubleword, out, obj, offset, null_checker);
+      } else {
+        __ Dlsa(TMP, index.AsRegister<GpuRegister>(), obj, TIMES_8);
+        __ LoadFromOffset(kLoadDoubleword, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kFloat32: {
+      FpuRegister out = out_loc.AsFpuRegister<FpuRegister>();
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4) + data_offset;
+        __ LoadFpuFromOffset(kLoadWord, out, obj, offset, null_checker);
+      } else {
+        __ Dlsa(TMP, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+        __ LoadFpuFromOffset(kLoadWord, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kFloat64: {
+      FpuRegister out = out_loc.AsFpuRegister<FpuRegister>();
+      if (index.IsConstant()) {
+        size_t offset =
+            (index.GetConstant()->AsIntConstant()->GetValue() << TIMES_8) + data_offset;
+        __ LoadFpuFromOffset(kLoadDoubleword, out, obj, offset, null_checker);
+      } else {
+        __ Dlsa(TMP, index.AsRegister<GpuRegister>(), obj, TIMES_8);
+        __ LoadFpuFromOffset(kLoadDoubleword, out, TMP, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kUint32:
+    case DataType::Type::kUint64:
+    case DataType::Type::kVoid:
+      LOG(FATAL) << "Unreachable type " << instruction->GetType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitArrayLength(HArrayLength* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitArrayLength(HArrayLength* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  uint32_t offset = CodeGenerator::GetArrayLengthOffset(instruction);
+  GpuRegister obj = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  __ LoadFromOffset(kLoadWord, out, obj, offset);
+  codegen_->MaybeRecordImplicitNullCheck(instruction);
+  // Mask out compression flag from String's array length.
+  if (mirror::kUseStringCompression && instruction->IsStringLength()) {
+    __ Srl(out, out, 1u);
+  }
+}
+
+Location LocationsBuilderRISCV64::RegisterOrZeroConstant(HInstruction* instruction) {
+  return (instruction->IsConstant() && instruction->AsConstant()->IsZeroBitPattern())
+      ? Location::ConstantLocation(instruction->AsConstant())
+      : Location::RequiresRegister();
+}
+
+Location LocationsBuilderRISCV64::FpuRegisterOrConstantForStore(HInstruction* instruction) {
+  // We can store 0.0 directly (from the ZERO register) without loading it into an FPU register.
+  // We can store a non-zero float or double constant without first loading it into the FPU,
+  // but we should only prefer this if the constant has a single use.
+  if (instruction->IsConstant() &&
+      (instruction->AsConstant()->IsZeroBitPattern() ||
+       instruction->GetUses().HasExactlyOneElement())) {
+    return Location::ConstantLocation(instruction->AsConstant());
+    // Otherwise fall through and require an FPU register for the constant.
+  }
+  return Location::RequiresFpuRegister();
+}
+
+void LocationsBuilderRISCV64::VisitArraySet(HArraySet* instruction) {
+  DataType::Type value_type = instruction->GetComponentType();
+
+  bool needs_write_barrier =
+      CodeGenerator::StoreNeedsWriteBarrier(value_type, instruction->GetValue());
+  bool may_need_runtime_call_for_type_check = instruction->NeedsTypeCheck();
+
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction,
+      may_need_runtime_call_for_type_check ?
+          LocationSummary::kCallOnSlowPath :
+          LocationSummary::kNoCall);
+
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RegisterOrConstant(instruction->InputAt(1)));
+  if (DataType::IsFloatingPointType(instruction->InputAt(2)->GetType())) {
+    locations->SetInAt(2, FpuRegisterOrConstantForStore(instruction->InputAt(2)));
+  } else {
+    locations->SetInAt(2, RegisterOrZeroConstant(instruction->InputAt(2)));
+  }
+  if (needs_write_barrier) {
+    // Temporary register for the write barrier.
+    locations->AddTemp(Location::RequiresRegister());  // Possibly used for ref. poisoning too.
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitArraySet(HArraySet* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  GpuRegister obj = locations->InAt(0).AsRegister<GpuRegister>();
+  Location index = locations->InAt(1);
+  Location value_location = locations->InAt(2);
+  DataType::Type value_type = instruction->GetComponentType();
+  bool may_need_runtime_call_for_type_check = instruction->NeedsTypeCheck();
+  bool needs_write_barrier =
+      CodeGenerator::StoreNeedsWriteBarrier(value_type, instruction->GetValue());
+  auto null_checker = GetImplicitNullChecker(instruction, codegen_);
+  GpuRegister base_reg = index.IsConstant() ? obj : TMP;
+
+  switch (value_type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8: {
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(uint8_t)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_1;
+      } else {
+        __ Daddu(base_reg, obj, index.AsRegister<GpuRegister>());
+      }
+      if (value_location.IsConstant()) {
+        int32_t value = CodeGenerator::GetInt32ValueOf(value_location.GetConstant());
+        __ StoreConstToOffset(kStoreByte, value, base_reg, data_offset, TMP, null_checker);
+      } else {
+        GpuRegister value = value_location.AsRegister<GpuRegister>();
+        __ StoreToOffset(kStoreByte, value, base_reg, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16: {
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(uint16_t)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_2;
+      } else {
+        __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_2);
+      }
+      if (value_location.IsConstant()) {
+        int32_t value = CodeGenerator::GetInt32ValueOf(value_location.GetConstant());
+        __ StoreConstToOffset(kStoreHalfword, value, base_reg, data_offset, TMP, null_checker);
+      } else {
+        GpuRegister value = value_location.AsRegister<GpuRegister>();
+        __ StoreToOffset(kStoreHalfword, value, base_reg, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kInt32: {
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(int32_t)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4;
+      } else {
+        __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+      }
+      if (value_location.IsConstant()) {
+        int32_t value = CodeGenerator::GetInt32ValueOf(value_location.GetConstant());
+        __ StoreConstToOffset(kStoreWord, value, base_reg, data_offset, TMP, null_checker);
+      } else {
+        GpuRegister value = value_location.AsRegister<GpuRegister>();
+        __ StoreToOffset(kStoreWord, value, base_reg, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kReference: {
+      if (value_location.IsConstant()) {
+        // Just setting null.
+        uint32_t data_offset = mirror::Array::DataOffset(sizeof(int32_t)).Uint32Value();
+        if (index.IsConstant()) {
+          data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4;
+        } else {
+          __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+        }
+        int32_t value = CodeGenerator::GetInt32ValueOf(value_location.GetConstant());
+        DCHECK_EQ(value, 0);
+        __ StoreConstToOffset(kStoreWord, value, base_reg, data_offset, TMP, null_checker);
+        DCHECK(!needs_write_barrier);
+        DCHECK(!may_need_runtime_call_for_type_check);
+        break;
+      }
+
+      DCHECK(needs_write_barrier);
+      GpuRegister value = value_location.AsRegister<GpuRegister>();
+      GpuRegister temp1 = locations->GetTemp(0).AsRegister<GpuRegister>();
+      GpuRegister temp2 = TMP;  // Doesn't need to survive slow path.
+      uint32_t class_offset = mirror::Object::ClassOffset().Int32Value();
+      uint32_t super_offset = mirror::Class::SuperClassOffset().Int32Value();
+      uint32_t component_offset = mirror::Class::ComponentTypeOffset().Int32Value();
+      Riscv64Label done;
+      SlowPathCodeRISCV64* slow_path = nullptr;
+
+      if (may_need_runtime_call_for_type_check) {
+        slow_path = new (codegen_->GetScopedAllocator()) ArraySetSlowPathRISCV64(instruction);
+        codegen_->AddSlowPath(slow_path);
+        if (instruction->GetValueCanBeNull()) {
+          Riscv64Label non_zero;
+          __ Bnezc(value, &non_zero);
+          uint32_t data_offset = mirror::Array::DataOffset(sizeof(int32_t)).Uint32Value();
+          if (index.IsConstant()) {
+            data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4;
+          } else {
+            __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+          }
+          __ StoreToOffset(kStoreWord, value, base_reg, data_offset, null_checker);
+          __ Bc(&done);
+          __ Bind(&non_zero);
+        }
+
+        // Note that when read barriers are enabled, the type checks
+        // are performed without read barriers.  This is fine, even in
+        // the case where a class object is in the from-space after
+        // the flip, as a comparison involving such a type would not
+        // produce a false positive; it may of course produce a false
+        // negative, in which case we would take the ArraySet slow
+        // path.
+
+        // /* HeapReference<Class> */ temp1 = obj->klass_
+        __ LoadFromOffset(kLoadUnsignedWord, temp1, obj, class_offset, null_checker);
+        __ MaybeUnpoisonHeapReference(temp1);
+
+        // /* HeapReference<Class> */ temp1 = temp1->component_type_
+        __ LoadFromOffset(kLoadUnsignedWord, temp1, temp1, component_offset);
+        // /* HeapReference<Class> */ temp2 = value->klass_
+        __ LoadFromOffset(kLoadUnsignedWord, temp2, value, class_offset);
+        // If heap poisoning is enabled, no need to unpoison `temp1`
+        // nor `temp2`, as we are comparing two poisoned references.
+
+        if (instruction->StaticTypeOfArrayIsObjectArray()) {
+          Riscv64Label do_put;
+          __ Beqc(temp1, temp2, &do_put);
+          // If heap poisoning is enabled, the `temp1` reference has
+          // not been unpoisoned yet; unpoison it now.
+          __ MaybeUnpoisonHeapReference(temp1);
+
+          // /* HeapReference<Class> */ temp1 = temp1->super_class_
+          __ LoadFromOffset(kLoadUnsignedWord, temp1, temp1, super_offset);
+          // If heap poisoning is enabled, no need to unpoison
+          // `temp1`, as we are comparing against null below.
+          __ Bnezc(temp1, slow_path->GetEntryLabel());
+          __ Bind(&do_put);
+        } else {
+          __ Bnec(temp1, temp2, slow_path->GetEntryLabel());
+        }
+      }
+
+      GpuRegister source = value;
+      if (kPoisonHeapReferences) {
+        // Note that in the case where `value` is a null reference,
+        // we do not enter this block, as a null reference does not
+        // need poisoning.
+        __ Move(temp1, value);
+        __ PoisonHeapReference(temp1);
+        source = temp1;
+      }
+
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(int32_t)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4;
+      } else {
+        __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+      }
+      __ StoreToOffset(kStoreWord, source, base_reg, data_offset);
+
+      if (!may_need_runtime_call_for_type_check) {
+        codegen_->MaybeRecordImplicitNullCheck(instruction);
+      }
+
+      codegen_->MarkGCCard(obj, value, instruction->GetValueCanBeNull());
+
+      if (done.IsLinked()) {
+        __ Bind(&done);
+      }
+
+      if (slow_path != nullptr) {
+        __ Bind(slow_path->GetExitLabel());
+      }
+      break;
+    }
+
+    case DataType::Type::kInt64: {
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(int64_t)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_8;
+      } else {
+        __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_8);
+      }
+      if (value_location.IsConstant()) {
+        int64_t value = CodeGenerator::GetInt64ValueOf(value_location.GetConstant());
+        __ StoreConstToOffset(kStoreDoubleword, value, base_reg, data_offset, TMP, null_checker);
+      } else {
+        GpuRegister value = value_location.AsRegister<GpuRegister>();
+        __ StoreToOffset(kStoreDoubleword, value, base_reg, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kFloat32: {
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(float)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_4;
+      } else {
+        __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_4);
+      }
+      if (value_location.IsConstant()) {
+        int32_t value = CodeGenerator::GetInt32ValueOf(value_location.GetConstant());
+        __ StoreConstToOffset(kStoreWord, value, base_reg, data_offset, TMP, null_checker);
+      } else {
+        FpuRegister value = value_location.AsFpuRegister<FpuRegister>();
+        __ StoreFpuToOffset(kStoreWord, value, base_reg, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kFloat64: {
+      uint32_t data_offset = mirror::Array::DataOffset(sizeof(double)).Uint32Value();
+      if (index.IsConstant()) {
+        data_offset += index.GetConstant()->AsIntConstant()->GetValue() << TIMES_8;
+      } else {
+        __ Dlsa(base_reg, index.AsRegister<GpuRegister>(), obj, TIMES_8);
+      }
+      if (value_location.IsConstant()) {
+        int64_t value = CodeGenerator::GetInt64ValueOf(value_location.GetConstant());
+        __ StoreConstToOffset(kStoreDoubleword, value, base_reg, data_offset, TMP, null_checker);
+      } else {
+        FpuRegister value = value_location.AsFpuRegister<FpuRegister>();
+        __ StoreFpuToOffset(kStoreDoubleword, value, base_reg, data_offset, null_checker);
+      }
+      break;
+    }
+
+    case DataType::Type::kUint32:
+    case DataType::Type::kUint64:
+    case DataType::Type::kVoid:
+      LOG(FATAL) << "Unreachable type " << instruction->GetType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitBoundsCheck(HBoundsCheck* instruction) {
+  RegisterSet caller_saves = RegisterSet::Empty();
+  InvokeRuntimeCallingConvention calling_convention;
+  caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+  LocationSummary* locations = codegen_->CreateThrowingSlowPathLocations(instruction, caller_saves);
+
+  HInstruction* index = instruction->InputAt(0);
+  HInstruction* length = instruction->InputAt(1);
+
+  bool const_index = false;
+  bool const_length = false;
+
+  if (index->IsConstant()) {
+    if (length->IsConstant()) {
+      const_index = true;
+      const_length = true;
+    } else {
+      int32_t index_value = index->AsIntConstant()->GetValue();
+      if (index_value < 0 || IsInt<12>(index_value + 1)) {
+        const_index = true;
+      }
+    }
+  } else if (length->IsConstant()) {
+    int32_t length_value = length->AsIntConstant()->GetValue();
+    if (IsUint<11>(length_value)) {
+      const_length = true;
+    }
+  }
+
+  locations->SetInAt(0, const_index
+      ? Location::ConstantLocation(index->AsConstant())
+      : Location::RequiresRegister());
+  locations->SetInAt(1, const_length
+      ? Location::ConstantLocation(length->AsConstant())
+      : Location::RequiresRegister());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitBoundsCheck(HBoundsCheck* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  Location index_loc = locations->InAt(0);
+  Location length_loc = locations->InAt(1);
+
+  if (length_loc.IsConstant()) {
+    int32_t length = length_loc.GetConstant()->AsIntConstant()->GetValue();
+    if (index_loc.IsConstant()) {
+      int32_t index = index_loc.GetConstant()->AsIntConstant()->GetValue();
+      if (index < 0 || index >= length) {
+        BoundsCheckSlowPathRISCV64* slow_path =
+            new (codegen_->GetScopedAllocator()) BoundsCheckSlowPathRISCV64(instruction);
+        codegen_->AddSlowPath(slow_path);
+        __ Bc(slow_path->GetEntryLabel());
+      } else {
+        // Nothing to be done.
+      }
+      return;
+    }
+
+    BoundsCheckSlowPathRISCV64* slow_path =
+        new (codegen_->GetScopedAllocator()) BoundsCheckSlowPathRISCV64(instruction);
+    codegen_->AddSlowPath(slow_path);
+    GpuRegister index = index_loc.AsRegister<GpuRegister>();
+    if (length == 0) {
+      __ Bc(slow_path->GetEntryLabel());
+    } else if (length == 1) {
+      __ Bnezc(index, slow_path->GetEntryLabel());
+    } else {
+      DCHECK(IsUint<11>(length)) << length;
+      __ Sltiu(TMP, index, length);
+      __ Beqzc(TMP, slow_path->GetEntryLabel());
+    }
+  } else {
+    GpuRegister length = length_loc.AsRegister<GpuRegister>();
+    BoundsCheckSlowPathRISCV64* slow_path =
+        new (codegen_->GetScopedAllocator()) BoundsCheckSlowPathRISCV64(instruction);
+    codegen_->AddSlowPath(slow_path);
+    if (index_loc.IsConstant()) {
+      int32_t index = index_loc.GetConstant()->AsIntConstant()->GetValue();
+      if (index < 0) {
+        __ Bc(slow_path->GetEntryLabel());
+      } else if (index == 0) {
+        __ Blezc(length, slow_path->GetEntryLabel());
+      } else {
+        DCHECK(IsInt<12>(index + 1)) << index;
+        __ Sltiu(TMP, length, index + 1);
+        __ Bnezc(TMP, slow_path->GetEntryLabel());
+      }
+    } else {
+      GpuRegister index = index_loc.AsRegister<GpuRegister>();
+      __ Bgeuc(index, length, slow_path->GetEntryLabel());
+    }
+  }
+}
+
+// Temp is used for read barrier.
+static size_t NumberOfInstanceOfTemps(TypeCheckKind type_check_kind) {
+  if (kEmitCompilerReadBarrier &&
+      !(kUseBakerReadBarrier && kBakerReadBarrierThunksEnableForFields) &&
+      (kUseBakerReadBarrier ||
+       type_check_kind == TypeCheckKind::kAbstractClassCheck ||
+       type_check_kind == TypeCheckKind::kClassHierarchyCheck ||
+       type_check_kind == TypeCheckKind::kArrayObjectCheck)) {
+    return 1;
+  }
+  return 0;
+}
+
+// Extra temp is used for read barrier.
+static size_t NumberOfCheckCastTemps(TypeCheckKind type_check_kind) {
+  return 1 + NumberOfInstanceOfTemps(type_check_kind);
+}
+
+void LocationsBuilderRISCV64::VisitCheckCast(HCheckCast* instruction) {
+  TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
+  LocationSummary::CallKind call_kind = CodeGenerator::GetCheckCastCallKind(instruction);
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  locations->SetInAt(0, Location::RequiresRegister());
+  if (type_check_kind == TypeCheckKind::kBitstringCheck) {
+    locations->SetInAt(1, Location::ConstantLocation(instruction->InputAt(1)->AsConstant()));
+    locations->SetInAt(2, Location::ConstantLocation(instruction->InputAt(2)->AsConstant()));
+    locations->SetInAt(3, Location::ConstantLocation(instruction->InputAt(3)->AsConstant()));
+  } else {
+    locations->SetInAt(1, Location::RequiresRegister());
+  }
+  locations->AddRegisterTemps(NumberOfCheckCastTemps(type_check_kind));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitCheckCast(HCheckCast* instruction) {
+  TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
+  LocationSummary* locations = instruction->GetLocations();
+  Location obj_loc = locations->InAt(0);
+  GpuRegister obj = obj_loc.AsRegister<GpuRegister>();
+  Location cls = locations->InAt(1);
+  Location temp_loc = locations->GetTemp(0);
+  GpuRegister temp = temp_loc.AsRegister<GpuRegister>();
+  const size_t num_temps = NumberOfCheckCastTemps(type_check_kind);
+  DCHECK_LE(num_temps, 2u);
+  Location maybe_temp2_loc = (num_temps >= 2) ? locations->GetTemp(1) : Location::NoLocation();
+  const uint32_t class_offset = mirror::Object::ClassOffset().Int32Value();
+  const uint32_t super_offset = mirror::Class::SuperClassOffset().Int32Value();
+  const uint32_t component_offset = mirror::Class::ComponentTypeOffset().Int32Value();
+  const uint32_t primitive_offset = mirror::Class::PrimitiveTypeOffset().Int32Value();
+  const uint32_t iftable_offset = mirror::Class::IfTableOffset().Uint32Value();
+  const uint32_t array_length_offset = mirror::Array::LengthOffset().Uint32Value();
+  const uint32_t object_array_data_offset =
+      mirror::Array::DataOffset(kHeapReferenceSize).Uint32Value();
+  Riscv64Label done;
+
+  bool is_type_check_slow_path_fatal = CodeGenerator::IsTypeCheckSlowPathFatal(instruction);
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) TypeCheckSlowPathRISCV64(
+          instruction, is_type_check_slow_path_fatal);
+  codegen_->AddSlowPath(slow_path);
+
+  // Avoid this check if we know `obj` is not null.
+  if (instruction->MustDoNullCheck()) {
+    __ Beqzc(obj, &done);
+  }
+
+  switch (type_check_kind) {
+    case TypeCheckKind::kExactCheck:
+    case TypeCheckKind::kArrayCheck: {
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+      // Jump to slow path for throwing the exception or doing a
+      // more involved array check.
+      __ Bnec(temp, cls.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+      break;
+    }
+
+    case TypeCheckKind::kAbstractClassCheck: {
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+      // If the class is abstract, we eagerly fetch the super class of the
+      // object to avoid doing a comparison we know will fail.
+      Riscv64Label loop;
+      __ Bind(&loop);
+      // /* HeapReference<Class> */ temp = temp->super_class_
+      GenerateReferenceLoadOneRegister(instruction,
+                                       temp_loc,
+                                       super_offset,
+                                       maybe_temp2_loc,
+                                       kWithoutReadBarrier);
+      // If the class reference currently in `temp` is null, jump to the slow path to throw the
+      // exception.
+      __ Beqzc(temp, slow_path->GetEntryLabel());
+      // Otherwise, compare the classes.
+      __ Bnec(temp, cls.AsRegister<GpuRegister>(), &loop);
+      break;
+    }
+
+    case TypeCheckKind::kClassHierarchyCheck: {
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+      // Walk over the class hierarchy to find a match.
+      Riscv64Label loop;
+      __ Bind(&loop);
+      __ Beqc(temp, cls.AsRegister<GpuRegister>(), &done);
+      // /* HeapReference<Class> */ temp = temp->super_class_
+      GenerateReferenceLoadOneRegister(instruction,
+                                       temp_loc,
+                                       super_offset,
+                                       maybe_temp2_loc,
+                                       kWithoutReadBarrier);
+      // If the class reference currently in `temp` is null, jump to the slow path to throw the
+      // exception. Otherwise, jump to the beginning of the loop.
+      __ Bnezc(temp, &loop);
+      __ Bc(slow_path->GetEntryLabel());
+      break;
+    }
+
+    case TypeCheckKind::kArrayObjectCheck: {
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+      // Do an exact check.
+      __ Beqc(temp, cls.AsRegister<GpuRegister>(), &done);
+      // Otherwise, we need to check that the object's class is a non-primitive array.
+      // /* HeapReference<Class> */ temp = temp->component_type_
+      GenerateReferenceLoadOneRegister(instruction,
+                                       temp_loc,
+                                       component_offset,
+                                       maybe_temp2_loc,
+                                       kWithoutReadBarrier);
+      // If the component type is null, jump to the slow path to throw the exception.
+      __ Beqzc(temp, slow_path->GetEntryLabel());
+      // Otherwise, the object is indeed an array, further check that this component
+      // type is not a primitive type.
+      __ LoadFromOffset(kLoadUnsignedHalfword, temp, temp, primitive_offset);
+      static_assert(Primitive::kPrimNot == 0, "Expected 0 for kPrimNot");
+      __ Bnezc(temp, slow_path->GetEntryLabel());
+      break;
+    }
+
+    case TypeCheckKind::kUnresolvedCheck:
+      // We always go into the type check slow path for the unresolved check case.
+      // We cannot directly call the CheckCast runtime entry point
+      // without resorting to a type checking slow path here (i.e. by
+      // calling InvokeRuntime directly), as it would require to
+      // assign fixed registers for the inputs of this HInstanceOf
+      // instruction (following the runtime calling convention), which
+      // might be cluttered by the potential first read barrier
+      // emission at the beginning of this method.
+      __ Bc(slow_path->GetEntryLabel());
+      break;
+
+    case TypeCheckKind::kInterfaceCheck: {
+      // Avoid read barriers to improve performance of the fast path. We can not get false
+      // positives by doing this.
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+      // /* HeapReference<Class> */ temp = temp->iftable_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        temp_loc,
+                                        iftable_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+      // Iftable is never null.
+      __ Lw(TMP, temp, array_length_offset);
+      // Loop through the iftable and check if any class matches.
+      Riscv64Label loop;
+      __ Bind(&loop);
+      __ Beqzc(TMP, slow_path->GetEntryLabel());
+      __ Lwu(AT, temp, object_array_data_offset);
+      __ MaybeUnpoisonHeapReference(AT);
+      // Go to next interface.
+      __ Daddiu(temp, temp, 2 * kHeapReferenceSize);
+      __ Addiu(TMP, TMP, -2);
+      // Compare the classes and continue the loop if they do not match.
+      __ Bnec(AT, cls.AsRegister<GpuRegister>(), &loop);
+      break;
+    }
+
+    case TypeCheckKind::kBitstringCheck: {
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        temp_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp2_loc,
+                                        kWithoutReadBarrier);
+
+      GenerateBitstringTypeCheckCompare(instruction, temp);
+      __ Bnezc(temp, slow_path->GetEntryLabel());
+      break;
+    }
+  }
+
+  __ Bind(&done);
+  __ Bind(slow_path->GetExitLabel());
+}
+
+void LocationsBuilderRISCV64::VisitClinitCheck(HClinitCheck* check) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(check, LocationSummary::kCallOnSlowPath);
+  locations->SetInAt(0, Location::RequiresRegister());
+  if (check->HasUses()) {
+    locations->SetOut(Location::SameAsFirstInput());
+  }
+  // Rely on the type initialization to save everything we need.
+  locations->SetCustomSlowPathCallerSaves(OneRegInReferenceOutSaveEverythingCallerSaves());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitClinitCheck(HClinitCheck* check) {
+  // We assume the class is not null.
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) LoadClassSlowPathRISCV64(check->GetLoadClass(), check);
+  codegen_->AddSlowPath(slow_path);
+  GenerateClassInitializationCheck(slow_path,
+                                   check->GetLocations()->InAt(0).AsRegister<GpuRegister>());
+}
+
+void LocationsBuilderRISCV64::VisitCompare(HCompare* compare) {
+  DataType::Type in_type = compare->InputAt(0)->GetType();
+
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(compare);
+
+  switch (in_type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RegisterOrConstant(compare->InputAt(1)));
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+
+    default:
+      LOG(FATAL) << "Unexpected type for compare operation " << in_type;
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitCompare(HCompare* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  GpuRegister res = locations->Out().AsRegister<GpuRegister>();
+  DataType::Type in_type = instruction->InputAt(0)->GetType();
+
+  //  0 if: left == right
+  //  1 if: left  > right
+  // -1 if: left  < right
+  switch (in_type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+      Location rhs_location = locations->InAt(1);
+      bool use_imm = rhs_location.IsConstant();
+      GpuRegister rhs = ZERO;
+      if (use_imm) {
+        if (in_type == DataType::Type::kInt64) {
+          int64_t value = CodeGenerator::GetInt64ValueOf(rhs_location.GetConstant()->AsConstant());
+          if (value != 0) {
+            rhs = AT;
+            __ LoadConst64(rhs, value);
+          }
+        } else {
+          int32_t value = CodeGenerator::GetInt32ValueOf(rhs_location.GetConstant()->AsConstant());
+          if (value != 0) {
+            rhs = AT;
+            __ LoadConst32(rhs, value);
+          }
+        }
+      } else {
+        rhs = rhs_location.AsRegister<GpuRegister>();
+      }
+      __ Slt(TMP, lhs, rhs);
+      __ Slt(res, rhs, lhs);
+      __ Subu(res, res, TMP);
+      break;
+    }
+
+    case DataType::Type::kFloat32: {
+      FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+      Riscv64Label done;
+      __ CmpEqS(TMP, lhs, rhs);
+      __ LoadConst32(res, 0);
+      __ Bnez(TMP, &done);
+      if (instruction->IsGtBias()) {
+        __ CmpLtS(TMP, lhs, rhs);
+        __ LoadConst32(res, -1);
+        __ Bnez(TMP, &done);
+        __ LoadConst32(res, 1);
+      } else {
+        __ CmpLtS(TMP, rhs, lhs);
+        __ LoadConst32(res, 1);
+        __ Bnez(TMP, &done);
+        __ LoadConst32(res, -1);
+      }
+      __ Bind(&done);
+      break;
+    }
+
+    case DataType::Type::kFloat64: {
+      FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+      Riscv64Label done;
+      __ CmpEqD(TMP, lhs, rhs);
+      __ LoadConst32(res, 0);
+      __ Bnez(TMP, &done);
+      if (instruction->IsGtBias()) {
+        __ CmpLtD(TMP, lhs, rhs);
+        __ LoadConst32(res, -1);
+        __ Bnez(TMP, &done);
+        __ LoadConst32(res, 1);
+      } else {
+        __ CmpLtD(TMP, rhs, lhs);
+        __ LoadConst32(res, 1);
+        __ Bnez(TMP, &done);
+        __ LoadConst32(res, -1);
+      }
+      __ Bind(&done);
+      break;
+    }
+
+    default:
+      LOG(FATAL) << "Unimplemented compare type " << in_type;
+  }
+}
+
+void LocationsBuilderRISCV64::HandleCondition(HCondition* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  switch (instruction->InputAt(0)->GetType()) {
+    default:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RegisterOrConstant(instruction->InputAt(1)));
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      break;
+  }
+  if (!instruction->IsEmittedAtUseSite()) {
+    locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::HandleCondition(HCondition* instruction) {
+  if (instruction->IsEmittedAtUseSite()) {
+    return;
+  }
+
+  DataType::Type type = instruction->InputAt(0)->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+  switch (type) {
+    default:
+      // Integer case.
+      GenerateIntLongCompare(instruction->GetCondition(), /* is64bit= */ false, locations);
+      return;
+    case DataType::Type::kInt64:
+      GenerateIntLongCompare(instruction->GetCondition(), /* is64bit= */ true, locations);
+      return;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      GenerateFpCompare(instruction->GetCondition(), instruction->IsGtBias(), type, locations);
+     return;
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::DivRemOneOrMinusOne(HBinaryOperation* instruction) {
+  DCHECK(instruction->IsDiv() || instruction->IsRem());
+  DataType::Type type = instruction->GetResultType();
+
+  LocationSummary* locations = instruction->GetLocations();
+  Location second = locations->InAt(1);
+  DCHECK(second.IsConstant());
+
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  GpuRegister dividend = locations->InAt(0).AsRegister<GpuRegister>();
+  int64_t imm = Int64FromConstant(second.GetConstant());
+  DCHECK(imm == 1 || imm == -1);
+
+  if (instruction->IsRem()) {
+    __ Move(out, ZERO);
+  } else {
+    if (imm == -1) {
+      if (type == DataType::Type::kInt32) {
+        __ Subu(out, ZERO, dividend);
+      } else {
+        DCHECK_EQ(type, DataType::Type::kInt64);
+        __ Dsubu(out, ZERO, dividend);
+      }
+    } else if (out != dividend) {
+      __ Move(out, dividend);
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::DivRemByPowerOfTwo(HBinaryOperation* instruction) {
+  DCHECK(instruction->IsDiv() || instruction->IsRem());
+  DataType::Type type = instruction->GetResultType();
+
+  LocationSummary* locations = instruction->GetLocations();
+  Location second = locations->InAt(1);
+  DCHECK(second.IsConstant());
+
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  GpuRegister dividend = locations->InAt(0).AsRegister<GpuRegister>();
+  int64_t imm = Int64FromConstant(second.GetConstant());
+  uint64_t abs_imm = static_cast<uint64_t>(AbsOrMin(imm));
+  int ctz_imm = CTZ(abs_imm);
+
+  if (instruction->IsDiv()) {
+    if (type == DataType::Type::kInt32) {
+      if (ctz_imm == 1) {
+        // Fast path for division by +/-2, which is very common.
+        __ Srl(TMP, dividend, 31);
+      } else {
+        __ Sra(TMP, dividend, 31);
+        __ Srl(TMP, TMP, 32 - ctz_imm);
+      }
+      __ Addu(out, dividend, TMP);
+      __ Sra(out, out, ctz_imm);
+      if (imm < 0) {
+        __ Subu(out, ZERO, out);
+      }
+    } else {
+      DCHECK_EQ(type, DataType::Type::kInt64);
+      if (ctz_imm == 1) {
+        // Fast path for division by +/-2, which is very common.
+        __ Dsrl32(TMP, dividend, 31);
+      } else {
+        __ Dsra32(TMP, dividend, 31);
+        if (ctz_imm > 32) {
+          __ Dsrl(TMP, TMP, 64 - ctz_imm);
+        } else {
+          __ Dsrl32(TMP, TMP, 32 - ctz_imm);
+        }
+      }
+      __ Daddu(out, dividend, TMP);
+      if (ctz_imm < 32) {
+        __ Dsra(out, out, ctz_imm);
+      } else {
+        __ Dsra32(out, out, ctz_imm - 32);
+      }
+      if (imm < 0) {
+        __ Dsubu(out, ZERO, out);
+      }
+    }
+  } else {
+    if (type == DataType::Type::kInt32) {
+      if (ctz_imm == 1) {
+        // Fast path for modulo +/-2, which is very common.
+        __ Sra(TMP, dividend, 31);
+        __ Subu(out, dividend, TMP);
+        __ Andi(out, out, 1);
+        __ Addu(out, out, TMP);
+      } else {
+        __ Sra(TMP, dividend, 31);
+        __ Srl(TMP, TMP, 32 - ctz_imm);
+        __ Addu(out, dividend, TMP);
+        __ Ins(out, ZERO, ctz_imm, 32 - ctz_imm);
+        __ Subu(out, out, TMP);
+      }
+    } else {
+      DCHECK_EQ(type, DataType::Type::kInt64);
+      if (ctz_imm == 1) {
+        // Fast path for modulo +/-2, which is very common.
+        __ Dsra32(TMP, dividend, 31);
+        __ Dsubu(out, dividend, TMP);
+        __ Andi(out, out, 1);
+        __ Daddu(out, out, TMP);
+      } else {
+        __ Dsra32(TMP, dividend, 31);
+        if (ctz_imm > 32) {
+          __ Dsrl(TMP, TMP, 64 - ctz_imm);
+        } else {
+          __ Dsrl32(TMP, TMP, 32 - ctz_imm);
+        }
+        __ Daddu(out, dividend, TMP);
+        __ DblIns(out, ZERO, ctz_imm, 64 - ctz_imm);
+        __ Dsubu(out, out, TMP);
+      }
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateDivRemWithAnyConstant(HBinaryOperation* instruction) {
+  DCHECK(instruction->IsDiv() || instruction->IsRem());
+
+  LocationSummary* locations = instruction->GetLocations();
+  Location second = locations->InAt(1);
+  DCHECK(second.IsConstant());
+
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  GpuRegister dividend = locations->InAt(0).AsRegister<GpuRegister>();
+  int64_t imm = Int64FromConstant(second.GetConstant());
+
+  DataType::Type type = instruction->GetResultType();
+  DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64) << type;
+
+  int64_t magic;
+  int shift;
+  CalculateMagicAndShiftForDivRem(imm,
+                                  (type == DataType::Type::kInt64),
+                                  &magic,
+                                  &shift);
+
+  if (type == DataType::Type::kInt32) {
+    __ LoadConst32(TMP, magic);
+    __ MuhR6(TMP, dividend, TMP);
+
+    if (imm > 0 && magic < 0) {
+      __ Addu(TMP, TMP, dividend);
+    } else if (imm < 0 && magic > 0) {
+      __ Subu(TMP, TMP, dividend);
+    }
+
+    if (shift != 0) {
+      __ Sra(TMP, TMP, shift);
+    }
+
+    if (instruction->IsDiv()) {
+      __ Sra(out, TMP, 31);
+      __ Subu(out, TMP, out);
+    } else {
+      __ Sra(AT, TMP, 31);
+      __ Subu(AT, TMP, AT);
+      __ LoadConst32(TMP, imm);
+      __ MulR6(TMP, AT, TMP);
+      __ Subu(out, dividend, TMP);
+    }
+  } else {
+    __ LoadConst64(TMP, magic);
+    __ Dmuh(TMP, dividend, TMP);
+
+    if (imm > 0 && magic < 0) {
+      __ Daddu(TMP, TMP, dividend);
+    } else if (imm < 0 && magic > 0) {
+      __ Dsubu(TMP, TMP, dividend);
+    }
+
+    if (shift >= 32) {
+      __ Dsra32(TMP, TMP, shift - 32);
+    } else if (shift > 0) {
+      __ Dsra(TMP, TMP, shift);
+    }
+
+    if (instruction->IsDiv()) {
+      __ Dsra32(out, TMP, 31);
+      __ Dsubu(out, TMP, out);
+    } else {
+      __ Dsra32(AT, TMP, 31);
+      __ Dsubu(AT, TMP, AT);
+      __ LoadConst64(TMP, imm);
+      __ Dmul(TMP, AT, TMP);
+      __ Dsubu(out, dividend, TMP);
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateDivRemIntegral(HBinaryOperation* instruction) {
+  DCHECK(instruction->IsDiv() || instruction->IsRem());
+  DataType::Type type = instruction->GetResultType();
+  DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64) << type;
+
+  LocationSummary* locations = instruction->GetLocations();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  Location second = locations->InAt(1);
+
+  if (second.IsConstant()) {
+    int64_t imm = Int64FromConstant(second.GetConstant());
+    // Skip it to simplify the porting.
+    #if 0
+    if (imm == 0) {
+      // Do not generate anything. DivZeroCheck would prevent any code to be executed.
+    } else if (imm == 1 || imm == -1) {
+      DivRemOneOrMinusOne(instruction);
+    } else if (IsPowerOfTwo(AbsOrMin(imm))) {
+      DivRemByPowerOfTwo(instruction);
+    } else {
+      DCHECK(imm <= -2 || imm >= 2);
+      GenerateDivRemWithAnyConstant(instruction);
+    }
+    #else
+    GpuRegister divisor = TMP;
+    // Load const divisor into register
+    __ LoadConst64(TMP, imm);
+
+    // Do div/rem with both dividend and divisor are in registers
+    GpuRegister dividend = locations->InAt(0).AsRegister<GpuRegister>();
+    if (instruction->IsDiv()) {
+      if (type == DataType::Type::kInt32)
+        __ DivR6(out, dividend, divisor);
+      else
+        __ Ddiv(out, dividend, divisor);
+    } else {
+      if (type == DataType::Type::kInt32)
+        __ ModR6(out, dividend, divisor);
+      else
+        __ Dmod(out, dividend, divisor);
+    }
+    #endif
+  } else {
+    GpuRegister dividend = locations->InAt(0).AsRegister<GpuRegister>();
+    GpuRegister divisor = second.AsRegister<GpuRegister>();
+    if (instruction->IsDiv()) {
+      if (type == DataType::Type::kInt32)
+        __ DivR6(out, dividend, divisor);
+      else
+        __ Ddiv(out, dividend, divisor);
+    } else {
+      if (type == DataType::Type::kInt32)
+        __ ModR6(out, dividend, divisor);
+      else
+        __ Dmod(out, dividend, divisor);
+    }
+  }
+}
+
+void LocationsBuilderRISCV64::VisitDiv(HDiv* div) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(div, LocationSummary::kNoCall);
+  switch (div->GetResultType()) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RegisterOrConstant(div->InputAt(1)));
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+
+    default:
+      LOG(FATAL) << "Unexpected div type " << div->GetResultType();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitDiv(HDiv* instruction) {
+  DataType::Type type = instruction->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      GenerateDivRemIntegral(instruction);
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64: {
+      FpuRegister dst = locations->Out().AsFpuRegister<FpuRegister>();
+      FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+      if (type == DataType::Type::kFloat32)
+        __ DivS(dst, lhs, rhs);
+      else
+        __ DivD(dst, lhs, rhs);
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected div type " << type;
+  }
+}
+
+void LocationsBuilderRISCV64::VisitDivZeroCheck(HDivZeroCheck* instruction) {
+  LocationSummary* locations = codegen_->CreateThrowingSlowPathLocations(instruction);
+  locations->SetInAt(0, Location::RegisterOrConstant(instruction->InputAt(0)));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitDivZeroCheck(HDivZeroCheck* instruction) {
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) DivZeroCheckSlowPathRISCV64(instruction);
+  codegen_->AddSlowPath(slow_path);
+  Location value = instruction->GetLocations()->InAt(0);
+
+  DataType::Type type = instruction->GetType();
+
+  if (!DataType::IsIntegralType(type)) {
+    LOG(FATAL) << "Unexpected type " << type << " for DivZeroCheck.";
+    UNREACHABLE();
+  }
+
+  if (value.IsConstant()) {
+    int64_t divisor = codegen_->GetInt64ValueOf(value.GetConstant()->AsConstant());
+    if (divisor == 0) {
+      __ Bc(slow_path->GetEntryLabel());
+    } else {
+      // A division by a non-null constant is valid. We don't need to perform
+      // any check, so simply fall through.
+    }
+  } else {
+    __ Beqzc(value.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+  }
+}
+
+void LocationsBuilderRISCV64::VisitDoubleConstant(HDoubleConstant* constant) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  locations->SetOut(Location::ConstantLocation(constant));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitDoubleConstant(HDoubleConstant* cst ATTRIBUTE_UNUSED) {
+  // Will be generated at use site.
+}
+
+void LocationsBuilderRISCV64::VisitExit(HExit* exit) {
+  exit->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitExit(HExit* exit ATTRIBUTE_UNUSED) {
+}
+
+void LocationsBuilderRISCV64::VisitFloatConstant(HFloatConstant* constant) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(constant, LocationSummary::kNoCall);
+  locations->SetOut(Location::ConstantLocation(constant));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitFloatConstant(HFloatConstant* constant ATTRIBUTE_UNUSED) {
+  // Will be generated at use site.
+}
+
+void InstructionCodeGeneratorRISCV64::HandleGoto(HInstruction* got, HBasicBlock* successor) {
+  if (successor->IsExitBlock()) {
+    DCHECK(got->GetPrevious()->AlwaysThrows());
+    return;  // no code needed
+  }
+
+  HBasicBlock* block = got->GetBlock();
+  HInstruction* previous = got->GetPrevious();
+  HLoopInformation* info = block->GetLoopInformation();
+
+  if (info != nullptr && info->IsBackEdge(*block) && info->HasSuspendCheck()) {
+    if (codegen_->GetCompilerOptions().CountHotnessInCompiledCode()) {
+      __ Ld(AT, SP, kCurrentMethodStackOffset);
+      __ Lhu(TMP, AT, ArtMethod::HotnessCountOffset().Int32Value());
+      __ Addiu(TMP, TMP, 1);
+      __ Sh(TMP, AT, ArtMethod::HotnessCountOffset().Int32Value());
+    }
+    GenerateSuspendCheck(info->GetSuspendCheck(), successor);
+    return;
+  }
+  if (block->IsEntryBlock() && (previous != nullptr) && previous->IsSuspendCheck()) {
+    GenerateSuspendCheck(previous->AsSuspendCheck(), nullptr);
+  }
+  if (!codegen_->GoesToNextBlock(block, successor)) {
+    __ Bc(codegen_->GetLabelOf(successor));
+  }
+}
+
+void LocationsBuilderRISCV64::VisitGoto(HGoto* got) {
+  got->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitGoto(HGoto* got) {
+  HandleGoto(got, got->GetSuccessor());
+}
+
+void LocationsBuilderRISCV64::VisitTryBoundary(HTryBoundary* try_boundary) {
+  try_boundary->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitTryBoundary(HTryBoundary* try_boundary) {
+  HBasicBlock* successor = try_boundary->GetNormalFlowSuccessor();
+  if (!successor->IsExitBlock()) {
+    HandleGoto(try_boundary, successor);
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateIntLongCompare(IfCondition cond,
+                                                            bool is64bit,
+                                                            LocationSummary* locations) {
+  GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+  GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+  Location rhs_location = locations->InAt(1);
+  GpuRegister rhs_reg = ZERO;
+  int64_t rhs_imm = 0;
+  bool use_imm = rhs_location.IsConstant();
+  if (use_imm) {
+    if (is64bit) {
+      rhs_imm = CodeGenerator::GetInt64ValueOf(rhs_location.GetConstant());
+    } else {
+      rhs_imm = CodeGenerator::GetInt32ValueOf(rhs_location.GetConstant());
+    }
+  } else {
+    rhs_reg = rhs_location.AsRegister<GpuRegister>();
+  }
+  int64_t rhs_imm_plus_one = rhs_imm + UINT64_C(1);
+
+  switch (cond) {
+    case kCondEQ:
+    case kCondNE:
+      if (use_imm && IsInt<12>(-rhs_imm)) {
+        if (rhs_imm == 0) {
+          if (cond == kCondEQ) {
+            __ Sltiu(dst, lhs, 1);
+          } else {
+            __ Sltu(dst, ZERO, lhs);
+          }
+        } else {
+          if (is64bit) {
+            __ Daddiu(dst, lhs, -rhs_imm);
+          } else {
+            __ Addiu(dst, lhs, -rhs_imm);
+          }
+          if (cond == kCondEQ) {
+            __ Sltiu(dst, dst, 1);
+          } else {
+            __ Sltu(dst, ZERO, dst);
+          }
+        }
+      } else {
+        // Use 11-bit here for avoiding sign-extension.
+        if (use_imm && IsUint<11>(rhs_imm)) {
+          __ Xori(dst, lhs, rhs_imm);
+        } else {
+          if (use_imm) {
+            rhs_reg = TMP;
+            __ LoadConst64(rhs_reg, rhs_imm);
+          }
+          __ Xor(dst, lhs, rhs_reg);
+        }
+        if (cond == kCondEQ) {
+          __ Sltiu(dst, dst, 1);
+        } else {
+          __ Sltu(dst, ZERO, dst);
+        }
+      }
+      break;
+
+    case kCondLT:
+    case kCondGE:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && IsInt<11>(rhs_imm)) {
+        __ Slti(dst, lhs, rhs_imm);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Slt(dst, lhs, rhs_reg);
+      }
+      if (cond == kCondGE) {
+        // Simulate lhs >= rhs via !(lhs < rhs) since there's
+        // only the slt instruction but no sge.
+        __ Xori(dst, dst, 1);
+      }
+      break;
+
+    case kCondLE:
+    case kCondGT:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && IsInt<11>(rhs_imm_plus_one)) {
+        // Simulate lhs <= rhs via lhs < rhs + 1.
+        __ Slti(dst, lhs, rhs_imm_plus_one);
+        if (cond == kCondGT) {
+          // Simulate lhs > rhs via !(lhs <= rhs) since there's
+          // only the slti instruction but no sgti.
+          __ Xori(dst, dst, 1);
+        }
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Slt(dst, rhs_reg, lhs);
+        if (cond == kCondLE) {
+          // Simulate lhs <= rhs via !(rhs < lhs) since there's
+          // only the slt instruction but no sle.
+          __ Xori(dst, dst, 1);
+        }
+      }
+      break;
+
+    case kCondB:
+    case kCondAE:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && IsInt<11>(rhs_imm)) {
+        // Sltiu sign-extends its 16-bit immediate operand before
+        // the comparison and thus lets us compare directly with
+        // unsigned values in the ranges [0, 0x7fff] and
+        // [0x[ffffffff]ffff8000, 0x[ffffffff]ffffffff].
+        __ Sltiu(dst, lhs, rhs_imm);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Sltu(dst, lhs, rhs_reg);
+      }
+      if (cond == kCondAE) {
+        // Simulate lhs >= rhs via !(lhs < rhs) since there's
+        // only the sltu instruction but no sgeu.
+        __ Xori(dst, dst, 1);
+      }
+      break;
+
+    case kCondBE:
+    case kCondA:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && (rhs_imm_plus_one != 0) && IsInt<11>(rhs_imm_plus_one)) {
+        // Simulate lhs <= rhs via lhs < rhs + 1.
+        // Note that this only works if rhs + 1 does not overflow
+        // to 0, hence the check above.
+        // Sltiu sign-extends its 16-bit immediate operand before
+        // the comparison and thus lets us compare directly with
+        // unsigned values in the ranges [0, 0x7fff] and
+        // [0x[ffffffff]ffff8000, 0x[ffffffff]ffffffff].
+        __ Sltiu(dst, lhs, rhs_imm_plus_one);
+        if (cond == kCondA) {
+          // Simulate lhs > rhs via !(lhs <= rhs) since there's
+          // only the sltiu instruction but no sgtiu.
+          __ Xori(dst, dst, 1);
+        }
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Sltu(dst, rhs_reg, lhs);
+        if (cond == kCondBE) {
+          // Simulate lhs <= rhs via !(rhs < lhs) since there's
+          // only the sltu instruction but no sleu.
+          __ Xori(dst, dst, 1);
+        }
+      }
+      break;
+  }
+}
+
+bool InstructionCodeGeneratorRISCV64::MaterializeIntLongCompare(IfCondition cond,
+                                                               bool is64bit,
+                                                               LocationSummary* input_locations,
+                                                               GpuRegister dst) {
+  GpuRegister lhs = input_locations->InAt(0).AsRegister<GpuRegister>();
+  Location rhs_location = input_locations->InAt(1);
+  GpuRegister rhs_reg = ZERO;
+  int64_t rhs_imm = 0;
+  bool use_imm = rhs_location.IsConstant();
+  if (use_imm) {
+    if (is64bit) {
+      rhs_imm = CodeGenerator::GetInt64ValueOf(rhs_location.GetConstant());
+    } else {
+      rhs_imm = CodeGenerator::GetInt32ValueOf(rhs_location.GetConstant());
+    }
+  } else {
+    rhs_reg = rhs_location.AsRegister<GpuRegister>();
+  }
+  int64_t rhs_imm_plus_one = rhs_imm + UINT64_C(1);
+
+  switch (cond) {
+    case kCondEQ:
+    case kCondNE:
+      if (use_imm && IsInt<12>(-rhs_imm)) {
+        if (is64bit) {
+          __ Daddiu(dst, lhs, -rhs_imm);
+        } else {
+          __ Addiu(dst, lhs, -rhs_imm);
+        }
+      } else if (use_imm && IsUint<11>(rhs_imm)) {
+      // Use 11-bit here for avoiding sign-extension.
+        __ Xori(dst, lhs, rhs_imm);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Xor(dst, lhs, rhs_reg);
+      }
+      return (cond == kCondEQ);
+
+    case kCondLT:
+    case kCondGE:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && IsInt<11>(rhs_imm)) {
+        __ Slti(dst, lhs, rhs_imm);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Slt(dst, lhs, rhs_reg);
+      }
+      return (cond == kCondGE);
+
+    case kCondLE:
+    case kCondGT:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && IsInt<11>(rhs_imm_plus_one)) {
+        // Simulate lhs <= rhs via lhs < rhs + 1.
+        __ Slti(dst, lhs, rhs_imm_plus_one);
+        return (cond == kCondGT);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Slt(dst, rhs_reg, lhs);
+        return (cond == kCondLE);
+      }
+
+    case kCondB:
+    case kCondAE:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && IsInt<11>(rhs_imm)) {
+        // Sltiu sign-extends its 16-bit immediate operand before
+        // the comparison and thus lets us compare directly with
+        // unsigned values in the ranges [0, 0x7fff] and
+        // [0x[ffffffff]ffff8000, 0x[ffffffff]ffffffff].
+        __ Sltiu(dst, lhs, rhs_imm);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Sltu(dst, lhs, rhs_reg);
+      }
+      return (cond == kCondAE);
+
+    case kCondBE:
+    case kCondA:
+      // Use 11-bit here for avoiding sign-extension.
+      if (use_imm && (rhs_imm_plus_one != 0) && IsInt<11>(rhs_imm_plus_one)) {
+        // Simulate lhs <= rhs via lhs < rhs + 1.
+        // Note that this only works if rhs + 1 does not overflow
+        // to 0, hence the check above.
+        // Sltiu sign-extends its 16-bit immediate operand before
+        // the comparison and thus lets us compare directly with
+        // unsigned values in the ranges [0, 0x7fff] and
+        // [0x[ffffffff]ffff8000, 0x[ffffffff]ffffffff].
+        __ Sltiu(dst, lhs, rhs_imm_plus_one);
+        return (cond == kCondA);
+      } else {
+        if (use_imm) {
+          rhs_reg = TMP;
+          __ LoadConst64(rhs_reg, rhs_imm);
+        }
+        __ Sltu(dst, rhs_reg, lhs);
+        return (cond == kCondBE);
+      }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateIntLongCompareAndBranch(IfCondition cond,
+                                                                     bool is64bit,
+                                                                     LocationSummary* locations,
+                                                                     Riscv64Label* label) {
+  GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+  Location rhs_location = locations->InAt(1);
+  GpuRegister rhs_reg = ZERO;
+  int64_t rhs_imm = 0;
+  bool use_imm = rhs_location.IsConstant();
+  if (use_imm) {
+    if (is64bit) {
+      rhs_imm = CodeGenerator::GetInt64ValueOf(rhs_location.GetConstant());
+    } else {
+      rhs_imm = CodeGenerator::GetInt32ValueOf(rhs_location.GetConstant());
+    }
+  } else {
+    rhs_reg = rhs_location.AsRegister<GpuRegister>();
+  }
+
+  if (use_imm && rhs_imm == 0) {
+    switch (cond) {
+      case kCondEQ:
+      case kCondBE:  // <= 0 if zero
+        __ Beqzc(lhs, label);
+        break;
+      case kCondNE:
+      case kCondA:  // > 0 if non-zero
+        __ Bnezc(lhs, label);
+        break;
+      case kCondLT:
+        __ Bltzc(lhs, label);
+        break;
+      case kCondGE:
+        __ Bgezc(lhs, label);
+        break;
+      case kCondLE:
+        __ Blezc(lhs, label);
+        break;
+      case kCondGT:
+        __ Bgtzc(lhs, label);
+        break;
+      case kCondB:  // always false
+        break;
+      case kCondAE:  // always true
+        __ Bc(label);
+        break;
+    }
+  } else {
+    if (use_imm) {
+      rhs_reg = TMP;
+      __ LoadConst64(rhs_reg, rhs_imm);
+    }
+    switch (cond) {
+      case kCondEQ:
+        __ Beqc(lhs, rhs_reg, label);
+        break;
+      case kCondNE:
+        __ Bnec(lhs, rhs_reg, label);
+        break;
+      case kCondLT:
+        __ Bltc(lhs, rhs_reg, label);
+        break;
+      case kCondGE:
+        __ Bgec(lhs, rhs_reg, label);
+        break;
+      case kCondLE:
+        __ Bgec(rhs_reg, lhs, label);
+        break;
+      case kCondGT:
+        __ Bltc(rhs_reg, lhs, label);
+        break;
+      case kCondB:
+        __ Bltuc(lhs, rhs_reg, label);
+        break;
+      case kCondAE:
+        __ Bgeuc(lhs, rhs_reg, label);
+        break;
+      case kCondBE:
+        __ Bgeuc(rhs_reg, lhs, label);
+        break;
+      case kCondA:
+        __ Bltuc(rhs_reg, lhs, label);
+        break;
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateFpCompare(IfCondition cond,
+                                                       bool gt_bias,
+                                                       DataType::Type type,
+                                                       LocationSummary* locations) {
+  GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+  FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+  if (type == DataType::Type::kFloat32) {
+    switch (cond) {
+      case kCondEQ:
+        __ CmpEqS(dst, lhs, rhs);
+        break;
+      case kCondNE:
+        __ CmpEqS(dst, lhs, rhs);
+        __ Xori(dst, dst, 1);
+        break;
+      case kCondLT:
+        if (gt_bias) {
+          __ CmpLtS(dst, lhs, rhs);
+        } else {
+          __ CmpUltS(dst, lhs, rhs);
+        }
+        break;
+      case kCondLE:
+        if (gt_bias) {
+          __ CmpLeS(dst, lhs, rhs);
+        } else {
+          __ CmpUleS(dst, lhs, rhs);
+        }
+        break;
+      case kCondGT:
+        if (gt_bias) {
+          __ CmpUltS(dst, rhs, lhs);
+        } else {
+          __ CmpLtS(dst, rhs, lhs);
+        }
+        break;
+      case kCondGE:
+        if (gt_bias) {
+          __ CmpUleS(dst, rhs, lhs);
+        } else {
+          __ CmpLeS(dst, rhs, lhs);
+        }
+        break;
+      default:
+        LOG(FATAL) << "Unexpected non-floating-point condition " << cond;
+        UNREACHABLE();
+    }
+  } else {
+    DCHECK_EQ(type, DataType::Type::kFloat64);
+    switch (cond) {
+      case kCondEQ:
+        __ CmpEqD(dst, lhs, rhs);
+        break;
+      case kCondNE:
+        __ CmpEqD(dst, lhs, rhs);
+        __ Xori(dst, dst, 1);
+        break;
+      case kCondLT:
+        if (gt_bias) {
+          __ CmpLtD(dst, lhs, rhs);
+        } else {
+          __ CmpUltD(dst, lhs, rhs);
+        }
+        break;
+      case kCondLE:
+        if (gt_bias) {
+          __ CmpLeD(dst, lhs, rhs);
+        } else {
+          __ CmpUleD(dst, lhs, rhs);
+        }
+        break;
+      case kCondGT:
+        if (gt_bias) {
+          __ CmpUltD(dst, rhs, lhs);
+        } else {
+          __ CmpLtD(dst, rhs, lhs);
+        }
+        break;
+      case kCondGE:
+        if (gt_bias) {
+          __ CmpUleD(dst, rhs, lhs);
+        } else {
+          __ CmpLeD(dst, rhs, lhs);
+        }
+        break;
+      default:
+        LOG(FATAL) << "Unexpected non-floating-point condition " << cond;
+        UNREACHABLE();
+    }
+  }
+}
+
+bool InstructionCodeGeneratorRISCV64::MaterializeFpCompare(IfCondition cond,
+                                                          bool gt_bias,
+                                                          DataType::Type type,
+                                                          LocationSummary* input_locations,
+                                                          GpuRegister dst) {
+  FpuRegister lhs = input_locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister rhs = input_locations->InAt(1).AsFpuRegister<FpuRegister>();
+  if (type == DataType::Type::kFloat32) {
+    switch (cond) {
+      case kCondEQ:
+        __ CmpEqS(dst, lhs, rhs);
+        return false;
+      case kCondNE:
+        __ CmpEqS(dst, lhs, rhs);
+        return true;
+      case kCondLT:
+        if (gt_bias) {
+          __ CmpLtS(dst, lhs, rhs);
+        } else {
+          __ CmpUltS(dst, lhs, rhs);
+        }
+        return false;
+      case kCondLE:
+        if (gt_bias) {
+          __ CmpLeS(dst, lhs, rhs);
+        } else {
+          __ CmpUleS(dst, lhs, rhs);
+        }
+        return false;
+      case kCondGT:
+        if (gt_bias) {
+          __ CmpUltS(dst, rhs, lhs);
+        } else {
+          __ CmpLtS(dst, rhs, lhs);
+        }
+        return false;
+      case kCondGE:
+        if (gt_bias) {
+          __ CmpUleS(dst, rhs, lhs);
+        } else {
+          __ CmpLeS(dst, rhs, lhs);
+        }
+        return false;
+      default:
+        LOG(FATAL) << "Unexpected non-floating-point condition " << cond;
+        UNREACHABLE();
+    }
+  } else {
+    DCHECK_EQ(type, DataType::Type::kFloat64);
+    switch (cond) {
+      case kCondEQ:
+        __ CmpEqD(dst, lhs, rhs);
+        return false;
+      case kCondNE:
+        __ CmpEqD(dst, lhs, rhs);
+        return true;
+      case kCondLT:
+        if (gt_bias) {
+          __ CmpLtD(dst, lhs, rhs);
+        } else {
+          __ CmpUltD(dst, lhs, rhs);
+        }
+        return false;
+      case kCondLE:
+        if (gt_bias) {
+          __ CmpLeD(dst, lhs, rhs);
+        } else {
+          __ CmpUleD(dst, lhs, rhs);
+        }
+        return false;
+      case kCondGT:
+        if (gt_bias) {
+          __ CmpUltD(dst, rhs, lhs);
+        } else {
+          __ CmpLtD(dst, rhs, lhs);
+        }
+        return false;
+      case kCondGE:
+        if (gt_bias) {
+          __ CmpUleD(dst, rhs, lhs);
+        } else {
+          __ CmpLeD(dst, rhs, lhs);
+        }
+        return false;
+      default:
+        LOG(FATAL) << "Unexpected non-floating-point condition " << cond;
+        UNREACHABLE();
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateFpCompareAndBranch(IfCondition cond,
+                                                                bool gt_bias,
+                                                                DataType::Type type,
+                                                                LocationSummary* locations,
+                                                                Riscv64Label* label) {
+  FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+  if (type == DataType::Type::kFloat32) {
+    switch (cond) {
+      case kCondEQ:
+        __ CmpEqS(TMP, lhs, rhs);
+        __ Bnez(TMP, label);
+        break;
+      case kCondNE:
+        __ CmpEqS(TMP, lhs, rhs);
+        __ Beqz(TMP, label);
+        break;
+      case kCondLT:
+        if (gt_bias) {
+          __ CmpLtS(TMP, lhs, rhs);
+        } else {
+          __ CmpUltS(TMP, lhs, rhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      case kCondLE:
+        if (gt_bias) {
+          __ CmpLeS(TMP, lhs, rhs);
+        } else {
+          __ CmpUleS(TMP, lhs, rhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      case kCondGT:
+        if (gt_bias) {
+          __ CmpUltS(TMP, rhs, lhs);
+        } else {
+          __ CmpLtS(TMP, rhs, lhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      case kCondGE:
+        if (gt_bias) {
+          __ CmpUleS(TMP, rhs, lhs);
+        } else {
+          __ CmpLeS(TMP, rhs, lhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      default:
+        LOG(FATAL) << "Unexpected non-floating-point condition";
+        UNREACHABLE();
+    }
+  } else {
+    DCHECK_EQ(type, DataType::Type::kFloat64);
+    switch (cond) {
+      case kCondEQ:
+        __ CmpEqD(TMP, lhs, rhs);
+        __ Bnez(TMP, label);
+        break;
+      case kCondNE:
+        __ CmpEqD(TMP, lhs, rhs);
+        __ Beqz(TMP, label);
+        break;
+      case kCondLT:
+        if (gt_bias) {
+          __ CmpLtD(TMP, lhs, rhs);
+        } else {
+          __ CmpUltD(TMP, lhs, rhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      case kCondLE:
+        if (gt_bias) {
+          __ CmpLeD(TMP, lhs, rhs);
+        } else {
+          __ CmpUleD(TMP, lhs, rhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      case kCondGT:
+        if (gt_bias) {
+          __ CmpUltD(TMP, rhs, lhs);
+        } else {
+          __ CmpLtD(TMP, rhs, lhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      case kCondGE:
+        if (gt_bias) {
+          __ CmpUleD(TMP, rhs, lhs);
+        } else {
+          __ CmpLeD(TMP, rhs, lhs);
+        }
+        __ Bnez(TMP, label);
+        break;
+      default:
+        LOG(FATAL) << "Unexpected non-floating-point condition";
+        UNREACHABLE();
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateTestAndBranch(HInstruction* instruction,
+                                                           size_t condition_input_index,
+                                                           Riscv64Label* true_target,
+                                                           Riscv64Label* false_target) {
+  HInstruction* cond = instruction->InputAt(condition_input_index);
+
+  if (true_target == nullptr && false_target == nullptr) {
+    // Nothing to do. The code always falls through.
+    return;
+  } else if (cond->IsIntConstant()) {
+    // Constant condition, statically compared against "true" (integer value 1).
+    if (cond->AsIntConstant()->IsTrue()) {
+      if (true_target != nullptr) {
+        __ Bc(true_target);
+      }
+    } else {
+      DCHECK(cond->AsIntConstant()->IsFalse()) << cond->AsIntConstant()->GetValue();
+      if (false_target != nullptr) {
+        __ Bc(false_target);
+      }
+    }
+    return;
+  }
+
+  // The following code generates these patterns:
+  //  (1) true_target == nullptr && false_target != nullptr
+  //        - opposite condition true => branch to false_target
+  //  (2) true_target != nullptr && false_target == nullptr
+  //        - condition true => branch to true_target
+  //  (3) true_target != nullptr && false_target != nullptr
+  //        - condition true => branch to true_target
+  //        - branch to false_target
+  if (IsBooleanValueOrMaterializedCondition(cond)) {
+    // The condition instruction has been materialized, compare the output to 0.
+    Location cond_val = instruction->GetLocations()->InAt(condition_input_index);
+    DCHECK(cond_val.IsRegister());
+    if (true_target == nullptr) {
+      __ Beqzc(cond_val.AsRegister<GpuRegister>(), false_target);
+    } else {
+      __ Bnezc(cond_val.AsRegister<GpuRegister>(), true_target);
+    }
+  } else {
+    // The condition instruction has not been materialized, use its inputs as
+    // the comparison and its condition as the branch condition.
+    HCondition* condition = cond->AsCondition();
+    DataType::Type type = condition->InputAt(0)->GetType();
+    LocationSummary* locations = cond->GetLocations();
+    IfCondition if_cond = condition->GetCondition();
+    Riscv64Label* branch_target = true_target;
+
+    if (true_target == nullptr) {
+      if_cond = condition->GetOppositeCondition();
+      branch_target = false_target;
+    }
+
+    switch (type) {
+      default:
+        GenerateIntLongCompareAndBranch(if_cond, /* is64bit= */ false, locations, branch_target);
+        break;
+      case DataType::Type::kInt64:
+        GenerateIntLongCompareAndBranch(if_cond, /* is64bit= */ true, locations, branch_target);
+        break;
+      case DataType::Type::kFloat32:
+      case DataType::Type::kFloat64:
+        GenerateFpCompareAndBranch(if_cond, condition->IsGtBias(), type, locations, branch_target);
+        break;
+    }
+  }
+
+  // If neither branch falls through (case 3), the conditional branch to `true_target`
+  // was already emitted (case 2) and we need to emit a jump to `false_target`.
+  if (true_target != nullptr && false_target != nullptr) {
+    __ Bc(false_target);
+  }
+}
+
+void LocationsBuilderRISCV64::VisitIf(HIf* if_instr) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(if_instr);
+  if (IsBooleanValueOrMaterializedCondition(if_instr->InputAt(0))) {
+    locations->SetInAt(0, Location::RequiresRegister());
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitIf(HIf* if_instr) {
+  HBasicBlock* true_successor = if_instr->IfTrueSuccessor();
+  HBasicBlock* false_successor = if_instr->IfFalseSuccessor();
+  Riscv64Label* true_target = codegen_->GoesToNextBlock(if_instr->GetBlock(), true_successor) ?
+      nullptr : codegen_->GetLabelOf(true_successor);
+  Riscv64Label* false_target = codegen_->GoesToNextBlock(if_instr->GetBlock(), false_successor) ?
+      nullptr : codegen_->GetLabelOf(false_successor);
+  GenerateTestAndBranch(if_instr, /* condition_input_index= */ 0, true_target, false_target);
+}
+
+void LocationsBuilderRISCV64::VisitDeoptimize(HDeoptimize* deoptimize) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator())
+      LocationSummary(deoptimize, LocationSummary::kCallOnSlowPath);
+  InvokeRuntimeCallingConvention calling_convention;
+  RegisterSet caller_saves = RegisterSet::Empty();
+  caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetCustomSlowPathCallerSaves(caller_saves);
+  if (IsBooleanValueOrMaterializedCondition(deoptimize->InputAt(0))) {
+    locations->SetInAt(0, Location::RequiresRegister());
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitDeoptimize(HDeoptimize* deoptimize) {
+  SlowPathCodeRISCV64* slow_path =
+      deopt_slow_paths_.NewSlowPath<DeoptimizationSlowPathRISCV64>(deoptimize);
+  GenerateTestAndBranch(deoptimize,
+                        /* condition_input_index= */ 0,
+                        slow_path->GetEntryLabel(),
+                        /* false_target= */ nullptr);
+}
+
+// This function returns true if a conditional move can be generated for HSelect.
+// Otherwise it returns false and HSelect must be implemented in terms of conditonal
+// branches and regular moves.
+//
+// If `locations_to_set` isn't nullptr, its inputs and outputs are set for HSelect.
+//
+// While determining feasibility of a conditional move and setting inputs/outputs
+// are two distinct tasks, this function does both because they share quite a bit
+// of common logic.
+static bool CanMoveConditionally(HSelect* select, LocationSummary* locations_to_set) {
+  bool materialized = IsBooleanValueOrMaterializedCondition(select->GetCondition());
+  HInstruction* cond = select->InputAt(/* i= */ 2);
+  HCondition* condition = cond->AsCondition();
+
+  DataType::Type cond_type =
+      materialized ? DataType::Type::kInt32 : condition->InputAt(0)->GetType();
+  DataType::Type dst_type = select->GetType();
+
+  HConstant* cst_true_value = select->GetTrueValue()->AsConstant();
+  HConstant* cst_false_value = select->GetFalseValue()->AsConstant();
+  bool is_true_value_zero_constant =
+      (cst_true_value != nullptr && cst_true_value->IsZeroBitPattern());
+  bool is_false_value_zero_constant =
+      (cst_false_value != nullptr && cst_false_value->IsZeroBitPattern());
+
+  bool can_move_conditionally = false;
+  bool use_const_for_false_in = false;
+  bool use_const_for_true_in = false;
+
+  if (!cond->IsConstant()) {
+    if (!DataType::IsFloatingPointType(cond_type)) {
+      if (!DataType::IsFloatingPointType(dst_type)) {
+        // Moving int/long on int/long condition.
+        if (is_true_value_zero_constant) {
+          // seleqz out_reg, false_reg, cond_reg
+          can_move_conditionally = true;
+          use_const_for_true_in = true;
+        } else if (is_false_value_zero_constant) {
+          // selnez out_reg, true_reg, cond_reg
+          can_move_conditionally = true;
+          use_const_for_false_in = true;
+        } else if (materialized) {
+          // Not materializing unmaterialized int conditions
+          // to keep the instruction count low.
+          // selnez AT, true_reg, cond_reg
+          // seleqz TMP, false_reg, cond_reg
+          // or out_reg, AT, TMP
+          can_move_conditionally = true;
+        }
+      } else {
+        // Moving float/double on int/long condition.
+        if (materialized) {
+          // Not materializing unmaterialized int conditions
+          // to keep the instruction count low.
+          can_move_conditionally = true;
+          if (is_true_value_zero_constant) {
+            // sltu TMP, ZERO, cond_reg
+            // mtc1 TMP, temp_cond_reg
+            // seleqz.fmt out_reg, false_reg, temp_cond_reg
+            use_const_for_true_in = true;
+          } else if (is_false_value_zero_constant) {
+            // sltu TMP, ZERO, cond_reg
+            // mtc1 TMP, temp_cond_reg
+            // selnez.fmt out_reg, true_reg, temp_cond_reg
+            use_const_for_false_in = true;
+          } else {
+            // sltu TMP, ZERO, cond_reg
+            // mtc1 TMP, temp_cond_reg
+            // sel.fmt temp_cond_reg, false_reg, true_reg
+            // mov.fmt out_reg, temp_cond_reg
+          }
+        }
+      }
+    } else {
+      if (!DataType::IsFloatingPointType(dst_type)) {
+        // Moving int/long on float/double condition.
+        can_move_conditionally = true;
+        if (is_true_value_zero_constant) {
+          // mfc1 TMP, temp_cond_reg
+          // seleqz out_reg, false_reg, TMP
+          use_const_for_true_in = true;
+        } else if (is_false_value_zero_constant) {
+          // mfc1 TMP, temp_cond_reg
+          // selnez out_reg, true_reg, TMP
+          use_const_for_false_in = true;
+        } else {
+          // mfc1 TMP, temp_cond_reg
+          // selnez AT, true_reg, TMP
+          // seleqz TMP, false_reg, TMP
+          // or out_reg, AT, TMP
+        }
+      } else {
+        // Moving float/double on float/double condition.
+        can_move_conditionally = true;
+        if (is_true_value_zero_constant) {
+          // seleqz.fmt out_reg, false_reg, temp_cond_reg
+          use_const_for_true_in = true;
+        } else if (is_false_value_zero_constant) {
+          // selnez.fmt out_reg, true_reg, temp_cond_reg
+          use_const_for_false_in = true;
+        } else {
+          // sel.fmt temp_cond_reg, false_reg, true_reg
+          // mov.fmt out_reg, temp_cond_reg
+        }
+      }
+    }
+  }
+
+  if (can_move_conditionally) {
+    DCHECK(!use_const_for_false_in || !use_const_for_true_in);
+  } else {
+    DCHECK(!use_const_for_false_in);
+    DCHECK(!use_const_for_true_in);
+  }
+
+  if (locations_to_set != nullptr) {
+    if (use_const_for_false_in) {
+      locations_to_set->SetInAt(0, Location::ConstantLocation(cst_false_value));
+    } else {
+      locations_to_set->SetInAt(0,
+                                DataType::IsFloatingPointType(dst_type)
+                                    ? Location::RequiresFpuRegister()
+                                    : Location::RequiresRegister());
+    }
+    if (use_const_for_true_in) {
+      locations_to_set->SetInAt(1, Location::ConstantLocation(cst_true_value));
+    } else {
+      locations_to_set->SetInAt(1,
+                                DataType::IsFloatingPointType(dst_type)
+                                    ? Location::RequiresFpuRegister()
+                                    : Location::RequiresRegister());
+    }
+    if (materialized) {
+      locations_to_set->SetInAt(2, Location::RequiresRegister());
+    }
+
+    if (can_move_conditionally) {
+      locations_to_set->SetOut(DataType::IsFloatingPointType(dst_type)
+                                   ? Location::RequiresFpuRegister()
+                                   : Location::RequiresRegister());
+    } else {
+      locations_to_set->SetOut(Location::SameAsFirstInput());
+    }
+  }
+
+  return can_move_conditionally;
+}
+
+
+void InstructionCodeGeneratorRISCV64::GenConditionalMove(HSelect* select) {
+  LocationSummary* locations = select->GetLocations();
+  Location dst = locations->Out();
+  Location false_src = locations->InAt(0);
+  Location true_src = locations->InAt(1);
+  HInstruction* cond = select->InputAt(/* i= */ 2);
+  GpuRegister cond_reg = TMP;
+  FpuRegister fcond_reg = FTMP;
+  DataType::Type cond_type = DataType::Type::kInt32;
+  bool cond_inverted = false;
+  DataType::Type dst_type = select->GetType();
+
+  if (IsBooleanValueOrMaterializedCondition(cond)) {
+    cond_reg = locations->InAt(/* at= */ 2).AsRegister<GpuRegister>();
+  } else {
+    HCondition* condition = cond->AsCondition();
+    LocationSummary* cond_locations = cond->GetLocations();
+    IfCondition if_cond = condition->GetCondition();
+    cond_type = condition->InputAt(0)->GetType();
+    switch (cond_type) {
+      default:
+        cond_inverted = MaterializeIntLongCompare(if_cond,
+                                                  /* is64bit= */ false,
+                                                  cond_locations,
+                                                  cond_reg);
+        break;
+      case DataType::Type::kInt64:
+        cond_inverted = MaterializeIntLongCompare(if_cond,
+                                                  /* is64bit= */ true,
+                                                  cond_locations,
+                                                  cond_reg);
+        break;
+      case DataType::Type::kFloat32:
+      case DataType::Type::kFloat64:
+        cond_inverted = MaterializeFpCompare(if_cond,
+                                             condition->IsGtBias(),
+                                             cond_type,
+                                             cond_locations,
+                                             cond_reg);
+        break;
+    }
+  }
+
+  if (true_src.IsConstant()) {
+    DCHECK(true_src.GetConstant()->IsZeroBitPattern());
+  }
+  if (false_src.IsConstant()) {
+    DCHECK(false_src.GetConstant()->IsZeroBitPattern());
+  }
+
+  switch (dst_type) {
+    default:
+      /*if (DataType::IsFloatingPointType(cond_type)) {
+        __ Mfc1(cond_reg, fcond_reg);
+      }*/
+      if (true_src.IsConstant()) {
+        if (cond_inverted) {
+          __ Selnez(dst.AsRegister<GpuRegister>(), false_src.AsRegister<GpuRegister>(), cond_reg);
+        } else {
+          __ Seleqz(dst.AsRegister<GpuRegister>(), false_src.AsRegister<GpuRegister>(), cond_reg);
+        }
+      } else if (false_src.IsConstant()) {
+        if (cond_inverted) {
+          __ Seleqz(dst.AsRegister<GpuRegister>(), true_src.AsRegister<GpuRegister>(), cond_reg);
+        } else {
+          __ Selnez(dst.AsRegister<GpuRegister>(), true_src.AsRegister<GpuRegister>(), cond_reg);
+        }
+      } else {
+        DCHECK_NE(cond_reg, AT);
+        if (cond_inverted) {
+          __ Seleqz(AT, true_src.AsRegister<GpuRegister>(), cond_reg);
+          __ Selnez(TMP, false_src.AsRegister<GpuRegister>(), cond_reg);
+        } else {
+          __ Selnez(AT, true_src.AsRegister<GpuRegister>(), cond_reg);
+          __ Seleqz(TMP, false_src.AsRegister<GpuRegister>(), cond_reg);
+        }
+        __ Or(dst.AsRegister<GpuRegister>(), AT, TMP);
+      }
+      break;
+    case DataType::Type::kFloat32: {
+      if (!DataType::IsFloatingPointType(cond_type)) {
+        // sel*.fmt tests bit 0 of the condition register, account for that.
+        __ Sltu(TMP, ZERO, cond_reg);
+        __ Mtc1(TMP, fcond_reg);
+      } else {
+        __ Mtc1(cond_reg, fcond_reg);
+      }
+
+      FpuRegister dst_reg = dst.AsFpuRegister<FpuRegister>();
+      if (true_src.IsConstant()) {
+        FpuRegister src_reg = false_src.AsFpuRegister<FpuRegister>();
+        if (cond_inverted) {
+          __ SelnezS(dst_reg, src_reg, fcond_reg);
+        } else {
+          __ SeleqzS(dst_reg, src_reg, fcond_reg);
+        }
+      } else if (false_src.IsConstant()) {
+        FpuRegister src_reg = true_src.AsFpuRegister<FpuRegister>();
+        if (cond_inverted) {
+          __ SeleqzS(dst_reg, src_reg, fcond_reg);
+        } else {
+          __ SelnezS(dst_reg, src_reg, fcond_reg);
+        }
+      } else {
+        if (cond_inverted) {
+          __ SelS(fcond_reg,
+                  true_src.AsFpuRegister<FpuRegister>(),
+                  false_src.AsFpuRegister<FpuRegister>());
+        } else {
+          __ SelS(fcond_reg,
+                  false_src.AsFpuRegister<FpuRegister>(),
+                  true_src.AsFpuRegister<FpuRegister>());
+        }
+        __ MovS(dst_reg, fcond_reg);
+      }
+      break;
+    }
+    case DataType::Type::kFloat64: {
+      if (!DataType::IsFloatingPointType(cond_type)) {
+        // sel*.fmt tests bit 0 of the condition register, account for that.
+        __ Sltu(TMP, ZERO, cond_reg);
+        __ Mtc1(TMP, fcond_reg);
+      } else {
+        __ Mtc1(cond_reg, fcond_reg);
+      }
+
+      FpuRegister dst_reg = dst.AsFpuRegister<FpuRegister>();
+      if (true_src.IsConstant()) {
+        FpuRegister src_reg = false_src.AsFpuRegister<FpuRegister>();
+        if (cond_inverted) {
+          __ SelnezD(dst_reg, src_reg, fcond_reg);
+        } else {
+          __ SeleqzD(dst_reg, src_reg, fcond_reg);
+        }
+      } else if (false_src.IsConstant()) {
+        FpuRegister src_reg = true_src.AsFpuRegister<FpuRegister>();
+        if (cond_inverted) {
+          __ SeleqzD(dst_reg, src_reg, fcond_reg);
+        } else {
+          __ SelnezD(dst_reg, src_reg, fcond_reg);
+        }
+      } else {
+        if (cond_inverted) {
+          __ SelD(fcond_reg,
+                  true_src.AsFpuRegister<FpuRegister>(),
+                  false_src.AsFpuRegister<FpuRegister>());
+        } else {
+          __ SelD(fcond_reg,
+                  false_src.AsFpuRegister<FpuRegister>(),
+                  true_src.AsFpuRegister<FpuRegister>());
+        }
+        __ MovD(dst_reg, fcond_reg);
+      }
+      break;
+    }
+  }
+}
+
+void LocationsBuilderRISCV64::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* flag) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator())
+      LocationSummary(flag, LocationSummary::kNoCall);
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitShouldDeoptimizeFlag(HShouldDeoptimizeFlag* flag) {
+  __ LoadFromOffset(kLoadWord,
+                    flag->GetLocations()->Out().AsRegister<GpuRegister>(),
+                    SP,
+                    codegen_->GetStackOffsetOfShouldDeoptimizeFlag());
+}
+
+void LocationsBuilderRISCV64::VisitSelect(HSelect* select) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(select);
+  CanMoveConditionally(select, locations);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitSelect(HSelect* select) {
+  if (CanMoveConditionally(select, /* locations_to_set= */ nullptr)) {
+    GenConditionalMove(select);
+  } else {
+    LocationSummary* locations = select->GetLocations();
+    Riscv64Label false_target;
+    GenerateTestAndBranch(select,
+                          /* condition_input_index= */ 2,
+                          /* true_target= */ nullptr,
+                          &false_target);
+    codegen_->MoveLocation(locations->Out(), locations->InAt(1), select->GetType());
+    __ Bind(&false_target);
+  }
+}
+
+void LocationsBuilderRISCV64::VisitNativeDebugInfo(HNativeDebugInfo* info) {
+  new (GetGraph()->GetAllocator()) LocationSummary(info);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNativeDebugInfo(HNativeDebugInfo*) {
+  // MaybeRecordNativeDebugInfo is already called implicitly in CodeGenerator::Compile.
+}
+
+void CodeGeneratorRISCV64::GenerateNop() {
+  __ Nop();
+}
+
+void LocationsBuilderRISCV64::HandleFieldGet(HInstruction* instruction,
+                                            const FieldInfo& field_info) {
+  DataType::Type field_type = field_info.GetFieldType();
+  bool object_field_get_with_read_barrier =
+      kEmitCompilerReadBarrier && (field_type == DataType::Type::kReference);
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction,
+      object_field_get_with_read_barrier
+          ? LocationSummary::kCallOnSlowPath
+          : LocationSummary::kNoCall);
+  if (object_field_get_with_read_barrier && kUseBakerReadBarrier) {
+    locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
+  }
+  locations->SetInAt(0, Location::RequiresRegister());
+  if (DataType::IsFloatingPointType(instruction->GetType())) {
+    locations->SetOut(Location::RequiresFpuRegister());
+  } else {
+    // The output overlaps in the case of an object field get with
+    // read barriers enabled: we do not want the move to overwrite the
+    // object's location, as we need it to emit the read barrier.
+    locations->SetOut(Location::RequiresRegister(),
+                      object_field_get_with_read_barrier
+                          ? Location::kOutputOverlap
+                          : Location::kNoOutputOverlap);
+  }
+  if (object_field_get_with_read_barrier && kUseBakerReadBarrier) {
+    // We need a temporary register for the read barrier marking slow
+    // path in CodeGeneratorRISCV64::GenerateFieldLoadWithBakerReadBarrier.
+    if (!kBakerReadBarrierThunksEnableForFields) {
+      locations->AddTemp(Location::RequiresRegister());
+    }
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::HandleFieldGet(HInstruction* instruction,
+                                                    const FieldInfo& field_info) {
+  DCHECK_EQ(DataType::Size(field_info.GetFieldType()), DataType::Size(instruction->GetType()));
+  DataType::Type type = instruction->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+  Location obj_loc = locations->InAt(0);
+  GpuRegister obj = obj_loc.AsRegister<GpuRegister>();
+  Location dst_loc = locations->Out();
+  LoadOperandType load_type = kLoadUnsignedByte;
+  bool is_volatile = field_info.IsVolatile();
+  uint32_t offset = field_info.GetFieldOffset().Uint32Value();
+  auto null_checker = GetImplicitNullChecker(instruction, codegen_);
+
+  switch (type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+      load_type = kLoadUnsignedByte;
+      break;
+    case DataType::Type::kInt8:
+      load_type = kLoadSignedByte;
+      break;
+    case DataType::Type::kUint16:
+      load_type = kLoadUnsignedHalfword;
+      break;
+    case DataType::Type::kInt16:
+      load_type = kLoadSignedHalfword;
+      break;
+    case DataType::Type::kInt32:
+    case DataType::Type::kFloat32:
+      load_type = kLoadWord;
+      break;
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat64:
+      load_type = kLoadDoubleword;
+      break;
+    case DataType::Type::kReference:
+      load_type = kLoadUnsignedWord;
+      break;
+    case DataType::Type::kUint32:
+    case DataType::Type::kUint64:
+    case DataType::Type::kVoid:
+      LOG(FATAL) << "Unreachable type " << type;
+      UNREACHABLE();
+  }
+  if (!DataType::IsFloatingPointType(type)) {
+    DCHECK(dst_loc.IsRegister());
+    GpuRegister dst = dst_loc.AsRegister<GpuRegister>();
+    if (type == DataType::Type::kReference) {
+      // /* HeapReference<Object> */ dst = *(obj + offset)
+      if (kEmitCompilerReadBarrier && kUseBakerReadBarrier) {
+        Location temp_loc =
+            kBakerReadBarrierThunksEnableForFields ? Location::NoLocation() : locations->GetTemp(0);
+        // Note that a potential implicit null check is handled in this
+        // CodeGeneratorRISCV64::GenerateFieldLoadWithBakerReadBarrier call.
+        codegen_->GenerateFieldLoadWithBakerReadBarrier(instruction,
+                                                        dst_loc,
+                                                        obj,
+                                                        offset,
+                                                        temp_loc,
+                                                        /* needs_null_check= */ true);
+        if (is_volatile) {
+          GenerateMemoryBarrier(MemBarrierKind::kLoadAny);
+        }
+      } else {
+        __ LoadFromOffset(kLoadUnsignedWord, dst, obj, offset, null_checker);
+        if (is_volatile) {
+          GenerateMemoryBarrier(MemBarrierKind::kLoadAny);
+        }
+        // If read barriers are enabled, emit read barriers other than
+        // Baker's using a slow path (and also unpoison the loaded
+        // reference, if heap poisoning is enabled).
+        codegen_->MaybeGenerateReadBarrierSlow(instruction, dst_loc, dst_loc, obj_loc, offset);
+      }
+    } else {
+      __ LoadFromOffset(load_type, dst, obj, offset, null_checker);
+    }
+  } else {
+    DCHECK(dst_loc.IsFpuRegister());
+    FpuRegister dst = dst_loc.AsFpuRegister<FpuRegister>();
+    __ LoadFpuFromOffset(load_type, dst, obj, offset, null_checker);
+  }
+
+  // Memory barriers, in the case of references, are handled in the
+  // previous switch statement.
+  if (is_volatile && (type != DataType::Type::kReference)) {
+    GenerateMemoryBarrier(MemBarrierKind::kLoadAny);
+  }
+}
+
+void LocationsBuilderRISCV64::HandleFieldSet(HInstruction* instruction,
+                                            const FieldInfo& field_info ATTRIBUTE_UNUSED) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  locations->SetInAt(0, Location::RequiresRegister());
+  if (DataType::IsFloatingPointType(instruction->InputAt(1)->GetType())) {
+    locations->SetInAt(1, FpuRegisterOrConstantForStore(instruction->InputAt(1)));
+  } else {
+    locations->SetInAt(1, RegisterOrZeroConstant(instruction->InputAt(1)));
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::HandleFieldSet(HInstruction* instruction,
+                                                    const FieldInfo& field_info,
+                                                    bool value_can_be_null) {
+  DataType::Type type = field_info.GetFieldType();
+  LocationSummary* locations = instruction->GetLocations();
+  GpuRegister obj = locations->InAt(0).AsRegister<GpuRegister>();
+  Location value_location = locations->InAt(1);
+  StoreOperandType store_type = kStoreByte;
+  bool is_volatile = field_info.IsVolatile();
+  uint32_t offset = field_info.GetFieldOffset().Uint32Value();
+  bool needs_write_barrier = CodeGenerator::StoreNeedsWriteBarrier(type, instruction->InputAt(1));
+  auto null_checker = GetImplicitNullChecker(instruction, codegen_);
+
+  switch (type) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      store_type = kStoreByte;
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      store_type = kStoreHalfword;
+      break;
+    case DataType::Type::kInt32:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kReference:
+      store_type = kStoreWord;
+      break;
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat64:
+      store_type = kStoreDoubleword;
+      break;
+    case DataType::Type::kUint32:
+    case DataType::Type::kUint64:
+    case DataType::Type::kVoid:
+      LOG(FATAL) << "Unreachable type " << type;
+      UNREACHABLE();
+  }
+
+  if (is_volatile) {
+    GenerateMemoryBarrier(MemBarrierKind::kAnyStore);
+  }
+
+  if (value_location.IsConstant()) {
+    int64_t value = CodeGenerator::GetInt64ValueOf(value_location.GetConstant());
+    __ StoreConstToOffset(store_type, value, obj, offset, TMP, null_checker);
+  } else {
+    if (!DataType::IsFloatingPointType(type)) {
+      DCHECK(value_location.IsRegister());
+      GpuRegister src = value_location.AsRegister<GpuRegister>();
+      if (kPoisonHeapReferences && needs_write_barrier) {
+        // Note that in the case where `value` is a null reference,
+        // we do not enter this block, as a null reference does not
+        // need poisoning.
+        DCHECK_EQ(type, DataType::Type::kReference);
+        __ PoisonHeapReference(TMP, src);
+        __ StoreToOffset(store_type, TMP, obj, offset, null_checker);
+      } else {
+        __ StoreToOffset(store_type, src, obj, offset, null_checker);
+      }
+    } else {
+      DCHECK(value_location.IsFpuRegister());
+      FpuRegister src = value_location.AsFpuRegister<FpuRegister>();
+      __ StoreFpuToOffset(store_type, src, obj, offset, null_checker);
+    }
+  }
+
+  if (needs_write_barrier) {
+    DCHECK(value_location.IsRegister());
+    GpuRegister src = value_location.AsRegister<GpuRegister>();
+    codegen_->MarkGCCard(obj, src, value_can_be_null);
+  }
+
+  if (is_volatile) {
+    GenerateMemoryBarrier(MemBarrierKind::kAnyAny);
+  }
+}
+
+void LocationsBuilderRISCV64::VisitInstanceFieldGet(HInstanceFieldGet* instruction) {
+  HandleFieldGet(instruction, instruction->GetFieldInfo());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInstanceFieldGet(HInstanceFieldGet* instruction) {
+  HandleFieldGet(instruction, instruction->GetFieldInfo());
+}
+
+void LocationsBuilderRISCV64::VisitInstanceFieldSet(HInstanceFieldSet* instruction) {
+  HandleFieldSet(instruction, instruction->GetFieldInfo());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInstanceFieldSet(HInstanceFieldSet* instruction) {
+  HandleFieldSet(instruction, instruction->GetFieldInfo(), instruction->GetValueCanBeNull());
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateReferenceLoadOneRegister(
+    HInstruction* instruction,
+    Location out,
+    uint32_t offset,
+    Location maybe_temp,
+    ReadBarrierOption read_barrier_option) {
+  GpuRegister out_reg = out.AsRegister<GpuRegister>();
+  if (read_barrier_option == kWithReadBarrier) {
+    CHECK(kEmitCompilerReadBarrier);
+    if (!kUseBakerReadBarrier || !kBakerReadBarrierThunksEnableForFields) {
+      DCHECK(maybe_temp.IsRegister()) << maybe_temp;
+    }
+    if (kUseBakerReadBarrier) {
+      // Load with fast path based Baker's read barrier.
+      // /* HeapReference<Object> */ out = *(out + offset)
+      codegen_->GenerateFieldLoadWithBakerReadBarrier(instruction,
+                                                      out,
+                                                      out_reg,
+                                                      offset,
+                                                      maybe_temp,
+                                                      /* needs_null_check= */ false);
+    } else {
+      // Load with slow path based read barrier.
+      // Save the value of `out` into `maybe_temp` before overwriting it
+      // in the following move operation, as we will need it for the
+      // read barrier below.
+      __ Move(maybe_temp.AsRegister<GpuRegister>(), out_reg);
+      // /* HeapReference<Object> */ out = *(out + offset)
+      __ LoadFromOffset(kLoadUnsignedWord, out_reg, out_reg, offset);
+      codegen_->GenerateReadBarrierSlow(instruction, out, out, maybe_temp, offset);
+    }
+  } else {
+    // Plain load with no read barrier.
+    // /* HeapReference<Object> */ out = *(out + offset)
+    __ LoadFromOffset(kLoadUnsignedWord, out_reg, out_reg, offset);
+    __ MaybeUnpoisonHeapReference(out_reg);
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateReferenceLoadTwoRegisters(
+    HInstruction* instruction,
+    Location out,
+    Location obj,
+    uint32_t offset,
+    Location maybe_temp,
+    ReadBarrierOption read_barrier_option) {
+  GpuRegister out_reg = out.AsRegister<GpuRegister>();
+  GpuRegister obj_reg = obj.AsRegister<GpuRegister>();
+  if (read_barrier_option == kWithReadBarrier) {
+    CHECK(kEmitCompilerReadBarrier);
+    if (kUseBakerReadBarrier) {
+      if (!kBakerReadBarrierThunksEnableForFields) {
+        DCHECK(maybe_temp.IsRegister()) << maybe_temp;
+      }
+      // Load with fast path based Baker's read barrier.
+      // /* HeapReference<Object> */ out = *(obj + offset)
+      codegen_->GenerateFieldLoadWithBakerReadBarrier(instruction,
+                                                      out,
+                                                      obj_reg,
+                                                      offset,
+                                                      maybe_temp,
+                                                      /* needs_null_check= */ false);
+    } else {
+      // Load with slow path based read barrier.
+      // /* HeapReference<Object> */ out = *(obj + offset)
+      __ LoadFromOffset(kLoadUnsignedWord, out_reg, obj_reg, offset);
+      codegen_->GenerateReadBarrierSlow(instruction, out, out, obj, offset);
+    }
+  } else {
+    // Plain load with no read barrier.
+    // /* HeapReference<Object> */ out = *(obj + offset)
+    __ LoadFromOffset(kLoadUnsignedWord, out_reg, obj_reg, offset);
+    __ MaybeUnpoisonHeapReference(out_reg);
+  }
+}
+
+static inline int GetBakerMarkThunkNumber(GpuRegister reg) {
+  static_assert(BAKER_MARK_INTROSPECTION_REGISTER_COUNT == 21, "Expecting equal");
+  if (reg >= T0 && reg <= S0) {  // 13 consequtive regs.
+    return reg - T0;
+  } else if (reg >= A0 && reg <= S10) {  // 6 consequtive regs.
+    return 4 + (reg - A0);
+  }
+  LOG(FATAL) << "Unexpected register " << reg;
+  UNREACHABLE();
+}
+
+static inline int GetBakerMarkFieldArrayThunkDisplacement(GpuRegister reg, bool short_offset) {
+  int num = GetBakerMarkThunkNumber(reg) +
+      (short_offset ? BAKER_MARK_INTROSPECTION_REGISTER_COUNT : 0);
+  return num * BAKER_MARK_INTROSPECTION_FIELD_ARRAY_ENTRY_SIZE;
+}
+
+static inline int GetBakerMarkGcRootThunkDisplacement(GpuRegister reg) {
+  return GetBakerMarkThunkNumber(reg) * BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRY_SIZE +
+      BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRIES_OFFSET;
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateGcRootFieldLoad(HInstruction* instruction,
+                                                             Location root,
+                                                             GpuRegister obj,
+                                                             uint32_t offset,
+                                                             ReadBarrierOption read_barrier_option,
+                                                             Riscv64Label* label_low) {
+  if (label_low != nullptr) {
+    DCHECK_EQ(offset, 0x678u);
+  }
+  GpuRegister root_reg = root.AsRegister<GpuRegister>();
+  if (read_barrier_option == kWithReadBarrier) {
+    DCHECK(kEmitCompilerReadBarrier);
+    if (kUseBakerReadBarrier) {
+      // Fast path implementation of art::ReadBarrier::BarrierForRoot when
+      // Baker's read barrier are used:
+      if (kBakerReadBarrierThunksEnableForGcRoots) {
+        // Note that we do not actually check the value of `GetIsGcMarking()`
+        // to decide whether to mark the loaded GC root or not.  Instead, we
+        // load into `temp` (6) the read barrier mark introspection entrypoint.
+        // If `temp` is null, it means that `GetIsGcMarking()` is false, and
+        // vice versa.
+        //
+        // We use thunks for the slow path. That thunk checks the reference
+        // and jumps to the entrypoint if needed.
+        //
+        //     temp = Thread::Current()->pReadBarrierMarkReg00
+        //     // AKA &art_quick_read_barrier_mark_introspection.
+        //     GcRoot<mirror::Object> root = *(obj+offset);  // Original reference load.
+        //     if (temp != nullptr) {
+        //        temp = &gc_root_thunk<root_reg>
+        //        root = temp(root)
+        //     }
+
+        const int32_t entry_point_offset =
+            Thread::ReadBarrierMarkEntryPointsOffset<kRiscv64PointerSize>(0);
+        const int thunk_disp = GetBakerMarkGcRootThunkDisplacement(root_reg);
+        int16_t offset_low = Low16Bits(offset);
+        int16_t offset_high = High16Bits(offset - offset_low);  // Accounts for sign
+                                                                // extension in lwu.
+        bool short_offset = IsInt<16>(static_cast<int32_t>(offset));
+        GpuRegister base = short_offset ? obj : TMP;
+        // Loading the entrypoint does not require a load acquire since it is only changed when
+        // threads are suspended or running a checkpoint.
+        __ LoadFromOffset(kLoadDoubleword, T6, TR, entry_point_offset);
+        if (!short_offset) {
+          DCHECK(!label_low);
+          __ Daui(base, obj, offset_high);
+        }
+        // /* GcRoot<mirror::Object> */ root = *(obj + offset)
+        __ LoadFromOffset(kLoadUnsignedWord, root_reg, base, offset_low);  // Single instruction
+                                                                           // Shouldn't be in delay slot.
+        Riscv64Label skip_call;
+        __ Beqzc(T6, &skip_call, /* is_bare= */ true);
+        if (label_low != nullptr) {
+          DCHECK(short_offset);
+          __ Bind(label_low);
+        }
+        __ Nop();  // Just for safety. Separate 2 jump instructions
+        __ Jialc(T6, thunk_disp);
+        __ Bind(&skip_call);
+      } else {
+        // Note that we do not actually check the value of `GetIsGcMarking()`
+        // to decide whether to mark the loaded GC root or not.  Instead, we
+        // load into `temp` (T6) the read barrier mark entry point corresponding
+        // to register `root`. If `temp` is null, it means that `GetIsGcMarking()`
+        // is false, and vice versa.
+        //
+        //     GcRoot<mirror::Object> root = *(obj+offset);  // Original reference load.
+        //     temp = Thread::Current()->pReadBarrierMarkReg ## root.reg()
+        //     if (temp != null) {
+        //       root = temp(root)
+        //     }
+
+        if (label_low != nullptr) {
+          __ Bind(label_low);
+        }
+        // /* GcRoot<mirror::Object> */ root = *(obj + offset)
+        __ LoadFromOffset(kLoadUnsignedWord, root_reg, obj, offset);
+        static_assert(
+            sizeof(mirror::CompressedReference<mirror::Object>) == sizeof(GcRoot<mirror::Object>),
+            "art::mirror::CompressedReference<mirror::Object> and art::GcRoot<mirror::Object> "
+            "have different sizes.");
+        static_assert(sizeof(mirror::CompressedReference<mirror::Object>) == sizeof(int32_t),
+                      "art::mirror::CompressedReference<mirror::Object> and int32_t "
+                      "have different sizes.");
+
+        // Slow path marking the GC root `root`.
+        Location temp = Location::RegisterLocation(T6);
+        SlowPathCodeRISCV64* slow_path =
+            new (codegen_->GetScopedAllocator()) ReadBarrierMarkSlowPathRISCV64(
+                instruction,
+                root,
+                /*entrypoint*/ temp);
+        codegen_->AddSlowPath(slow_path);
+
+        const int32_t entry_point_offset =
+            Thread::ReadBarrierMarkEntryPointsOffset<kRiscv64PointerSize>(root.reg() - 1);
+        // Loading the entrypoint does not require a load acquire since it is only changed when
+        // threads are suspended or running a checkpoint.
+        __ LoadFromOffset(kLoadDoubleword, temp.AsRegister<GpuRegister>(), TR, entry_point_offset);
+        __ Bnezc(temp.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+        __ Bind(slow_path->GetExitLabel());
+      }
+    } else {
+      if (label_low != nullptr) {
+        __ Bind(label_low);
+      }
+      // GC root loaded through a slow path for read barriers other
+      // than Baker's.
+      // /* GcRoot<mirror::Object>* */ root = obj + offset
+      __ Daddiu64(root_reg, obj, static_cast<int32_t>(offset));
+      // /* mirror::Object* */ root = root->Read()
+      codegen_->GenerateReadBarrierForRootSlow(instruction, root, root);
+    }
+  } else {
+    if (label_low != nullptr) {
+      __ Bind(label_low);
+    }
+    // Plain GC root load with no read barrier.
+    // /* GcRoot<mirror::Object> */ root = *(obj + offset)
+    __ LoadFromOffset(kLoadUnsignedWord, root_reg, obj, offset);
+    // Note that GC roots are not affected by heap poisoning, thus we
+    // do not have to unpoison `root_reg` here.
+  }
+}
+
+void CodeGeneratorRISCV64::GenerateFieldLoadWithBakerReadBarrier(HInstruction* instruction,
+                                                                Location ref,
+                                                                GpuRegister obj,
+                                                                uint32_t offset,
+                                                                Location temp,
+                                                                bool needs_null_check) {
+  DCHECK(kEmitCompilerReadBarrier);
+  DCHECK(kUseBakerReadBarrier);
+
+  if (kBakerReadBarrierThunksEnableForFields) {
+    // Note that we do not actually check the value of `GetIsGcMarking()`
+    // to decide whether to mark the loaded reference or not.  Instead, we
+    // load into `temp` (T6) the read barrier mark introspection entrypoint.
+    // If `temp` is null, it means that `GetIsGcMarking()` is false, and
+    // vice versa.
+    //
+    // We use thunks for the slow path. That thunk checks the reference
+    // and jumps to the entrypoint if needed. If the holder is not gray,
+    // it issues a load-load memory barrier and returns to the original
+    // reference load.
+    //
+    //     temp = Thread::Current()->pReadBarrierMarkReg00
+    //     // AKA &art_quick_read_barrier_mark_introspection.
+    //     if (temp != nullptr) {
+    //        temp = &field_array_thunk<holder_reg>
+    //        temp()
+    //     }
+    //   not_gray_return_address:
+    //     // If the offset is too large to fit into the lw instruction, we
+    //     // use an adjusted base register (TMP) here. This register
+    //     // receives bits 16 ... 31 of the offset before the thunk invocation
+    //     // and the thunk benefits from it.
+    //     HeapReference<mirror::Object> reference = *(obj+offset);  // Original reference load.
+    //   gray_return_address:
+
+    DCHECK(temp.IsInvalid());
+    bool short_offset = IsInt<16>(static_cast<int32_t>(offset));
+    const int32_t entry_point_offset =
+        Thread::ReadBarrierMarkEntryPointsOffset<kRiscv64PointerSize>(0);
+    // There may have or may have not been a null check if the field offset is smaller than
+    // the page size.
+    // There must've been a null check in case it's actually a load from an array.
+    // We will, however, perform an explicit null check in the thunk as it's easier to
+    // do it than not.
+    if (instruction->IsArrayGet()) {
+      DCHECK(!needs_null_check);
+    }
+    const int thunk_disp = GetBakerMarkFieldArrayThunkDisplacement(obj, short_offset);
+    // Loading the entrypoint does not require a load acquire since it is only changed when
+    // threads are suspended or running a checkpoint.
+    __ LoadFromOffset(kLoadDoubleword, T6, TR, entry_point_offset);
+    GpuRegister ref_reg = ref.AsRegister<GpuRegister>();
+    Riscv64Label skip_call;
+    if (short_offset) {
+      __ Beqzc(T6, &skip_call, /* is_bare= */ true);
+      __ Nop();  // In forbidden slot.
+      __ Jialc(T6, thunk_disp);
+      __ Bind(&skip_call);
+      // /* HeapReference<Object> */ ref = *(obj + offset)
+      __ LoadFromOffset(kLoadUnsignedWord, ref_reg, obj, offset);  // Single instruction.
+    } else {
+      int16_t offset_low = Low16Bits(offset);
+      int16_t offset_high = High16Bits(offset - offset_low);  // Accounts for sign extension in lwu.
+      __ Daui(TMP, obj, offset_high);  // Shouldn't In delay slot.
+      __ Beqzc(T6, &skip_call, /* is_bare= */ true);
+      __ Nop();  // Just for safety. Separate 2 jump instructions
+      __ Jialc(T6, thunk_disp);
+      __ Bind(&skip_call);
+      // /* HeapReference<Object> */ ref = *(obj + offset)
+      __ LoadFromOffset(kLoadUnsignedWord, ref_reg, TMP, offset_low);  // Single instruction.
+    }
+    if (needs_null_check) {
+      MaybeRecordImplicitNullCheck(instruction);
+    }
+    __ MaybeUnpoisonHeapReference(ref_reg);
+    return;
+  }
+
+  // /* HeapReference<Object> */ ref = *(obj + offset)
+  Location no_index = Location::NoLocation();
+  ScaleFactor no_scale_factor = TIMES_1;
+  GenerateReferenceLoadWithBakerReadBarrier(instruction,
+                                            ref,
+                                            obj,
+                                            offset,
+                                            no_index,
+                                            no_scale_factor,
+                                            temp,
+                                            needs_null_check);
+}
+
+void CodeGeneratorRISCV64::GenerateArrayLoadWithBakerReadBarrier(HInstruction* instruction,
+                                                                Location ref,
+                                                                GpuRegister obj,
+                                                                uint32_t data_offset,
+                                                                Location index,
+                                                                Location temp,
+                                                                bool needs_null_check) {
+  DCHECK(kEmitCompilerReadBarrier);
+  DCHECK(kUseBakerReadBarrier);
+
+  static_assert(
+      sizeof(mirror::HeapReference<mirror::Object>) == sizeof(int32_t),
+      "art::mirror::HeapReference<art::mirror::Object> and int32_t have different sizes.");
+  ScaleFactor scale_factor = TIMES_4;
+
+  if (kBakerReadBarrierThunksEnableForArrays) {
+    // Note that we do not actually check the value of `GetIsGcMarking()`
+    // to decide whether to mark the loaded reference or not.  Instead, we
+    // load into `temp` (T6) the read barrier mark introspection entrypoint.
+    // If `temp` is null, it means that `GetIsGcMarking()` is false, and
+    // vice versa.
+    //
+    // We use thunks for the slow path. That thunk checks the reference
+    // and jumps to the entrypoint if needed. If the holder is not gray,
+    // it issues a load-load memory barrier and returns to the original
+    // reference load.
+    //
+    //     temp = Thread::Current()->pReadBarrierMarkReg00
+    //     // AKA &art_quick_read_barrier_mark_introspection.
+    //     if (temp != nullptr) {
+    //        temp = &field_array_thunk<holder_reg>
+    //        temp()
+    //     }
+    //   not_gray_return_address:
+    //     // The element address is pre-calculated in the TMP register before the
+    //     // thunk invocation and the thunk benefits from it.
+    //     HeapReference<mirror::Object> reference = data[index];  // Original reference load.
+    //   gray_return_address:
+
+    DCHECK(temp.IsInvalid());
+    DCHECK(index.IsValid());
+    const int32_t entry_point_offset =
+        Thread::ReadBarrierMarkEntryPointsOffset<kRiscv64PointerSize>(0);
+    // We will not do the explicit null check in the thunk as some form of a null check
+    // must've been done earlier.
+    DCHECK(!needs_null_check);
+    const int thunk_disp = GetBakerMarkFieldArrayThunkDisplacement(obj, /* short_offset= */ false);
+    // Loading the entrypoint does not require a load acquire since it is only changed when
+    // threads are suspended or running a checkpoint.
+    __ LoadFromOffset(kLoadDoubleword, T6, TR, entry_point_offset);
+    Riscv64Label skip_call;
+    GpuRegister ref_reg = ref.AsRegister<GpuRegister>();
+    GpuRegister index_reg = index.AsRegister<GpuRegister>();
+    __ Dlsa(TMP, index_reg, obj, scale_factor);  // Shouldn't be In delay slot.
+    __ Beqzc(T6, &skip_call, /* is_bare= */ true);
+
+    __ Nop();   // Just for safety. Separate 2 jump instructions
+    __ Jialc(T6, thunk_disp);
+    __ Bind(&skip_call);
+    // /* HeapReference<Object> */ ref = *(obj + data_offset + (index << scale_factor))
+    DCHECK(IsInt<16>(static_cast<int32_t>(data_offset))) << data_offset;
+    __ LoadFromOffset(kLoadUnsignedWord, ref_reg, TMP, data_offset);  // Single instruction.
+    __ MaybeUnpoisonHeapReference(ref_reg);
+    return;
+  }
+
+  // /* HeapReference<Object> */ ref =
+  //     *(obj + data_offset + index * sizeof(HeapReference<Object>))
+  GenerateReferenceLoadWithBakerReadBarrier(instruction,
+                                            ref,
+                                            obj,
+                                            data_offset,
+                                            index,
+                                            scale_factor,
+                                            temp,
+                                            needs_null_check);
+}
+
+void CodeGeneratorRISCV64::GenerateReferenceLoadWithBakerReadBarrier(HInstruction* instruction,
+                                                                    Location ref,
+                                                                    GpuRegister obj,
+                                                                    uint32_t offset,
+                                                                    Location index,
+                                                                    ScaleFactor scale_factor,
+                                                                    Location temp,
+                                                                    bool needs_null_check,
+                                                                    bool always_update_field) {
+  DCHECK(kEmitCompilerReadBarrier);
+  DCHECK(kUseBakerReadBarrier);
+
+  // In slow path based read barriers, the read barrier call is
+  // inserted after the original load. However, in fast path based
+  // Baker's read barriers, we need to perform the load of
+  // mirror::Object::monitor_ *before* the original reference load.
+  // This load-load ordering is required by the read barrier.
+  // The fast path/slow path (for Baker's algorithm) should look like:
+  //
+  //   uint32_t rb_state = Lockword(obj->monitor_).ReadBarrierState();
+  //   lfence;  // Load fence or artificial data dependency to prevent load-load reordering
+  //   HeapReference<Object> ref = *src;  // Original reference load.
+  //   bool is_gray = (rb_state == ReadBarrier::GrayState());
+  //   if (is_gray) {
+  //     ref = ReadBarrier::Mark(ref);  // Performed by runtime entrypoint slow path.
+  //   }
+  //
+  // Note: the original implementation in ReadBarrier::Barrier is
+  // slightly more complex as it performs additional checks that we do
+  // not do here for performance reasons.
+
+  GpuRegister ref_reg = ref.AsRegister<GpuRegister>();
+  GpuRegister temp_reg = temp.AsRegister<GpuRegister>();
+  uint32_t monitor_offset = mirror::Object::MonitorOffset().Int32Value();
+
+  // /* int32_t */ monitor = obj->monitor_
+  __ LoadFromOffset(kLoadWord, temp_reg, obj, monitor_offset);
+  if (needs_null_check) {
+    MaybeRecordImplicitNullCheck(instruction);
+  }
+  // /* LockWord */ lock_word = LockWord(monitor)
+  static_assert(sizeof(LockWord) == sizeof(int32_t),
+                "art::LockWord and int32_t have different sizes.");
+
+  __ Sync(0);  // Barrier to prevent load-load reordering.
+
+  // The actual reference load.
+  if (index.IsValid()) {
+    // Load types involving an "index": ArrayGet,
+    // UnsafeGetObject/UnsafeGetObjectVolatile and UnsafeCASObject
+    // intrinsics.
+    // /* HeapReference<Object> */ ref = *(obj + offset + (index << scale_factor))
+    if (index.IsConstant()) {
+      size_t computed_offset =
+          (index.GetConstant()->AsIntConstant()->GetValue() << scale_factor) + offset;
+      __ LoadFromOffset(kLoadUnsignedWord, ref_reg, obj, computed_offset);
+    } else {
+      GpuRegister index_reg = index.AsRegister<GpuRegister>();
+      if (scale_factor == TIMES_1) {
+        __ Daddu(TMP, index_reg, obj);
+      } else {
+        __ Dlsa(TMP, index_reg, obj, scale_factor);
+      }
+      __ LoadFromOffset(kLoadUnsignedWord, ref_reg, TMP, offset);
+    }
+  } else {
+    // /* HeapReference<Object> */ ref = *(obj + offset)
+    __ LoadFromOffset(kLoadUnsignedWord, ref_reg, obj, offset);
+  }
+
+  // Object* ref = ref_addr->AsMirrorPtr()
+  __ MaybeUnpoisonHeapReference(ref_reg);
+
+  // Slow path marking the object `ref` when it is gray.
+  SlowPathCodeRISCV64* slow_path;
+  if (always_update_field) {
+    // ReadBarrierMarkAndUpdateFieldSlowPathRISCV64 only supports address
+    // of the form `obj + field_offset`, where `obj` is a register and
+    // `field_offset` is a register. Thus `offset` and `scale_factor`
+    // above are expected to be null in this code path.
+    DCHECK_EQ(offset, 0u);
+    DCHECK_EQ(scale_factor, ScaleFactor::TIMES_1);
+    slow_path = new (GetScopedAllocator())
+        ReadBarrierMarkAndUpdateFieldSlowPathRISCV64(instruction,
+                                                    ref,
+                                                    obj,
+                                                    /* field_offset= */ index,
+                                                    temp_reg);
+  } else {
+    slow_path = new (GetScopedAllocator()) ReadBarrierMarkSlowPathRISCV64(instruction, ref);
+  }
+  AddSlowPath(slow_path);
+
+  // if (rb_state == ReadBarrier::GrayState())
+  //   ref = ReadBarrier::Mark(ref);
+  // Given the numeric representation, it's enough to check the low bit of the
+  // rb_state. We do that by shifting the bit into the sign bit (31) and
+  // performing a branch on less than zero.
+  static_assert(ReadBarrier::NonGrayState() == 0, "Expecting non-gray to have value 0");
+  static_assert(ReadBarrier::GrayState() == 1, "Expecting gray to have value 1");
+  static_assert(LockWord::kReadBarrierStateSize == 1, "Expecting 1-bit read barrier state size");
+  __ Sll(temp_reg, temp_reg, 31 - LockWord::kReadBarrierStateShift);
+  __ Bltzc(temp_reg, slow_path->GetEntryLabel());
+  __ Bind(slow_path->GetExitLabel());
+}
+
+void CodeGeneratorRISCV64::GenerateReadBarrierSlow(HInstruction* instruction,
+                                                  Location out,
+                                                  Location ref,
+                                                  Location obj,
+                                                  uint32_t offset,
+                                                  Location index) {
+  DCHECK(kEmitCompilerReadBarrier);
+
+  // Insert a slow path based read barrier *after* the reference load.
+  //
+  // If heap poisoning is enabled, the unpoisoning of the loaded
+  // reference will be carried out by the runtime within the slow
+  // path.
+  //
+  // Note that `ref` currently does not get unpoisoned (when heap
+  // poisoning is enabled), which is alright as the `ref` argument is
+  // not used by the artReadBarrierSlow entry point.
+  //
+  // TODO: Unpoison `ref` when it is used by artReadBarrierSlow.
+  SlowPathCodeRISCV64* slow_path = new (GetScopedAllocator())
+      ReadBarrierForHeapReferenceSlowPathRISCV64(instruction, out, ref, obj, offset, index);
+  AddSlowPath(slow_path);
+
+  __ Bc(slow_path->GetEntryLabel());
+  __ Bind(slow_path->GetExitLabel());
+}
+
+void CodeGeneratorRISCV64::MaybeGenerateReadBarrierSlow(HInstruction* instruction,
+                                                       Location out,
+                                                       Location ref,
+                                                       Location obj,
+                                                       uint32_t offset,
+                                                       Location index) {
+  if (kEmitCompilerReadBarrier) {
+    // Baker's read barriers shall be handled by the fast path
+    // (CodeGeneratorRISCV64::GenerateReferenceLoadWithBakerReadBarrier).
+    DCHECK(!kUseBakerReadBarrier);
+    // If heap poisoning is enabled, unpoisoning will be taken care of
+    // by the runtime within the slow path.
+    GenerateReadBarrierSlow(instruction, out, ref, obj, offset, index);
+  } else if (kPoisonHeapReferences) {
+    __ UnpoisonHeapReference(out.AsRegister<GpuRegister>());
+  }
+}
+
+void CodeGeneratorRISCV64::GenerateReadBarrierForRootSlow(HInstruction* instruction,
+                                                         Location out,
+                                                         Location root) {
+  DCHECK(kEmitCompilerReadBarrier);
+
+  // Insert a slow path based read barrier *after* the GC root load.
+  //
+  // Note that GC roots are not affected by heap poisoning, so we do
+  // not need to do anything special for this here.
+  SlowPathCodeRISCV64* slow_path =
+      new (GetScopedAllocator()) ReadBarrierForRootSlowPathRISCV64(instruction, out, root);
+  AddSlowPath(slow_path);
+
+  __ Bc(slow_path->GetEntryLabel());
+  __ Bind(slow_path->GetExitLabel());
+}
+
+void LocationsBuilderRISCV64::VisitInstanceOf(HInstanceOf* instruction) {
+  LocationSummary::CallKind call_kind = LocationSummary::kNoCall;
+  TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
+  bool baker_read_barrier_slow_path = false;
+  switch (type_check_kind) {
+    case TypeCheckKind::kExactCheck:
+    case TypeCheckKind::kAbstractClassCheck:
+    case TypeCheckKind::kClassHierarchyCheck:
+    case TypeCheckKind::kArrayObjectCheck: {
+      bool needs_read_barrier = CodeGenerator::InstanceOfNeedsReadBarrier(instruction);
+      call_kind = needs_read_barrier ? LocationSummary::kCallOnSlowPath : LocationSummary::kNoCall;
+      baker_read_barrier_slow_path = kUseBakerReadBarrier && needs_read_barrier;
+      break;
+    }
+    case TypeCheckKind::kArrayCheck:
+    case TypeCheckKind::kUnresolvedCheck:
+    case TypeCheckKind::kInterfaceCheck:
+      call_kind = LocationSummary::kCallOnSlowPath;
+      break;
+    case TypeCheckKind::kBitstringCheck:
+      break;
+  }
+
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(instruction, call_kind);
+  if (baker_read_barrier_slow_path) {
+    locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
+  }
+  locations->SetInAt(0, Location::RequiresRegister());
+  if (type_check_kind == TypeCheckKind::kBitstringCheck) {
+    locations->SetInAt(1, Location::ConstantLocation(instruction->InputAt(1)->AsConstant()));
+    locations->SetInAt(2, Location::ConstantLocation(instruction->InputAt(2)->AsConstant()));
+    locations->SetInAt(3, Location::ConstantLocation(instruction->InputAt(3)->AsConstant()));
+  } else {
+    locations->SetInAt(1, Location::RequiresRegister());
+  }
+  // The output does overlap inputs.
+  // Note that TypeCheckSlowPathRISCV64 uses this register too.
+  locations->SetOut(Location::RequiresRegister(), Location::kOutputOverlap);
+  locations->AddRegisterTemps(NumberOfInstanceOfTemps(type_check_kind));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInstanceOf(HInstanceOf* instruction) {
+  TypeCheckKind type_check_kind = instruction->GetTypeCheckKind();
+  LocationSummary* locations = instruction->GetLocations();
+  Location obj_loc = locations->InAt(0);
+  GpuRegister obj = obj_loc.AsRegister<GpuRegister>();
+  Location cls = locations->InAt(1);
+  Location out_loc = locations->Out();
+  GpuRegister out = out_loc.AsRegister<GpuRegister>();
+  const size_t num_temps = NumberOfInstanceOfTemps(type_check_kind);
+  DCHECK_LE(num_temps, 1u);
+  Location maybe_temp_loc = (num_temps >= 1) ? locations->GetTemp(0) : Location::NoLocation();
+  uint32_t class_offset = mirror::Object::ClassOffset().Int32Value();
+  uint32_t super_offset = mirror::Class::SuperClassOffset().Int32Value();
+  uint32_t component_offset = mirror::Class::ComponentTypeOffset().Int32Value();
+  uint32_t primitive_offset = mirror::Class::PrimitiveTypeOffset().Int32Value();
+  Riscv64Label done;
+  SlowPathCodeRISCV64* slow_path = nullptr;
+
+  // Return 0 if `obj` is null.
+  // Avoid this check if we know `obj` is not null.
+  if (instruction->MustDoNullCheck()) {
+    __ Move(out, ZERO);
+    __ Beqzc(obj, &done);
+  }
+
+  switch (type_check_kind) {
+    case TypeCheckKind::kExactCheck: {
+      ReadBarrierOption read_barrier_option =
+          CodeGenerator::ReadBarrierOptionForInstanceOf(instruction);
+      // /* HeapReference<Class> */ out = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        out_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp_loc,
+                                        read_barrier_option);
+      // Classes must be equal for the instanceof to succeed.
+      __ Xor(out, out, cls.AsRegister<GpuRegister>());
+      __ Sltiu(out, out, 1);
+      break;
+    }
+
+    case TypeCheckKind::kAbstractClassCheck: {
+      ReadBarrierOption read_barrier_option =
+          CodeGenerator::ReadBarrierOptionForInstanceOf(instruction);
+      // /* HeapReference<Class> */ out = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        out_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp_loc,
+                                        read_barrier_option);
+      // If the class is abstract, we eagerly fetch the super class of the
+      // object to avoid doing a comparison we know will fail.
+      Riscv64Label loop;
+      __ Bind(&loop);
+      // /* HeapReference<Class> */ out = out->super_class_
+      GenerateReferenceLoadOneRegister(instruction,
+                                       out_loc,
+                                       super_offset,
+                                       maybe_temp_loc,
+                                       read_barrier_option);
+      // If `out` is null, we use it for the result, and jump to `done`.
+      __ Beqzc(out, &done);
+      __ Bnec(out, cls.AsRegister<GpuRegister>(), &loop);
+      __ LoadConst32(out, 1);
+      break;
+    }
+
+    case TypeCheckKind::kClassHierarchyCheck: {
+      ReadBarrierOption read_barrier_option =
+          CodeGenerator::ReadBarrierOptionForInstanceOf(instruction);
+      // /* HeapReference<Class> */ out = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        out_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp_loc,
+                                        read_barrier_option);
+      // Walk over the class hierarchy to find a match.
+      Riscv64Label loop, success;
+      __ Bind(&loop);
+      __ Beqc(out, cls.AsRegister<GpuRegister>(), &success);
+      // /* HeapReference<Class> */ out = out->super_class_
+      GenerateReferenceLoadOneRegister(instruction,
+                                       out_loc,
+                                       super_offset,
+                                       maybe_temp_loc,
+                                       read_barrier_option);
+      __ Bnezc(out, &loop);
+      // If `out` is null, we use it for the result, and jump to `done`.
+      __ Bc(&done);
+      __ Bind(&success);
+      __ LoadConst32(out, 1);
+      break;
+    }
+
+    case TypeCheckKind::kArrayObjectCheck: {
+      ReadBarrierOption read_barrier_option =
+          CodeGenerator::ReadBarrierOptionForInstanceOf(instruction);
+      // /* HeapReference<Class> */ out = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        out_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp_loc,
+                                        read_barrier_option);
+      // Do an exact check.
+      Riscv64Label success;
+      __ Beqc(out, cls.AsRegister<GpuRegister>(), &success);
+      // Otherwise, we need to check that the object's class is a non-primitive array.
+      // /* HeapReference<Class> */ out = out->component_type_
+      GenerateReferenceLoadOneRegister(instruction,
+                                       out_loc,
+                                       component_offset,
+                                       maybe_temp_loc,
+                                       read_barrier_option);
+      // If `out` is null, we use it for the result, and jump to `done`.
+      __ Beqzc(out, &done);
+      __ LoadFromOffset(kLoadUnsignedHalfword, out, out, primitive_offset);
+      static_assert(Primitive::kPrimNot == 0, "Expected 0 for kPrimNot");
+      __ Sltiu(out, out, 1);
+      __ Bc(&done);
+      __ Bind(&success);
+      __ LoadConst32(out, 1);
+      break;
+    }
+
+    case TypeCheckKind::kArrayCheck: {
+      // No read barrier since the slow path will retry upon failure.
+      // /* HeapReference<Class> */ out = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        out_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp_loc,
+                                        kWithoutReadBarrier);
+      DCHECK(locations->OnlyCallsOnSlowPath());
+      slow_path = new (codegen_->GetScopedAllocator()) TypeCheckSlowPathRISCV64(
+          instruction, /* is_fatal= */ false);
+      codegen_->AddSlowPath(slow_path);
+      __ Bnec(out, cls.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+      __ LoadConst32(out, 1);
+      break;
+    }
+
+    case TypeCheckKind::kUnresolvedCheck:
+    case TypeCheckKind::kInterfaceCheck: {
+      // Note that we indeed only call on slow path, but we always go
+      // into the slow path for the unresolved and interface check
+      // cases.
+      //
+      // We cannot directly call the InstanceofNonTrivial runtime
+      // entry point without resorting to a type checking slow path
+      // here (i.e. by calling InvokeRuntime directly), as it would
+      // require to assign fixed registers for the inputs of this
+      // HInstanceOf instruction (following the runtime calling
+      // convention), which might be cluttered by the potential first
+      // read barrier emission at the beginning of this method.
+      //
+      // TODO: Introduce a new runtime entry point taking the object
+      // to test (instead of its class) as argument, and let it deal
+      // with the read barrier issues. This will let us refactor this
+      // case of the `switch` code as it was previously (with a direct
+      // call to the runtime not using a type checking slow path).
+      // This should also be beneficial for the other cases above.
+      DCHECK(locations->OnlyCallsOnSlowPath());
+      slow_path = new (codegen_->GetScopedAllocator()) TypeCheckSlowPathRISCV64(
+          instruction, /* is_fatal= */ false);
+      codegen_->AddSlowPath(slow_path);
+      __ Bc(slow_path->GetEntryLabel());
+      break;
+    }
+
+    case TypeCheckKind::kBitstringCheck: {
+      // /* HeapReference<Class> */ temp = obj->klass_
+      GenerateReferenceLoadTwoRegisters(instruction,
+                                        out_loc,
+                                        obj_loc,
+                                        class_offset,
+                                        maybe_temp_loc,
+                                        kWithoutReadBarrier);
+
+      GenerateBitstringTypeCheckCompare(instruction, out);
+      __ Sltiu(out, out, 1);
+      break;
+    }
+  }
+
+  __ Bind(&done);
+
+  if (slow_path != nullptr) {
+    __ Bind(slow_path->GetExitLabel());
+  }
+}
+
+void LocationsBuilderRISCV64::VisitIntConstant(HIntConstant* constant) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(constant);
+  locations->SetOut(Location::ConstantLocation(constant));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitIntConstant(HIntConstant* constant ATTRIBUTE_UNUSED) {
+  // Will be generated at use site.
+}
+
+void LocationsBuilderRISCV64::VisitNullConstant(HNullConstant* constant) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(constant);
+  locations->SetOut(Location::ConstantLocation(constant));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNullConstant(HNullConstant* constant ATTRIBUTE_UNUSED) {
+  // Will be generated at use site.
+}
+
+void LocationsBuilderRISCV64::VisitInvokeUnresolved(HInvokeUnresolved* invoke) {
+  // The trampoline uses the same calling convention as dex calling conventions,
+  // except instead of loading arg0/r0 with the target Method*, arg0/r0 will contain
+  // the method_idx.
+  HandleInvoke(invoke);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInvokeUnresolved(HInvokeUnresolved* invoke) {
+  codegen_->GenerateInvokeUnresolvedRuntimeCall(invoke);
+}
+
+void LocationsBuilderRISCV64::HandleInvoke(HInvoke* invoke) {
+  InvokeDexCallingConventionVisitorRISCV64 calling_convention_visitor;
+  CodeGenerator::CreateCommonInvokeLocationSummary(invoke, &calling_convention_visitor);
+}
+
+void LocationsBuilderRISCV64::VisitInvokeInterface(HInvokeInterface* invoke) {
+  HandleInvoke(invoke);
+  // The register T0 is required to be used for the hidden argument in
+  // art_quick_imt_conflict_trampoline, so add the hidden argument.
+  invoke->GetLocations()->AddTemp(Location::RegisterLocation(T0));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInvokeInterface(HInvokeInterface* invoke) {
+  // TODO: b/18116999, our IMTs can miss an IncompatibleClassChangeError.
+  GpuRegister temp = invoke->GetLocations()->GetTemp(0).AsRegister<GpuRegister>();
+  Location receiver = invoke->GetLocations()->InAt(0);
+  uint32_t class_offset = mirror::Object::ClassOffset().Int32Value();
+  Offset entry_point = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kRiscv64PointerSize);
+
+  // Set the hidden argument.
+  __ LoadConst32(invoke->GetLocations()->GetTemp(1).AsRegister<GpuRegister>(),
+                 invoke->GetDexMethodIndex());
+
+  // temp = object->GetClass();
+  if (receiver.IsStackSlot()) {
+    __ LoadFromOffset(kLoadUnsignedWord, temp, SP, receiver.GetStackIndex());
+    __ LoadFromOffset(kLoadUnsignedWord, temp, temp, class_offset);
+  } else {
+    __ LoadFromOffset(kLoadUnsignedWord, temp, receiver.AsRegister<GpuRegister>(), class_offset);
+  }
+  codegen_->MaybeRecordImplicitNullCheck(invoke);
+  // Instead of simply (possibly) unpoisoning `temp` here, we should
+  // emit a read barrier for the previous class reference load.
+  // However this is not required in practice, as this is an
+  // intermediate/temporary reference and because the current
+  // concurrent copying collector keeps the from-space memory
+  // intact/accessible until the end of the marking phase (the
+  // concurrent copying collector may not in the future).
+  __ MaybeUnpoisonHeapReference(temp);
+  __ LoadFromOffset(kLoadDoubleword, temp, temp,
+      mirror::Class::ImtPtrOffset(kRiscv64PointerSize).Uint32Value());
+  uint32_t method_offset = static_cast<uint32_t>(ImTable::OffsetOfElement(
+      invoke->GetImtIndex(), kRiscv64PointerSize));
+  // temp = temp->GetImtEntryAt(method_offset);
+  __ LoadFromOffset(kLoadDoubleword, temp, temp, method_offset);
+  // T6 = temp->GetEntryPoint();
+  __ LoadFromOffset(kLoadDoubleword, T6, temp, entry_point.Int32Value());
+  // T6();
+  __ Jalr(T6);
+  DCHECK(!codegen_->IsLeafMethod());
+  codegen_->RecordPcInfo(invoke, invoke->GetDexPc());
+}
+
+void LocationsBuilderRISCV64::VisitInvokeVirtual(HInvokeVirtual* invoke) {
+  // FIXME: T-HEAD, Skip Generate Intrinsic code now.
+  #if 0
+  IntrinsicLocationsBuilderRISCV64 intrinsic(codegen_);
+  if (intrinsic.TryDispatch(invoke)) {
+    return;
+  }
+  #endif
+  HandleInvoke(invoke);
+}
+
+void LocationsBuilderRISCV64::VisitInvokeStaticOrDirect(HInvokeStaticOrDirect* invoke) {
+  // Explicit clinit checks triggered by static invokes must have been pruned by
+  // art::PrepareForRegisterAllocation.
+  DCHECK(!invoke->IsStaticWithExplicitClinitCheck());
+  // FIXME: T-HEAD, Skip Generate Intrinsic code now.
+  #if 0
+  IntrinsicLocationsBuilderRISCV64 intrinsic(codegen_);
+  if (intrinsic.TryDispatch(invoke)) {
+    return;
+  }
+  #endif
+  HandleInvoke(invoke);
+}
+
+void LocationsBuilderRISCV64::VisitInvokePolymorphic(HInvokePolymorphic* invoke) {
+  HandleInvoke(invoke);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInvokePolymorphic(HInvokePolymorphic* invoke) {
+  codegen_->GenerateInvokePolymorphicCall(invoke);
+}
+
+void LocationsBuilderRISCV64::VisitInvokeCustom(HInvokeCustom* invoke) {
+  HandleInvoke(invoke);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInvokeCustom(HInvokeCustom* invoke) {
+  codegen_->GenerateInvokeCustomCall(invoke);
+}
+
+static bool TryGenerateIntrinsicCode(HInvoke* invoke, CodeGeneratorRISCV64* codegen) {
+  // FIXME: T-HEAD, Skip Generate Intrinsic code now.
+  #if 0
+  if (invoke->GetLocations()->Intrinsified()) {
+    IntrinsicCodeGeneratorRISCV64 intrinsic(codegen);
+    intrinsic.Dispatch(invoke);
+    return true;
+  }
+  #endif
+  return false;
+}
+
+HLoadString::LoadKind CodeGeneratorRISCV64::GetSupportedLoadStringKind(
+    HLoadString::LoadKind desired_string_load_kind) {
+  bool fallback_load = false;
+  switch (desired_string_load_kind) {
+    case HLoadString::LoadKind::kBootImageLinkTimePcRelative:
+    case HLoadString::LoadKind::kBootImageRelRo:
+    case HLoadString::LoadKind::kBssEntry:
+      DCHECK(!Runtime::Current()->UseJitCompilation());
+      break;
+    case HLoadString::LoadKind::kJitBootImageAddress:
+    case HLoadString::LoadKind::kJitTableAddress:
+      DCHECK(Runtime::Current()->UseJitCompilation());
+      break;
+    case HLoadString::LoadKind::kRuntimeCall:
+      break;
+  }
+  if (fallback_load) {
+    desired_string_load_kind = HLoadString::LoadKind::kRuntimeCall;
+  }
+  return desired_string_load_kind;
+}
+
+HLoadClass::LoadKind CodeGeneratorRISCV64::GetSupportedLoadClassKind(
+    HLoadClass::LoadKind desired_class_load_kind) {
+  bool fallback_load = false;
+  switch (desired_class_load_kind) {
+    case HLoadClass::LoadKind::kInvalid:
+      LOG(FATAL) << "UNREACHABLE";
+      UNREACHABLE();
+    case HLoadClass::LoadKind::kReferrersClass:
+      break;
+    case HLoadClass::LoadKind::kBootImageLinkTimePcRelative:
+    case HLoadClass::LoadKind::kBootImageRelRo:
+    case HLoadClass::LoadKind::kBssEntry:
+      DCHECK(!Runtime::Current()->UseJitCompilation());
+      break;
+    case HLoadClass::LoadKind::kJitBootImageAddress:
+    case HLoadClass::LoadKind::kJitTableAddress:
+      DCHECK(Runtime::Current()->UseJitCompilation());
+      break;
+    case HLoadClass::LoadKind::kRuntimeCall:
+      break;
+  }
+  if (fallback_load) {
+    desired_class_load_kind = HLoadClass::LoadKind::kRuntimeCall;
+  }
+  return desired_class_load_kind;
+}
+
+HInvokeStaticOrDirect::DispatchInfo CodeGeneratorRISCV64::GetSupportedInvokeStaticOrDirectDispatch(
+      const HInvokeStaticOrDirect::DispatchInfo& desired_dispatch_info,
+      ArtMethod* method ATTRIBUTE_UNUSED) {
+  // On RISCV64 we support all dispatch types.
+  return desired_dispatch_info;
+}
+
+void CodeGeneratorRISCV64::GenerateStaticOrDirectCall(
+    HInvokeStaticOrDirect* invoke, Location temp, SlowPathCode* slow_path) {
+  // All registers are assumed to be correctly set up per the calling convention.
+  Location callee_method = temp;  // For all kinds except kRecursive, callee will be in temp.
+  HInvokeStaticOrDirect::MethodLoadKind method_load_kind = invoke->GetMethodLoadKind();
+  HInvokeStaticOrDirect::CodePtrLocation code_ptr_location = invoke->GetCodePtrLocation();
+
+  switch (method_load_kind) {
+    case HInvokeStaticOrDirect::MethodLoadKind::kStringInit: {
+      // temp = thread->string_init_entrypoint
+      uint32_t offset =
+          GetThreadOffset<kRiscv64PointerSize>(invoke->GetStringInitEntryPoint()).Int32Value();
+      __ LoadFromOffset(kLoadDoubleword,
+                        temp.AsRegister<GpuRegister>(),
+                        TR,
+                        offset);
+      break;
+    }
+    case HInvokeStaticOrDirect::MethodLoadKind::kRecursive:
+      callee_method = invoke->GetLocations()->InAt(invoke->GetSpecialInputIndex());
+      break;
+    case HInvokeStaticOrDirect::MethodLoadKind::kBootImageLinkTimePcRelative: {
+      DCHECK(GetCompilerOptions().IsBootImage());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_high =
+          NewBootImageMethodPatch(invoke->GetTargetMethod());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          NewBootImageMethodPatch(invoke->GetTargetMethod(), info_high);
+      EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      __ Addi(temp.AsRegister<GpuRegister>(), AT, /* imm12= */ 0x678);
+      break;
+    }
+    case HInvokeStaticOrDirect::MethodLoadKind::kBootImageRelRo: {
+      uint32_t boot_image_offset = GetBootImageOffset(invoke);
+      PcRelativePatchInfo* info_high = NewBootImageRelRoPatch(boot_image_offset);
+      PcRelativePatchInfo* info_low = NewBootImageRelRoPatch(boot_image_offset, info_high);
+      EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      // Note: Boot image is in the low 4GiB and the entry is 32-bit, so emit a 32-bit load.
+      __ Lwu(temp.AsRegister<GpuRegister>(), AT, /* imm12= */ 0x678);
+      break;
+    }
+    case HInvokeStaticOrDirect::MethodLoadKind::kBssEntry: {
+      PcRelativePatchInfo* info_high = NewMethodBssEntryPatch(
+          MethodReference(&GetGraph()->GetDexFile(), invoke->GetDexMethodIndex()));
+      PcRelativePatchInfo* info_low = NewMethodBssEntryPatch(
+          MethodReference(&GetGraph()->GetDexFile(), invoke->GetDexMethodIndex()), info_high);
+      EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      __ Ld(temp.AsRegister<GpuRegister>(), AT, /* imm12= */ 0x678);
+      break;
+    }
+    case HInvokeStaticOrDirect::MethodLoadKind::kJitDirectAddress:
+      __ LoadLiteral(temp.AsRegister<GpuRegister>(),
+                     kLoadDoubleword,
+                     DeduplicateUint64Literal(invoke->GetMethodAddress()));
+      break;
+    case HInvokeStaticOrDirect::MethodLoadKind::kRuntimeCall: {
+      GenerateInvokeStaticOrDirectRuntimeCall(invoke, temp, slow_path);
+      return;  // No code pointer retrieval; the runtime performs the call directly.
+    }
+  }
+
+  switch (code_ptr_location) {
+    case HInvokeStaticOrDirect::CodePtrLocation::kCallSelf:
+      __ Balc(&frame_entry_label_);
+      break;
+    case HInvokeStaticOrDirect::CodePtrLocation::kCallArtMethod:
+      // T6 = callee_method->entry_point_from_quick_compiled_code_;
+      __ LoadFromOffset(kLoadDoubleword,
+                        T6,
+                        callee_method.AsRegister<GpuRegister>(),
+                        ArtMethod::EntryPointFromQuickCompiledCodeOffset(
+                            kRiscv64PointerSize).Int32Value());
+      // T6()
+      __ Jalr(T6);
+      break;
+  }
+  RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+
+  DCHECK(!IsLeafMethod());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInvokeStaticOrDirect(HInvokeStaticOrDirect* invoke) {
+  // Explicit clinit checks triggered by static invokes must have been pruned by
+  // art::PrepareForRegisterAllocation.
+  DCHECK(!invoke->IsStaticWithExplicitClinitCheck());
+  // FIXME: T-HEAD, Skip Generate Intrinsic code now.
+  #if 0
+  if (TryGenerateIntrinsicCode(invoke, codegen_)) {
+    return;
+  }
+  #endif
+  LocationSummary* locations = invoke->GetLocations();
+  codegen_->GenerateStaticOrDirectCall(invoke,
+                                       locations->HasTemps()
+                                           ? locations->GetTemp(0)
+                                           : Location::NoLocation());
+}
+
+void CodeGeneratorRISCV64::GenerateVirtualCall(
+    HInvokeVirtual* invoke, Location temp_location, SlowPathCode* slow_path) {
+  // Use the calling convention instead of the location of the receiver, as
+  // intrinsics may have put the receiver in a different register. In the intrinsics
+  // slow path, the arguments have been moved to the right place, so here we are
+  // guaranteed that the receiver is the first register of the calling convention.
+  InvokeDexCallingConvention calling_convention;
+  GpuRegister receiver = calling_convention.GetRegisterAt(0);
+
+  GpuRegister temp = temp_location.AsRegister<GpuRegister>();
+  size_t method_offset = mirror::Class::EmbeddedVTableEntryOffset(
+      invoke->GetVTableIndex(), kRiscv64PointerSize).SizeValue();
+  uint32_t class_offset = mirror::Object::ClassOffset().Int32Value();
+  Offset entry_point = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kRiscv64PointerSize);
+
+  // temp = object->GetClass();
+  __ LoadFromOffset(kLoadUnsignedWord, temp, receiver, class_offset);
+  MaybeRecordImplicitNullCheck(invoke);
+  // Instead of simply (possibly) unpoisoning `temp` here, we should
+  // emit a read barrier for the previous class reference load.
+  // However this is not required in practice, as this is an
+  // intermediate/temporary reference and because the current
+  // concurrent copying collector keeps the from-space memory
+  // intact/accessible until the end of the marking phase (the
+  // concurrent copying collector may not in the future).
+  __ MaybeUnpoisonHeapReference(temp);
+  // temp = temp->GetMethodAt(method_offset);
+  __ LoadFromOffset(kLoadDoubleword, temp, temp, method_offset);
+  // T6 = temp->GetEntryPoint();
+  __ LoadFromOffset(kLoadDoubleword, T6, temp, entry_point.Int32Value());
+  // T6();
+  __ Jalr(T6);
+  RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitInvokeVirtual(HInvokeVirtual* invoke) {
+  // FIXME: T-HEAD, Skip Generate Intrinsic code now.
+  #if 0
+  if (TryGenerateIntrinsicCode(invoke, codegen_)) {
+    return;
+  }
+  #endif
+  codegen_->GenerateVirtualCall(invoke, invoke->GetLocations()->GetTemp(0));
+  DCHECK(!codegen_->IsLeafMethod());
+}
+
+void LocationsBuilderRISCV64::VisitLoadClass(HLoadClass* cls) {
+  HLoadClass::LoadKind load_kind = cls->GetLoadKind();
+  if (load_kind == HLoadClass::LoadKind::kRuntimeCall) {
+    InvokeRuntimeCallingConvention calling_convention;
+    Location loc = Location::RegisterLocation(calling_convention.GetRegisterAt(0));
+    CodeGenerator::CreateLoadClassRuntimeCallLocationSummary(cls, loc, loc);
+    return;
+  }
+  DCHECK(!cls->NeedsAccessCheck());
+
+  const bool requires_read_barrier = kEmitCompilerReadBarrier && !cls->IsInBootImage();
+  LocationSummary::CallKind call_kind = (cls->NeedsEnvironment() || requires_read_barrier)
+      ? LocationSummary::kCallOnSlowPath
+      : LocationSummary::kNoCall;
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(cls, call_kind);
+  if (kUseBakerReadBarrier && requires_read_barrier && !cls->NeedsEnvironment()) {
+    locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
+  }
+  if (load_kind == HLoadClass::LoadKind::kReferrersClass) {
+    locations->SetInAt(0, Location::RequiresRegister());
+  }
+  locations->SetOut(Location::RequiresRegister());
+  if (load_kind == HLoadClass::LoadKind::kBssEntry) {
+    if (!kUseReadBarrier || kUseBakerReadBarrier) {
+      // Rely on the type resolution or initialization and marking to save everything we need.
+      locations->SetCustomSlowPathCallerSaves(OneRegInReferenceOutSaveEverythingCallerSaves());
+    } else {
+      // For non-Baker read barriers we have a temp-clobbering call.
+    }
+  }
+}
+
+// NO_THREAD_SAFETY_ANALYSIS as we manipulate handles whose internal object we know does not
+// move.
+void InstructionCodeGeneratorRISCV64::VisitLoadClass(HLoadClass* cls) NO_THREAD_SAFETY_ANALYSIS {
+  HLoadClass::LoadKind load_kind = cls->GetLoadKind();
+  if (load_kind == HLoadClass::LoadKind::kRuntimeCall) {
+    codegen_->GenerateLoadClassRuntimeCall(cls);
+    return;
+  }
+  DCHECK(!cls->NeedsAccessCheck());
+
+  LocationSummary* locations = cls->GetLocations();
+  Location out_loc = locations->Out();
+  GpuRegister out = out_loc.AsRegister<GpuRegister>();
+  GpuRegister current_method_reg = ZERO;
+  if (load_kind == HLoadClass::LoadKind::kReferrersClass ||
+      load_kind == HLoadClass::LoadKind::kRuntimeCall) {
+      current_method_reg = locations->InAt(0).AsRegister<GpuRegister>();
+  }
+
+  const ReadBarrierOption read_barrier_option = cls->IsInBootImage()
+      ? kWithoutReadBarrier
+      : kCompilerReadBarrierOption;
+  bool generate_null_check = false;
+  switch (load_kind) {
+    case HLoadClass::LoadKind::kReferrersClass:
+      DCHECK(!cls->CanCallRuntime());
+      DCHECK(!cls->MustGenerateClinitCheck());
+      // /* GcRoot<mirror::Class> */ out = current_method->declaring_class_
+      GenerateGcRootFieldLoad(cls,
+                              out_loc,
+                              current_method_reg,
+                              ArtMethod::DeclaringClassOffset().Int32Value(),
+                              read_barrier_option);
+      break;
+    case HLoadClass::LoadKind::kBootImageLinkTimePcRelative: {
+      DCHECK(codegen_->GetCompilerOptions().IsBootImage());
+      DCHECK_EQ(read_barrier_option, kWithoutReadBarrier);
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_high =
+          codegen_->NewBootImageTypePatch(cls->GetDexFile(), cls->GetTypeIndex());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          codegen_->NewBootImageTypePatch(cls->GetDexFile(), cls->GetTypeIndex(), info_high);
+      codegen_->EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      __ Addi(out, AT, /* imm12= */ 0x678);
+      break;
+    }
+    case HLoadClass::LoadKind::kBootImageRelRo: {
+      DCHECK(!codegen_->GetCompilerOptions().IsBootImage());
+      uint32_t boot_image_offset = codegen_->GetBootImageOffset(cls);
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_high =
+          codegen_->NewBootImageRelRoPatch(boot_image_offset);
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          codegen_->NewBootImageRelRoPatch(boot_image_offset, info_high);
+      codegen_->EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      __ Lwu(out, AT, /* imm12= */ 0x678);
+      break;
+    }
+    case HLoadClass::LoadKind::kBssEntry: {
+      CodeGeneratorRISCV64::PcRelativePatchInfo* bss_info_high =
+          codegen_->NewTypeBssEntryPatch(cls->GetDexFile(), cls->GetTypeIndex());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          codegen_->NewTypeBssEntryPatch(cls->GetDexFile(), cls->GetTypeIndex(), bss_info_high);
+      codegen_->EmitPcRelativeAddressPlaceholderHigh(bss_info_high, out);
+      GenerateGcRootFieldLoad(cls,
+                              out_loc,
+                              out,
+                              /* offset= */ 0x678,
+                              read_barrier_option,
+                              &info_low->label);
+      generate_null_check = true;
+      break;
+    }
+    case HLoadClass::LoadKind::kJitBootImageAddress: {
+      DCHECK_EQ(read_barrier_option, kWithoutReadBarrier);
+      uint32_t address = reinterpret_cast32<uint32_t>(cls->GetClass().Get());
+      DCHECK_NE(address, 0u);
+      __ LoadLiteral(out,
+                     kLoadUnsignedWord,
+                     codegen_->DeduplicateBootImageAddressLiteral(address));
+      break;
+    }
+    case HLoadClass::LoadKind::kJitTableAddress:
+      __ LoadLiteral(out,
+                     kLoadUnsignedWord,
+                     codegen_->DeduplicateJitClassLiteral(cls->GetDexFile(),
+                                                          cls->GetTypeIndex(),
+                                                          cls->GetClass()));
+      GenerateGcRootFieldLoad(cls, out_loc, out, 0, read_barrier_option);
+      break;
+    case HLoadClass::LoadKind::kRuntimeCall:
+    case HLoadClass::LoadKind::kInvalid:
+      LOG(FATAL) << "UNREACHABLE";
+      UNREACHABLE();
+  }
+
+  if (generate_null_check || cls->MustGenerateClinitCheck()) {
+    DCHECK(cls->CanCallRuntime());
+    SlowPathCodeRISCV64* slow_path =
+        new (codegen_->GetScopedAllocator()) LoadClassSlowPathRISCV64(cls, cls);
+    codegen_->AddSlowPath(slow_path);
+    if (generate_null_check) {
+      __ Beqzc(out, slow_path->GetEntryLabel());
+    }
+    if (cls->MustGenerateClinitCheck()) {
+      GenerateClassInitializationCheck(slow_path, out);
+    } else {
+      __ Bind(slow_path->GetExitLabel());
+    }
+  }
+}
+
+void LocationsBuilderRISCV64::VisitLoadMethodHandle(HLoadMethodHandle* load) {
+  InvokeRuntimeCallingConvention calling_convention;
+  Location loc = Location::RegisterLocation(calling_convention.GetRegisterAt(0));
+  CodeGenerator::CreateLoadMethodHandleRuntimeCallLocationSummary(load, loc, loc);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitLoadMethodHandle(HLoadMethodHandle* load) {
+  codegen_->GenerateLoadMethodHandleRuntimeCall(load);
+}
+
+void LocationsBuilderRISCV64::VisitLoadMethodType(HLoadMethodType* load) {
+  InvokeRuntimeCallingConvention calling_convention;
+  Location loc = Location::RegisterLocation(calling_convention.GetRegisterAt(0));
+  CodeGenerator::CreateLoadMethodTypeRuntimeCallLocationSummary(load, loc, loc);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitLoadMethodType(HLoadMethodType* load) {
+  codegen_->GenerateLoadMethodTypeRuntimeCall(load);
+}
+
+static int32_t GetExceptionTlsOffset() {
+  return Thread::ExceptionOffset<kRiscv64PointerSize>().Int32Value();
+}
+
+void LocationsBuilderRISCV64::VisitLoadException(HLoadException* load) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(load, LocationSummary::kNoCall);
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitLoadException(HLoadException* load) {
+  GpuRegister out = load->GetLocations()->Out().AsRegister<GpuRegister>();
+  __ LoadFromOffset(kLoadUnsignedWord, out, TR, GetExceptionTlsOffset());
+}
+
+void LocationsBuilderRISCV64::VisitClearException(HClearException* clear) {
+  new (GetGraph()->GetAllocator()) LocationSummary(clear, LocationSummary::kNoCall);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitClearException(HClearException* clear ATTRIBUTE_UNUSED) {
+  __ StoreToOffset(kStoreWord, ZERO, TR, GetExceptionTlsOffset());
+}
+
+void LocationsBuilderRISCV64::VisitLoadString(HLoadString* load) {
+  HLoadString::LoadKind load_kind = load->GetLoadKind();
+  LocationSummary::CallKind call_kind = CodeGenerator::GetLoadStringCallKind(load);
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(load, call_kind);
+  if (load_kind == HLoadString::LoadKind::kRuntimeCall) {
+    InvokeRuntimeCallingConvention calling_convention;
+    locations->SetOut(Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  } else {
+    locations->SetOut(Location::RequiresRegister());
+    if (load_kind == HLoadString::LoadKind::kBssEntry) {
+      if (!kUseReadBarrier || kUseBakerReadBarrier) {
+        // Rely on the pResolveString and marking to save everything we need.
+        locations->SetCustomSlowPathCallerSaves(OneRegInReferenceOutSaveEverythingCallerSaves());
+      } else {
+        // For non-Baker read barriers we have a temp-clobbering call.
+      }
+    }
+  }
+}
+
+// NO_THREAD_SAFETY_ANALYSIS as we manipulate handles whose internal object we know does not
+// move.
+void InstructionCodeGeneratorRISCV64::VisitLoadString(HLoadString* load) NO_THREAD_SAFETY_ANALYSIS {
+  HLoadString::LoadKind load_kind = load->GetLoadKind();
+  LocationSummary* locations = load->GetLocations();
+  Location out_loc = locations->Out();
+  GpuRegister out = out_loc.AsRegister<GpuRegister>();
+
+  switch (load_kind) {
+    case HLoadString::LoadKind::kBootImageLinkTimePcRelative: {
+      DCHECK(codegen_->GetCompilerOptions().IsBootImage());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_high =
+          codegen_->NewBootImageStringPatch(load->GetDexFile(), load->GetStringIndex());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          codegen_->NewBootImageStringPatch(load->GetDexFile(), load->GetStringIndex(), info_high);
+      codegen_->EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      __ Addi(out, AT, /* imm12= */ 0x678);
+      return;
+    }
+    case HLoadString::LoadKind::kBootImageRelRo: {
+      DCHECK(!codegen_->GetCompilerOptions().IsBootImage());
+      uint32_t boot_image_offset = codegen_->GetBootImageOffset(load);
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_high =
+          codegen_->NewBootImageRelRoPatch(boot_image_offset);
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          codegen_->NewBootImageRelRoPatch(boot_image_offset, info_high);
+      codegen_->EmitPcRelativeAddressPlaceholderHigh(info_high, AT, info_low);
+      __ Lwu(out, AT, /* imm12= */ 0x678);
+      return;
+    }
+    case HLoadString::LoadKind::kBssEntry: {
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_high =
+          codegen_->NewStringBssEntryPatch(load->GetDexFile(), load->GetStringIndex());
+      CodeGeneratorRISCV64::PcRelativePatchInfo* info_low =
+          codegen_->NewStringBssEntryPatch(load->GetDexFile(), load->GetStringIndex(), info_high);
+      codegen_->EmitPcRelativeAddressPlaceholderHigh(info_high, out);
+      GenerateGcRootFieldLoad(load,
+                              out_loc,
+                              out,
+                              /* offset= */ 0x678,
+                              kCompilerReadBarrierOption,
+                              &info_low->label);
+      SlowPathCodeRISCV64* slow_path =
+          new (codegen_->GetScopedAllocator()) LoadStringSlowPathRISCV64(load);
+      codegen_->AddSlowPath(slow_path);
+      __ Beqzc(out, slow_path->GetEntryLabel());
+      __ Bind(slow_path->GetExitLabel());
+      return;
+    }
+    case HLoadString::LoadKind::kJitBootImageAddress: {
+      uint32_t address = reinterpret_cast32<uint32_t>(load->GetString().Get());
+      DCHECK_NE(address, 0u);
+      __ LoadLiteral(out,
+                     kLoadUnsignedWord,
+                     codegen_->DeduplicateBootImageAddressLiteral(address));
+      return;
+    }
+    case HLoadString::LoadKind::kJitTableAddress:
+      __ LoadLiteral(out,
+                     kLoadUnsignedWord,
+                     codegen_->DeduplicateJitStringLiteral(load->GetDexFile(),
+                                                           load->GetStringIndex(),
+                                                           load->GetString()));
+      GenerateGcRootFieldLoad(load, out_loc, out, 0, kCompilerReadBarrierOption);
+      return;
+    default:
+      break;
+  }
+
+  // TODO: Re-add the compiler code to do string dex cache lookup again.
+  DCHECK(load_kind == HLoadString::LoadKind::kRuntimeCall);
+  InvokeRuntimeCallingConvention calling_convention;
+  DCHECK_EQ(calling_convention.GetRegisterAt(0), out);
+  __ LoadConst32(calling_convention.GetRegisterAt(0), load->GetStringIndex().index_);
+  codegen_->InvokeRuntime(kQuickResolveString, load, load->GetDexPc());
+  CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
+}
+
+void LocationsBuilderRISCV64::VisitLongConstant(HLongConstant* constant) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(constant);
+  locations->SetOut(Location::ConstantLocation(constant));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitLongConstant(HLongConstant* constant ATTRIBUTE_UNUSED) {
+  // Will be generated at use site.
+}
+
+void LocationsBuilderRISCV64::VisitMonitorOperation(HMonitorOperation* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction, LocationSummary::kCallOnMainOnly);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitMonitorOperation(HMonitorOperation* instruction) {
+  codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject : kQuickUnlockObject,
+                          instruction,
+                          instruction->GetDexPc());
+  if (instruction->IsEnter()) {
+    CheckEntrypointTypes<kQuickLockObject, void, mirror::Object*>();
+  } else {
+    CheckEntrypointTypes<kQuickUnlockObject, void, mirror::Object*>();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitMul(HMul* mul) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(mul, LocationSummary::kNoCall);
+  switch (mul->GetResultType()) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RequiresRegister());
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+
+    default:
+      LOG(FATAL) << "Unexpected mul type " << mul->GetResultType();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitMul(HMul* instruction) {
+  DataType::Type type = instruction->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+      GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+      GpuRegister rhs = locations->InAt(1).AsRegister<GpuRegister>();
+      if (type == DataType::Type::kInt32)
+        __ MulR6(dst, lhs, rhs);
+      else
+        __ Dmul(dst, lhs, rhs);
+      break;
+    }
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64: {
+      FpuRegister dst = locations->Out().AsFpuRegister<FpuRegister>();
+      FpuRegister lhs = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister rhs = locations->InAt(1).AsFpuRegister<FpuRegister>();
+      if (type == DataType::Type::kFloat32)
+        __ MulS(dst, lhs, rhs);
+      else
+        __ MulD(dst, lhs, rhs);
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected mul type " << type;
+  }
+}
+
+void LocationsBuilderRISCV64::VisitNeg(HNeg* neg) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(neg, LocationSummary::kNoCall);
+  switch (neg->GetResultType()) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+
+    default:
+      LOG(FATAL) << "Unexpected neg type " << neg->GetResultType();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNeg(HNeg* instruction) {
+  DataType::Type type = instruction->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+      GpuRegister src = locations->InAt(0).AsRegister<GpuRegister>();
+      if (type == DataType::Type::kInt32)
+        __ Subu(dst, ZERO, src);
+      else
+        __ Dsubu(dst, ZERO, src);
+      break;
+    }
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64: {
+      FpuRegister dst = locations->Out().AsFpuRegister<FpuRegister>();
+      FpuRegister src = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      if (type == DataType::Type::kFloat32)
+        __ NegS(dst, src);
+      else
+        __ NegD(dst, src);
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected neg type " << type;
+  }
+}
+
+void LocationsBuilderRISCV64::VisitNewArray(HNewArray* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction, LocationSummary::kCallOnMainOnly);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNewArray(HNewArray* instruction) {
+  // Note: if heap poisoning is enabled, the entry point takes care of poisoning the reference.
+  QuickEntrypointEnum entrypoint = CodeGenerator::GetArrayAllocationEntrypoint(instruction);
+  codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+  CheckEntrypointTypes<kQuickAllocArrayResolved, void*, mirror::Class*, int32_t>();
+  DCHECK(!codegen_->IsLeafMethod());
+}
+
+void LocationsBuilderRISCV64::VisitNewInstance(HNewInstance* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction, LocationSummary::kCallOnMainOnly);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kReference));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNewInstance(HNewInstance* instruction) {
+  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction, instruction->GetDexPc());
+  CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
+}
+
+void LocationsBuilderRISCV64::VisitNot(HNot* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNot(HNot* instruction) {
+  DataType::Type type = instruction->GetType();
+  LocationSummary* locations = instruction->GetLocations();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64: {
+      GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+      GpuRegister src = locations->InAt(0).AsRegister<GpuRegister>();
+      __ Not(dst, src);
+      break;
+    }
+
+    default:
+      LOG(FATAL) << "Unexpected type for not operation " << instruction->GetResultType();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitBooleanNot(HBooleanNot* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitBooleanNot(HBooleanNot* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  __ Xori(locations->Out().AsRegister<GpuRegister>(),
+          locations->InAt(0).AsRegister<GpuRegister>(),
+          1);
+}
+
+void LocationsBuilderRISCV64::VisitNullCheck(HNullCheck* instruction) {
+  LocationSummary* locations = codegen_->CreateThrowingSlowPathLocations(instruction);
+  locations->SetInAt(0, Location::RequiresRegister());
+}
+
+void CodeGeneratorRISCV64::GenerateImplicitNullCheck(HNullCheck* instruction) {
+  if (CanMoveNullCheckToUser(instruction)) {
+    return;
+  }
+  Location obj = instruction->GetLocations()->InAt(0);
+
+  __ Lw(ZERO, obj.AsRegister<GpuRegister>(), 0);
+  RecordPcInfo(instruction, instruction->GetDexPc());
+}
+
+void CodeGeneratorRISCV64::GenerateExplicitNullCheck(HNullCheck* instruction) {
+  SlowPathCodeRISCV64* slow_path =
+      new (GetScopedAllocator()) NullCheckSlowPathRISCV64(instruction);
+  AddSlowPath(slow_path);
+
+  Location obj = instruction->GetLocations()->InAt(0);
+
+  __ Beqzc(obj.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNullCheck(HNullCheck* instruction) {
+  codegen_->GenerateNullCheck(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitOr(HOr* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitOr(HOr* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitParallelMove(HParallelMove* instruction ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "Unreachable";
+}
+
+void InstructionCodeGeneratorRISCV64::VisitParallelMove(HParallelMove* instruction) {
+  if (instruction->GetNext()->IsSuspendCheck() &&
+      instruction->GetBlock()->GetLoopInformation() != nullptr) {
+    HSuspendCheck* suspend_check = instruction->GetNext()->AsSuspendCheck();
+    // The back edge will generate the suspend check.
+    codegen_->ClearSpillSlotsFromLoopPhisInStackMap(suspend_check, instruction);
+  }
+
+  codegen_->GetMoveResolver()->EmitNativeCode(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitParameterValue(HParameterValue* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  Location location = parameter_visitor_.GetNextLocation(instruction->GetType());
+  if (location.IsStackSlot()) {
+    location = Location::StackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
+  } else if (location.IsDoubleStackSlot()) {
+    location = Location::DoubleStackSlot(location.GetStackIndex() + codegen_->GetFrameSize());
+  }
+  locations->SetOut(location);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitParameterValue(HParameterValue* instruction
+                                                         ATTRIBUTE_UNUSED) {
+  // Nothing to do, the parameter is already at its location.
+}
+
+void LocationsBuilderRISCV64::VisitCurrentMethod(HCurrentMethod* instruction) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  locations->SetOut(Location::RegisterLocation(kMethodRegisterArgument));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitCurrentMethod(HCurrentMethod* instruction
+                                                        ATTRIBUTE_UNUSED) {
+  // Nothing to do, the method is already at its location.
+}
+
+void LocationsBuilderRISCV64::VisitPhi(HPhi* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  for (size_t i = 0, e = locations->GetInputCount(); i < e; ++i) {
+    locations->SetInAt(i, Location::Any());
+  }
+  locations->SetOut(Location::Any());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitPhi(HPhi* instruction ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "Unreachable";
+}
+
+void LocationsBuilderRISCV64::VisitRem(HRem* rem) {
+  DataType::Type type = rem->GetResultType();
+  LocationSummary::CallKind call_kind =
+      DataType::IsFloatingPointType(type) ? LocationSummary::kCallOnMainOnly
+                                          : LocationSummary::kNoCall;
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(rem, call_kind);
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RegisterOrConstant(rem->InputAt(1)));
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64: {
+      InvokeRuntimeCallingConvention calling_convention;
+      locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
+      locations->SetInAt(1, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(1)));
+      locations->SetOut(calling_convention.GetReturnLocation(type));
+      break;
+    }
+
+    default:
+      LOG(FATAL) << "Unexpected rem type " << type;
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitRem(HRem* instruction) {
+  DataType::Type type = instruction->GetType();
+
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      GenerateDivRemIntegral(instruction);
+      break;
+
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64: {
+      QuickEntrypointEnum entrypoint =
+          (type == DataType::Type::kFloat32) ? kQuickFmodf : kQuickFmod;
+      codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+      if (type == DataType::Type::kFloat32) {
+        CheckEntrypointTypes<kQuickFmodf, float, float, float>();
+      } else {
+        CheckEntrypointTypes<kQuickFmod, double, double, double>();
+      }
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected rem type " << type;
+  }
+}
+
+static void CreateMinMaxLocations(ArenaAllocator* allocator, HBinaryOperation* minmax) {
+  LocationSummary* locations = new (allocator) LocationSummary(minmax);
+  switch (minmax->GetResultType()) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RequiresRegister());
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+    default:
+      LOG(FATAL) << "Unexpected type for HMinMax " << minmax->GetResultType();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateMinMaxInt(LocationSummary* locations, bool is_min) {
+  GpuRegister lhs = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister rhs = locations->InAt(1).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (lhs == rhs) {
+    if (out != lhs) {
+      __ Move(out, lhs);
+    }
+  } else {
+    // Some architectures, such as ARM and MIPS (prior to r6), have a
+    // conditional move instruction which only changes the target
+    // (output) register if the condition is true (MIPS prior to r6 had
+    // MOVF, MOVT, and MOVZ). The SELEQZ and SELNEZ instructions always
+    // change the target (output) register.  If the condition is true the
+    // output register gets the contents of the "rs" register; otherwise,
+    // the output register is set to zero. One consequence of this is
+    // that to implement something like "rd = c==0 ? rs : rt" RISCV64r6
+    // needs to use a pair of SELEQZ/SELNEZ instructions.  After
+    // executing this pair of instructions one of the output registers
+    // from the pair will necessarily contain zero. Then the code ORs the
+    // output registers from the SELEQZ/SELNEZ instructions to get the
+    // final result.
+    //
+    // The initial test to see if the output register is same as the
+    // first input register is needed to make sure that value in the
+    // first input register isn't clobbered before we've finished
+    // computing the output value. The logic in the corresponding else
+    // clause performs the same task but makes sure the second input
+    // register isn't clobbered in the event that it's the same register
+    // as the output register; the else clause also handles the case
+    // where the output register is distinct from both the first, and the
+    // second input registers.
+    if (out == lhs) {
+      __ Slt(AT, rhs, lhs);
+      if (is_min) {
+        __ Seleqz(out, lhs, AT);
+        __ Selnez(AT, rhs, AT);
+      } else {
+        __ Selnez(out, lhs, AT);
+        __ Seleqz(AT, rhs, AT);
+      }
+    } else {
+      __ Slt(AT, lhs, rhs);
+      if (is_min) {
+        __ Seleqz(out, rhs, AT);
+        __ Selnez(AT, lhs, AT);
+      } else {
+        __ Selnez(out, rhs, AT);
+        __ Seleqz(AT, lhs, AT);
+      }
+    }
+    __ Or(out, out, AT);
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateMinMaxFP(LocationSummary* locations,
+                                                      bool is_min,
+                                                      DataType::Type type) {
+  FpuRegister a = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister b = locations->InAt(1).AsFpuRegister<FpuRegister>();
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+
+  Riscv64Label noNaNs;
+  Riscv64Label done;
+  FpuRegister ftmp = ((out != a) && (out != b)) ? out : FTMP;
+
+  // When Java computes min/max it prefers a NaN to a number; the
+  // behavior of MIPSR6 is to prefer numbers to NaNs, i.e., if one of
+  // the inputs is a NaN and the other is a valid number, the MIPS
+  // instruction will return the number; Java wants the NaN value
+  // returned. This is why there is extra logic preceding the use of
+  // the MIPS min.fmt/max.fmt instructions. If either a, or b holds a
+  // NaN, return the NaN, otherwise return the min/max.
+  if (type == DataType::Type::kFloat64) {
+    __ CmpUnD(TMP, a, b);
+    __ Beqz(TMP, &noNaNs);
+
+    // One of the inputs is a NaN
+    __ CmpEqD(TMP, a, a);
+    // If a == a then b is the NaN, otherwise a is the NaN.
+    __ Dmtc1(TMP, ftmp);  // todo: checkout int 1 convert to float 1, bit0 is 1;
+    __ SelD(ftmp, a, b);
+
+    if (ftmp != out) {
+      __ MovD(out, ftmp);
+    }
+
+    __ Bc(&done);
+
+    __ Bind(&noNaNs);
+
+    if (is_min) {
+      __ MinD(out, a, b);
+    } else {
+      __ MaxD(out, a, b);
+    }
+  } else {
+    DCHECK_EQ(type, DataType::Type::kFloat32);
+    __ CmpUnS(TMP, a, b);
+    __ Beqz(TMP, &noNaNs);
+
+    // One of the inputs is a NaN
+    __ CmpEqS(TMP, a, a);
+    // If a == a then b is the NaN, otherwise a is the NaN.
+    __ Dmtc1(TMP, ftmp);  // todo: checkout int 1 convert to float 1, bit0 is 1;
+    __ SelS(ftmp, a, b);
+
+    if (ftmp != out) {
+      __ MovS(out, ftmp);
+    }
+
+    __ Bc(&done);
+
+    __ Bind(&noNaNs);
+
+    if (is_min) {
+      __ MinS(out, a, b);
+    } else {
+      __ MaxS(out, a, b);
+    }
+  }
+
+  __ Bind(&done);
+}
+
+void InstructionCodeGeneratorRISCV64::GenerateMinMax(HBinaryOperation* minmax, bool is_min) {
+  DataType::Type type = minmax->GetResultType();
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      GenerateMinMaxInt(minmax->GetLocations(), is_min);
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      GenerateMinMaxFP(minmax->GetLocations(), is_min, type);
+      break;
+    default:
+      LOG(FATAL) << "Unexpected type for HMinMax " << type;
+  }
+}
+
+void LocationsBuilderRISCV64::VisitMin(HMin* min) {
+  CreateMinMaxLocations(GetGraph()->GetAllocator(), min);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitMin(HMin* min) {
+  GenerateMinMax(min, /*is_min*/ true);
+}
+
+void LocationsBuilderRISCV64::VisitMax(HMax* max) {
+  CreateMinMaxLocations(GetGraph()->GetAllocator(), max);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitMax(HMax* max) {
+  GenerateMinMax(max, /*is_min*/ false);
+}
+
+void LocationsBuilderRISCV64::VisitAbs(HAbs* abs) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(abs);
+  switch (abs->GetResultType()) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+    default:
+      LOG(FATAL) << "Unexpected abs type " << abs->GetResultType();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitAbs(HAbs* abs) {
+  LocationSummary* locations = abs->GetLocations();
+  switch (abs->GetResultType()) {
+    case DataType::Type::kInt32: {
+      GpuRegister in  = locations->InAt(0).AsRegister<GpuRegister>();
+      GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+      __ Sra(AT, in, 31);
+      __ Xor(out, in, AT);
+      __ Subu(out, out, AT);
+      break;
+    }
+    case DataType::Type::kInt64: {
+      GpuRegister in  = locations->InAt(0).AsRegister<GpuRegister>();
+      GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+      __ Dsra32(AT, in, 31);
+      __ Xor(out, in, AT);
+      __ Dsubu(out, out, AT);
+      break;
+    }
+    case DataType::Type::kFloat32: {
+      FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+      __ AbsS(out, in);
+      break;
+    }
+    case DataType::Type::kFloat64: {
+      FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+      FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+      __ AbsD(out, in);
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unexpected abs type " << abs->GetResultType();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitConstructorFence(HConstructorFence* constructor_fence) {
+  constructor_fence->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitConstructorFence(
+    HConstructorFence* constructor_fence ATTRIBUTE_UNUSED) {
+  GenerateMemoryBarrier(MemBarrierKind::kStoreStore);
+}
+
+void LocationsBuilderRISCV64::VisitMemoryBarrier(HMemoryBarrier* memory_barrier) {
+  memory_barrier->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitMemoryBarrier(HMemoryBarrier* memory_barrier) {
+  GenerateMemoryBarrier(memory_barrier->GetBarrierKind());
+}
+
+void LocationsBuilderRISCV64::VisitReturn(HReturn* ret) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(ret);
+  DataType::Type return_type = ret->InputAt(0)->GetType();
+  locations->SetInAt(0, Riscv64ReturnLocation(return_type));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitReturn(HReturn* ret ATTRIBUTE_UNUSED) {
+  codegen_->GenerateFrameExit();
+}
+
+void LocationsBuilderRISCV64::VisitReturnVoid(HReturnVoid* ret) {
+  ret->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitReturnVoid(HReturnVoid* ret ATTRIBUTE_UNUSED) {
+  codegen_->GenerateFrameExit();
+}
+
+void LocationsBuilderRISCV64::VisitRor(HRor* ror) {
+  HandleShift(ror);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitRor(HRor* ror) {
+  HandleShift(ror);
+}
+
+void LocationsBuilderRISCV64::VisitShl(HShl* shl) {
+  HandleShift(shl);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitShl(HShl* shl) {
+  HandleShift(shl);
+}
+
+void LocationsBuilderRISCV64::VisitShr(HShr* shr) {
+  HandleShift(shr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitShr(HShr* shr) {
+  HandleShift(shr);
+}
+
+void LocationsBuilderRISCV64::VisitSub(HSub* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitSub(HSub* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitStaticFieldGet(HStaticFieldGet* instruction) {
+  HandleFieldGet(instruction, instruction->GetFieldInfo());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitStaticFieldGet(HStaticFieldGet* instruction) {
+  HandleFieldGet(instruction, instruction->GetFieldInfo());
+}
+
+void LocationsBuilderRISCV64::VisitStaticFieldSet(HStaticFieldSet* instruction) {
+  HandleFieldSet(instruction, instruction->GetFieldInfo());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitStaticFieldSet(HStaticFieldSet* instruction) {
+  HandleFieldSet(instruction, instruction->GetFieldInfo(), instruction->GetValueCanBeNull());
+}
+
+void LocationsBuilderRISCV64::VisitUnresolvedInstanceFieldGet(
+    HUnresolvedInstanceFieldGet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->CreateUnresolvedFieldLocationSummary(
+      instruction, instruction->GetFieldType(), calling_convention);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitUnresolvedInstanceFieldGet(
+    HUnresolvedInstanceFieldGet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->GenerateUnresolvedFieldAccess(instruction,
+                                          instruction->GetFieldType(),
+                                          instruction->GetFieldIndex(),
+                                          instruction->GetDexPc(),
+                                          calling_convention);
+}
+
+void LocationsBuilderRISCV64::VisitUnresolvedInstanceFieldSet(
+    HUnresolvedInstanceFieldSet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->CreateUnresolvedFieldLocationSummary(
+      instruction, instruction->GetFieldType(), calling_convention);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitUnresolvedInstanceFieldSet(
+    HUnresolvedInstanceFieldSet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->GenerateUnresolvedFieldAccess(instruction,
+                                          instruction->GetFieldType(),
+                                          instruction->GetFieldIndex(),
+                                          instruction->GetDexPc(),
+                                          calling_convention);
+}
+
+void LocationsBuilderRISCV64::VisitUnresolvedStaticFieldGet(
+    HUnresolvedStaticFieldGet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->CreateUnresolvedFieldLocationSummary(
+      instruction, instruction->GetFieldType(), calling_convention);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitUnresolvedStaticFieldGet(
+    HUnresolvedStaticFieldGet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->GenerateUnresolvedFieldAccess(instruction,
+                                          instruction->GetFieldType(),
+                                          instruction->GetFieldIndex(),
+                                          instruction->GetDexPc(),
+                                          calling_convention);
+}
+
+void LocationsBuilderRISCV64::VisitUnresolvedStaticFieldSet(
+    HUnresolvedStaticFieldSet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->CreateUnresolvedFieldLocationSummary(
+      instruction, instruction->GetFieldType(), calling_convention);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitUnresolvedStaticFieldSet(
+    HUnresolvedStaticFieldSet* instruction) {
+  FieldAccessCallingConventionRISCV64 calling_convention;
+  codegen_->GenerateUnresolvedFieldAccess(instruction,
+                                          instruction->GetFieldType(),
+                                          instruction->GetFieldIndex(),
+                                          instruction->GetDexPc(),
+                                          calling_convention);
+}
+
+void LocationsBuilderRISCV64::VisitSuspendCheck(HSuspendCheck* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction, LocationSummary::kCallOnSlowPath);
+  // In suspend check slow path, usually there are no caller-save registers at all.
+  // If SIMD instructions are present, however, we force spilling all live SIMD
+  // registers in full width (since the runtime only saves/restores lower part).
+  locations->SetCustomSlowPathCallerSaves(
+      GetGraph()->HasSIMD() ? RegisterSet::AllFpu() : RegisterSet::Empty());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitSuspendCheck(HSuspendCheck* instruction) {
+  HBasicBlock* block = instruction->GetBlock();
+  if (block->GetLoopInformation() != nullptr) {
+    DCHECK(block->GetLoopInformation()->GetSuspendCheck() == instruction);
+    // The back edge will generate the suspend check.
+    return;
+  }
+  if (block->IsEntryBlock() && instruction->GetNext()->IsGoto()) {
+    // The goto will generate the suspend check.
+    return;
+  }
+  GenerateSuspendCheck(instruction, nullptr);
+}
+
+void LocationsBuilderRISCV64::VisitThrow(HThrow* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(
+      instruction, LocationSummary::kCallOnMainOnly);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+}
+
+void InstructionCodeGeneratorRISCV64::VisitThrow(HThrow* instruction) {
+  codegen_->InvokeRuntime(kQuickDeliverException, instruction, instruction->GetDexPc());
+  CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
+}
+
+void LocationsBuilderRISCV64::VisitTypeConversion(HTypeConversion* conversion) {
+  DataType::Type input_type = conversion->GetInputType();
+  DataType::Type result_type = conversion->GetResultType();
+  DCHECK(!DataType::IsTypeConversionImplicit(input_type, result_type))
+      << input_type << " -> " << result_type;
+
+  if ((input_type == DataType::Type::kReference) || (input_type == DataType::Type::kVoid) ||
+      (result_type == DataType::Type::kReference) || (result_type == DataType::Type::kVoid)) {
+    LOG(FATAL) << "Unexpected type conversion from " << input_type << " to " << result_type;
+  }
+
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(conversion);
+
+  if (DataType::IsFloatingPointType(input_type)) {
+    locations->SetInAt(0, Location::RequiresFpuRegister());
+  } else {
+    locations->SetInAt(0, Location::RequiresRegister());
+  }
+
+  if (DataType::IsFloatingPointType(result_type)) {
+    locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+  } else {
+    locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitTypeConversion(HTypeConversion* conversion) {
+  LocationSummary* locations = conversion->GetLocations();
+  DataType::Type result_type = conversion->GetResultType();
+  DataType::Type input_type = conversion->GetInputType();
+
+  DCHECK(!DataType::IsTypeConversionImplicit(input_type, result_type))
+      << input_type << " -> " << result_type;
+
+  if (DataType::IsIntegralType(result_type) && DataType::IsIntegralType(input_type)) {
+    GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+    GpuRegister src = locations->InAt(0).AsRegister<GpuRegister>();
+
+    switch (result_type) {
+      case DataType::Type::kUint8:
+        __ Andi(dst, src, 0xFF);
+        break;
+      case DataType::Type::kInt8:
+        // Convert to Uint8
+        __ Andi(dst, src, 0xFF);
+        // Sign-extend Uint8
+        __ Slli(dst, dst, 56);
+        __ Srai(dst, dst, 56);
+        break;
+      case DataType::Type::kUint16:
+        __ LoadConst32(TMP, 0xFFFF);
+        __ And(dst, src, TMP);
+        break;
+      case DataType::Type::kInt16:
+        // Convert to Uint16
+        __ LoadConst32(TMP, 0xFFFF);
+        __ And(dst, src, TMP);
+        // Sign-extend Uint16
+        __ Slli(dst, dst, 48);
+        __ Srai(dst, dst, 48);
+        break;
+      case DataType::Type::kInt32:
+      case DataType::Type::kInt64:
+        // Sign-extend 32-bit int into bits 32 through 63 for int-to-long and long-to-int
+        // conversions, except when the input and output registers are the same and we are not
+        // converting longs to shorter types. In these cases, do nothing.
+        if ((input_type == DataType::Type::kInt64) || (dst != src)) {
+          __ Slliw(dst, src, 0);
+        }
+        break;
+
+      default:
+        LOG(FATAL) << "Unexpected type conversion from " << input_type
+                   << " to " << result_type;
+    }
+  } else if (DataType::IsFloatingPointType(result_type) && DataType::IsIntegralType(input_type)) {
+    FpuRegister dst = locations->Out().AsFpuRegister<FpuRegister>();
+    GpuRegister src = locations->InAt(0).AsRegister<GpuRegister>();
+    if (input_type == DataType::Type::kInt64) {
+      if (result_type == DataType::Type::kFloat32) {
+        __ FCvtSL(dst, src);
+      } else {
+        __ FCvtDL(dst, src);
+      }
+    } else {
+      __ Mtc1(src, FTMP);
+      if (result_type == DataType::Type::kFloat32) {
+        __ FCvtSW(dst, src);
+      } else {
+        __ FCvtDW(dst, src);
+      }
+    }
+  } else if (DataType::IsIntegralType(result_type) && DataType::IsFloatingPointType(input_type)) {
+    CHECK(result_type == DataType::Type::kInt32 || result_type == DataType::Type::kInt64);
+    GpuRegister dst = locations->Out().AsRegister<GpuRegister>();
+    FpuRegister src = locations->InAt(0).AsFpuRegister<FpuRegister>();
+
+    if (result_type == DataType::Type::kInt64) {
+      if (input_type == DataType::Type::kFloat32) {
+        __ TruncLS(dst, src);
+      } else {
+        __ TruncLD(dst, src);
+      }
+    } else {
+      if (input_type == DataType::Type::kFloat32) {
+        __ TruncWS(dst, src);
+      } else {
+        __ TruncWD(dst, src);
+      }
+    }
+  } else if (DataType::IsFloatingPointType(result_type) &&
+             DataType::IsFloatingPointType(input_type)) {
+    FpuRegister dst = locations->Out().AsFpuRegister<FpuRegister>();
+    FpuRegister src = locations->InAt(0).AsFpuRegister<FpuRegister>();
+    if (result_type == DataType::Type::kFloat32) {
+      __ Cvtsd(dst, src);
+    } else {
+      __ Cvtds(dst, src);
+    }
+  } else {
+    LOG(FATAL) << "Unexpected or unimplemented type conversion from " << input_type
+                << " to " << result_type;
+  }
+}
+
+void LocationsBuilderRISCV64::VisitUShr(HUShr* ushr) {
+  HandleShift(ushr);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitUShr(HUShr* ushr) {
+  HandleShift(ushr);
+}
+
+void LocationsBuilderRISCV64::VisitXor(HXor* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitXor(HXor* instruction) {
+  HandleBinaryOp(instruction);
+}
+
+void LocationsBuilderRISCV64::VisitBoundType(HBoundType* instruction ATTRIBUTE_UNUSED) {
+  // Nothing to do, this should be removed during prepare for register allocator.
+  LOG(FATAL) << "Unreachable";
+}
+
+void InstructionCodeGeneratorRISCV64::VisitBoundType(HBoundType* instruction ATTRIBUTE_UNUSED) {
+  // Nothing to do, this should be removed during prepare for register allocator.
+  LOG(FATAL) << "Unreachable";
+}
+
+void LocationsBuilderRISCV64::VisitEqual(HEqual* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitEqual(HEqual* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitNotEqual(HNotEqual* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitNotEqual(HNotEqual* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitLessThan(HLessThan* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitLessThan(HLessThan* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitLessThanOrEqual(HLessThanOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitLessThanOrEqual(HLessThanOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitGreaterThan(HGreaterThan* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitGreaterThan(HGreaterThan* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitGreaterThanOrEqual(HGreaterThanOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitGreaterThanOrEqual(HGreaterThanOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitBelow(HBelow* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitBelow(HBelow* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitBelowOrEqual(HBelowOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitBelowOrEqual(HBelowOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitAbove(HAbove* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitAbove(HAbove* comp) {
+  HandleCondition(comp);
+}
+
+void LocationsBuilderRISCV64::VisitAboveOrEqual(HAboveOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitAboveOrEqual(HAboveOrEqual* comp) {
+  HandleCondition(comp);
+}
+
+// Simple implementation of packed switch - generate cascaded compare/jumps.
+void LocationsBuilderRISCV64::VisitPackedSwitch(HPackedSwitch* switch_instr) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(switch_instr, LocationSummary::kNoCall);
+  locations->SetInAt(0, Location::RequiresRegister());
+}
+
+void InstructionCodeGeneratorRISCV64::GenPackedSwitchWithCompares(GpuRegister value_reg,
+                                                                 int32_t lower_bound,
+                                                                 uint32_t num_entries,
+                                                                 HBasicBlock* switch_block,
+                                                                 HBasicBlock* default_block) {
+  // Create a set of compare/jumps.
+  GpuRegister temp_reg = TMP;
+  __ Addiu32(temp_reg, value_reg, -lower_bound);
+  // Jump to default if index is negative
+  // Note: We don't check the case that index is positive while value < lower_bound, because in
+  // this case, index >= num_entries must be true. So that we can save one branch instruction.
+  __ Bltzc(temp_reg, codegen_->GetLabelOf(default_block));
+
+  const ArenaVector<HBasicBlock*>& successors = switch_block->GetSuccessors();
+  // Jump to successors[0] if value == lower_bound.
+  __ Beqzc(temp_reg, codegen_->GetLabelOf(successors[0]));
+  int32_t last_index = 0;
+  for (; num_entries - last_index > 2; last_index += 2) {
+    __ Addiu(temp_reg, temp_reg, -2);
+    // Jump to successors[last_index + 1] if value < case_value[last_index + 2].
+    __ Bltzc(temp_reg, codegen_->GetLabelOf(successors[last_index + 1]));
+    // Jump to successors[last_index + 2] if value == case_value[last_index + 2].
+    __ Beqzc(temp_reg, codegen_->GetLabelOf(successors[last_index + 2]));
+  }
+  if (num_entries - last_index == 2) {
+    // The last missing case_value.
+    __ Addiu(temp_reg, temp_reg, -1);
+    __ Beqzc(temp_reg, codegen_->GetLabelOf(successors[last_index + 1]));
+  }
+
+  // And the default for any other value.
+  if (!codegen_->GoesToNextBlock(switch_block, default_block)) {
+    __ Bc(codegen_->GetLabelOf(default_block));
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::GenTableBasedPackedSwitch(GpuRegister value_reg,
+                                                               int32_t lower_bound,
+                                                               uint32_t num_entries,
+                                                               HBasicBlock* switch_block,
+                                                               HBasicBlock* default_block) {
+  // Create a jump table.
+  std::vector<Riscv64Label*> labels(num_entries);
+  const ArenaVector<HBasicBlock*>& successors = switch_block->GetSuccessors();
+  for (uint32_t i = 0; i < num_entries; i++) {
+    labels[i] = codegen_->GetLabelOf(successors[i]);
+  }
+  JumpTable* table = __ CreateJumpTable(std::move(labels));
+
+  // Is the value in range?
+  __ Addiu32(TMP, value_reg, -lower_bound);
+  __ LoadConst32(AT, num_entries);
+  __ Bgeuc(TMP, AT, codegen_->GetLabelOf(default_block));
+
+  // We are in the range of the table.
+  // Load the target address from the jump table, indexing by the value.
+  __ LoadLabelAddress(AT, table->GetLabel());
+  __ Dlsa(TMP, TMP, AT, 2);
+  __ Lw(TMP, TMP, 0);
+  // Compute the absolute target address by adding the table start address
+  // (the table contains offsets to targets relative to its start).
+  __ Daddu(TMP, TMP, AT);
+  // And jump.
+  __ Jr(TMP);
+  __ Nop();
+}
+
+void InstructionCodeGeneratorRISCV64::VisitPackedSwitch(HPackedSwitch* switch_instr) {
+  int32_t lower_bound = switch_instr->GetStartValue();
+  uint32_t num_entries = switch_instr->GetNumEntries();
+  LocationSummary* locations = switch_instr->GetLocations();
+  GpuRegister value_reg = locations->InAt(0).AsRegister<GpuRegister>();
+  HBasicBlock* switch_block = switch_instr->GetBlock();
+  HBasicBlock* default_block = switch_instr->GetDefaultBlock();
+
+  if (num_entries > kPackedSwitchJumpTableThreshold) {
+    GenTableBasedPackedSwitch(value_reg,
+                              lower_bound,
+                              num_entries,
+                              switch_block,
+                              default_block);
+  } else {
+    GenPackedSwitchWithCompares(value_reg,
+                                lower_bound,
+                                num_entries,
+                                switch_block,
+                                default_block);
+  }
+}
+
+void LocationsBuilderRISCV64::VisitClassTableGet(HClassTableGet* instruction) {
+  LocationSummary* locations =
+      new (GetGraph()->GetAllocator()) LocationSummary(instruction, LocationSummary::kNoCall);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void InstructionCodeGeneratorRISCV64::VisitClassTableGet(HClassTableGet* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  if (instruction->GetTableKind() == HClassTableGet::TableKind::kVTable) {
+    uint32_t method_offset = mirror::Class::EmbeddedVTableEntryOffset(
+        instruction->GetIndex(), kRiscv64PointerSize).SizeValue();
+    __ LoadFromOffset(kLoadDoubleword,
+                      locations->Out().AsRegister<GpuRegister>(),
+                      locations->InAt(0).AsRegister<GpuRegister>(),
+                      method_offset);
+  } else {
+    uint32_t method_offset = static_cast<uint32_t>(ImTable::OffsetOfElement(
+        instruction->GetIndex(), kRiscv64PointerSize));
+    __ LoadFromOffset(kLoadDoubleword,
+                      locations->Out().AsRegister<GpuRegister>(),
+                      locations->InAt(0).AsRegister<GpuRegister>(),
+                      mirror::Class::ImtPtrOffset(kRiscv64PointerSize).Uint32Value());
+    __ LoadFromOffset(kLoadDoubleword,
+                      locations->Out().AsRegister<GpuRegister>(),
+                      locations->Out().AsRegister<GpuRegister>(),
+                      method_offset);
+  }
+}
+
+void LocationsBuilderRISCV64::VisitIntermediateAddress(HIntermediateAddress* instruction
+                                                      ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "Unreachable";
+}
+
+void InstructionCodeGeneratorRISCV64::VisitIntermediateAddress(HIntermediateAddress* instruction
+                                                              ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "Unreachable";
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/optimizing/code_generator_riscv64.h b/compiler/optimizing/code_generator_riscv64.h
new file mode 100644
index 0000000000..db75d5adb7
--- /dev/null
+++ b/compiler/optimizing/code_generator_riscv64.h
@@ -0,0 +1,693 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_CODE_GENERATOR_RISCV64_H_
+#define ART_COMPILER_OPTIMIZING_CODE_GENERATOR_RISCV64_H_
+
+#include "code_generator.h"
+#include "dex/type_reference.h"
+#include "driver/compiler_options.h"
+#include "nodes.h"
+#include "parallel_move_resolver.h"
+#include "utils/riscv64/assembler_riscv64.h"
+
+namespace art {
+namespace riscv64 {
+
+// InvokeDexCallingConvention registers
+
+static constexpr GpuRegister kParameterCoreRegisters[] =
+    { A1, A2, A3, A4, A5, A6, A7 };
+static constexpr size_t kParameterCoreRegistersLength = arraysize(kParameterCoreRegisters);
+
+static constexpr FpuRegister kParameterFpuRegisters[] =
+    { FA0, FA1, FA2, FA3, FA4, FA5, FA6, FA7 };
+static constexpr size_t kParameterFpuRegistersLength = arraysize(kParameterFpuRegisters);
+
+
+// InvokeRuntimeCallingConvention registers
+
+static constexpr GpuRegister kRuntimeParameterCoreRegisters[] =
+    { A0, A1, A2, A3, A4, A5, A6, A7 };
+static constexpr size_t kRuntimeParameterCoreRegistersLength =
+    arraysize(kRuntimeParameterCoreRegisters);
+
+static constexpr FpuRegister kRuntimeParameterFpuRegisters[] =
+    { FA0, FA1, FA2, FA3, FA4, FA5, FA6, FA7 };
+static constexpr size_t kRuntimeParameterFpuRegistersLength =
+    arraysize(kRuntimeParameterFpuRegisters);
+
+
+static constexpr GpuRegister kCoreCalleeSaves[] =
+    { S1, S2, S3, S4, S5, S6, S7, S8, S9, S10, S11, S0, RA };
+static constexpr FpuRegister kFpuCalleeSaves[] =
+    { FS0, FS1, FS2, FS3, FS4, FS5, FS6, FS7, FS8, FS9, FS10, FS11};
+
+
+class CodeGeneratorRISCV64;
+
+VectorRegister VectorRegisterFrom(Location location);
+
+class InvokeDexCallingConvention : public CallingConvention<GpuRegister, FpuRegister> {
+ public:
+  InvokeDexCallingConvention()
+      : CallingConvention(kParameterCoreRegisters,
+                          kParameterCoreRegistersLength,
+                          kParameterFpuRegisters,
+                          kParameterFpuRegistersLength,
+                          kRiscv64PointerSize) {}
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(InvokeDexCallingConvention);
+};
+
+class InvokeDexCallingConventionVisitorRISCV64 : public InvokeDexCallingConventionVisitor {
+ public:
+  InvokeDexCallingConventionVisitorRISCV64() {}
+  virtual ~InvokeDexCallingConventionVisitorRISCV64() {}
+
+  Location GetNextLocation(DataType::Type type) override;
+  Location GetReturnLocation(DataType::Type type) const override;
+  Location GetMethodLocation() const override;
+
+ private:
+  InvokeDexCallingConvention calling_convention;
+
+  DISALLOW_COPY_AND_ASSIGN(InvokeDexCallingConventionVisitorRISCV64);
+};
+
+class InvokeRuntimeCallingConvention : public CallingConvention<GpuRegister, FpuRegister> {
+ public:
+  InvokeRuntimeCallingConvention()
+      : CallingConvention(kRuntimeParameterCoreRegisters,
+                          kRuntimeParameterCoreRegistersLength,
+                          kRuntimeParameterFpuRegisters,
+                          kRuntimeParameterFpuRegistersLength,
+                          kRiscv64PointerSize) {}
+
+  Location GetReturnLocation(DataType::Type return_type);
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(InvokeRuntimeCallingConvention);
+};
+
+class FieldAccessCallingConventionRISCV64 : public FieldAccessCallingConvention {
+ public:
+  FieldAccessCallingConventionRISCV64() {}
+
+  Location GetObjectLocation() const override {
+    return Location::RegisterLocation(A1);
+  }
+  Location GetFieldIndexLocation() const override {
+    return Location::RegisterLocation(A0);
+  }
+  Location GetReturnLocation(DataType::Type type ATTRIBUTE_UNUSED) const override {
+    return Location::RegisterLocation(V0);
+  }
+  Location GetSetValueLocation(DataType::Type type ATTRIBUTE_UNUSED,
+                               bool is_instance) const override {
+    return is_instance
+        ? Location::RegisterLocation(A2)
+        : Location::RegisterLocation(A1);
+  }
+  Location GetFpuLocation(DataType::Type type ATTRIBUTE_UNUSED) const override {
+    return Location::FpuRegisterLocation(FT0);
+  }
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(FieldAccessCallingConventionRISCV64);
+};
+
+class ParallelMoveResolverRISCV64 : public ParallelMoveResolverWithSwap {
+ public:
+  ParallelMoveResolverRISCV64(ArenaAllocator* allocator, CodeGeneratorRISCV64* codegen)
+      : ParallelMoveResolverWithSwap(allocator), codegen_(codegen) {}
+
+  void EmitMove(size_t index) override;
+  void EmitSwap(size_t index) override;
+  void SpillScratch(int reg) override;
+  void RestoreScratch(int reg) override;
+
+  void Exchange(int index1, int index2, bool double_slot);
+  void ExchangeQuadSlots(int index1, int index2);
+
+  Riscv64Assembler* GetAssembler() const;
+
+ private:
+  CodeGeneratorRISCV64* const codegen_;
+
+  DISALLOW_COPY_AND_ASSIGN(ParallelMoveResolverRISCV64);
+};
+
+class SlowPathCodeRISCV64 : public SlowPathCode {
+ public:
+  explicit SlowPathCodeRISCV64(HInstruction* instruction)
+      : SlowPathCode(instruction), entry_label_(), exit_label_() {}
+
+  Riscv64Label* GetEntryLabel() { return &entry_label_; }
+  Riscv64Label* GetExitLabel() { return &exit_label_; }
+
+ private:
+  Riscv64Label entry_label_;
+  Riscv64Label exit_label_;
+
+  DISALLOW_COPY_AND_ASSIGN(SlowPathCodeRISCV64);
+};
+
+class LocationsBuilderRISCV64 : public HGraphVisitor {
+ public:
+  LocationsBuilderRISCV64(HGraph* graph, CodeGeneratorRISCV64* codegen)
+      : HGraphVisitor(graph), codegen_(codegen) {}
+
+#define DECLARE_VISIT_INSTRUCTION(name, super)     \
+  void Visit##name(H##name* instr) override;
+
+  FOR_EACH_CONCRETE_INSTRUCTION_COMMON(DECLARE_VISIT_INSTRUCTION)
+  FOR_EACH_CONCRETE_INSTRUCTION_RISCV64(DECLARE_VISIT_INSTRUCTION)
+
+#undef DECLARE_VISIT_INSTRUCTION
+
+  void VisitInstruction(HInstruction* instruction) override {
+    LOG(FATAL) << "Unreachable instruction " << instruction->DebugName()
+               << " (id " << instruction->GetId() << ")";
+  }
+
+ private:
+  void HandleInvoke(HInvoke* invoke);
+  void HandleBinaryOp(HBinaryOperation* operation);
+  void HandleCondition(HCondition* instruction);
+  void HandleShift(HBinaryOperation* operation);
+  void HandleFieldSet(HInstruction* instruction, const FieldInfo& field_info);
+  void HandleFieldGet(HInstruction* instruction, const FieldInfo& field_info);
+  Location RegisterOrZeroConstant(HInstruction* instruction);
+  Location FpuRegisterOrConstantForStore(HInstruction* instruction);
+
+  InvokeDexCallingConventionVisitorRISCV64 parameter_visitor_;
+
+  CodeGeneratorRISCV64* const codegen_;
+
+  DISALLOW_COPY_AND_ASSIGN(LocationsBuilderRISCV64);
+};
+
+class InstructionCodeGeneratorRISCV64 : public InstructionCodeGenerator {
+ public:
+  InstructionCodeGeneratorRISCV64(HGraph* graph, CodeGeneratorRISCV64* codegen);
+
+#define DECLARE_VISIT_INSTRUCTION(name, super)     \
+  void Visit##name(H##name* instr) override;
+
+  FOR_EACH_CONCRETE_INSTRUCTION_COMMON(DECLARE_VISIT_INSTRUCTION)
+  FOR_EACH_CONCRETE_INSTRUCTION_RISCV64(DECLARE_VISIT_INSTRUCTION)
+
+#undef DECLARE_VISIT_INSTRUCTION
+
+  void VisitInstruction(HInstruction* instruction) override {
+    LOG(FATAL) << "Unreachable instruction " << instruction->DebugName()
+               << " (id " << instruction->GetId() << ")";
+  }
+
+  Riscv64Assembler* GetAssembler() const { return assembler_; }
+
+  // Compare-and-jump packed switch generates approx. 3 + 2.5 * N 32-bit
+  // instructions for N cases.
+  // Table-based packed switch generates approx. 11 32-bit instructions
+  // and N 32-bit data words for N cases.
+  // At N = 6 they come out as 18 and 17 32-bit words respectively.
+  // We switch to the table-based method starting with 7 cases.
+  static constexpr uint32_t kPackedSwitchJumpTableThreshold = 6;
+
+  void GenerateMemoryBarrier(MemBarrierKind kind);
+
+ private:
+  void GenerateClassInitializationCheck(SlowPathCodeRISCV64* slow_path, GpuRegister class_reg);
+  void GenerateBitstringTypeCheckCompare(HTypeCheckInstruction* check, GpuRegister temp);
+  void GenerateSuspendCheck(HSuspendCheck* check, HBasicBlock* successor);
+  void HandleBinaryOp(HBinaryOperation* operation);
+  void HandleCondition(HCondition* instruction);
+  void HandleShift(HBinaryOperation* operation);
+  void HandleFieldSet(HInstruction* instruction,
+                      const FieldInfo& field_info,
+                      bool value_can_be_null);
+  void HandleFieldGet(HInstruction* instruction, const FieldInfo& field_info);
+
+  void GenerateMinMaxInt(LocationSummary* locations, bool is_min);
+  void GenerateMinMaxFP(LocationSummary* locations, bool is_min, DataType::Type type);
+  void GenerateMinMax(HBinaryOperation* minmax, bool is_min);
+
+  // Generate a heap reference load using one register `out`:
+  //
+  //   out <- *(out + offset)
+  //
+  // while honoring heap poisoning and/or read barriers (if any).
+  //
+  // Location `maybe_temp` is used when generating a read barrier and
+  // shall be a register in that case; it may be an invalid location
+  // otherwise.
+  void GenerateReferenceLoadOneRegister(HInstruction* instruction,
+                                        Location out,
+                                        uint32_t offset,
+                                        Location maybe_temp,
+                                        ReadBarrierOption read_barrier_option);
+  // Generate a heap reference load using two different registers
+  // `out` and `obj`:
+  //
+  //   out <- *(obj + offset)
+  //
+  // while honoring heap poisoning and/or read barriers (if any).
+  //
+  // Location `maybe_temp` is used when generating a Baker's (fast
+  // path) read barrier and shall be a register in that case; it may
+  // be an invalid location otherwise.
+  void GenerateReferenceLoadTwoRegisters(HInstruction* instruction,
+                                         Location out,
+                                         Location obj,
+                                         uint32_t offset,
+                                         Location maybe_temp,
+                                         ReadBarrierOption read_barrier_option);
+
+  // Generate a GC root reference load:
+  //
+  //   root <- *(obj + offset)
+  //
+  // while honoring read barriers (if any).
+  void GenerateGcRootFieldLoad(HInstruction* instruction,
+                               Location root,
+                               GpuRegister obj,
+                               uint32_t offset,
+                               ReadBarrierOption read_barrier_option,
+                               Riscv64Label* label_low = nullptr);
+
+  void GenerateTestAndBranch(HInstruction* instruction,
+                             size_t condition_input_index,
+                             Riscv64Label* true_target,
+                             Riscv64Label* false_target);
+  void DivRemOneOrMinusOne(HBinaryOperation* instruction);
+  void DivRemByPowerOfTwo(HBinaryOperation* instruction);
+  void GenerateDivRemWithAnyConstant(HBinaryOperation* instruction);
+  void GenerateDivRemIntegral(HBinaryOperation* instruction);
+  void GenerateIntLongCompare(IfCondition cond, bool is64bit, LocationSummary* locations);
+  // When the function returns `false` it means that the condition holds if `dst` is non-zero
+  // and doesn't hold if `dst` is zero. If it returns `true`, the roles of zero and non-zero
+  // `dst` are exchanged.
+  bool MaterializeIntLongCompare(IfCondition cond,
+                                 bool is64bit,
+                                 LocationSummary* input_locations,
+                                 GpuRegister dst);
+  void GenerateIntLongCompareAndBranch(IfCondition cond,
+                                       bool is64bit,
+                                       LocationSummary* locations,
+                                       Riscv64Label* label);
+  void GenerateFpCompare(IfCondition cond,
+                         bool gt_bias,
+                         DataType::Type type,
+                         LocationSummary* locations);
+  // When the function returns `false` it means that the condition holds if `dst` is non-zero
+  // and doesn't hold if `dst` is zero. If it returns `true`, the roles of zero and non-zero
+  // `dst` are exchanged.
+  bool MaterializeFpCompare(IfCondition cond,
+                            bool gt_bias,
+                            DataType::Type type,
+                            LocationSummary* input_locations,
+                            GpuRegister dst);
+  void GenerateFpCompareAndBranch(IfCondition cond,
+                                  bool gt_bias,
+                                  DataType::Type type,
+                                  LocationSummary* locations,
+                                  Riscv64Label* label);
+  void HandleGoto(HInstruction* got, HBasicBlock* successor);
+  void GenPackedSwitchWithCompares(GpuRegister value_reg,
+                                   int32_t lower_bound,
+                                   uint32_t num_entries,
+                                   HBasicBlock* switch_block,
+                                   HBasicBlock* default_block);
+  void GenTableBasedPackedSwitch(GpuRegister value_reg,
+                                 int32_t lower_bound,
+                                 uint32_t num_entries,
+                                 HBasicBlock* switch_block,
+                                 HBasicBlock* default_block);
+  int32_t VecAddress(LocationSummary* locations,
+                     size_t size,
+                     /* out */ GpuRegister* adjusted_base);
+  void GenConditionalMove(HSelect* select);
+
+  Riscv64Assembler* const assembler_;
+  CodeGeneratorRISCV64* const codegen_;
+
+  DISALLOW_COPY_AND_ASSIGN(InstructionCodeGeneratorRISCV64);
+};
+
+class CodeGeneratorRISCV64 : public CodeGenerator {
+ public:
+  CodeGeneratorRISCV64(HGraph* graph,
+                      const CompilerOptions& compiler_options,
+                      OptimizingCompilerStats* stats = nullptr);
+  virtual ~CodeGeneratorRISCV64() {}
+
+  void GenerateFrameEntry() override;
+  void GenerateFrameExit() override;
+
+  void Bind(HBasicBlock* block) override;
+
+  size_t GetWordSize() const override { return kRiscv64DoublewordSize; }
+
+  size_t GetFloatingPointSpillSlotSize() const override {
+    return GetGraph()->HasSIMD()
+        ? 2 * kRiscv64DoublewordSize   // 16 bytes for each spill.
+        : 1 * kRiscv64DoublewordSize;  //  8 bytes for each spill.
+  }
+
+  uintptr_t GetAddressOf(HBasicBlock* block) override {
+    return assembler_.GetLabelLocation(GetLabelOf(block));
+  }
+
+  HGraphVisitor* GetLocationBuilder() override { return &location_builder_; }
+  HGraphVisitor* GetInstructionVisitor() override { return &instruction_visitor_; }
+  Riscv64Assembler* GetAssembler() override { return &assembler_; }
+  const Riscv64Assembler& GetAssembler() const override { return assembler_; }
+
+  // Emit linker patches.
+  void EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* linker_patches) override;
+  void EmitJitRootPatches(uint8_t* code, const uint8_t* roots_data) override;
+
+  // Fast path implementation of ReadBarrier::Barrier for a heap
+  // reference field load when Baker's read barriers are used.
+  void GenerateFieldLoadWithBakerReadBarrier(HInstruction* instruction,
+                                             Location ref,
+                                             GpuRegister obj,
+                                             uint32_t offset,
+                                             Location temp,
+                                             bool needs_null_check);
+  // Fast path implementation of ReadBarrier::Barrier for a heap
+  // reference array load when Baker's read barriers are used.
+  void GenerateArrayLoadWithBakerReadBarrier(HInstruction* instruction,
+                                             Location ref,
+                                             GpuRegister obj,
+                                             uint32_t data_offset,
+                                             Location index,
+                                             Location temp,
+                                             bool needs_null_check);
+
+  // Factored implementation, used by GenerateFieldLoadWithBakerReadBarrier,
+  // GenerateArrayLoadWithBakerReadBarrier and some intrinsics.
+  //
+  // Load the object reference located at the address
+  // `obj + offset + (index << scale_factor)`, held by object `obj`, into
+  // `ref`, and mark it if needed.
+  //
+  // If `always_update_field` is true, the value of the reference is
+  // atomically updated in the holder (`obj`).
+  void GenerateReferenceLoadWithBakerReadBarrier(HInstruction* instruction,
+                                                 Location ref,
+                                                 GpuRegister obj,
+                                                 uint32_t offset,
+                                                 Location index,
+                                                 ScaleFactor scale_factor,
+                                                 Location temp,
+                                                 bool needs_null_check,
+                                                 bool always_update_field = false);
+
+  // Generate a read barrier for a heap reference within `instruction`
+  // using a slow path.
+  //
+  // A read barrier for an object reference read from the heap is
+  // implemented as a call to the artReadBarrierSlow runtime entry
+  // point, which is passed the values in locations `ref`, `obj`, and
+  // `offset`:
+  //
+  //   mirror::Object* artReadBarrierSlow(mirror::Object* ref,
+  //                                      mirror::Object* obj,
+  //                                      uint32_t offset);
+  //
+  // The `out` location contains the value returned by
+  // artReadBarrierSlow.
+  //
+  // When `index` is provided (i.e. for array accesses), the offset
+  // value passed to artReadBarrierSlow is adjusted to take `index`
+  // into account.
+  void GenerateReadBarrierSlow(HInstruction* instruction,
+                               Location out,
+                               Location ref,
+                               Location obj,
+                               uint32_t offset,
+                               Location index = Location::NoLocation());
+
+  // If read barriers are enabled, generate a read barrier for a heap
+  // reference using a slow path. If heap poisoning is enabled, also
+  // unpoison the reference in `out`.
+  void MaybeGenerateReadBarrierSlow(HInstruction* instruction,
+                                    Location out,
+                                    Location ref,
+                                    Location obj,
+                                    uint32_t offset,
+                                    Location index = Location::NoLocation());
+
+  // Generate a read barrier for a GC root within `instruction` using
+  // a slow path.
+  //
+  // A read barrier for an object reference GC root is implemented as
+  // a call to the artReadBarrierForRootSlow runtime entry point,
+  // which is passed the value in location `root`:
+  //
+  //   mirror::Object* artReadBarrierForRootSlow(GcRoot<mirror::Object>* root);
+  //
+  // The `out` location contains the value returned by
+  // artReadBarrierForRootSlow.
+  void GenerateReadBarrierForRootSlow(HInstruction* instruction, Location out, Location root);
+
+  void MarkGCCard(GpuRegister object, GpuRegister value, bool value_can_be_null);
+
+  // Register allocation.
+
+  void SetupBlockedRegisters() const override;
+
+  size_t SaveCoreRegister(size_t stack_index, uint32_t reg_id) override;
+  size_t RestoreCoreRegister(size_t stack_index, uint32_t reg_id) override;
+  size_t SaveFloatingPointRegister(size_t stack_index, uint32_t reg_id) override;
+  size_t RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) override;
+
+  void DumpCoreRegister(std::ostream& stream, int reg) const override;
+  void DumpFloatingPointRegister(std::ostream& stream, int reg) const override;
+
+  InstructionSet GetInstructionSet() const override { return InstructionSet::kRiscv64; }
+
+  const Riscv64InstructionSetFeatures& GetInstructionSetFeatures() const;
+
+  Riscv64Label* GetLabelOf(HBasicBlock* block) const {
+    return CommonGetLabelOf<Riscv64Label>(block_labels_, block);
+  }
+
+  void Initialize() override {
+    block_labels_ = CommonInitializeLabels<Riscv64Label>();
+  }
+
+  // We prefer aligned loads and stores (less code), so spill and restore registers in slow paths
+  // at aligned locations.
+  uint32_t GetPreferredSlotsAlignment() const override { return kRiscv64DoublewordSize; }
+
+  void Finalize(CodeAllocator* allocator) override;
+
+  // Code generation helpers.
+  void MoveLocation(Location dst, Location src, DataType::Type dst_type) override;
+
+  void MoveConstant(Location destination, int32_t value) override;
+
+  void AddLocationAsTemp(Location location, LocationSummary* locations) override;
+
+
+  void SwapLocations(Location loc1, Location loc2, DataType::Type type);
+
+  // Generate code to invoke a runtime entry point.
+  void InvokeRuntime(QuickEntrypointEnum entrypoint,
+                     HInstruction* instruction,
+                     uint32_t dex_pc,
+                     SlowPathCode* slow_path = nullptr) override;
+
+  // Generate code to invoke a runtime entry point, but do not record
+  // PC-related information in a stack map.
+  void InvokeRuntimeWithoutRecordingPcInfo(int32_t entry_point_offset,
+                                           HInstruction* instruction,
+                                           SlowPathCode* slow_path);
+
+  void GenerateInvokeRuntime(int32_t entry_point_offset);
+
+  ParallelMoveResolver* GetMoveResolver() override { return &move_resolver_; }
+
+  bool NeedsTwoRegisters(DataType::Type type ATTRIBUTE_UNUSED) const override { return false; }
+
+  // Check if the desired_string_load_kind is supported. If it is, return it,
+  // otherwise return a fall-back kind that should be used instead.
+  HLoadString::LoadKind GetSupportedLoadStringKind(
+      HLoadString::LoadKind desired_string_load_kind) override;
+
+  // Check if the desired_class_load_kind is supported. If it is, return it,
+  // otherwise return a fall-back kind that should be used instead.
+  HLoadClass::LoadKind GetSupportedLoadClassKind(
+      HLoadClass::LoadKind desired_class_load_kind) override;
+
+  // Check if the desired_dispatch_info is supported. If it is, return it,
+  // otherwise return a fall-back info that should be used instead.
+  HInvokeStaticOrDirect::DispatchInfo GetSupportedInvokeStaticOrDirectDispatch(
+      const HInvokeStaticOrDirect::DispatchInfo& desired_dispatch_info,
+      ArtMethod* method) override;
+
+  void GenerateStaticOrDirectCall(
+      HInvokeStaticOrDirect* invoke, Location temp, SlowPathCode* slow_path = nullptr) override;
+  void GenerateVirtualCall(
+      HInvokeVirtual* invoke, Location temp, SlowPathCode* slow_path = nullptr) override;
+
+  void MoveFromReturnRegister(Location trg ATTRIBUTE_UNUSED,
+                              DataType::Type type ATTRIBUTE_UNUSED) override {
+    UNIMPLEMENTED(FATAL) << "Not implemented on RISCV64";
+  }
+
+  void GenerateNop() override;
+  void GenerateImplicitNullCheck(HNullCheck* instruction) override;
+  void GenerateExplicitNullCheck(HNullCheck* instruction) override;
+
+  // The PcRelativePatchInfo is used for PC-relative addressing of methods/strings/types,
+  // whether through .data.bimg.rel.ro, .bss, or directly in the boot image.
+  //
+  // The 16-bit halves of the 32-bit PC-relative offset are patched separately, necessitating
+  // two patches/infos. There can be more than two patches/infos if the instruction supplying
+  // the high half is shared with e.g. a slow path, while the low half is supplied by separate
+  // instructions, e.g.:
+  //     auipc r1, high       // patch
+  //     lwu   r2, low(r1)    // patch
+  //     beqzc r2, slow_path
+  //   back:
+  //     ...
+  //   slow_path:
+  //     ...
+  //     sw    r2, low(r1)    // patch
+  //     bc    back
+  struct PcRelativePatchInfo : PatchInfo<Riscv64Label> {
+    PcRelativePatchInfo(const DexFile* dex_file,
+                        uint32_t off_or_idx,
+                        const PcRelativePatchInfo* info_high)
+        : PatchInfo<Riscv64Label>(dex_file, off_or_idx),
+          patch_info_high(info_high) { }
+
+    // Pointer to the info for the high half patch or nullptr if this is the high half patch info.
+    const PcRelativePatchInfo* patch_info_high;
+
+   private:
+    PcRelativePatchInfo(PcRelativePatchInfo&& other) = delete;
+    DISALLOW_COPY_AND_ASSIGN(PcRelativePatchInfo);
+  };
+
+  PcRelativePatchInfo* NewBootImageIntrinsicPatch(uint32_t intrinsic_data,
+                                                  const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewBootImageRelRoPatch(uint32_t boot_image_offset,
+                                              const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewBootImageMethodPatch(MethodReference target_method,
+                                               const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewMethodBssEntryPatch(MethodReference target_method,
+                                              const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewBootImageTypePatch(const DexFile& dex_file,
+                                             dex::TypeIndex type_index,
+                                             const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewTypeBssEntryPatch(const DexFile& dex_file,
+                                            dex::TypeIndex type_index,
+                                            const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewBootImageStringPatch(const DexFile& dex_file,
+                                               dex::StringIndex string_index,
+                                               const PcRelativePatchInfo* info_high = nullptr);
+  PcRelativePatchInfo* NewStringBssEntryPatch(const DexFile& dex_file,
+                                              dex::StringIndex string_index,
+                                              const PcRelativePatchInfo* info_high = nullptr);
+  Literal* DeduplicateBootImageAddressLiteral(uint64_t address);
+
+  void EmitPcRelativeAddressPlaceholderHigh(PcRelativePatchInfo* info_high,
+                                            GpuRegister out,
+                                            PcRelativePatchInfo* info_low = nullptr);
+
+  void LoadBootImageAddress(GpuRegister reg, uint32_t boot_image_reference);
+  void AllocateInstanceForIntrinsic(HInvokeStaticOrDirect* invoke, uint32_t boot_image_offset);
+
+  void PatchJitRootUse(uint8_t* code,
+                       const uint8_t* roots_data,
+                       const Literal* literal,
+                       uint64_t index_in_table) const;
+  Literal* DeduplicateJitStringLiteral(const DexFile& dex_file,
+                                       dex::StringIndex string_index,
+                                       Handle<mirror::String> handle);
+  Literal* DeduplicateJitClassLiteral(const DexFile& dex_file,
+                                      dex::TypeIndex type_index,
+                                      Handle<mirror::Class> handle);
+
+ private:
+  using Uint32ToLiteralMap = ArenaSafeMap<uint32_t, Literal*>;
+  using Uint64ToLiteralMap = ArenaSafeMap<uint64_t, Literal*>;
+  using StringToLiteralMap = ArenaSafeMap<StringReference,
+                                          Literal*,
+                                          StringReferenceValueComparator>;
+  using TypeToLiteralMap = ArenaSafeMap<TypeReference,
+                                        Literal*,
+                                        TypeReferenceValueComparator>;
+
+  Literal* DeduplicateUint32Literal(uint32_t value, Uint32ToLiteralMap* map);
+  Literal* DeduplicateUint64Literal(uint64_t value);
+
+  PcRelativePatchInfo* NewPcRelativePatch(const DexFile* dex_file,
+                                          uint32_t offset_or_index,
+                                          const PcRelativePatchInfo* info_high,
+                                          ArenaDeque<PcRelativePatchInfo>* patches);
+
+  template <linker::LinkerPatch (*Factory)(size_t, const DexFile*, uint32_t, uint32_t)>
+  void EmitPcRelativeLinkerPatches(const ArenaDeque<PcRelativePatchInfo>& infos,
+                                   ArenaVector<linker::LinkerPatch>* linker_patches);
+
+  // Labels for each block that will be compiled.
+  Riscv64Label* block_labels_;  // Indexed by block id.
+  Riscv64Label frame_entry_label_;
+  LocationsBuilderRISCV64 location_builder_;
+  InstructionCodeGeneratorRISCV64 instruction_visitor_;
+  ParallelMoveResolverRISCV64 move_resolver_;
+  Riscv64Assembler assembler_;
+
+  // Deduplication map for 32-bit literals, used for non-patchable boot image addresses.
+  Uint32ToLiteralMap uint32_literals_;
+  // Deduplication map for 64-bit literals, used for non-patchable method address or method code
+  // address.
+  Uint64ToLiteralMap uint64_literals_;
+  // PC-relative method patch info for kBootImageLinkTimePcRelative/kBootImageRelRo.
+  // Also used for type/string patches for kBootImageRelRo (same linker patch as for methods).
+  ArenaDeque<PcRelativePatchInfo> boot_image_method_patches_;
+  // PC-relative method patch info for kBssEntry.
+  ArenaDeque<PcRelativePatchInfo> method_bss_entry_patches_;
+  // PC-relative type patch info for kBootImageLinkTimePcRelative.
+  ArenaDeque<PcRelativePatchInfo> boot_image_type_patches_;
+  // PC-relative type patch info for kBssEntry.
+  ArenaDeque<PcRelativePatchInfo> type_bss_entry_patches_;
+  // PC-relative String patch info for kBootImageLinkTimePcRelative.
+  ArenaDeque<PcRelativePatchInfo> boot_image_string_patches_;
+  // PC-relative type patch info for kBssEntry.
+  ArenaDeque<PcRelativePatchInfo> string_bss_entry_patches_;
+  // PC-relative patch info for IntrinsicObjects.
+  ArenaDeque<PcRelativePatchInfo> boot_image_intrinsic_patches_;
+
+  // Patches for string root accesses in JIT compiled code.
+  StringToLiteralMap jit_string_patches_;
+  // Patches for class root accesses in JIT compiled code.
+  TypeToLiteralMap jit_class_patches_;
+
+  DISALLOW_COPY_AND_ASSIGN(CodeGeneratorRISCV64);
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_CODE_GENERATOR_RISCV64_H_
diff --git a/compiler/optimizing/code_generator_vector_riscv64.cc b/compiler/optimizing/code_generator_vector_riscv64.cc
new file mode 100644
index 0000000000..2e6e13ef68
--- /dev/null
+++ b/compiler/optimizing/code_generator_vector_riscv64.cc
@@ -0,0 +1,1428 @@
+/*
+ * Copyright (C) 2017 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "code_generator_riscv64.h"
+#include "mirror/array-inl.h"
+
+namespace art {
+namespace riscv64 {
+
+// NOLINT on __ macro to suppress wrong warning/fix (misc-macro-parentheses) from clang-tidy.
+#define __ down_cast<Riscv64Assembler*>(GetAssembler())->  // NOLINT
+
+VectorRegister VectorRegisterFrom(Location location) {
+  DCHECK(location.IsFpuRegister());
+  return static_cast<VectorRegister>(location.AsFpuRegister<FpuRegister>());
+}
+
+void LocationsBuilderRISCV64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetOut(Location::RequiresFpuRegister());
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ FillB(dst, locations->InAt(0).AsRegister<GpuRegister>());
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ FillH(dst, locations->InAt(0).AsRegister<GpuRegister>());
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FillW(dst, locations->InAt(0).AsRegister<GpuRegister>());
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FillD(dst, locations->InAt(0).AsRegister<GpuRegister>());
+      break;
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ ReplicateFPToVectorRegister(dst,
+                                     locations->InAt(0).AsFpuRegister<FpuRegister>(),
+                                     /* is_double= */ false);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ ReplicateFPToVectorRegister(dst,
+                                     locations->InAt(0).AsFpuRegister<FpuRegister>(),
+                                     /* is_double= */ true);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecExtractScalar(HVecExtractScalar* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresRegister());
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::SameAsFirstInput());
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecExtractScalar(HVecExtractScalar* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister src = VectorRegisterFrom(locations->InAt(0));
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ Copy_sW(locations->Out().AsRegister<GpuRegister>(), src, 0);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ Copy_sD(locations->Out().AsRegister<GpuRegister>(), src, 0);
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 4u);
+      DCHECK(locations->InAt(0).Equals(locations->Out()));  // no code required
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+// Helper to set up locations for vector unary operations.
+static void CreateVecUnOpLocations(ArenaAllocator* allocator, HVecUnaryOperation* instruction) {
+  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  DataType::Type type = instruction->GetPackedType();
+  switch (type) {
+    case DataType::Type::kBool:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(),
+                        instruction->IsVecNot() ? Location::kOutputOverlap
+                                                : Location::kNoOutputOverlap);
+      break;
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(),
+                        (instruction->IsVecNeg() || instruction->IsVecAbs() ||
+                            (instruction->IsVecReduce() && type == DataType::Type::kInt64))
+                            ? Location::kOutputOverlap
+                            : Location::kNoOutputOverlap);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecReduce(HVecReduce* instruction) {
+  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecReduce(HVecReduce* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister src = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  VectorRegister tmp = static_cast<VectorRegister>(FTMP);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      switch (instruction->GetReductionKind()) {
+        case HVecReduce::kSum:
+          __ Hadd_sD(tmp, src, src);
+          __ IlvlD(dst, tmp, tmp);
+          __ AddvW(dst, dst, tmp);
+          break;
+        case HVecReduce::kMin:
+          __ IlvodW(tmp, src, src);
+          __ Min_sW(tmp, src, tmp);
+          __ IlvlW(dst, tmp, tmp);
+          __ Min_sW(dst, dst, tmp);
+          break;
+        case HVecReduce::kMax:
+          __ IlvodW(tmp, src, src);
+          __ Max_sW(tmp, src, tmp);
+          __ IlvlW(dst, tmp, tmp);
+          __ Max_sW(dst, dst, tmp);
+          break;
+      }
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      switch (instruction->GetReductionKind()) {
+        case HVecReduce::kSum:
+          __ IlvlD(dst, src, src);
+          __ AddvD(dst, dst, src);
+          break;
+        case HVecReduce::kMin:
+          __ IlvlD(dst, src, src);
+          __ Min_sD(dst, dst, src);
+          break;
+        case HVecReduce::kMax:
+          __ IlvlD(dst, src, src);
+          __ Max_sD(dst, dst, src);
+          break;
+      }
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecCnv(HVecCnv* instruction) {
+  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecCnv(HVecCnv* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister src = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  DataType::Type from = instruction->GetInputType();
+  DataType::Type to = instruction->GetResultType();
+  if (from == DataType::Type::kInt32 && to == DataType::Type::kFloat32) {
+    DCHECK_EQ(4u, instruction->GetVectorLength());
+    __ Ffint_sW(dst, src);
+  } else {
+    LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+    UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecNeg(HVecNeg* instruction) {
+  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecNeg(HVecNeg* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister src = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ FillB(dst, ZERO);
+      __ SubvB(dst, dst, src);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ FillH(dst, ZERO);
+      __ SubvH(dst, dst, src);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FillW(dst, ZERO);
+      __ SubvW(dst, dst, src);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FillD(dst, ZERO);
+      __ SubvD(dst, dst, src);
+      break;
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FillW(dst, ZERO);
+      __ FsubW(dst, dst, src);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FillD(dst, ZERO);
+      __ FsubD(dst, dst, src);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecAbs(HVecAbs* instruction) {
+  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecAbs(HVecAbs* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister src = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ FillB(dst, ZERO);       // all zeroes
+      __ Add_aB(dst, dst, src);  // dst = abs(0) + abs(src)
+      break;
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ FillH(dst, ZERO);       // all zeroes
+      __ Add_aH(dst, dst, src);  // dst = abs(0) + abs(src)
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FillW(dst, ZERO);       // all zeroes
+      __ Add_aW(dst, dst, src);  // dst = abs(0) + abs(src)
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FillD(dst, ZERO);       // all zeroes
+      __ Add_aD(dst, dst, src);  // dst = abs(0) + abs(src)
+      break;
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ LdiW(dst, -1);          // all ones
+      __ SrliW(dst, dst, 1);
+      __ AndV(dst, dst, src);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ LdiD(dst, -1);          // all ones
+      __ SrliD(dst, dst, 1);
+      __ AndV(dst, dst, src);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecNot(HVecNot* instruction) {
+  CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecNot(HVecNot* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister src = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:  // special case boolean-not
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ LdiB(dst, 1);
+      __ XorV(dst, dst, src);
+      break;
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 16u);
+      __ NorV(dst, src, src);  // lanes do not matter
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+// Helper to set up locations for vector binary operations.
+static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
+  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecAdd(HVecAdd* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecAdd(HVecAdd* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ AddvB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ AddvH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ AddvW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ AddvD(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FaddW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FaddD(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
+  LOG(FATAL) << "Unsupported SIMD " << instruction->GetId();
+}
+
+void LocationsBuilderRISCV64::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      instruction->IsRounded()
+          ? __ Aver_uB(dst, lhs, rhs)
+          : __ Ave_uB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      instruction->IsRounded()
+          ? __ Aver_sB(dst, lhs, rhs)
+          : __ Ave_sB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      instruction->IsRounded()
+          ? __ Aver_uH(dst, lhs, rhs)
+          : __ Ave_uH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      instruction->IsRounded()
+          ? __ Aver_sH(dst, lhs, rhs)
+          : __ Ave_sH(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecSub(HVecSub* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecSub(HVecSub* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ SubvB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ SubvH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ SubvW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ SubvD(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FsubW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FsubD(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecSaturationSub(HVecSaturationSub* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecSaturationSub(HVecSaturationSub* instruction) {
+  LOG(FATAL) << "Unsupported SIMD " << instruction->GetId();
+}
+
+void LocationsBuilderRISCV64::VisitVecMul(HVecMul* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecMul(HVecMul* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ MulvB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ MulvH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ MulvW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ MulvD(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FmulW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FmulD(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecDiv(HVecDiv* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecDiv(HVecDiv* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FdivW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FdivD(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecMin(HVecMin* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecMin(HVecMin* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ Min_uB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ Min_sB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ Min_uH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ Min_sH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ Min_uW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ Min_sW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ Min_uD(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ Min_sD(dst, lhs, rhs);
+      break;
+    // When one of arguments is NaN, fmin.df returns other argument, but Java expects a NaN value.
+    // TODO: Fix min(x, NaN) cases for float and double.
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FminW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FminD(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecMax(HVecMax* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecMax(HVecMax* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ Max_uB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ Max_sB(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ Max_uH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ Max_sH(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ Max_uW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ Max_sW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kUint64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ Max_uD(dst, lhs, rhs);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ Max_sD(dst, lhs, rhs);
+      break;
+    // When one of arguments is NaN, fmax.df returns other argument, but Java expects a NaN value.
+    // TODO: Fix max(x, NaN) cases for float and double.
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ FmaxW(dst, lhs, rhs);
+      break;
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ FmaxD(dst, lhs, rhs);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecAnd(HVecAnd* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecAnd(HVecAnd* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 16u);
+      __ AndV(dst, lhs, rhs);  // lanes do not matter
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecAndNot(HVecAndNot* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecAndNot(HVecAndNot* instruction) {
+  LOG(FATAL) << "No SIMD for " << instruction->GetId();
+}
+
+void LocationsBuilderRISCV64::VisitVecOr(HVecOr* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecOr(HVecOr* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 16u);
+      __ OrV(dst, lhs, rhs);  // lanes do not matter
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecXor(HVecXor* instruction) {
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecXor(HVecXor* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister rhs = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 16u);
+      __ XorV(dst, lhs, rhs);  // lanes do not matter
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+// Helper to set up locations for vector shift operations.
+static void CreateVecShiftLocations(ArenaAllocator* allocator, HVecBinaryOperation* instruction) {
+  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::ConstantLocation(instruction->InputAt(1)->AsConstant()));
+      locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecShl(HVecShl* instruction) {
+  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecShl(HVecShl* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  int32_t value = locations->InAt(1).GetConstant()->AsIntConstant()->GetValue();
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ SlliB(dst, lhs, value);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ SlliH(dst, lhs, value);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ SlliW(dst, lhs, value);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ SlliD(dst, lhs, value);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecShr(HVecShr* instruction) {
+  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecShr(HVecShr* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  int32_t value = locations->InAt(1).GetConstant()->AsIntConstant()->GetValue();
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ SraiB(dst, lhs, value);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ SraiH(dst, lhs, value);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ SraiW(dst, lhs, value);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ SraiD(dst, lhs, value);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecUShr(HVecUShr* instruction) {
+  CreateVecShiftLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecUShr(HVecUShr* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister lhs = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+  int32_t value = locations->InAt(1).GetConstant()->AsIntConstant()->GetValue();
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ SrliB(dst, lhs, value);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ SrliH(dst, lhs, value);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ SrliW(dst, lhs, value);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ SrliD(dst, lhs, value);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecSetScalars(HVecSetScalars* instruction) {
+  LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
+
+  DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
+
+  HInstruction* input = instruction->InputAt(0);
+  bool is_zero = IsZeroBitPattern(input);
+
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, is_zero ? Location::ConstantLocation(input->AsConstant())
+                                    : Location::RequiresRegister());
+      locations->SetOut(Location::RequiresFpuRegister());
+      break;
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, is_zero ? Location::ConstantLocation(input->AsConstant())
+                                    : Location::RequiresFpuRegister());
+      locations->SetOut(Location::RequiresFpuRegister());
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecSetScalars(HVecSetScalars* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister dst = VectorRegisterFrom(locations->Out());
+
+  DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
+
+  // Zero out all other elements first.
+  __ FillW(dst, ZERO);
+
+  // Shorthand for any type of zero.
+  if (IsZeroBitPattern(instruction->InputAt(0))) {
+    return;
+  }
+
+  // Set required elements.
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ InsertB(dst, locations->InAt(0).AsRegister<GpuRegister>(), 0);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ InsertH(dst, locations->InAt(0).AsRegister<GpuRegister>(), 0);
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ InsertW(dst, locations->InAt(0).AsRegister<GpuRegister>(), 0);
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ InsertD(dst, locations->InAt(0).AsRegister<GpuRegister>(), 0);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+// Helper to set up locations for vector accumulations.
+static void CreateVecAccumLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
+  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+      locations->SetInAt(0, Location::RequiresFpuRegister());
+      locations->SetInAt(1, Location::RequiresFpuRegister());
+      locations->SetInAt(2, Location::RequiresFpuRegister());
+      locations->SetOut(Location::SameAsFirstInput());
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
+  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecMultiplyAccumulate(HVecMultiplyAccumulate* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister acc = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister left = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister right = VectorRegisterFrom(locations->InAt(2));
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      if (instruction->GetOpKind() == HInstruction::kAdd) {
+        __ MaddvB(acc, left, right);
+      } else {
+        __ MsubvB(acc, left, right);
+      }
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      if (instruction->GetOpKind() == HInstruction::kAdd) {
+        __ MaddvH(acc, left, right);
+      } else {
+        __ MsubvH(acc, left, right);
+      }
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      if (instruction->GetOpKind() == HInstruction::kAdd) {
+        __ MaddvW(acc, left, right);
+      } else {
+        __ MsubvW(acc, left, right);
+      }
+      break;
+    case DataType::Type::kInt64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      if (instruction->GetOpKind() == HInstruction::kAdd) {
+        __ MaddvD(acc, left, right);
+      } else {
+        __ MsubvD(acc, left, right);
+      }
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
+  CreateVecAccumLocations(GetGraph()->GetAllocator(), instruction);
+  LocationSummary* locations = instruction->GetLocations();
+  // All conversions require at least one temporary register.
+  locations->AddTemp(Location::RequiresFpuRegister());
+  // Some conversions require a second temporary register.
+  HVecOperation* a = instruction->InputAt(1)->AsVecOperation();
+  HVecOperation* b = instruction->InputAt(2)->AsVecOperation();
+  DCHECK_EQ(HVecOperation::ToSignedType(a->GetPackedType()),
+            HVecOperation::ToSignedType(b->GetPackedType()));
+  switch (a->GetPackedType()) {
+    case DataType::Type::kInt32:
+      if (instruction->GetPackedType() == DataType::Type::kInt32) {
+        break;
+      }
+      FALLTHROUGH_INTENDED;
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      locations->AddTemp(Location::RequiresFpuRegister());
+      break;
+    default:
+      break;
+  }
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecSADAccumulate(HVecSADAccumulate* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  VectorRegister acc = VectorRegisterFrom(locations->InAt(0));
+  VectorRegister left = VectorRegisterFrom(locations->InAt(1));
+  VectorRegister right = VectorRegisterFrom(locations->InAt(2));
+  VectorRegister tmp = static_cast<VectorRegister>(FTMP);
+  VectorRegister tmp1 = VectorRegisterFrom(locations->GetTemp(0));
+
+  DCHECK(locations->InAt(0).Equals(locations->Out()));
+
+  // Handle all feasible acc_T += sad(a_S, b_S) type combinations (T x S).
+  HVecOperation* a = instruction->InputAt(1)->AsVecOperation();
+  HVecOperation* b = instruction->InputAt(2)->AsVecOperation();
+  DCHECK_EQ(HVecOperation::ToSignedType(a->GetPackedType()),
+            HVecOperation::ToSignedType(b->GetPackedType()));
+  switch (a->GetPackedType()) {
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, a->GetVectorLength());
+      switch (instruction->GetPackedType()) {
+        case DataType::Type::kUint16:
+        case DataType::Type::kInt16: {
+          DCHECK_EQ(8u, instruction->GetVectorLength());
+          VectorRegister tmp2 = VectorRegisterFrom(locations->GetTemp(1));
+          __ FillB(tmp, ZERO);
+          __ Hadd_sH(tmp1, left, tmp);
+          __ Hadd_sH(tmp2, right, tmp);
+          __ Asub_sH(tmp1, tmp1, tmp2);
+          __ AddvH(acc, acc, tmp1);
+          __ Hadd_sH(tmp1, tmp, left);
+          __ Hadd_sH(tmp2, tmp, right);
+          __ Asub_sH(tmp1, tmp1, tmp2);
+          __ AddvH(acc, acc, tmp1);
+          break;
+        }
+        case DataType::Type::kInt32: {
+          DCHECK_EQ(4u, instruction->GetVectorLength());
+          VectorRegister tmp2 = VectorRegisterFrom(locations->GetTemp(1));
+          __ FillB(tmp, ZERO);
+          __ Hadd_sH(tmp1, left, tmp);
+          __ Hadd_sH(tmp2, right, tmp);
+          __ Asub_sH(tmp1, tmp1, tmp2);
+          __ Hadd_sW(tmp1, tmp1, tmp1);
+          __ AddvW(acc, acc, tmp1);
+          __ Hadd_sH(tmp1, tmp, left);
+          __ Hadd_sH(tmp2, tmp, right);
+          __ Asub_sH(tmp1, tmp1, tmp2);
+          __ Hadd_sW(tmp1, tmp1, tmp1);
+          __ AddvW(acc, acc, tmp1);
+          break;
+        }
+        case DataType::Type::kInt64: {
+          DCHECK_EQ(2u, instruction->GetVectorLength());
+          VectorRegister tmp2 = VectorRegisterFrom(locations->GetTemp(1));
+          __ FillB(tmp, ZERO);
+          __ Hadd_sH(tmp1, left, tmp);
+          __ Hadd_sH(tmp2, right, tmp);
+          __ Asub_sH(tmp1, tmp1, tmp2);
+          __ Hadd_sW(tmp1, tmp1, tmp1);
+          __ Hadd_sD(tmp1, tmp1, tmp1);
+          __ AddvD(acc, acc, tmp1);
+          __ Hadd_sH(tmp1, tmp, left);
+          __ Hadd_sH(tmp2, tmp, right);
+          __ Asub_sH(tmp1, tmp1, tmp2);
+          __ Hadd_sW(tmp1, tmp1, tmp1);
+          __ Hadd_sD(tmp1, tmp1, tmp1);
+          __ AddvD(acc, acc, tmp1);
+          break;
+        }
+        default:
+          LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+          UNREACHABLE();
+      }
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, a->GetVectorLength());
+      switch (instruction->GetPackedType()) {
+        case DataType::Type::kInt32: {
+          DCHECK_EQ(4u, instruction->GetVectorLength());
+          VectorRegister tmp2 = VectorRegisterFrom(locations->GetTemp(1));
+          __ FillH(tmp, ZERO);
+          __ Hadd_sW(tmp1, left, tmp);
+          __ Hadd_sW(tmp2, right, tmp);
+          __ Asub_sW(tmp1, tmp1, tmp2);
+          __ AddvW(acc, acc, tmp1);
+          __ Hadd_sW(tmp1, tmp, left);
+          __ Hadd_sW(tmp2, tmp, right);
+          __ Asub_sW(tmp1, tmp1, tmp2);
+          __ AddvW(acc, acc, tmp1);
+          break;
+        }
+        case DataType::Type::kInt64: {
+          DCHECK_EQ(2u, instruction->GetVectorLength());
+          VectorRegister tmp2 = VectorRegisterFrom(locations->GetTemp(1));
+          __ FillH(tmp, ZERO);
+          __ Hadd_sW(tmp1, left, tmp);
+          __ Hadd_sW(tmp2, right, tmp);
+          __ Asub_sW(tmp1, tmp1, tmp2);
+          __ Hadd_sD(tmp1, tmp1, tmp1);
+          __ AddvD(acc, acc, tmp1);
+          __ Hadd_sW(tmp1, tmp, left);
+          __ Hadd_sW(tmp2, tmp, right);
+          __ Asub_sW(tmp1, tmp1, tmp2);
+          __ Hadd_sD(tmp1, tmp1, tmp1);
+          __ AddvD(acc, acc, tmp1);
+          break;
+        }
+        default:
+          LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+          UNREACHABLE();
+      }
+      break;
+    case DataType::Type::kInt32:
+      DCHECK_EQ(4u, a->GetVectorLength());
+      switch (instruction->GetPackedType()) {
+        case DataType::Type::kInt32: {
+          DCHECK_EQ(4u, instruction->GetVectorLength());
+          __ FillW(tmp, ZERO);
+          __ SubvW(tmp1, left, right);
+          __ Add_aW(tmp1, tmp1, tmp);
+          __ AddvW(acc, acc, tmp1);
+          break;
+        }
+        case DataType::Type::kInt64: {
+          DCHECK_EQ(2u, instruction->GetVectorLength());
+          VectorRegister tmp2 = VectorRegisterFrom(locations->GetTemp(1));
+          __ FillW(tmp, ZERO);
+          __ Hadd_sD(tmp1, left, tmp);
+          __ Hadd_sD(tmp2, right, tmp);
+          __ Asub_sD(tmp1, tmp1, tmp2);
+          __ AddvD(acc, acc, tmp1);
+          __ Hadd_sD(tmp1, tmp, left);
+          __ Hadd_sD(tmp2, tmp, right);
+          __ Asub_sD(tmp1, tmp1, tmp2);
+          __ AddvD(acc, acc, tmp1);
+          break;
+        }
+        default:
+          LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+          UNREACHABLE();
+      }
+      break;
+    case DataType::Type::kInt64: {
+      DCHECK_EQ(2u, a->GetVectorLength());
+      switch (instruction->GetPackedType()) {
+        case DataType::Type::kInt64: {
+          DCHECK_EQ(2u, instruction->GetVectorLength());
+          __ FillD(tmp, ZERO);
+          __ SubvD(tmp1, left, right);
+          __ Add_aD(tmp1, tmp1, tmp);
+          __ AddvD(acc, acc, tmp1);
+          break;
+        }
+        default:
+          LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+          UNREACHABLE();
+      }
+      break;
+    }
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecDotProd(HVecDotProd* instruction) {
+  LOG(FATAL) << "No SIMD for " << instruction->GetId();
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecDotProd(HVecDotProd* instruction) {
+  LOG(FATAL) << "No SIMD for " << instruction->GetId();
+}
+
+// Helper to set up locations for vector memory operations.
+static void CreateVecMemLocations(ArenaAllocator* allocator,
+                                  HVecMemoryOperation* instruction,
+                                  bool is_load) {
+  LocationSummary* locations = new (allocator) LocationSummary(instruction);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+    case DataType::Type::kInt32:
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat32:
+    case DataType::Type::kFloat64:
+      locations->SetInAt(0, Location::RequiresRegister());
+      locations->SetInAt(1, Location::RegisterOrConstant(instruction->InputAt(1)));
+      if (is_load) {
+        locations->SetOut(Location::RequiresFpuRegister());
+      } else {
+        locations->SetInAt(2, Location::RequiresFpuRegister());
+      }
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+// Helper to prepare register and offset for vector memory operations. Returns the offset and sets
+// the output parameter adjusted_base to the original base or to a reserved temporary register (AT).
+int32_t InstructionCodeGeneratorRISCV64::VecAddress(LocationSummary* locations,
+                                                   size_t size,
+                                                   /* out */ GpuRegister* adjusted_base) {
+  GpuRegister base = locations->InAt(0).AsRegister<GpuRegister>();
+  Location index = locations->InAt(1);
+  int scale = TIMES_1;
+  switch (size) {
+    case 2: scale = TIMES_2; break;
+    case 4: scale = TIMES_4; break;
+    case 8: scale = TIMES_8; break;
+    default: break;
+  }
+  int32_t offset = mirror::Array::DataOffset(size).Int32Value();
+
+  if (index.IsConstant()) {
+    offset += index.GetConstant()->AsIntConstant()->GetValue() << scale;
+    __ AdjustBaseOffsetAndElementSizeShift(base, offset, scale);
+    *adjusted_base = base;
+  } else {
+    GpuRegister index_reg = index.AsRegister<GpuRegister>();
+    if (scale != TIMES_1) {
+      __ Dlsa(AT, index_reg, base, scale);
+    } else {
+      __ Daddu(AT, base, index_reg);
+    }
+    *adjusted_base = AT;
+  }
+  return offset;
+}
+
+void LocationsBuilderRISCV64::VisitVecLoad(HVecLoad* instruction) {
+  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /* is_load= */ true);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecLoad(HVecLoad* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  size_t size = DataType::Size(instruction->GetPackedType());
+  VectorRegister reg = VectorRegisterFrom(locations->Out());
+  GpuRegister base;
+  int32_t offset = VecAddress(locations, size, &base);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ LdB(reg, base, offset);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      // Loading 8-bytes (needed if dealing with compressed strings in StringCharAt) from unaligned
+      // memory address may cause a trap to the kernel if the CPU doesn't directly support unaligned
+      // loads and stores.
+      // TODO: Implement support for StringCharAt.
+      DCHECK(!instruction->IsStringCharAt());
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ LdH(reg, base, offset);
+      break;
+    case DataType::Type::kInt32:
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ LdW(reg, base, offset);
+      break;
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ LdD(reg, base, offset);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+void LocationsBuilderRISCV64::VisitVecStore(HVecStore* instruction) {
+  CreateVecMemLocations(GetGraph()->GetAllocator(), instruction, /* is_load= */ false);
+}
+
+void InstructionCodeGeneratorRISCV64::VisitVecStore(HVecStore* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  size_t size = DataType::Size(instruction->GetPackedType());
+  VectorRegister reg = VectorRegisterFrom(locations->InAt(2));
+  GpuRegister base;
+  int32_t offset = VecAddress(locations, size, &base);
+  switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kUint8:
+    case DataType::Type::kInt8:
+      DCHECK_EQ(16u, instruction->GetVectorLength());
+      __ StB(reg, base, offset);
+      break;
+    case DataType::Type::kUint16:
+    case DataType::Type::kInt16:
+      DCHECK_EQ(8u, instruction->GetVectorLength());
+      __ StH(reg, base, offset);
+      break;
+    case DataType::Type::kInt32:
+    case DataType::Type::kFloat32:
+      DCHECK_EQ(4u, instruction->GetVectorLength());
+      __ StW(reg, base, offset);
+      break;
+    case DataType::Type::kInt64:
+    case DataType::Type::kFloat64:
+      DCHECK_EQ(2u, instruction->GetVectorLength());
+      __ StD(reg, base, offset);
+      break;
+    default:
+      LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
+      UNREACHABLE();
+  }
+}
+
+#undef __
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/optimizing/codegen_test.cc b/compiler/optimizing/codegen_test.cc
index b5a7c137f6..34d1a529f6 100644
--- a/compiler/optimizing/codegen_test.cc
+++ b/compiler/optimizing/codegen_test.cc
@@ -42,6 +42,9 @@ namespace art {
 static ::std::vector<CodegenTargetConfig> GetTargetConfigs() {
   ::std::vector<CodegenTargetConfig> v;
   ::std::vector<CodegenTargetConfig> test_config_candidates = {
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    CodegenTargetConfig(InstructionSet::kRiscv64, create_codegen_riscv64),
+#endif
 #ifdef ART_ENABLE_CODEGEN_arm
     // TODO: Should't this be `kThumb2` instead of `kArm` here?
     CodegenTargetConfig(InstructionSet::kArm, create_codegen_arm_vixl32),
diff --git a/compiler/optimizing/codegen_test_utils.h b/compiler/optimizing/codegen_test_utils.h
index dde39d46f3..7cfd63172d 100644
--- a/compiler/optimizing/codegen_test_utils.h
+++ b/compiler/optimizing/codegen_test_utils.h
@@ -22,6 +22,8 @@
 #include "arch/mips/registers_mips.h"
 #include "arch/mips64/registers_mips64.h"
 #include "arch/x86/registers_x86.h"
+#include "register_allocator.h"
+#include "code_generator.h"
 #include "code_simulator.h"
 #include "code_simulator_container.h"
 #include "common_compiler_test.h"
@@ -53,6 +55,10 @@
 #include "code_generator_mips64.h"
 #endif
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+#include "code_generator_riscv64.h"
+#endif
+
 namespace art {
 
 typedef CodeGenerator* (*CreateCodegenFn)(HGraph*, const CompilerOptions&);
@@ -358,6 +364,12 @@ CodeGenerator* create_codegen_mips64(HGraph* graph, const CompilerOptions& compi
 }
 #endif
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+CodeGenerator* create_codegen_riscv64(HGraph* graph, const CompilerOptions& compiler_options) {
+  return new (graph->GetAllocator()) riscv64::CodeGeneratorRISCV64(graph, compiler_options);
+}
+#endif
+
 }  // namespace art
 
 #endif  // ART_COMPILER_OPTIMIZING_CODEGEN_TEST_UTILS_H_
diff --git a/compiler/optimizing/intrinsics_riscv64.cc b/compiler/optimizing/intrinsics_riscv64.cc
new file mode 100644
index 0000000000..fe61a9cdf8
--- /dev/null
+++ b/compiler/optimizing/intrinsics_riscv64.cc
@@ -0,0 +1,2382 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "intrinsics_riscv64.h"
+
+#include "arch/riscv64/instruction_set_features_riscv64.h"
+#include "art_method.h"
+#include "code_generator_riscv64.h"
+#include "entrypoints/quick/quick_entrypoints.h"
+#include "heap_poisoning.h"
+#include "intrinsics.h"
+#include "mirror/array-inl.h"
+#include "mirror/object_array-inl.h"
+#include "mirror/string.h"
+#include "scoped_thread_state_change-inl.h"
+#include "thread.h"
+#include "utils/riscv64/assembler_riscv64.h"
+#include "utils/riscv64/constants_riscv64.h"
+
+namespace art {
+
+namespace riscv64 {
+
+IntrinsicLocationsBuilderRISCV64::IntrinsicLocationsBuilderRISCV64(CodeGeneratorRISCV64* codegen)
+  : codegen_(codegen), allocator_(codegen->GetGraph()->GetAllocator()) {
+}
+
+Riscv64Assembler* IntrinsicCodeGeneratorRISCV64::GetAssembler() {
+  return reinterpret_cast<Riscv64Assembler*>(codegen_->GetAssembler());
+}
+
+ArenaAllocator* IntrinsicCodeGeneratorRISCV64::GetAllocator() {
+  return codegen_->GetGraph()->GetAllocator();
+}
+
+inline bool IntrinsicCodeGeneratorRISCV64::HasMsa() const {
+  return false;  // codegen_->GetInstructionSetFeatures().HasMsa();
+}
+
+#define __ codegen->GetAssembler()->
+
+static void MoveFromReturnRegister(Location trg,
+                                   DataType::Type type,
+                                   CodeGeneratorRISCV64* codegen) {
+  if (!trg.IsValid()) {
+    DCHECK_EQ(type, DataType::Type::kVoid);
+    return;
+  }
+
+  DCHECK_NE(type, DataType::Type::kVoid);
+
+  if (DataType::IsIntegralType(type) || type == DataType::Type::kReference) {
+    GpuRegister trg_reg = trg.AsRegister<GpuRegister>();
+    if (trg_reg != V0) {
+      __ Move(V0, trg_reg);
+    }
+  } else {
+    FpuRegister trg_reg = trg.AsFpuRegister<FpuRegister>();
+    if (trg_reg != F0) {
+      if (type == DataType::Type::kFloat32) {
+        __ MovS(F0, trg_reg);
+      } else {
+        __ MovD(F0, trg_reg);
+      }
+    }
+  }
+}
+
+static void MoveArguments(HInvoke* invoke, CodeGeneratorRISCV64* codegen) {
+  InvokeDexCallingConventionVisitorRISCV64 calling_convention_visitor;
+  IntrinsicVisitor::MoveArguments(invoke, codegen, &calling_convention_visitor);
+}
+
+// Slow-path for fallback (calling the managed code to handle the
+// intrinsic) in an intrinsified call. This will copy the arguments
+// into the positions for a regular call.
+//
+// Note: The actual parameters are required to be in the locations
+//       given by the invoke's location summary. If an intrinsic
+//       modifies those locations before a slowpath call, they must be
+//       restored!
+class IntrinsicSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  explicit IntrinsicSlowPathRISCV64(HInvoke* invoke)
+     : SlowPathCodeRISCV64(invoke), invoke_(invoke) { }
+
+  void EmitNativeCode(CodeGenerator* codegen_in) override {
+    CodeGeneratorRISCV64* codegen = down_cast<CodeGeneratorRISCV64*>(codegen_in);
+
+    __ Bind(GetEntryLabel());
+
+    SaveLiveRegisters(codegen, invoke_->GetLocations());
+
+    MoveArguments(invoke_, codegen);
+
+    if (invoke_->IsInvokeStaticOrDirect()) {
+      codegen->GenerateStaticOrDirectCall(
+          invoke_->AsInvokeStaticOrDirect(), Location::RegisterLocation(A0), this);
+    } else {
+      codegen->GenerateVirtualCall(
+          invoke_->AsInvokeVirtual(), Location::RegisterLocation(A0), this);
+    }
+
+    // Copy the result back to the expected output.
+    Location out = invoke_->GetLocations()->Out();
+    if (out.IsValid()) {
+      DCHECK(out.IsRegister());  // TODO: Replace this when we support output in memory.
+      DCHECK(!invoke_->GetLocations()->GetLiveRegisters()->ContainsCoreRegister(out.reg()));
+      MoveFromReturnRegister(out, invoke_->GetType(), codegen);
+    }
+
+    RestoreLiveRegisters(codegen, invoke_->GetLocations());
+    __ Bc(GetExitLabel());
+  }
+
+  const char* GetDescription() const override { return "IntrinsicSlowPathRISCV64"; }
+
+ private:
+  // The instruction where this slow path is happening.
+  HInvoke* const invoke_;
+
+  DISALLOW_COPY_AND_ASSIGN(IntrinsicSlowPathRISCV64);
+};
+
+#undef __
+
+bool IntrinsicLocationsBuilderRISCV64::TryDispatch(HInvoke* invoke) {
+  Dispatch(invoke);
+  LocationSummary* res = invoke->GetLocations();
+  return res != nullptr && res->Intrinsified();
+}
+
+#define __ assembler->
+
+static void CreateFPToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresFpuRegister());
+  locations->SetOut(Location::RequiresRegister());
+}
+
+static void MoveFPToInt(LocationSummary* locations, bool is64bit, Riscv64Assembler* assembler) {
+  FpuRegister in  = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (is64bit) {
+    __ Dmfc1(out, in);
+  } else {
+    __ Mfc1(out, in);
+  }
+}
+
+// long java.lang.Double.doubleToRawLongBits(double)
+void IntrinsicLocationsBuilderRISCV64::VisitDoubleDoubleToRawLongBits(HInvoke* invoke) {
+  CreateFPToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitDoubleDoubleToRawLongBits(HInvoke* invoke) {
+  MoveFPToInt(invoke->GetLocations(), /* is64bit= */ true, GetAssembler());
+}
+
+// int java.lang.Float.floatToRawIntBits(float)
+void IntrinsicLocationsBuilderRISCV64::VisitFloatFloatToRawIntBits(HInvoke* invoke) {
+  CreateFPToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitFloatFloatToRawIntBits(HInvoke* invoke) {
+  MoveFPToInt(invoke->GetLocations(), /* is64bit= */ false, GetAssembler());
+}
+
+static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresFpuRegister());
+}
+
+static void MoveIntToFP(LocationSummary* locations, bool is64bit, Riscv64Assembler* assembler) {
+  GpuRegister in  = locations->InAt(0).AsRegister<GpuRegister>();
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+
+  if (is64bit) {
+    __ Dmtc1(in, out);
+  } else {
+    __ Mtc1(in, out);
+  }
+}
+
+// double java.lang.Double.longBitsToDouble(long)
+void IntrinsicLocationsBuilderRISCV64::VisitDoubleLongBitsToDouble(HInvoke* invoke) {
+  CreateIntToFPLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitDoubleLongBitsToDouble(HInvoke* invoke) {
+  MoveIntToFP(invoke->GetLocations(), /* is64bit= */ true, GetAssembler());
+}
+
+// float java.lang.Float.intBitsToFloat(int)
+void IntrinsicLocationsBuilderRISCV64::VisitFloatIntBitsToFloat(HInvoke* invoke) {
+  CreateIntToFPLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitFloatIntBitsToFloat(HInvoke* invoke) {
+  MoveIntToFP(invoke->GetLocations(), /* is64bit= */ false, GetAssembler());
+}
+
+static void CreateIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+}
+
+static void GenReverseBytes(LocationSummary* locations,
+                            DataType::Type type,
+                            Riscv64Assembler* assembler) {
+  GpuRegister in  = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  switch (type) {
+    case DataType::Type::kInt16:
+      __ Dsbh(out, in);
+      __ Seh(out, out);
+      break;
+    case DataType::Type::kInt32:
+      __ Rotr(out, in, 16);
+      __ Wsbh(out, out);
+      break;
+    case DataType::Type::kInt64:
+      __ Dsbh(out, in);
+      __ Dshd(out, out);
+      break;
+    default:
+      LOG(FATAL) << "Unexpected size for reverse-bytes: " << type;
+      UNREACHABLE();
+  }
+}
+
+// int java.lang.Integer.reverseBytes(int)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerReverseBytes(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerReverseBytes(HInvoke* invoke) {
+  GenReverseBytes(invoke->GetLocations(), DataType::Type::kInt32, GetAssembler());
+}
+
+// long java.lang.Long.reverseBytes(long)
+void IntrinsicLocationsBuilderRISCV64::VisitLongReverseBytes(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongReverseBytes(HInvoke* invoke) {
+  GenReverseBytes(invoke->GetLocations(), DataType::Type::kInt64, GetAssembler());
+}
+
+// short java.lang.Short.reverseBytes(short)
+void IntrinsicLocationsBuilderRISCV64::VisitShortReverseBytes(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitShortReverseBytes(HInvoke* invoke) {
+  GenReverseBytes(invoke->GetLocations(), DataType::Type::kInt16, GetAssembler());
+}
+
+static void GenNumberOfLeadingZeroes(LocationSummary* locations,
+                                     bool is64bit,
+                                     Riscv64Assembler* assembler) {
+  GpuRegister in  = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (is64bit) {
+    __ Dclz(out, in);
+  } else {
+    __ Clz(out, in);
+  }
+}
+
+// int java.lang.Integer.numberOfLeadingZeros(int i)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerNumberOfLeadingZeros(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerNumberOfLeadingZeros(HInvoke* invoke) {
+  GenNumberOfLeadingZeroes(invoke->GetLocations(), /* is64bit= */ false, GetAssembler());
+}
+
+// int java.lang.Long.numberOfLeadingZeros(long i)
+void IntrinsicLocationsBuilderRISCV64::VisitLongNumberOfLeadingZeros(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongNumberOfLeadingZeros(HInvoke* invoke) {
+  GenNumberOfLeadingZeroes(invoke->GetLocations(), /* is64bit= */ true, GetAssembler());
+}
+
+static void GenNumberOfTrailingZeroes(LocationSummary* locations,
+                                      bool is64bit,
+                                      Riscv64Assembler* assembler) {
+  Location in = locations->InAt(0);
+  Location out = locations->Out();
+
+  if (is64bit) {
+    __ Dsbh(out.AsRegister<GpuRegister>(), in.AsRegister<GpuRegister>());
+    __ Dshd(out.AsRegister<GpuRegister>(), out.AsRegister<GpuRegister>());
+    __ Dbitswap(out.AsRegister<GpuRegister>(), out.AsRegister<GpuRegister>());
+    __ Dclz(out.AsRegister<GpuRegister>(), out.AsRegister<GpuRegister>());
+  } else {
+    __ Rotr(out.AsRegister<GpuRegister>(), in.AsRegister<GpuRegister>(), 16);
+    __ Wsbh(out.AsRegister<GpuRegister>(), out.AsRegister<GpuRegister>());
+    __ Bitswap(out.AsRegister<GpuRegister>(), out.AsRegister<GpuRegister>());
+    __ Clz(out.AsRegister<GpuRegister>(), out.AsRegister<GpuRegister>());
+  }
+}
+
+// int java.lang.Integer.numberOfTrailingZeros(int i)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerNumberOfTrailingZeros(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerNumberOfTrailingZeros(HInvoke* invoke) {
+  GenNumberOfTrailingZeroes(invoke->GetLocations(), /* is64bit= */ false, GetAssembler());
+}
+
+// int java.lang.Long.numberOfTrailingZeros(long i)
+void IntrinsicLocationsBuilderRISCV64::VisitLongNumberOfTrailingZeros(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongNumberOfTrailingZeros(HInvoke* invoke) {
+  GenNumberOfTrailingZeroes(invoke->GetLocations(), /* is64bit= */ true, GetAssembler());
+}
+
+static void GenReverse(LocationSummary* locations,
+                       DataType::Type type,
+                       Riscv64Assembler* assembler) {
+  DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64);
+
+  GpuRegister in  = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (type == DataType::Type::kInt32) {
+    __ Rotr(out, in, 16);
+    __ Wsbh(out, out);
+    __ Bitswap(out, out);
+  } else {
+    __ Dsbh(out, in);
+    __ Dshd(out, out);
+    __ Dbitswap(out, out);
+  }
+}
+
+// int java.lang.Integer.reverse(int)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerReverse(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerReverse(HInvoke* invoke) {
+  GenReverse(invoke->GetLocations(), DataType::Type::kInt32, GetAssembler());
+}
+
+// long java.lang.Long.reverse(long)
+void IntrinsicLocationsBuilderRISCV64::VisitLongReverse(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongReverse(HInvoke* invoke) {
+  GenReverse(invoke->GetLocations(), DataType::Type::kInt64, GetAssembler());
+}
+
+static void CreateFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresFpuRegister());
+  locations->SetOut(Location::RequiresFpuRegister(), Location::kNoOutputOverlap);
+}
+
+static void GenBitCount(LocationSummary* locations,
+                        const DataType::Type type,
+                        const bool hasMsa,
+                        Riscv64Assembler* assembler) {
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  GpuRegister in = locations->InAt(0).AsRegister<GpuRegister>();
+
+  DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64);
+
+  // https://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel
+  //
+  // A generalization of the best bit counting method to integers of
+  // bit-widths up to 128 (parameterized by type T) is this:
+  //
+  // v = v - ((v >> 1) & (T)~(T)0/3);                           // temp
+  // v = (v & (T)~(T)0/15*3) + ((v >> 2) & (T)~(T)0/15*3);      // temp
+  // v = (v + (v >> 4)) & (T)~(T)0/255*15;                      // temp
+  // c = (T)(v * ((T)~(T)0/255)) >> (sizeof(T) - 1) * BITS_PER_BYTE; // count
+  //
+  // For comparison, for 32-bit quantities, this algorithm can be executed
+  // using 20 MIPS instructions (the calls to LoadConst32() generate two
+  // machine instructions each for the values being used in this algorithm).
+  // A(n unrolled) loop-based algorithm requires 25 instructions.
+  //
+  // For a 64-bit operand this can be performed in 24 instructions compared
+  // to a(n unrolled) loop based algorithm which requires 38 instructions.
+  //
+  // There are algorithms which are faster in the cases where very few
+  // bits are set but the algorithm here attempts to minimize the total
+  // number of instructions executed even when a large number of bits
+  // are set.
+  if (hasMsa) {
+    if (type == DataType::Type::kInt32) {
+      __ Mtc1(in, FTMP);
+      __ PcntW(static_cast<VectorRegister>(FTMP), static_cast<VectorRegister>(FTMP));
+      __ Mfc1(out, FTMP);
+    } else {
+      __ Dmtc1(in, FTMP);
+      __ PcntD(static_cast<VectorRegister>(FTMP), static_cast<VectorRegister>(FTMP));
+      __ Dmfc1(out, FTMP);
+    }
+  } else {
+    if (type == DataType::Type::kInt32) {
+      __ Srl(TMP, in, 1);
+      __ LoadConst32(AT, 0x55555555);
+      __ And(TMP, TMP, AT);
+      __ Subu(TMP, in, TMP);
+      __ LoadConst32(AT, 0x33333333);
+      __ And(out, TMP, AT);
+      __ Srl(TMP, TMP, 2);
+      __ And(TMP, TMP, AT);
+      __ Addu(TMP, out, TMP);
+      __ Srl(out, TMP, 4);
+      __ Addu(out, out, TMP);
+      __ LoadConst32(AT, 0x0F0F0F0F);
+      __ And(out, out, AT);
+      __ LoadConst32(TMP, 0x01010101);
+      __ MulR6(out, out, TMP);
+      __ Srl(out, out, 24);
+    } else {
+      __ Dsrl(TMP, in, 1);
+      __ LoadConst64(AT, 0x5555555555555555L);
+      __ And(TMP, TMP, AT);
+      __ Dsubu(TMP, in, TMP);
+      __ LoadConst64(AT, 0x3333333333333333L);
+      __ And(out, TMP, AT);
+      __ Dsrl(TMP, TMP, 2);
+      __ And(TMP, TMP, AT);
+      __ Daddu(TMP, out, TMP);
+      __ Dsrl(out, TMP, 4);
+      __ Daddu(out, out, TMP);
+      __ LoadConst64(AT, 0x0F0F0F0F0F0F0F0FL);
+      __ And(out, out, AT);
+      __ LoadConst64(TMP, 0x0101010101010101L);
+      __ Dmul(out, out, TMP);
+      __ Dsrl32(out, out, 24);
+    }
+  }
+}
+
+// int java.lang.Integer.bitCount(int)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerBitCount(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerBitCount(HInvoke* invoke) {
+  GenBitCount(invoke->GetLocations(), DataType::Type::kInt32, HasMsa(), GetAssembler());
+}
+
+// int java.lang.Long.bitCount(long)
+void IntrinsicLocationsBuilderRISCV64::VisitLongBitCount(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongBitCount(HInvoke* invoke) {
+  GenBitCount(invoke->GetLocations(), DataType::Type::kInt64, HasMsa(), GetAssembler());
+}
+
+// double java.lang.Math.sqrt(double)
+void IntrinsicLocationsBuilderRISCV64::VisitMathSqrt(HInvoke* invoke) {
+  CreateFPToFPLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathSqrt(HInvoke* invoke) {
+  LocationSummary* locations = invoke->GetLocations();
+  Riscv64Assembler* assembler = GetAssembler();
+  FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+
+  __ SqrtD(out, in);
+}
+
+static void CreateFPToFP(ArenaAllocator* allocator,
+                         HInvoke* invoke,
+                         Location::OutputOverlap overlaps = Location::kOutputOverlap) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresFpuRegister());
+  locations->SetOut(Location::RequiresFpuRegister(), overlaps);
+}
+
+// double java.lang.Math.rint(double)
+void IntrinsicLocationsBuilderRISCV64::VisitMathRint(HInvoke* invoke) {
+  CreateFPToFP(allocator_, invoke, Location::kNoOutputOverlap);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathRint(HInvoke* invoke) {
+  LocationSummary* locations = invoke->GetLocations();
+  Riscv64Assembler* assembler = GetAssembler();
+  FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+
+  __ RintD(out, in);
+}
+
+// double java.lang.Math.floor(double)
+void IntrinsicLocationsBuilderRISCV64::VisitMathFloor(HInvoke* invoke) {
+  CreateFPToFP(allocator_, invoke);
+}
+
+const constexpr uint16_t kFPLeaveUnchanged = kPositiveZero |
+                                             kPositiveInfinity |
+                                             kNegativeZero |
+                                             kNegativeInfinity |
+                                             kQuietNaN |
+                                             kSignalingNaN;
+
+enum FloatRoundingMode {
+  kFloor,
+  kCeil,
+};
+
+static void GenRoundingMode(LocationSummary* locations,
+                            FloatRoundingMode mode,
+                            Riscv64Assembler* assembler) {
+  FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+
+  DCHECK_NE(in, out);
+
+  Riscv64Label done;
+
+  // double floor/ceil(double in) {
+  //     if in.isNaN || in.isInfinite || in.isZero {
+  //         return in;
+  //     }
+  __ ClassD(out, in);
+  __ Dmfc1(AT, out);
+  __ Andi(AT, AT, kFPLeaveUnchanged);   // +0.0 | +Inf | -0.0 | -Inf | qNaN | sNaN
+  __ MovD(out, in);
+  __ Bnezc(AT, &done);
+
+  //     Long outLong = floor/ceil(in);
+  //     if (outLong == Long.MAX_VALUE) || (outLong == Long.MIN_VALUE) {
+  //         // floor()/ceil() has almost certainly returned a value
+  //         // which can't be successfully represented as a signed
+  //         // 64-bit number.  Java expects that the input value will
+  //         // be returned in these cases.
+  //         // There is also a small probability that floor(in)/ceil(in)
+  //         // correctly truncates/rounds up the input value to
+  //         // Long.MAX_VALUE or Long.MIN_VALUE. In these cases, this
+  //         // exception handling code still does the correct thing.
+  //         return in;
+  //     }
+  if (mode == kFloor) {
+    __ FloorLD(out, in);
+  } else  if (mode == kCeil) {
+    __ CeilLD(out, in);
+  }
+  __ Dmfc1(AT, out);
+  __ MovD(out, in);
+  __ Daddiu(TMP, AT, 1);
+  __ Dati(TMP, 0x8000);  // TMP = AT + 0x8000 0000 0000 0001
+                         // or    AT - 0x7FFF FFFF FFFF FFFF.
+                         // IOW, TMP = 1 if AT = Long.MIN_VALUE
+                         // or   TMP = 0 if AT = Long.MAX_VALUE.
+  __ Dsrl(TMP, TMP, 1);  // TMP = 0 if AT = Long.MIN_VALUE
+                         //         or AT = Long.MAX_VALUE.
+  __ Beqzc(TMP, &done);
+
+  //     double out = outLong;
+  //     return out;
+  __ Dmtc1(AT, out);
+  __ Cvtdl(out, out);
+  __ Bind(&done);
+  // }
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathFloor(HInvoke* invoke) {
+  GenRoundingMode(invoke->GetLocations(), kFloor, GetAssembler());
+}
+
+// double java.lang.Math.ceil(double)
+void IntrinsicLocationsBuilderRISCV64::VisitMathCeil(HInvoke* invoke) {
+  CreateFPToFP(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathCeil(HInvoke* invoke) {
+  GenRoundingMode(invoke->GetLocations(), kCeil, GetAssembler());
+}
+
+static void GenRound(LocationSummary* locations, Riscv64Assembler* assembler, DataType::Type type) {
+  FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  FpuRegister half = locations->GetTemp(0).AsFpuRegister<FpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  DCHECK(type == DataType::Type::kFloat32 || type == DataType::Type::kFloat64);
+
+  Riscv64Label done;
+
+  // out = floor(in);
+  //
+  // if (out != MAX_VALUE && out != MIN_VALUE) {
+  //   TMP = ((in - out) >= 0.5) ? 1 : 0;
+  //   return out += TMP;
+  // }
+  // return out;
+
+  // out = floor(in);
+  if (type == DataType::Type::kFloat64) {
+    __ FloorLD(FTMP, in);
+    __ Dmfc1(out, FTMP);
+  } else {
+    __ FloorWS(FTMP, in);
+    __ Mfc1(out, FTMP);
+  }
+
+  // if (out != MAX_VALUE && out != MIN_VALUE)
+  if (type == DataType::Type::kFloat64) {
+    __ Daddiu(TMP, out, 1);
+    __ Dati(TMP, 0x8000);  // TMP = out + 0x8000 0000 0000 0001
+                           // or    out - 0x7FFF FFFF FFFF FFFF.
+                           // IOW, TMP = 1 if out = Long.MIN_VALUE
+                           // or   TMP = 0 if out = Long.MAX_VALUE.
+    __ Dsrl(TMP, TMP, 1);  // TMP = 0 if out = Long.MIN_VALUE
+                           //         or out = Long.MAX_VALUE.
+    __ Beqzc(TMP, &done);
+  } else {
+    __ Addiu(TMP, out, 1);
+    __ Aui(TMP, TMP, 0x8000);  // TMP = out + 0x8000 0001
+                               // or    out - 0x7FFF FFFF.
+                               // IOW, TMP = 1 if out = Int.MIN_VALUE
+                               // or   TMP = 0 if out = Int.MAX_VALUE.
+    __ Srl(TMP, TMP, 1);       // TMP = 0 if out = Int.MIN_VALUE
+                               //         or out = Int.MAX_VALUE.
+    __ Beqzc(TMP, &done);
+  }
+
+  // TMP = (0.5 <= (in - out)) ? -1 : 0;
+  if (type == DataType::Type::kFloat64) {
+    __ Cvtdl(FTMP, FTMP);  // Convert output of floor.l.d back to "double".
+    __ LoadConst64(AT, bit_cast<int64_t, double>(0.5));
+    __ SubD(FTMP, in, FTMP);
+    __ Dmtc1(AT, half);
+    __ CmpLeD(TMP, half, FTMP);
+  } else {
+    __ Cvtsw(FTMP, FTMP);  // Convert output of floor.w.s back to "float".
+    __ LoadConst32(AT, bit_cast<int32_t, float>(0.5f));
+    __ SubS(FTMP, in, FTMP);
+    __ Mtc1(AT, half);
+    __ CmpLeS(TMP, half, FTMP);
+  }
+
+  // Return out -= TMP.
+  if (type == DataType::Type::kFloat64) {
+    __ Dsubu(out, out, TMP);
+  } else {
+    __ Subu(out, out, TMP);
+  }
+
+  __ Bind(&done);
+}
+
+// int java.lang.Math.round(float)
+void IntrinsicLocationsBuilderRISCV64::VisitMathRoundFloat(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresFpuRegister());
+  locations->AddTemp(Location::RequiresFpuRegister());
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathRoundFloat(HInvoke* invoke) {
+  GenRound(invoke->GetLocations(), GetAssembler(), DataType::Type::kFloat32);
+}
+
+// long java.lang.Math.round(double)
+void IntrinsicLocationsBuilderRISCV64::VisitMathRoundDouble(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresFpuRegister());
+  locations->AddTemp(Location::RequiresFpuRegister());
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathRoundDouble(HInvoke* invoke) {
+  GenRound(invoke->GetLocations(), GetAssembler(), DataType::Type::kFloat64);
+}
+
+// byte libcore.io.Memory.peekByte(long address)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPeekByte(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPeekByte(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = invoke->GetLocations()->Out().AsRegister<GpuRegister>();
+
+  __ Lb(out, adr, 0);
+}
+
+// short libcore.io.Memory.peekShort(long address)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPeekShortNative(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPeekShortNative(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = invoke->GetLocations()->Out().AsRegister<GpuRegister>();
+
+  __ Lh(out, adr, 0);
+}
+
+// int libcore.io.Memory.peekInt(long address)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPeekIntNative(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPeekIntNative(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = invoke->GetLocations()->Out().AsRegister<GpuRegister>();
+
+  __ Lw(out, adr, 0);
+}
+
+// long libcore.io.Memory.peekLong(long address)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPeekLongNative(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPeekLongNative(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = invoke->GetLocations()->Out().AsRegister<GpuRegister>();
+
+  __ Ld(out, adr, 0);
+}
+
+static void CreateIntIntToVoidLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RequiresRegister());
+}
+
+// void libcore.io.Memory.pokeByte(long address, byte value)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPokeByte(HInvoke* invoke) {
+  CreateIntIntToVoidLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPokeByte(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister val = invoke->GetLocations()->InAt(1).AsRegister<GpuRegister>();
+
+  __ Sb(val, adr, 0);
+}
+
+// void libcore.io.Memory.pokeShort(long address, short value)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPokeShortNative(HInvoke* invoke) {
+  CreateIntIntToVoidLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPokeShortNative(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister val = invoke->GetLocations()->InAt(1).AsRegister<GpuRegister>();
+
+  __ Sh(val, adr, 0);
+}
+
+// void libcore.io.Memory.pokeInt(long address, int value)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPokeIntNative(HInvoke* invoke) {
+  CreateIntIntToVoidLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPokeIntNative(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister val = invoke->GetLocations()->InAt(1).AsRegister<GpuRegister>();
+
+  __ Sw(val, adr, 00);
+}
+
+// void libcore.io.Memory.pokeLong(long address, long value)
+void IntrinsicLocationsBuilderRISCV64::VisitMemoryPokeLongNative(HInvoke* invoke) {
+  CreateIntIntToVoidLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMemoryPokeLongNative(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister adr = invoke->GetLocations()->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister val = invoke->GetLocations()->InAt(1).AsRegister<GpuRegister>();
+
+  __ Sd(val, adr, 0);
+}
+
+// Thread java.lang.Thread.currentThread()
+void IntrinsicLocationsBuilderRISCV64::VisitThreadCurrentThread(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitThreadCurrentThread(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister out = invoke->GetLocations()->Out().AsRegister<GpuRegister>();
+
+  __ LoadFromOffset(kLoadUnsignedWord,
+                    out,
+                    TR,
+                    Thread::PeerOffset<kRiscv64PointerSize>().Int32Value());
+}
+
+static void CreateIntIntIntToIntLocations(ArenaAllocator* allocator,
+                                          HInvoke* invoke,
+                                          DataType::Type type) {
+  bool can_call = kEmitCompilerReadBarrier &&
+      (invoke->GetIntrinsic() == Intrinsics::kUnsafeGetObject ||
+       invoke->GetIntrinsic() == Intrinsics::kUnsafeGetObjectVolatile);
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke,
+                                      can_call
+                                          ? LocationSummary::kCallOnSlowPath
+                                          : LocationSummary::kNoCall,
+                                      kIntrinsified);
+  if (can_call && kUseBakerReadBarrier) {
+    locations->SetCustomSlowPathCallerSaves(RegisterSet::Empty());  // No caller-save registers.
+  }
+  locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
+  locations->SetInAt(1, Location::RequiresRegister());
+  locations->SetInAt(2, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister(),
+                    (can_call ? Location::kOutputOverlap : Location::kNoOutputOverlap));
+  if (type == DataType::Type::kReference && kEmitCompilerReadBarrier && kUseBakerReadBarrier) {
+    // We need a temporary register for the read barrier marking slow
+    // path in InstructionCodeGeneratorRISCV64::GenerateReferenceLoadWithBakerReadBarrier.
+    locations->AddTemp(Location::RequiresRegister());
+  }
+}
+
+// Note that the caller must supply a properly aligned memory address.
+// If they do not, the behavior is undefined (atomicity not guaranteed, exception may occur).
+static void GenUnsafeGet(HInvoke* invoke,
+                         DataType::Type type,
+                         bool is_volatile,
+                         CodeGeneratorRISCV64* codegen) {
+  LocationSummary* locations = invoke->GetLocations();
+  DCHECK((type == DataType::Type::kInt32) ||
+         (type == DataType::Type::kInt64) ||
+         (type == DataType::Type::kReference)) << type;
+  Riscv64Assembler* assembler = codegen->GetAssembler();
+  // Target register.
+  Location trg_loc = locations->Out();
+  GpuRegister trg = trg_loc.AsRegister<GpuRegister>();
+  // Object pointer.
+  Location base_loc = locations->InAt(1);
+  GpuRegister base = base_loc.AsRegister<GpuRegister>();
+  // Long offset.
+  Location offset_loc = locations->InAt(2);
+  GpuRegister offset = offset_loc.AsRegister<GpuRegister>();
+
+  if (!(kEmitCompilerReadBarrier && kUseBakerReadBarrier && (type == DataType::Type::kReference))) {
+    __ Daddu(TMP, base, offset);
+  }
+
+  switch (type) {
+    case DataType::Type::kInt64:
+      __ Ld(trg, TMP, 0);
+      if (is_volatile) {
+        __ Sync(0);
+      }
+      break;
+
+    case DataType::Type::kInt32:
+      __ Lw(trg, TMP, 0);
+      if (is_volatile) {
+        __ Sync(0);
+      }
+      break;
+
+    case DataType::Type::kReference:
+      if (kEmitCompilerReadBarrier) {
+        if (kUseBakerReadBarrier) {
+          Location temp = locations->GetTemp(0);
+          codegen->GenerateReferenceLoadWithBakerReadBarrier(invoke,
+                                                             trg_loc,
+                                                             base,
+                                                             /* offset= */ 0U,
+                                                             /* index= */ offset_loc,
+                                                             TIMES_1,
+                                                             temp,
+                                                             /* needs_null_check= */ false);
+          if (is_volatile) {
+            __ Sync(0);
+          }
+        } else {
+          __ Lwu(trg, TMP, 0);
+          if (is_volatile) {
+            __ Sync(0);
+          }
+          codegen->GenerateReadBarrierSlow(invoke,
+                                           trg_loc,
+                                           trg_loc,
+                                           base_loc,
+                                           /* offset= */ 0U,
+                                           /* index= */ offset_loc);
+        }
+      } else {
+        __ Lwu(trg, TMP, 0);
+        if (is_volatile) {
+          __ Sync(0);
+        }
+        __ MaybeUnpoisonHeapReference(trg);
+      }
+      break;
+
+    default:
+      LOG(FATAL) << "Unsupported op size " << type;
+      UNREACHABLE();
+  }
+}
+
+// int sun.misc.Unsafe.getInt(Object o, long offset)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeGet(HInvoke* invoke) {
+  CreateIntIntIntToIntLocations(allocator_, invoke, DataType::Type::kInt32);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeGet(HInvoke* invoke) {
+  GenUnsafeGet(invoke, DataType::Type::kInt32, /* is_volatile= */ false, codegen_);
+}
+
+// int sun.misc.Unsafe.getIntVolatile(Object o, long offset)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeGetVolatile(HInvoke* invoke) {
+  CreateIntIntIntToIntLocations(allocator_, invoke, DataType::Type::kInt32);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeGetVolatile(HInvoke* invoke) {
+  GenUnsafeGet(invoke, DataType::Type::kInt32, /* is_volatile= */ true, codegen_);
+}
+
+// long sun.misc.Unsafe.getLong(Object o, long offset)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeGetLong(HInvoke* invoke) {
+  CreateIntIntIntToIntLocations(allocator_, invoke, DataType::Type::kInt64);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeGetLong(HInvoke* invoke) {
+  GenUnsafeGet(invoke, DataType::Type::kInt64, /* is_volatile= */ false, codegen_);
+}
+
+// long sun.misc.Unsafe.getLongVolatile(Object o, long offset)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeGetLongVolatile(HInvoke* invoke) {
+  CreateIntIntIntToIntLocations(allocator_, invoke, DataType::Type::kInt64);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeGetLongVolatile(HInvoke* invoke) {
+  GenUnsafeGet(invoke, DataType::Type::kInt64, /* is_volatile= */ true, codegen_);
+}
+
+// Object sun.misc.Unsafe.getObject(Object o, long offset)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeGetObject(HInvoke* invoke) {
+  CreateIntIntIntToIntLocations(allocator_, invoke, DataType::Type::kReference);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeGetObject(HInvoke* invoke) {
+  GenUnsafeGet(invoke, DataType::Type::kReference, /* is_volatile= */ false, codegen_);
+}
+
+// Object sun.misc.Unsafe.getObjectVolatile(Object o, long offset)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeGetObjectVolatile(HInvoke* invoke) {
+  CreateIntIntIntToIntLocations(allocator_, invoke, DataType::Type::kReference);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeGetObjectVolatile(HInvoke* invoke) {
+  GenUnsafeGet(invoke, DataType::Type::kReference, /* is_volatile= */ true, codegen_);
+}
+
+static void CreateIntIntIntIntToVoid(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
+  locations->SetInAt(1, Location::RequiresRegister());
+  locations->SetInAt(2, Location::RequiresRegister());
+  locations->SetInAt(3, Location::RequiresRegister());
+}
+
+// Note that the caller must supply a properly aligned memory address.
+// If they do not, the behavior is undefined (atomicity not guaranteed, exception may occur).
+static void GenUnsafePut(LocationSummary* locations,
+                         DataType::Type type,
+                         bool is_volatile,
+                         bool is_ordered,
+                         CodeGeneratorRISCV64* codegen) {
+  DCHECK((type == DataType::Type::kInt32) ||
+         (type == DataType::Type::kInt64) ||
+         (type == DataType::Type::kReference));
+  Riscv64Assembler* assembler = codegen->GetAssembler();
+  // Object pointer.
+  GpuRegister base = locations->InAt(1).AsRegister<GpuRegister>();
+  // Long offset.
+  GpuRegister offset = locations->InAt(2).AsRegister<GpuRegister>();
+  GpuRegister value = locations->InAt(3).AsRegister<GpuRegister>();
+
+  __ Daddu(TMP, base, offset);
+  if (is_volatile || is_ordered) {
+    __ Sync(0);
+  }
+  switch (type) {
+    case DataType::Type::kInt32:
+    case DataType::Type::kReference:
+      if (kPoisonHeapReferences && type == DataType::Type::kReference) {
+        __ PoisonHeapReference(AT, value);
+        __ Sw(AT, TMP, 0);
+      } else {
+        __ Sw(value, TMP, 0);
+      }
+      break;
+
+    case DataType::Type::kInt64:
+      __ Sd(value, TMP, 0);
+      break;
+
+    default:
+      LOG(FATAL) << "Unsupported op size " << type;
+      UNREACHABLE();
+  }
+  if (is_volatile) {
+    __ Sync(0);
+  }
+
+  if (type == DataType::Type::kReference) {
+    bool value_can_be_null = true;  // TODO: Worth finding out this information?
+    codegen->MarkGCCard(base, value, value_can_be_null);
+  }
+}
+
+// void sun.misc.Unsafe.putInt(Object o, long offset, int x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePut(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePut(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kInt32,
+               /* is_volatile= */ false,
+               /* is_ordered= */ false,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutOrdered(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutOrdered(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kInt32,
+               /* is_volatile= */ false,
+               /* is_ordered= */ true,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putIntVolatile(Object o, long offset, int x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutVolatile(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutVolatile(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kInt32,
+               /* is_volatile= */ true,
+               /* is_ordered= */ false,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putObject(Object o, long offset, Object x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutObject(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutObject(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kReference,
+               /* is_volatile= */ false,
+               /* is_ordered= */ false,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kReference,
+               /* is_volatile= */ false,
+               /* is_ordered= */ true,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putObjectVolatile(Object o, long offset, Object x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kReference,
+               /* is_volatile= */ true,
+               /* is_ordered= */ false,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putLong(Object o, long offset, long x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutLong(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutLong(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kInt64,
+               /* is_volatile= */ false,
+               /* is_ordered= */ false,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutLongOrdered(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutLongOrdered(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kInt64,
+               /* is_volatile= */ false,
+               /* is_ordered= */ true,
+               codegen_);
+}
+
+// void sun.misc.Unsafe.putLongVolatile(Object o, long offset, long x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutLongVolatile(HInvoke* invoke) {
+  CreateIntIntIntIntToVoid(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutLongVolatile(HInvoke* invoke) {
+  GenUnsafePut(invoke->GetLocations(),
+               DataType::Type::kInt64,
+               /* is_volatile= */ true,
+               /* is_ordered= */ false,
+               codegen_);
+}
+
+static void CreateIntIntIntIntIntToIntPlusTemps(ArenaAllocator* allocator, HInvoke* invoke) {
+  bool can_call = kEmitCompilerReadBarrier &&
+      kUseBakerReadBarrier &&
+      (invoke->GetIntrinsic() == Intrinsics::kUnsafeCASObject);
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke,
+                                      can_call
+                                          ? LocationSummary::kCallOnSlowPath
+                                          : LocationSummary::kNoCall,
+                                      kIntrinsified);
+  locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
+  locations->SetInAt(1, Location::RequiresRegister());
+  locations->SetInAt(2, Location::RequiresRegister());
+  locations->SetInAt(3, Location::RequiresRegister());
+  locations->SetInAt(4, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister());
+
+  // Temporary register used in CAS by (Baker) read barrier.
+  if (can_call) {
+    locations->AddTemp(Location::RequiresRegister());
+  }
+}
+
+// Note that the caller must supply a properly aligned memory address.
+// If they do not, the behavior is undefined (atomicity not guaranteed, exception may occur).
+static void GenCas(HInvoke* invoke, DataType::Type type, CodeGeneratorRISCV64* codegen) {
+  Riscv64Assembler* assembler = codegen->GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+  GpuRegister base = locations->InAt(1).AsRegister<GpuRegister>();
+  Location offset_loc = locations->InAt(2);
+  GpuRegister offset = offset_loc.AsRegister<GpuRegister>();
+  GpuRegister expected = locations->InAt(3).AsRegister<GpuRegister>();
+  GpuRegister value = locations->InAt(4).AsRegister<GpuRegister>();
+  Location out_loc = locations->Out();
+  GpuRegister out = out_loc.AsRegister<GpuRegister>();
+
+  DCHECK_NE(base, out);
+  DCHECK_NE(offset, out);
+  DCHECK_NE(expected, out);
+
+  if (type == DataType::Type::kReference) {
+    // The only read barrier implementation supporting the
+    // UnsafeCASObject intrinsic is the Baker-style read barriers.
+    DCHECK(!kEmitCompilerReadBarrier || kUseBakerReadBarrier);
+
+    // Mark card for object assuming new value is stored. Worst case we will mark an unchanged
+    // object and scan the receiver at the next GC for nothing.
+    bool value_can_be_null = true;  // TODO: Worth finding out this information?
+    codegen->MarkGCCard(base, value, value_can_be_null);
+
+    if (kEmitCompilerReadBarrier && kUseBakerReadBarrier) {
+      Location temp = locations->GetTemp(0);
+      // Need to make sure the reference stored in the field is a to-space
+      // one before attempting the CAS or the CAS could fail incorrectly.
+      codegen->GenerateReferenceLoadWithBakerReadBarrier(
+          invoke,
+          out_loc,  // Unused, used only as a "temporary" within the read barrier.
+          base,
+          /* offset= */ 0u,
+          /* index= */ offset_loc,
+          ScaleFactor::TIMES_1,
+          temp,
+          /* needs_null_check= */ false,
+          /* always_update_field= */ true);
+    }
+  }
+
+  Riscv64Label loop_head, exit_loop;
+  __ Daddu(TMP, base, offset);
+
+  if (kPoisonHeapReferences && type == DataType::Type::kReference) {
+    __ PoisonHeapReference(expected);
+    // Do not poison `value`, if it is the same register as
+    // `expected`, which has just been poisoned.
+    if (value != expected) {
+      __ PoisonHeapReference(value);
+    }
+  }
+
+  // do {
+  //   tmp_value = [tmp_ptr] - expected;
+  // } while (tmp_value == 0 && failure([tmp_ptr] <- r_new_value));
+  // result = tmp_value != 0;
+
+  __ Sync(0);
+  __ Bind(&loop_head);
+  if (type == DataType::Type::kInt64) {
+    __ Lld(out, TMP);
+  } else {
+    // Note: We will need a read barrier here, when read barrier
+    // support is added to the RISCV64 back end.
+    __ Ll(out, TMP);
+    if (type == DataType::Type::kReference) {
+      // The LL instruction sign-extends the 32-bit value, but
+      // 32-bit references must be zero-extended. Zero-extend `out`.
+      __ Dext(out, out, 0, 32);
+    }
+  }
+  __ Dsubu(out, out, expected);         // If we didn't get the 'expected'
+  __ Sltiu(out, out, 1);                // value, set 'out' to false, and
+  __ Beqzc(out, &exit_loop);            // return.
+  __ Move(out, value);  // Use 'out' for the 'store conditional' instruction.
+                        // If we use 'value' directly, we would lose 'value'
+                        // in the case that the store fails.  Whether the
+                        // store succeeds, or fails, it will load the
+                        // correct Boolean value into the 'out' register.
+  if (type == DataType::Type::kInt64) {
+    __ Scd(out, TMP);
+  } else {
+    __ Sc(out, TMP);
+  }
+
+  // return in out 0: success, 1 : fail
+  __ Bnezc(out, &loop_head);    // If we couldn't do the read-modify-write
+                                // cycle atomically then retry.
+  __ Bind(&exit_loop);
+  __ Sync(0);
+
+  if (kPoisonHeapReferences && type == DataType::Type::kReference) {
+    __ UnpoisonHeapReference(expected);
+    // Do not unpoison `value`, if it is the same register as
+    // `expected`, which has just been unpoisoned.
+    if (value != expected) {
+      __ UnpoisonHeapReference(value);
+    }
+  }
+}
+
+// boolean sun.misc.Unsafe.compareAndSwapInt(Object o, long offset, int expected, int x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeCASInt(HInvoke* invoke) {
+  CreateIntIntIntIntIntToIntPlusTemps(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeCASInt(HInvoke* invoke) {
+  GenCas(invoke, DataType::Type::kInt32, codegen_);
+}
+
+// boolean sun.misc.Unsafe.compareAndSwapLong(Object o, long offset, long expected, long x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeCASLong(HInvoke* invoke) {
+  CreateIntIntIntIntIntToIntPlusTemps(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeCASLong(HInvoke* invoke) {
+  GenCas(invoke, DataType::Type::kInt64, codegen_);
+}
+
+// boolean sun.misc.Unsafe.compareAndSwapObject(Object o, long offset, Object expected, Object x)
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafeCASObject(HInvoke* invoke) {
+  // The only read barrier implementation supporting the
+  // UnsafeCASObject intrinsic is the Baker-style read barriers.
+  if (kEmitCompilerReadBarrier && !kUseBakerReadBarrier) {
+    return;
+  }
+
+  CreateIntIntIntIntIntToIntPlusTemps(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafeCASObject(HInvoke* invoke) {
+  // The only read barrier implementation supporting the
+  // UnsafeCASObject intrinsic is the Baker-style read barriers.
+  DCHECK(!kEmitCompilerReadBarrier || kUseBakerReadBarrier);
+
+  GenCas(invoke, DataType::Type::kReference, codegen_);
+}
+
+// int java.lang.String.compareTo(String anotherString)
+void IntrinsicLocationsBuilderRISCV64::VisitStringCompareTo(HInvoke* invoke) {
+  LocationSummary* locations = new (allocator_) LocationSummary(
+      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+  Location outLocation = calling_convention.GetReturnLocation(DataType::Type::kInt32);
+  locations->SetOut(Location::RegisterLocation(outLocation.AsRegister<GpuRegister>()));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringCompareTo(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+
+  // Note that the null check must have been done earlier.
+  DCHECK(!invoke->CanDoImplicitNullCheckOn(invoke->InputAt(0)));
+
+  GpuRegister argument = locations->InAt(1).AsRegister<GpuRegister>();
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) IntrinsicSlowPathRISCV64(invoke);
+  codegen_->AddSlowPath(slow_path);
+  __ Beqzc(argument, slow_path->GetEntryLabel());
+
+  codegen_->InvokeRuntime(kQuickStringCompareTo, invoke, invoke->GetDexPc(), slow_path);
+  __ Bind(slow_path->GetExitLabel());
+}
+
+// boolean java.lang.String.equals(Object anObject)
+void IntrinsicLocationsBuilderRISCV64::VisitStringEquals(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister());
+
+  // Temporary registers to store lengths of strings and for calculations.
+  locations->AddTemp(Location::RequiresRegister());
+  locations->AddTemp(Location::RequiresRegister());
+  locations->AddTemp(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringEquals(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+
+  GpuRegister str = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister arg = locations->InAt(1).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  GpuRegister temp1 = locations->GetTemp(0).AsRegister<GpuRegister>();
+  GpuRegister temp2 = locations->GetTemp(1).AsRegister<GpuRegister>();
+  GpuRegister temp3 = locations->GetTemp(2).AsRegister<GpuRegister>();
+
+  Riscv64Label loop;
+  Riscv64Label end;
+  Riscv64Label return_true;
+  Riscv64Label return_false;
+
+  // Get offsets of count, value, and class fields within a string object.
+  const int32_t count_offset = mirror::String::CountOffset().Int32Value();
+  const int32_t value_offset = mirror::String::ValueOffset().Int32Value();
+  const int32_t class_offset = mirror::Object::ClassOffset().Int32Value();
+
+  // Note that the null check must have been done earlier.
+  DCHECK(!invoke->CanDoImplicitNullCheckOn(invoke->InputAt(0)));
+
+  // If the register containing the pointer to "this", and the register
+  // containing the pointer to "anObject" are the same register then
+  // "this", and "anObject" are the same object and we can
+  // short-circuit the logic to a true result.
+  if (str == arg) {
+    __ LoadConst64(out, 1);
+    return;
+  }
+
+  StringEqualsOptimizations optimizations(invoke);
+  if (!optimizations.GetArgumentNotNull()) {
+    // Check if input is null, return false if it is.
+    __ Beqzc(arg, &return_false);
+  }
+
+  // Reference equality check, return true if same reference.
+  __ Beqc(str, arg, &return_true);
+
+  if (!optimizations.GetArgumentIsString()) {
+    // Instanceof check for the argument by comparing class fields.
+    // All string objects must have the same type since String cannot be subclassed.
+    // Receiver must be a string object, so its class field is equal to all strings' class fields.
+    // If the argument is a string object, its class field must be equal to receiver's class field.
+    //
+    // As the String class is expected to be non-movable, we can read the class
+    // field from String.equals' arguments without read barriers.
+    AssertNonMovableStringClass();
+    // /* HeapReference<Class> */ temp1 = str->klass_
+    __ Lw(temp1, str, class_offset);
+    // /* HeapReference<Class> */ temp2 = arg->klass_
+    __ Lw(temp2, arg, class_offset);
+    // Also, because we use the previously loaded class references only in the
+    // following comparison, we don't need to unpoison them.
+    __ Bnec(temp1, temp2, &return_false);
+  }
+
+  // Load `count` fields of this and argument strings.
+  __ Lw(temp1, str, count_offset);
+  __ Lw(temp2, arg, count_offset);
+  // Check if `count` fields are equal, return false if they're not.
+  // Also compares the compression style, if differs return false.
+  __ Bnec(temp1, temp2, &return_false);
+  // Return true if both strings are empty. Even with string compression `count == 0` means empty.
+  static_assert(static_cast<uint32_t>(mirror::StringCompressionFlag::kCompressed) == 0u,
+                "Expecting 0=compressed, 1=uncompressed");
+  __ Beqzc(temp1, &return_true);
+
+  // Don't overwrite input registers
+  __ Move(TMP, str);
+  __ Move(temp3, arg);
+
+  // Assertions that must hold in order to compare strings 8 bytes at a time.
+  DCHECK_ALIGNED(value_offset, 8);
+  static_assert(IsAligned<8>(kObjectAlignment), "String of odd length is not zero padded");
+
+  if (mirror::kUseStringCompression) {
+    // For string compression, calculate the number of bytes to compare (not chars).
+    __ Dext(temp2, temp1, 0, 1);         // Extract compression flag.
+    __ Srl(temp1, temp1, 1);             // Extract length.
+    __ Sllv(temp1, temp1, temp2);        // Double the byte count if uncompressed.
+  }
+
+  // Loop to compare strings 8 bytes at a time starting at the beginning of the string.
+  // Ok to do this because strings are zero-padded to kObjectAlignment.
+  __ Bind(&loop);
+  __ Ld(out, TMP, value_offset);
+  __ Ld(temp2, temp3, value_offset);
+  __ Bnec(out, temp2, &return_false);
+  __ Daddiu(TMP, TMP, 8);
+  __ Daddiu(temp3, temp3, 8);
+  // With string compression, we have compared 8 bytes, otherwise 4 chars.
+  __ Addiu(temp1, temp1, mirror::kUseStringCompression ? -8 : -4);
+  __ Bgtzc(temp1, &loop);
+
+  // Return true and exit the function.
+  // If loop does not result in returning false, we return true.
+  __ Bind(&return_true);
+  __ LoadConst64(out, 1);
+  __ Bc(&end);
+
+  // Return false and exit the function.
+  __ Bind(&return_false);
+  __ LoadConst64(out, 0);
+  __ Bind(&end);
+}
+
+static void GenerateStringIndexOf(HInvoke* invoke,
+                                  Riscv64Assembler* assembler,
+                                  CodeGeneratorRISCV64* codegen,
+                                  bool start_at_zero) {
+  LocationSummary* locations = invoke->GetLocations();
+  GpuRegister tmp_reg = start_at_zero ? locations->GetTemp(0).AsRegister<GpuRegister>() : TMP;
+
+  // Note that the null check must have been done earlier.
+  DCHECK(!invoke->CanDoImplicitNullCheckOn(invoke->InputAt(0)));
+
+  // Check for code points > 0xFFFF. Either a slow-path check when we don't know statically,
+  // or directly dispatch for a large constant, or omit slow-path for a small constant or a char.
+  SlowPathCodeRISCV64* slow_path = nullptr;
+  HInstruction* code_point = invoke->InputAt(1);
+  if (code_point->IsIntConstant()) {
+    if (!IsUint<16>(code_point->AsIntConstant()->GetValue())) {
+      // Always needs the slow-path. We could directly dispatch to it,
+      // but this case should be rare, so for simplicity just put the
+      // full slow-path down and branch unconditionally.
+      slow_path = new (codegen->GetScopedAllocator()) IntrinsicSlowPathRISCV64(invoke);
+      codegen->AddSlowPath(slow_path);
+      __ Bc(slow_path->GetEntryLabel());
+      __ Bind(slow_path->GetExitLabel());
+      return;
+    }
+  } else if (code_point->GetType() != DataType::Type::kUint16) {
+    GpuRegister char_reg = locations->InAt(1).AsRegister<GpuRegister>();
+    __ LoadConst32(tmp_reg, std::numeric_limits<uint16_t>::max());
+    slow_path = new (codegen->GetScopedAllocator()) IntrinsicSlowPathRISCV64(invoke);
+    codegen->AddSlowPath(slow_path);
+    __ Bltuc(tmp_reg, char_reg, slow_path->GetEntryLabel());    // UTF-16 required
+  }
+
+  if (start_at_zero) {
+    DCHECK_EQ(tmp_reg, A2);
+    // Start-index = 0.
+    __ Clear(tmp_reg);
+  }
+
+  codegen->InvokeRuntime(kQuickIndexOf, invoke, invoke->GetDexPc(), slow_path);
+  CheckEntrypointTypes<kQuickIndexOf, int32_t, void*, uint32_t, uint32_t>();
+
+  if (slow_path != nullptr) {
+    __ Bind(slow_path->GetExitLabel());
+  }
+}
+
+// int java.lang.String.indexOf(int ch)
+void IntrinsicLocationsBuilderRISCV64::VisitStringIndexOf(HInvoke* invoke) {
+  LocationSummary* locations = new (allocator_) LocationSummary(
+      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  // We have a hand-crafted assembly stub that follows the runtime
+  // calling convention. So it's best to align the inputs accordingly.
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+  Location outLocation = calling_convention.GetReturnLocation(DataType::Type::kInt32);
+  locations->SetOut(Location::RegisterLocation(outLocation.AsRegister<GpuRegister>()));
+
+  // Need a temp for slow-path codepoint compare, and need to send start-index=0.
+  locations->AddTemp(Location::RegisterLocation(calling_convention.GetRegisterAt(2)));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringIndexOf(HInvoke* invoke) {
+  GenerateStringIndexOf(invoke, GetAssembler(), codegen_, /* start_at_zero= */ true);
+}
+
+// int java.lang.String.indexOf(int ch, int fromIndex)
+void IntrinsicLocationsBuilderRISCV64::VisitStringIndexOfAfter(HInvoke* invoke) {
+  LocationSummary* locations = new (allocator_) LocationSummary(
+      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  // We have a hand-crafted assembly stub that follows the runtime
+  // calling convention. So it's best to align the inputs accordingly.
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+  locations->SetInAt(2, Location::RegisterLocation(calling_convention.GetRegisterAt(2)));
+  Location outLocation = calling_convention.GetReturnLocation(DataType::Type::kInt32);
+  locations->SetOut(Location::RegisterLocation(outLocation.AsRegister<GpuRegister>()));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringIndexOfAfter(HInvoke* invoke) {
+  GenerateStringIndexOf(invoke, GetAssembler(), codegen_, /* start_at_zero= */ false);
+}
+
+// java.lang.StringFactory.newStringFromBytes(byte[] data, int high, int offset, int byteCount)
+void IntrinsicLocationsBuilderRISCV64::VisitStringNewStringFromBytes(HInvoke* invoke) {
+  LocationSummary* locations = new (allocator_) LocationSummary(
+      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+  locations->SetInAt(2, Location::RegisterLocation(calling_convention.GetRegisterAt(2)));
+  locations->SetInAt(3, Location::RegisterLocation(calling_convention.GetRegisterAt(3)));
+  Location outLocation = calling_convention.GetReturnLocation(DataType::Type::kInt32);
+  locations->SetOut(Location::RegisterLocation(outLocation.AsRegister<GpuRegister>()));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromBytes(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+
+  GpuRegister byte_array = locations->InAt(0).AsRegister<GpuRegister>();
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) IntrinsicSlowPathRISCV64(invoke);
+  codegen_->AddSlowPath(slow_path);
+  __ Beqzc(byte_array, slow_path->GetEntryLabel());
+
+  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, invoke->GetDexPc(), slow_path);
+  CheckEntrypointTypes<kQuickAllocStringFromBytes, void*, void*, int32_t, int32_t, int32_t>();
+  __ Bind(slow_path->GetExitLabel());
+}
+
+// java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
+void IntrinsicLocationsBuilderRISCV64::VisitStringNewStringFromChars(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  locations->SetInAt(1, Location::RegisterLocation(calling_convention.GetRegisterAt(1)));
+  locations->SetInAt(2, Location::RegisterLocation(calling_convention.GetRegisterAt(2)));
+  Location outLocation = calling_convention.GetReturnLocation(DataType::Type::kInt32);
+  locations->SetOut(Location::RegisterLocation(outLocation.AsRegister<GpuRegister>()));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromChars(HInvoke* invoke) {
+  // No need to emit code checking whether `locations->InAt(2)` is a null
+  // pointer, as callers of the native method
+  //
+  //   java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
+  //
+  // all include a null check on `data` before calling that method.
+  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke, invoke->GetDexPc());
+  CheckEntrypointTypes<kQuickAllocStringFromChars, void*, int32_t, int32_t, void*>();
+}
+
+// java.lang.StringFactory.newStringFromString(String toCopy)
+void IntrinsicLocationsBuilderRISCV64::VisitStringNewStringFromString(HInvoke* invoke) {
+  LocationSummary* locations = new (allocator_) LocationSummary(
+      invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+  InvokeRuntimeCallingConvention calling_convention;
+  locations->SetInAt(0, Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+  Location outLocation = calling_convention.GetReturnLocation(DataType::Type::kInt32);
+  locations->SetOut(Location::RegisterLocation(outLocation.AsRegister<GpuRegister>()));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromString(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+
+  GpuRegister string_to_copy = locations->InAt(0).AsRegister<GpuRegister>();
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) IntrinsicSlowPathRISCV64(invoke);
+  codegen_->AddSlowPath(slow_path);
+  __ Beqzc(string_to_copy, slow_path->GetEntryLabel());
+
+  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, invoke->GetDexPc(), slow_path);
+  CheckEntrypointTypes<kQuickAllocStringFromString, void*, void*>();
+  __ Bind(slow_path->GetExitLabel());
+}
+
+static void GenIsInfinite(LocationSummary* locations,
+                          bool is64bit,
+                          Riscv64Assembler* assembler) {
+  FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (is64bit) {
+    __ ClassD(FTMP, in);
+  } else {
+    __ ClassS(FTMP, in);
+  }
+  __ Mfc1(out, FTMP);
+  __ Andi(out, out, kPositiveInfinity | kNegativeInfinity);
+  __ Sltu(out, ZERO, out);
+}
+
+// boolean java.lang.Float.isInfinite(float)
+void IntrinsicLocationsBuilderRISCV64::VisitFloatIsInfinite(HInvoke* invoke) {
+  CreateFPToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitFloatIsInfinite(HInvoke* invoke) {
+  GenIsInfinite(invoke->GetLocations(), /* is64bit= */ false, GetAssembler());
+}
+
+// boolean java.lang.Double.isInfinite(double)
+void IntrinsicLocationsBuilderRISCV64::VisitDoubleIsInfinite(HInvoke* invoke) {
+  CreateFPToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitDoubleIsInfinite(HInvoke* invoke) {
+  GenIsInfinite(invoke->GetLocations(), /* is64bit= */ true, GetAssembler());
+}
+
+// void java.lang.String.getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin)
+void IntrinsicLocationsBuilderRISCV64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RequiresRegister());
+  locations->SetInAt(2, Location::RequiresRegister());
+  locations->SetInAt(3, Location::RequiresRegister());
+  locations->SetInAt(4, Location::RequiresRegister());
+
+  locations->AddTemp(Location::RequiresRegister());
+  locations->AddTemp(Location::RequiresRegister());
+  locations->AddTemp(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+
+  // Check assumption that sizeof(Char) is 2 (used in scaling below).
+  const size_t char_size = DataType::Size(DataType::Type::kUint16);
+  DCHECK_EQ(char_size, 2u);
+  const size_t char_shift = DataType::SizeShift(DataType::Type::kUint16);
+
+  GpuRegister srcObj = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister srcBegin = locations->InAt(1).AsRegister<GpuRegister>();
+  GpuRegister srcEnd = locations->InAt(2).AsRegister<GpuRegister>();
+  GpuRegister dstObj = locations->InAt(3).AsRegister<GpuRegister>();
+  GpuRegister dstBegin = locations->InAt(4).AsRegister<GpuRegister>();
+
+  GpuRegister dstPtr = locations->GetTemp(0).AsRegister<GpuRegister>();
+  GpuRegister srcPtr = locations->GetTemp(1).AsRegister<GpuRegister>();
+  GpuRegister numChrs = locations->GetTemp(2).AsRegister<GpuRegister>();
+
+  Riscv64Label done;
+  Riscv64Label loop;
+
+  // Location of data in char array buffer.
+  const uint32_t data_offset = mirror::Array::DataOffset(char_size).Uint32Value();
+
+  // Get offset of value field within a string object.
+  const int32_t value_offset = mirror::String::ValueOffset().Int32Value();
+
+  __ Beqc(srcEnd, srcBegin, &done);  // No characters to move.
+
+  // Calculate number of characters to be copied.
+  __ Dsubu(numChrs, srcEnd, srcBegin);
+
+  // Calculate destination address.
+  __ Daddiu(dstPtr, dstObj, data_offset);
+  __ Dlsa(dstPtr, dstBegin, dstPtr, char_shift);
+
+  if (mirror::kUseStringCompression) {
+    Riscv64Label uncompressed_copy, compressed_loop;
+    const uint32_t count_offset = mirror::String::CountOffset().Uint32Value();
+    // Load count field and extract compression flag.
+    __ LoadFromOffset(kLoadWord, TMP, srcObj, count_offset);
+    __ Dext(TMP, TMP, 0, 1);
+
+    // If string is uncompressed, use uncompressed path.
+    __ Bnezc(TMP, &uncompressed_copy);
+
+    // Copy loop for compressed src, copying 1 character (8-bit) to (16-bit) at a time.
+    __ Daddu(srcPtr, srcObj, srcBegin);
+    __ Bind(&compressed_loop);
+    __ LoadFromOffset(kLoadUnsignedByte, TMP, srcPtr, value_offset);
+    __ StoreToOffset(kStoreHalfword, TMP, dstPtr, 0);
+    __ Daddiu(numChrs, numChrs, -1);
+    __ Daddiu(srcPtr, srcPtr, 1);
+    __ Daddiu(dstPtr, dstPtr, 2);
+    __ Bnezc(numChrs, &compressed_loop);
+
+    __ Bc(&done);
+    __ Bind(&uncompressed_copy);
+  }
+
+  // Calculate source address.
+  __ Daddiu(srcPtr, srcObj, value_offset);
+  __ Dlsa(srcPtr, srcBegin, srcPtr, char_shift);
+
+  __ Bind(&loop);
+  __ Lh(AT, srcPtr, 0);
+  __ Daddiu(numChrs, numChrs, -1);
+  __ Daddiu(srcPtr, srcPtr, char_size);
+  __ Sh(AT, dstPtr, 0);
+  __ Daddiu(dstPtr, dstPtr, char_size);
+  __ Bnezc(numChrs, &loop);
+
+  __ Bind(&done);
+}
+
+// static void java.lang.System.arraycopy(Object src, int srcPos,
+//                                        Object dest, int destPos,
+//                                        int length)
+void IntrinsicLocationsBuilderRISCV64::VisitSystemArrayCopyChar(HInvoke* invoke) {
+  HIntConstant* src_pos = invoke->InputAt(1)->AsIntConstant();
+  HIntConstant* dest_pos = invoke->InputAt(3)->AsIntConstant();
+  HIntConstant* length = invoke->InputAt(4)->AsIntConstant();
+
+  // As long as we are checking, we might as well check to see if the src and dest
+  // positions are >= 0.
+  if ((src_pos != nullptr && src_pos->GetValue() < 0) ||
+      (dest_pos != nullptr && dest_pos->GetValue() < 0)) {
+    // We will have to fail anyways.
+    return;
+  }
+
+  // And since we are already checking, check the length too.
+  if (length != nullptr) {
+    int32_t len = length->GetValue();
+    if (len < 0) {
+      // Just call as normal.
+      return;
+    }
+  }
+
+  // Okay, it is safe to generate inline code.
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+  // arraycopy(Object src, int srcPos, Object dest, int destPos, int length).
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RegisterOrConstant(invoke->InputAt(1)));
+  locations->SetInAt(2, Location::RequiresRegister());
+  locations->SetInAt(3, Location::RegisterOrConstant(invoke->InputAt(3)));
+  locations->SetInAt(4, Location::RegisterOrConstant(invoke->InputAt(4)));
+
+  locations->AddTemp(Location::RequiresRegister());
+  locations->AddTemp(Location::RequiresRegister());
+  locations->AddTemp(Location::RequiresRegister());
+}
+
+// Utility routine to verify that "length(input) - pos >= length"
+static void EnoughItems(Riscv64Assembler* assembler,
+                        GpuRegister length_input_minus_pos,
+                        Location length,
+                        SlowPathCodeRISCV64* slow_path) {
+  if (length.IsConstant()) {
+    int32_t length_constant = length.GetConstant()->AsIntConstant()->GetValue();
+
+    if (IsInt<16>(length_constant)) {
+      __ Slti(TMP, length_input_minus_pos, length_constant);
+      __ Bnezc(TMP, slow_path->GetEntryLabel());
+    } else {
+      __ LoadConst32(TMP, length_constant);
+      __ Bltc(length_input_minus_pos, TMP, slow_path->GetEntryLabel());
+    }
+  } else {
+    __ Bltc(length_input_minus_pos, length.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+  }
+}
+
+static void CheckPosition(Riscv64Assembler* assembler,
+                          Location pos,
+                          GpuRegister input,
+                          Location length,
+                          SlowPathCodeRISCV64* slow_path,
+                          bool length_is_input_length = false) {
+  // Where is the length in the Array?
+  const uint32_t length_offset = mirror::Array::LengthOffset().Uint32Value();
+
+  // Calculate length(input) - pos.
+  if (pos.IsConstant()) {
+    int32_t pos_const = pos.GetConstant()->AsIntConstant()->GetValue();
+    if (pos_const == 0) {
+      if (!length_is_input_length) {
+        // Check that length(input) >= length.
+        __ LoadFromOffset(kLoadWord, AT, input, length_offset);
+        EnoughItems(assembler, AT, length, slow_path);
+      }
+    } else {
+      // Check that (length(input) - pos) >= zero.
+      __ LoadFromOffset(kLoadWord, AT, input, length_offset);
+      DCHECK_GT(pos_const, 0);
+      __ Addiu32(AT, AT, -pos_const);
+      __ Bltzc(AT, slow_path->GetEntryLabel());
+
+      // Verify that (length(input) - pos) >= length.
+      EnoughItems(assembler, AT, length, slow_path);
+    }
+  } else if (length_is_input_length) {
+    // The only way the copy can succeed is if pos is zero.
+    GpuRegister pos_reg = pos.AsRegister<GpuRegister>();
+    __ Bnezc(pos_reg, slow_path->GetEntryLabel());
+  } else {
+    // Verify that pos >= 0.
+    GpuRegister pos_reg = pos.AsRegister<GpuRegister>();
+    __ Bltzc(pos_reg, slow_path->GetEntryLabel());
+
+    // Check that (length(input) - pos) >= zero.
+    __ LoadFromOffset(kLoadWord, AT, input, length_offset);
+    __ Subu(AT, AT, pos_reg);
+    __ Bltzc(AT, slow_path->GetEntryLabel());
+
+    // Verify that (length(input) - pos) >= length.
+    EnoughItems(assembler, AT, length, slow_path);
+  }
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitSystemArrayCopyChar(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  LocationSummary* locations = invoke->GetLocations();
+
+  GpuRegister src = locations->InAt(0).AsRegister<GpuRegister>();
+  Location src_pos = locations->InAt(1);
+  GpuRegister dest = locations->InAt(2).AsRegister<GpuRegister>();
+  Location dest_pos = locations->InAt(3);
+  Location length = locations->InAt(4);
+
+  Riscv64Label loop;
+
+  GpuRegister dest_base = locations->GetTemp(0).AsRegister<GpuRegister>();
+  GpuRegister src_base = locations->GetTemp(1).AsRegister<GpuRegister>();
+  GpuRegister count = locations->GetTemp(2).AsRegister<GpuRegister>();
+
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) IntrinsicSlowPathRISCV64(invoke);
+  codegen_->AddSlowPath(slow_path);
+
+  // Bail out if the source and destination are the same (to handle overlap).
+  __ Beqc(src, dest, slow_path->GetEntryLabel());
+
+  // Bail out if the source is null.
+  __ Beqzc(src, slow_path->GetEntryLabel());
+
+  // Bail out if the destination is null.
+  __ Beqzc(dest, slow_path->GetEntryLabel());
+
+  // Load length into register for count.
+  if (length.IsConstant()) {
+    __ LoadConst32(count, length.GetConstant()->AsIntConstant()->GetValue());
+  } else {
+    // If the length is negative, bail out.
+    // We have already checked in the LocationsBuilder for the constant case.
+    __ Bltzc(length.AsRegister<GpuRegister>(), slow_path->GetEntryLabel());
+
+    __ Move(count, length.AsRegister<GpuRegister>());
+  }
+
+  // Validity checks: source.
+  CheckPosition(assembler, src_pos, src, Location::RegisterLocation(count), slow_path);
+
+  // Validity checks: dest.
+  CheckPosition(assembler, dest_pos, dest, Location::RegisterLocation(count), slow_path);
+
+  // If count is zero, we're done.
+  __ Beqzc(count, slow_path->GetExitLabel());
+
+  // Okay, everything checks out.  Finally time to do the copy.
+  // Check assumption that sizeof(Char) is 2 (used in scaling below).
+  const size_t char_size = DataType::Size(DataType::Type::kUint16);
+  DCHECK_EQ(char_size, 2u);
+
+  const size_t char_shift = DataType::SizeShift(DataType::Type::kUint16);
+
+  const uint32_t data_offset = mirror::Array::DataOffset(char_size).Uint32Value();
+
+  // Calculate source and destination addresses.
+  if (src_pos.IsConstant()) {
+    int32_t src_pos_const = src_pos.GetConstant()->AsIntConstant()->GetValue();
+
+    __ Daddiu64(src_base, src, data_offset + char_size * src_pos_const, TMP);
+  } else {
+    __ Daddiu64(src_base, src, data_offset, TMP);
+    __ Dlsa(src_base, src_pos.AsRegister<GpuRegister>(), src_base, char_shift);
+  }
+  if (dest_pos.IsConstant()) {
+    int32_t dest_pos_const = dest_pos.GetConstant()->AsIntConstant()->GetValue();
+
+    __ Daddiu64(dest_base, dest, data_offset + char_size * dest_pos_const, TMP);
+  } else {
+    __ Daddiu64(dest_base, dest, data_offset, TMP);
+    __ Dlsa(dest_base, dest_pos.AsRegister<GpuRegister>(), dest_base, char_shift);
+  }
+
+  __ Bind(&loop);
+  __ Lh(TMP, src_base, 0);
+  __ Daddiu(src_base, src_base, char_size);
+  __ Daddiu(count, count, -1);
+  __ Sh(TMP, dest_base, 0);
+  __ Daddiu(dest_base, dest_base, char_size);
+  __ Bnezc(count, &loop);
+
+  __ Bind(slow_path->GetExitLabel());
+}
+
+static void GenHighestOneBit(LocationSummary* locations,
+                             DataType::Type type,
+                             Riscv64Assembler* assembler) {
+  DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64) << type;
+
+  GpuRegister in = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (type == DataType::Type::kInt64) {
+    __ Dclz(TMP, in);
+    __ LoadConst64(AT, INT64_C(0x8000000000000000));
+    __ Dsrlv(AT, AT, TMP);
+  } else {
+    __ Clz(TMP, in);
+    __ LoadConst32(AT, 0x80000000);
+    __ Srlv(AT, AT, TMP);
+  }
+  // For either value of "type", when "in" is zero, "out" should also
+  // be zero. Without this extra "and" operation, when "in" is zero,
+  // "out" would be either Integer.MIN_VALUE, or Long.MIN_VALUE because
+  // the MIPS logical shift operations "dsrlv", and "srlv" don't use
+  // the shift amount (TMP) directly; they use either (TMP % 64) or
+  // (TMP % 32), respectively.
+  __ And(out, AT, in);
+}
+
+// int java.lang.Integer.highestOneBit(int)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerHighestOneBit(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerHighestOneBit(HInvoke* invoke) {
+  GenHighestOneBit(invoke->GetLocations(), DataType::Type::kInt32, GetAssembler());
+}
+
+// long java.lang.Long.highestOneBit(long)
+void IntrinsicLocationsBuilderRISCV64::VisitLongHighestOneBit(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongHighestOneBit(HInvoke* invoke) {
+  GenHighestOneBit(invoke->GetLocations(), DataType::Type::kInt64, GetAssembler());
+}
+
+static void GenLowestOneBit(LocationSummary* locations,
+                            DataType::Type type,
+                            Riscv64Assembler* assembler) {
+  DCHECK(type == DataType::Type::kInt32 || type == DataType::Type::kInt64) << type;
+
+  GpuRegister in = locations->InAt(0).AsRegister<GpuRegister>();
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+
+  if (type == DataType::Type::kInt64) {
+    __ Dsubu(TMP, ZERO, in);
+  } else {
+    __ Subu(TMP, ZERO, in);
+  }
+  __ And(out, TMP, in);
+}
+
+// int java.lang.Integer.lowestOneBit(int)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerLowestOneBit(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerLowestOneBit(HInvoke* invoke) {
+  GenLowestOneBit(invoke->GetLocations(), DataType::Type::kInt32, GetAssembler());
+}
+
+// long java.lang.Long.lowestOneBit(long)
+void IntrinsicLocationsBuilderRISCV64::VisitLongLowestOneBit(HInvoke* invoke) {
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitLongLowestOneBit(HInvoke* invoke) {
+  GenLowestOneBit(invoke->GetLocations(), DataType::Type::kInt64, GetAssembler());
+}
+
+static void CreateFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  InvokeRuntimeCallingConvention calling_convention;
+
+  locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
+  locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kFloat64));
+}
+
+static void CreateFPFPToFPCallLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnMainOnly, kIntrinsified);
+  InvokeRuntimeCallingConvention calling_convention;
+
+  locations->SetInAt(0, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(0)));
+  locations->SetInAt(1, Location::FpuRegisterLocation(calling_convention.GetFpuRegisterAt(1)));
+  locations->SetOut(calling_convention.GetReturnLocation(DataType::Type::kFloat64));
+}
+
+static void GenFPToFPCall(HInvoke* invoke,
+                          CodeGeneratorRISCV64* codegen,
+                          QuickEntrypointEnum entry) {
+  LocationSummary* locations = invoke->GetLocations();
+  FpuRegister in = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  DCHECK_EQ(in, FA0);
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+  DCHECK_EQ(out, FA0);
+
+  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+}
+
+static void GenFPFPToFPCall(HInvoke* invoke,
+                            CodeGeneratorRISCV64* codegen,
+                            QuickEntrypointEnum entry) {
+  LocationSummary* locations = invoke->GetLocations();
+  FpuRegister in0 = locations->InAt(0).AsFpuRegister<FpuRegister>();
+  DCHECK_EQ(in0, FA0);
+  FpuRegister in1 = locations->InAt(1).AsFpuRegister<FpuRegister>();
+  DCHECK_EQ(in1, FA1);
+  FpuRegister out = locations->Out().AsFpuRegister<FpuRegister>();
+  DCHECK_EQ(out, FA0);
+
+  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+}
+
+// static double java.lang.Math.cos(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathCos(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathCos(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickCos);
+}
+
+// static double java.lang.Math.sin(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathSin(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathSin(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickSin);
+}
+
+// static double java.lang.Math.acos(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathAcos(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathAcos(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickAcos);
+}
+
+// static double java.lang.Math.asin(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathAsin(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathAsin(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickAsin);
+}
+
+// static double java.lang.Math.atan(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathAtan(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathAtan(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickAtan);
+}
+
+// static double java.lang.Math.atan2(double y, double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathAtan2(HInvoke* invoke) {
+  CreateFPFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathAtan2(HInvoke* invoke) {
+  GenFPFPToFPCall(invoke, codegen_, kQuickAtan2);
+}
+
+// static double java.lang.Math.pow(double y, double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathPow(HInvoke* invoke) {
+  CreateFPFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathPow(HInvoke* invoke) {
+  GenFPFPToFPCall(invoke, codegen_, kQuickPow);
+}
+
+// static double java.lang.Math.cbrt(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathCbrt(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathCbrt(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickCbrt);
+}
+
+// static double java.lang.Math.cosh(double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathCosh(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathCosh(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickCosh);
+}
+
+// static double java.lang.Math.exp(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathExp(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathExp(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickExp);
+}
+
+// static double java.lang.Math.expm1(double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathExpm1(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathExpm1(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickExpm1);
+}
+
+// static double java.lang.Math.hypot(double x, double y)
+void IntrinsicLocationsBuilderRISCV64::VisitMathHypot(HInvoke* invoke) {
+  CreateFPFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathHypot(HInvoke* invoke) {
+  GenFPFPToFPCall(invoke, codegen_, kQuickHypot);
+}
+
+// static double java.lang.Math.log(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathLog(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathLog(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickLog);
+}
+
+// static double java.lang.Math.log10(double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathLog10(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathLog10(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickLog10);
+}
+
+// static double java.lang.Math.nextAfter(double start, double direction)
+void IntrinsicLocationsBuilderRISCV64::VisitMathNextAfter(HInvoke* invoke) {
+  CreateFPFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathNextAfter(HInvoke* invoke) {
+  GenFPFPToFPCall(invoke, codegen_, kQuickNextAfter);
+}
+
+// static double java.lang.Math.sinh(double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathSinh(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathSinh(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickSinh);
+}
+
+// static double java.lang.Math.tan(double a)
+void IntrinsicLocationsBuilderRISCV64::VisitMathTan(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathTan(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickTan);
+}
+
+// static double java.lang.Math.tanh(double x)
+void IntrinsicLocationsBuilderRISCV64::VisitMathTanh(HInvoke* invoke) {
+  CreateFPToFPCallLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMathTanh(HInvoke* invoke) {
+  GenFPToFPCall(invoke, codegen_, kQuickTanh);
+}
+
+// long java.lang.Integer.valueOf(long)
+void IntrinsicLocationsBuilderRISCV64::VisitIntegerValueOf(HInvoke* invoke) {
+  InvokeRuntimeCallingConvention calling_convention;
+  IntrinsicVisitor::ComputeIntegerValueOfLocations(
+      invoke,
+      codegen_,
+      calling_convention.GetReturnLocation(DataType::Type::kReference),
+      Location::RegisterLocation(calling_convention.GetRegisterAt(0)));
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitIntegerValueOf(HInvoke* invoke) {
+  IntrinsicVisitor::IntegerValueOfInfo info =
+      IntrinsicVisitor::ComputeIntegerValueOfInfo(invoke, codegen_->GetCompilerOptions());
+  LocationSummary* locations = invoke->GetLocations();
+  Riscv64Assembler* assembler = GetAssembler();
+  InstructionCodeGeneratorRISCV64* icodegen =
+      down_cast<InstructionCodeGeneratorRISCV64*>(codegen_->GetInstructionVisitor());
+
+  GpuRegister out = locations->Out().AsRegister<GpuRegister>();
+  if (invoke->InputAt(0)->IsConstant()) {
+    int32_t value = invoke->InputAt(0)->AsIntConstant()->GetValue();
+    if (static_cast<uint32_t>(value - info.low) < info.length) {
+      // Just embed the j.l.Integer in the code.
+      DCHECK_NE(info.value_boot_image_reference, IntegerValueOfInfo::kInvalidReference);
+      codegen_->LoadBootImageAddress(out, info.value_boot_image_reference);
+    } else {
+      DCHECK(locations->CanCall());
+      // Allocate and initialize a new j.l.Integer.
+      // TODO: If we JIT, we could allocate the j.l.Integer now, and store it in the
+      // JIT object table.
+      codegen_->AllocateInstanceForIntrinsic(invoke->AsInvokeStaticOrDirect(),
+                                             info.integer_boot_image_offset);
+      __ StoreConstToOffset(kStoreWord, value, out, info.value_offset, TMP);
+      // `value` is a final field :-( Ideally, we'd merge this memory barrier with the allocation
+      // one.
+      icodegen->GenerateMemoryBarrier(MemBarrierKind::kStoreStore);
+    }
+  } else {
+    DCHECK(locations->CanCall());
+    GpuRegister in = locations->InAt(0).AsRegister<GpuRegister>();
+    Riscv64Label allocate, done;
+
+    __ Addiu32(out, in, -info.low);
+    // As unsigned quantities is out < info.length ?
+    __ LoadConst32(AT, info.length);
+    // Branch if out >= info.length . This means that "in" is outside of the valid range.
+    __ Bgeuc(out, AT, &allocate);
+
+    // If the value is within the bounds, load the j.l.Integer directly from the array.
+    codegen_->LoadBootImageAddress(TMP, info.array_data_boot_image_reference);
+    __ Dlsa(out, out, TMP, TIMES_4);
+    __ Lwu(out, out, 0);
+    __ MaybeUnpoisonHeapReference(out);
+    __ Bc(&done);
+
+    __ Bind(&allocate);
+    // Otherwise allocate and initialize a new j.l.Integer.
+    codegen_->AllocateInstanceForIntrinsic(invoke->AsInvokeStaticOrDirect(),
+                                           info.integer_boot_image_offset);
+    __ StoreToOffset(kStoreWord, in, out, info.value_offset);
+    // `value` is a final field :-( Ideally, we'd merge this memory barrier with the allocation
+    // one.
+    icodegen->GenerateMemoryBarrier(MemBarrierKind::kStoreStore);
+    __ Bind(&done);
+  }
+}
+
+// static boolean java.lang.Thread.interrupted()
+void IntrinsicLocationsBuilderRISCV64::VisitThreadInterrupted(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitThreadInterrupted(HInvoke* invoke) {
+  Riscv64Assembler* assembler = GetAssembler();
+  GpuRegister out = invoke->GetLocations()->Out().AsRegister<GpuRegister>();
+  int32_t offset = Thread::InterruptedOffset<kRiscv64PointerSize>().Int32Value();
+  __ LoadFromOffset(kLoadWord, out, TR, offset);
+  Riscv64Label done;
+  __ Beqzc(out, &done);
+  __ Sync(0);
+  __ StoreToOffset(kStoreWord, ZERO, TR, offset);
+  __ Sync(0);
+  __ Bind(&done);
+}
+
+void IntrinsicLocationsBuilderRISCV64::VisitReachabilityFence(HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator_) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::Any());
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitReachabilityFence(HInvoke* invoke ATTRIBUTE_UNUSED) { }
+
+UNIMPLEMENTED_INTRINSIC(RISCV64, ReferenceGetReferent)
+UNIMPLEMENTED_INTRINSIC(RISCV64, SystemArrayCopy)
+UNIMPLEMENTED_INTRINSIC(RISCV64, CRC32Update)
+UNIMPLEMENTED_INTRINSIC(RISCV64, CRC32UpdateBytes)
+UNIMPLEMENTED_INTRINSIC(RISCV64, CRC32UpdateByteBuffer)
+
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringStringIndexOf);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringStringIndexOfAfter);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringBufferAppend);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringBufferLength);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringBufferToString);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringBuilderLength);
+UNIMPLEMENTED_INTRINSIC(RISCV64, StringBuilderToString);
+
+// 1.8.
+UNIMPLEMENTED_INTRINSIC(RISCV64, UnsafeGetAndAddInt)
+UNIMPLEMENTED_INTRINSIC(RISCV64, UnsafeGetAndAddLong)
+UNIMPLEMENTED_INTRINSIC(RISCV64, UnsafeGetAndSetInt)
+UNIMPLEMENTED_INTRINSIC(RISCV64, UnsafeGetAndSetLong)
+UNIMPLEMENTED_INTRINSIC(RISCV64, UnsafeGetAndSetObject)
+
+UNREACHABLE_INTRINSICS(RISCV64)
+
+#undef __
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/optimizing/intrinsics_riscv64.h b/compiler/optimizing/intrinsics_riscv64.h
new file mode 100644
index 0000000000..58ac8667e3
--- /dev/null
+++ b/compiler/optimizing/intrinsics_riscv64.h
@@ -0,0 +1,86 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_INTRINSICS_RISCV64_H_
+#define ART_COMPILER_OPTIMIZING_INTRINSICS_RISCV64_H_
+
+#include "intrinsics.h"
+
+namespace art {
+
+class ArenaAllocator;
+class HInvokeStaticOrDirect;
+class HInvokeVirtual;
+
+namespace riscv64 {
+
+class CodeGeneratorRISCV64;
+class Riscv64Assembler;
+
+class IntrinsicLocationsBuilderRISCV64 final : public IntrinsicVisitor {
+ public:
+  explicit IntrinsicLocationsBuilderRISCV64(CodeGeneratorRISCV64* codegen);
+
+  // Define visitor methods.
+
+#define OPTIMIZING_INTRINSICS(Name, IsStatic, NeedsEnvironmentOrCache, SideEffects, Exceptions, ...) \
+  void Visit ## Name(HInvoke* invoke) override;
+#include "intrinsics_list.h"
+  INTRINSICS_LIST(OPTIMIZING_INTRINSICS)
+#undef INTRINSICS_LIST
+#undef OPTIMIZING_INTRINSICS
+
+  // Check whether an invoke is an intrinsic, and if so, create a location summary. Returns whether
+  // a corresponding LocationSummary with the intrinsified_ flag set was generated and attached to
+  // the invoke.
+  bool TryDispatch(HInvoke* invoke);
+
+ private:
+  CodeGeneratorRISCV64* const codegen_;
+  ArenaAllocator* const allocator_;
+
+  DISALLOW_COPY_AND_ASSIGN(IntrinsicLocationsBuilderRISCV64);
+};
+
+class IntrinsicCodeGeneratorRISCV64 final : public IntrinsicVisitor {
+ public:
+  explicit IntrinsicCodeGeneratorRISCV64(CodeGeneratorRISCV64* codegen) : codegen_(codegen) {}
+
+  // Define visitor methods.
+
+#define OPTIMIZING_INTRINSICS(Name, IsStatic, NeedsEnvironmentOrCache, SideEffects, Exceptions, ...) \
+  void Visit ## Name(HInvoke* invoke) override;
+#include "intrinsics_list.h"
+  INTRINSICS_LIST(OPTIMIZING_INTRINSICS)
+#undef INTRINSICS_LIST
+#undef OPTIMIZING_INTRINSICS
+
+  bool HasMsa() const;
+
+ private:
+  Riscv64Assembler* GetAssembler();
+
+  ArenaAllocator* GetAllocator();
+
+  CodeGeneratorRISCV64* const codegen_;
+
+  DISALLOW_COPY_AND_ASSIGN(IntrinsicCodeGeneratorRISCV64);
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_INTRINSICS_RISCV64_H_
diff --git a/compiler/optimizing/loop_optimization.cc b/compiler/optimizing/loop_optimization.cc
index 6c76ab858b..b5d44d6337 100644
--- a/compiler/optimizing/loop_optimization.cc
+++ b/compiler/optimizing/loop_optimization.cc
@@ -31,7 +31,12 @@
 namespace art {
 
 // Enables vectorization (SIMDization) in the loop optimizer.
+#ifndef ART_ENABLE_CODEGEN_riscv64
 static constexpr bool kEnableVectorization = true;
+#else
+// FIXME: T-HEAD, Current Riscv64 doesn't support SIMD.
+static constexpr bool kEnableVectorization = false;
+#endif
 
 //
 // Static helpers.
diff --git a/compiler/optimizing/nodes.h b/compiler/optimizing/nodes.h
index fedad0c69a..482cada2e4 100644
--- a/compiler/optimizing/nodes.h
+++ b/compiler/optimizing/nodes.h
@@ -1483,7 +1483,7 @@ class HLoopInformationOutwardIterator : public ValueObject {
 /*
  * Instructions, shared across several (not all) architectures.
  */
-#if !defined(ART_ENABLE_CODEGEN_arm) && !defined(ART_ENABLE_CODEGEN_arm64)
+#if !defined(ART_ENABLE_CODEGEN_arm) && !defined(ART_ENABLE_CODEGEN_arm64) && !defined(ART_ENABLE_CODEGEN_riscv64)
 #define FOR_EACH_CONCRETE_INSTRUCTION_SHARED(M)
 #else
 #define FOR_EACH_CONCRETE_INSTRUCTION_SHARED(M)                         \
@@ -1497,6 +1497,8 @@ class HLoopInformationOutwardIterator : public ValueObject {
 
 #define FOR_EACH_CONCRETE_INSTRUCTION_ARM64(M)
 
+#define FOR_EACH_CONCRETE_INSTRUCTION_RISCV64(M)
+
 #ifndef ART_ENABLE_CODEGEN_mips
 #define FOR_EACH_CONCRETE_INSTRUCTION_MIPS(M)
 #else
@@ -1533,6 +1535,7 @@ class HLoopInformationOutwardIterator : public ValueObject {
   FOR_EACH_CONCRETE_INSTRUCTION_SHARED(M)                               \
   FOR_EACH_CONCRETE_INSTRUCTION_ARM(M)                                  \
   FOR_EACH_CONCRETE_INSTRUCTION_ARM64(M)                                \
+  FOR_EACH_CONCRETE_INSTRUCTION_RISCV64(M)                              \
   FOR_EACH_CONCRETE_INSTRUCTION_MIPS(M)                                 \
   FOR_EACH_CONCRETE_INSTRUCTION_MIPS64(M)                               \
   FOR_EACH_CONCRETE_INSTRUCTION_X86(M)                                  \
@@ -7786,7 +7789,7 @@ class HIntermediateAddress final : public HExpression<2> {
 
 #include "nodes_vector.h"
 
-#if defined(ART_ENABLE_CODEGEN_arm) || defined(ART_ENABLE_CODEGEN_arm64)
+#if defined(ART_ENABLE_CODEGEN_arm) || defined(ART_ENABLE_CODEGEN_arm64) || defined(ART_ENABLE_CODEGEN_riscv64)
 #include "nodes_shared.h"
 #endif
 #ifdef ART_ENABLE_CODEGEN_mips
@@ -7818,7 +7821,7 @@ class HGraphVisitor : public ValueObject {
 
   HGraph* GetGraph() const { return graph_; }
 
-  // Visit functions for instruction classes.
+  // Visit functions for instruction classes. WWD
 #define DECLARE_VISIT_INSTRUCTION(name, super)                                        \
   virtual void Visit##name(H##name* instr) { VisitInstruction(instr); }
 
@@ -7841,7 +7844,7 @@ class HGraphDelegateVisitor : public HGraphVisitor {
       : HGraphVisitor(graph, stats) {}
   virtual ~HGraphDelegateVisitor() {}
 
-  // Visit functions that delegate to to super class.
+  // Visit functions that delegate to to super class.  WWD
 #define DECLARE_VISIT_INSTRUCTION(name, super)                                        \
   void Visit##name(H##name* instr) override { Visit##super(instr); }
 
@@ -8003,7 +8006,7 @@ inline bool IsZeroBitPattern(HInstruction* instruction) {
     return results[static_cast<size_t>(GetKind())];                            \
   }
 
-  FOR_EACH_ABSTRACT_INSTRUCTION(INSTRUCTION_TYPE_CHECK)
+FOR_EACH_ABSTRACT_INSTRUCTION(INSTRUCTION_TYPE_CHECK)
 #undef INSTRUCTION_TYPE_CHECK
 #undef INSTRUCTION_TYPE_CHECK_RESULT
 
@@ -8015,7 +8018,7 @@ inline bool IsZeroBitPattern(HInstruction* instruction) {
     return Is##type() ? static_cast<H##type*>(this) : nullptr;                 \
   }
 
-  FOR_EACH_INSTRUCTION(INSTRUCTION_TYPE_CAST)
+FOR_EACH_INSTRUCTION(INSTRUCTION_TYPE_CAST)
 #undef INSTRUCTION_TYPE_CAST
 
 
diff --git a/compiler/optimizing/optimizing_compiler.cc b/compiler/optimizing/optimizing_compiler.cc
index f4bf11d3d3..b441a18b60 100644
--- a/compiler/optimizing/optimizing_compiler.cc
+++ b/compiler/optimizing/optimizing_compiler.cc
@@ -452,6 +452,7 @@ static bool IsInstructionSetSupported(InstructionSet instruction_set) {
       || instruction_set == InstructionSet::kThumb2
       || instruction_set == InstructionSet::kMips
       || instruction_set == InstructionSet::kMips64
+      || instruction_set == InstructionSet::kRiscv64
       || instruction_set == InstructionSet::kX86
       || instruction_set == InstructionSet::kX86_64;
 }
@@ -567,6 +568,20 @@ bool OptimizingCompiler::RunArchOptimizations(HGraph* graph,
                               mips64_optimizations);
     }
 #endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64: {
+      OptimizationDef riscv64_optimizations[] = {
+        OptDef(OptimizationPass::kSideEffectsAnalysis),
+        OptDef(OptimizationPass::kGlobalValueNumbering, "GVN$after_arch")
+      };
+      return RunOptimizations(graph,
+                              codegen,
+                              dex_compilation_unit,
+                              pass_observer,
+                              handles,
+                              riscv64_optimizations);
+    }
+#endif
 #ifdef ART_ENABLE_CODEGEN_x86
     case InstructionSet::kX86: {
       OptimizationDef x86_optimizations[] = {
@@ -1180,6 +1195,8 @@ CompiledMethod* OptimizingCompiler::JniCompile(uint32_t access_flags,
   ArenaStack arena_stack(runtime->GetArenaPool());
 
   const CompilerOptions& compiler_options = GetCompilerOptions();
+  // FIXME: T-HEAD: Implments Intrinsics compile in future.
+#if 0
   if (compiler_options.IsBootImage()) {
     ScopedObjectAccess soa(Thread::Current());
     ArtMethod* method = runtime->GetClassLinker()->LookupResolvedMethod(
@@ -1219,6 +1236,7 @@ CompiledMethod* OptimizingCompiler::JniCompile(uint32_t access_flags,
       }
     }
   }
+#endif
 
   JniCompiledMethod jni_compiled_method = ArtQuickJniCompileMethod(
       compiler_options, access_flags, method_idx, dex_file);
diff --git a/compiler/optimizing/register_allocator.cc b/compiler/optimizing/register_allocator.cc
index bad73e1b61..0e08d70484 100644
--- a/compiler/optimizing/register_allocator.cc
+++ b/compiler/optimizing/register_allocator.cc
@@ -74,6 +74,7 @@ bool RegisterAllocator::CanAllocateRegistersFor(const HGraph& graph ATTRIBUTE_UN
       || instruction_set == InstructionSet::kArm64
       || instruction_set == InstructionSet::kMips
       || instruction_set == InstructionSet::kMips64
+      || instruction_set == InstructionSet::kRiscv64
       || instruction_set == InstructionSet::kThumb2
       || instruction_set == InstructionSet::kX86
       || instruction_set == InstructionSet::kX86_64;
diff --git a/compiler/optimizing/stack_map_stream.cc b/compiler/optimizing/stack_map_stream.cc
index 60ca61c133..c87e7c26e2 100644
--- a/compiler/optimizing/stack_map_stream.cc
+++ b/compiler/optimizing/stack_map_stream.cc
@@ -28,7 +28,15 @@
 
 namespace art {
 
+/*
+  FIXME: T-HEAD,: riscv64 backend might modify the elements that are aready existing in stack_maps_.
+             skip stackmaps verify here for riscv64 arch.
+*/
+#ifdef ART_ENABLE_CODEGEN_riscv64
+constexpr static bool kVerifyStackMaps = false;
+#else
 constexpr static bool kVerifyStackMaps = kIsDebugBuild;
+#endif
 
 uint32_t StackMapStream::GetStackMapNativePcOffset(size_t i) {
   return StackMap::UnpackNativePc(stack_maps_[i][StackMap::kPackedNativePc], instruction_set_);
diff --git a/compiler/trampolines/trampoline_compiler.cc b/compiler/trampolines/trampoline_compiler.cc
index 26aa434c0d..8b93262bb6 100644
--- a/compiler/trampolines/trampoline_compiler.cc
+++ b/compiler/trampolines/trampoline_compiler.cc
@@ -44,6 +44,10 @@
 #include "utils/x86_64/assembler_x86_64.h"
 #endif
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+#include "utils/riscv64/assembler_riscv64.h"
+#endif
+
 #define __ assembler.
 
 namespace art {
@@ -199,6 +203,38 @@ static std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline(
 }  // namespace mips64
 #endif  // ART_ENABLE_CODEGEN_mips
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+namespace riscv64 {
+static std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline(
+    ArenaAllocator* allocator, EntryPointCallingConvention abi, ThreadOffset64 offset) {
+  Riscv64Assembler assembler(allocator);
+
+  switch (abi) {
+    case kInterpreterAbi:  // Thread* is first argument (A0) in interpreter ABI.
+      __ LoadFromOffset(kLoadDoubleword, T9, A0, offset.Int32Value());
+      break;
+    case kJniAbi:  // Load via Thread* held in JNIEnv* in first argument (A0).
+      __ LoadFromOffset(kLoadDoubleword, T9, A0, JNIEnvExt::SelfOffset(8).Int32Value());
+      __ LoadFromOffset(kLoadDoubleword, T9, T9, offset.Int32Value());
+      break;
+    case kQuickAbi:  // Fall-through.
+      __ LoadFromOffset(kLoadDoubleword, T9, S1, offset.Int32Value());
+  }
+  __ Jr(T9);
+  __ Nop();
+  __ Break();
+
+  __ FinalizeCode();
+  size_t cs = __ CodeSize();
+  std::unique_ptr<std::vector<uint8_t>> entry_stub(new std::vector<uint8_t>(cs));
+  MemoryRegion code(entry_stub->data(), entry_stub->size());
+  __ FinalizeInstructions(code);
+
+  return std::move(entry_stub);
+}
+}  // namespace riscv64
+#endif  // ART_ENABLE_CODEGEN_riscv64
+
 #ifdef ART_ENABLE_CODEGEN_x86
 namespace x86 {
 static std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline(ArenaAllocator* allocator,
@@ -258,6 +294,10 @@ std::unique_ptr<const std::vector<uint8_t>> CreateTrampoline64(InstructionSet is
 #ifdef ART_ENABLE_CODEGEN_x86_64
     case InstructionSet::kX86_64:
       return x86_64::CreateTrampoline(&allocator, offset);
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64:
+      return riscv64::CreateTrampoline(&allocator, abi, offset);
 #endif
     default:
       UNUSED(abi);
diff --git a/compiler/utils/assembler_test.h b/compiler/utils/assembler_test.h
index 9e23d11116..85dea2437e 100644
--- a/compiler/utils/assembler_test.h
+++ b/compiler/utils/assembler_test.h
@@ -144,6 +144,17 @@ class AssemblerTest : public testing::Test {
         fmt);
   }
 
+  std::string Repeatrrr(void (Ass::*f)(Reg, Reg, Reg), const std::string& fmt) {
+    return RepeatTemplatedRegisters<Reg, Reg, Reg>(f,
+        GetRegisters(),
+        GetRegisters(),
+        GetRegisters(),
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        fmt);
+  }
+
   std::string Repeatrb(void (Ass::*f)(Reg, Reg), const std::string& fmt) {
     return RepeatTemplatedRegisters<Reg, Reg>(f,
         GetRegisters(),
@@ -355,7 +366,8 @@ class AssemblerTest : public testing::Test {
                                              const std::vector<RegType*> registers,
                                              std::string (AssemblerTest::*GetName)(const RegType&),
                                              const std::string& fmt,
-                                             int bias) {
+                                             int bias = 0,
+                                             int multiplier = 1) {
     std::string str;
     std::vector<int64_t> imms = CreateImmediateValuesBits(abs(imm_bits), (imm_bits > 0));
 
@@ -363,7 +375,7 @@ class AssemblerTest : public testing::Test {
       for (int64_t imm : imms) {
         ImmType new_imm = CreateImmediate(imm);
         if (f != nullptr) {
-          (assembler_.get()->*f)(*reg, new_imm + bias);
+          (assembler_.get()->*f)(*reg, new_imm * multiplier + bias);
         }
         std::string base = fmt;
 
@@ -376,7 +388,7 @@ class AssemblerTest : public testing::Test {
         size_t imm_index = base.find(IMM_TOKEN);
         if (imm_index != std::string::npos) {
           std::ostringstream sreg;
-          sreg << imm + bias;
+          sreg << imm * multiplier + bias;
           std::string imm_string = sreg.str();
           base.replace(imm_index, ConstexprStrLen(IMM_TOKEN), imm_string);
         }
@@ -407,6 +419,23 @@ class AssemblerTest : public testing::Test {
         bias);
   }
 
+  template <typename ImmType>
+  std::string RepeatrrIb(void (Ass::*f)(Reg, Reg, ImmType),
+                         int imm_bits,
+                         const std::string& fmt,
+                         int bias = 0,
+                         int multiplier = 1) {
+    return RepeatTemplatedRegistersImmBits<Reg, Reg, ImmType>(f,
+        imm_bits,
+        GetRegisters(),
+        GetRegisters(),
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        fmt,
+        bias,
+        multiplier);
+  }
+
   template <typename ImmType>
   std::string RepeatRRRIb(void (Ass::*f)(Reg, Reg, Reg, ImmType),
                           int imm_bits,
@@ -434,6 +463,17 @@ class AssemblerTest : public testing::Test {
         bias);
   }
 
+  template <typename ImmType>
+  std::string RepeatrIb(void (Ass::*f)(Reg, ImmType), int imm_bits, std::string fmt, int bias = 0, int multiplier = 1) {
+    return RepeatTemplatedRegisterImmBits<Reg, ImmType>(f,
+        imm_bits,
+        GetRegisters(),
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        fmt,
+        bias,
+        multiplier);
+  }
+
   template <typename ImmType>
   std::string RepeatFRIb(void (Ass::*f)(FPReg, Reg, ImmType),
                          int imm_bits,
@@ -449,6 +489,21 @@ class AssemblerTest : public testing::Test {
         bias);
   }
 
+  template <typename ImmType>
+  std::string RepeatFrIb(void (Ass::*f)(FPReg, Reg, ImmType),
+                         int imm_bits,
+                         const std::string& fmt,
+                         int bias = 0) {
+    return RepeatTemplatedRegistersImmBits<FPReg, Reg, ImmType>(f,
+        imm_bits,
+        GetFPRegisters(),
+        GetRegisters(),
+        &AssemblerTest::GetFPRegName,
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        fmt,
+        bias);
+  }
+
   std::string RepeatFF(void (Ass::*f)(FPReg, FPReg), const std::string& fmt) {
     return RepeatTemplatedRegisters<FPReg, FPReg>(f,
                                                   GetFPRegisters(),
@@ -469,6 +524,19 @@ class AssemblerTest : public testing::Test {
                                                          fmt);
   }
 
+  std::string RepeatFFF(void (Ass::*f)(FPReg, FPReg, FPReg, FPReg), const std::string& fmt) {
+    return RepeatTemplatedRegisters<FPReg, FPReg, FPReg, FPReg>(f,
+                                                         GetFPRegisters(),
+                                                         GetFPRegisters(),
+                                                         GetFPRegisters(),
+                                                         GetFPRegisters(),
+                                                         &AssemblerTest::GetFPRegName,
+                                                         &AssemblerTest::GetFPRegName,
+                                                         &AssemblerTest::GetFPRegName,
+                                                         &AssemblerTest::GetFPRegName,
+                                                         fmt);
+  }
+
   std::string RepeatFFR(void (Ass::*f)(FPReg, FPReg, Reg), const std::string& fmt) {
     return RepeatTemplatedRegisters<FPReg, FPReg, Reg>(
         f,
@@ -555,6 +623,17 @@ class AssemblerTest : public testing::Test {
         fmt);
   }
 
+  std::string RepeatrFF(void (Ass::*f)(Reg, FPReg, FPReg), const std::string& fmt) {
+    return RepeatTemplatedRegisters<Reg, FPReg, FPReg>(f,
+        GetRegisters(),
+        GetFPRegisters(),
+        GetFPRegisters(),
+        &AssemblerTest::GetRegName<RegisterView::kUseSecondaryName>,
+        &AssemblerTest::GetFPRegName,
+        &AssemblerTest::GetFPRegName,
+        fmt);
+  }
+
   std::string RepeatI(void (Ass::*f)(const Imm&),
                       size_t imm_bytes,
                       const std::string& fmt,
@@ -1435,6 +1514,64 @@ class AssemblerTest : public testing::Test {
     return str;
   }
 
+  template <typename Reg1, typename Reg2, typename Reg3, typename Reg4>
+  std::string RepeatTemplatedRegisters(void (Ass::*f)(Reg1, Reg2, Reg3, Reg4),
+                                       const std::vector<Reg1*> reg1_registers,
+                                       const std::vector<Reg2*> reg2_registers,
+                                       const std::vector<Reg3*> reg3_registers,
+                                       const std::vector<Reg4*> reg4_registers,
+                                       std::string (AssemblerTest::*GetName1)(const Reg1&),
+                                       std::string (AssemblerTest::*GetName2)(const Reg2&),
+                                       std::string (AssemblerTest::*GetName3)(const Reg3&),
+                                       std::string (AssemblerTest::*GetName4)(const Reg4&),
+                                       const std::string& fmt) {
+    std::string str;
+    for (auto reg1 : reg1_registers) {
+      for (auto reg2 : reg2_registers) {
+        for (auto reg3 : reg3_registers) {
+          for (auto reg4 : reg4_registers) {
+            if (f != nullptr) {
+              (assembler_.get()->*f)(*reg1, *reg2, *reg3, *reg4);
+            }
+            std::string base = fmt;
+
+            std::string reg1_string = (this->*GetName1)(*reg1);
+            size_t reg1_index;
+            while ((reg1_index = base.find(REG1_TOKEN)) != std::string::npos) {
+              base.replace(reg1_index, ConstexprStrLen(REG1_TOKEN), reg1_string);
+            }
+
+            std::string reg2_string = (this->*GetName2)(*reg2);
+            size_t reg2_index;
+            while ((reg2_index = base.find(REG2_TOKEN)) != std::string::npos) {
+              base.replace(reg2_index, ConstexprStrLen(REG2_TOKEN), reg2_string);
+            }
+
+            std::string reg3_string = (this->*GetName3)(*reg3);
+            size_t reg3_index;
+            while ((reg3_index = base.find(REG3_TOKEN)) != std::string::npos) {
+              base.replace(reg3_index, ConstexprStrLen(REG3_TOKEN), reg3_string);
+            }
+
+            std::string reg4_string = (this->*GetName4)(*reg4);
+            size_t reg4_index;
+            while ((reg4_index = base.find(REG4_TOKEN)) != std::string::npos) {
+              base.replace(reg4_index, ConstexprStrLen(REG4_TOKEN), reg4_string);
+            }
+
+            if (str.size() > 0) {
+              str += "\n";
+            }
+            str += base;
+          }
+        }
+      }
+    }
+    // Add a newline at the end.
+    str += "\n";
+    return str;
+  }
+
   template <typename Reg1, typename Reg2>
   std::string RepeatTemplatedRegistersImm(void (Ass::*f)(Reg1, Reg2, const Imm&),
                                           const std::vector<Reg1*> reg1_registers,
@@ -1545,6 +1682,7 @@ class AssemblerTest : public testing::Test {
   static constexpr const char* REG1_TOKEN = "{reg1}";
   static constexpr const char* REG2_TOKEN = "{reg2}";
   static constexpr const char* REG3_TOKEN = "{reg3}";
+  static constexpr const char* REG4_TOKEN = "{reg4}";
   static constexpr const char* IMM_TOKEN = "{imm}";
 
  private:
diff --git a/compiler/utils/assembler_test_base.h b/compiler/utils/assembler_test_base.h
index 5fa0b3cd39..c182e88f80 100644
--- a/compiler/utils/assembler_test_base.h
+++ b/compiler/utils/assembler_test_base.h
@@ -33,7 +33,7 @@ namespace art {
 
 // If you want to take a look at the differences between the ART assembler and GCC, set this flag
 // to true. The disassembled files will then remain in the tmp directory.
-static constexpr bool kKeepDisassembledFiles = false;
+static constexpr bool kKeepDisassembledFiles = true;
 
 // Use a glocal static variable to keep the same name for all test data. Else we'll just spam the
 // temp directory.
diff --git a/compiler/utils/jni_macro_assembler.cc b/compiler/utils/jni_macro_assembler.cc
index 5f405f348c..b364c3098c 100644
--- a/compiler/utils/jni_macro_assembler.cc
+++ b/compiler/utils/jni_macro_assembler.cc
@@ -37,6 +37,9 @@
 #ifdef ART_ENABLE_CODEGEN_x86_64
 #include "x86_64/jni_macro_assembler_x86_64.h"
 #endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+#include "riscv64/jni_macro_assembler_riscv64.h"
+#endif
 #include "base/casts.h"
 #include "base/globals.h"
 #include "base/memory_region.h"
@@ -105,6 +108,10 @@ MacroAsm64UniquePtr JNIMacroAssembler<PointerSize::k64>::Create(
 #ifdef ART_ENABLE_CODEGEN_x86_64
     case InstructionSet::kX86_64:
       return MacroAsm64UniquePtr(new (allocator) x86_64::X86_64JNIMacroAssembler(allocator));
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64:
+      return MacroAsm64UniquePtr(new (allocator) riscv64::Riscv64JNIMacroAssembler(allocator));
 #endif
     default:
       UNUSED(allocator);
diff --git a/compiler/utils/label.h b/compiler/utils/label.h
index 3c91b2ffd1..dca135c786 100644
--- a/compiler/utils/label.h
+++ b/compiler/utils/label.h
@@ -45,6 +45,10 @@ namespace x86_64 {
 class X86_64Assembler;
 class NearLabel;
 }  // namespace x86_64
+namespace riscv64 {
+class Riscv64Assembler;
+class Riscv64Label;
+}  // namespace riscv64
 
 class ExternalLabel {
  public:
@@ -123,6 +127,7 @@ class Label {
   friend class x86::NearLabel;
   friend class x86_64::X86_64Assembler;
   friend class x86_64::NearLabel;
+  friend class riscv64::Riscv64Assembler;
 
   DISALLOW_COPY_AND_ASSIGN(Label);
 };
diff --git a/compiler/utils/managed_register.h b/compiler/utils/managed_register.h
index db9c36cc75..0604490445 100644
--- a/compiler/utils/managed_register.h
+++ b/compiler/utils/managed_register.h
@@ -45,6 +45,10 @@ namespace x86_64 {
 class X86_64ManagedRegister;
 }  // namespace x86_64
 
+namespace riscv64 {
+class Riscv64ManagedRegister;
+}  // namespace riscv64
+
 class ManagedRegister : public ValueObject {
  public:
   // ManagedRegister is a value class. There exists no method to change the
@@ -60,6 +64,7 @@ class ManagedRegister : public ValueObject {
   constexpr mips64::Mips64ManagedRegister AsMips64() const;
   constexpr x86::X86ManagedRegister AsX86() const;
   constexpr x86_64::X86_64ManagedRegister AsX86_64() const;
+  constexpr riscv64::Riscv64ManagedRegister AsRiscv64() const;
 
   // It is valid to invoke Equals on and with a NoRegister.
   constexpr bool Equals(const ManagedRegister& other) const {
diff --git a/compiler/utils/riscv64/assembler_riscv64.cc b/compiler/utils/riscv64/assembler_riscv64.cc
new file mode 100644
index 0000000000..4e2975a4a8
--- /dev/null
+++ b/compiler/utils/riscv64/assembler_riscv64.cc
@@ -0,0 +1,4874 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "assembler_riscv64.h"
+
+#include "base/bit_utils.h"
+#include "base/casts.h"
+#include "base/memory_region.h"
+#include "entrypoints/quick/quick_entrypoints.h"
+#include "entrypoints/quick/quick_entrypoints_enum.h"
+#include "thread.h"
+
+namespace art {
+namespace riscv64 {
+
+static_assert(static_cast<size_t>(kRiscv64PointerSize) == kRiscv64DoublewordSize,
+              "Unexpected Riscv64 pointer size.");
+static_assert(kRiscv64PointerSize == PointerSize::k64, "Unexpected Riscv64 pointer size.");
+
+
+void Riscv64Assembler::FinalizeCode() {
+  for (auto& exception_block : exception_blocks_) {
+    EmitExceptionPoll(&exception_block);
+  }
+  ReserveJumpTableSpace();
+  EmitLiterals();
+  PromoteBranches();
+}
+
+void Riscv64Assembler::FinalizeInstructions(const MemoryRegion& region) {
+  EmitBranches();
+  EmitJumpTables();
+  Assembler::FinalizeInstructions(region);
+  PatchCFI();
+}
+
+void Riscv64Assembler::PatchCFI() {
+  if (cfi().NumberOfDelayedAdvancePCs() == 0u) {
+    return;
+  }
+
+  using DelayedAdvancePC = DebugFrameOpCodeWriterForAssembler::DelayedAdvancePC;
+  const auto data = cfi().ReleaseStreamAndPrepareForDelayedAdvancePC();
+  const std::vector<uint8_t>& old_stream = data.first;
+  const std::vector<DelayedAdvancePC>& advances = data.second;
+
+  // Refill our data buffer with patched opcodes.
+  cfi().ReserveCFIStream(old_stream.size() + advances.size() + 16);
+  size_t stream_pos = 0;
+  for (const DelayedAdvancePC& advance : advances) {
+    DCHECK_GE(advance.stream_pos, stream_pos);
+    // Copy old data up to the point where advance was issued.
+    cfi().AppendRawData(old_stream, stream_pos, advance.stream_pos);
+    stream_pos = advance.stream_pos;
+    // Insert the advance command with its final offset.
+    size_t final_pc = GetAdjustedPosition(advance.pc);
+    cfi().AdvancePC(final_pc);
+  }
+  // Copy the final segment if any.
+  cfi().AppendRawData(old_stream, stream_pos, old_stream.size());
+}
+
+void Riscv64Assembler::EmitBranches() {
+  CHECK(!overwriting_);
+  // Switch from appending instructions at the end of the buffer to overwriting
+  // existing instructions (branch placeholders) in the buffer.
+  overwriting_ = true;
+  for (auto& branch : branches_) {
+    EmitBranch(&branch);
+  }
+  overwriting_ = false;
+}
+
+void Riscv64Assembler::Emit(uint32_t value) {
+  if (overwriting_) {
+    // Branches to labels are emitted into their placeholders here.
+    buffer_.Store<uint32_t>(overwrite_location_, value);
+    overwrite_location_ += sizeof(uint32_t);
+  } else {
+    // Other instructions are simply appended at the end here.
+    AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+    buffer_.Emit<uint32_t>(value);
+  }
+}
+
+void Riscv64Assembler::EmitRsd(int opcode, GpuRegister rs, GpuRegister rd,
+                              int shamt, int funct) {
+  CHECK_NE(rs, kNoGpuRegister);
+  CHECK_NE(rd, kNoGpuRegister);
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift |
+                      static_cast<uint32_t>(rs) << kRsShift |
+                      static_cast<uint32_t>(ZERO) << kRtShift |
+                      static_cast<uint32_t>(rd) << kRdShift |
+                      shamt << kShamtShift |
+                      funct;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitRtd(int opcode, GpuRegister rt, GpuRegister rd,
+                              int shamt, int funct) {
+  CHECK_NE(rt, kNoGpuRegister);
+  CHECK_NE(rd, kNoGpuRegister);
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift |
+                      static_cast<uint32_t>(ZERO) << kRsShift |
+                      static_cast<uint32_t>(rt) << kRtShift |
+                      static_cast<uint32_t>(rd) << kRdShift |
+                      shamt << kShamtShift |
+                      funct;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitI(int opcode, GpuRegister rs, GpuRegister rt, uint16_t imm) {
+  CHECK_NE(rs, kNoGpuRegister);
+  CHECK_NE(rt, kNoGpuRegister);
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift |
+                      static_cast<uint32_t>(rs) << kRsShift |
+                      static_cast<uint32_t>(rt) << kRtShift |
+                      imm;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitI21(int opcode, GpuRegister rs, uint32_t imm21) {
+  CHECK_NE(rs, kNoGpuRegister);
+  CHECK(IsUint<21>(imm21)) << imm21;
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift |
+                      static_cast<uint32_t>(rs) << kRsShift |
+                      imm21;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitI26(int opcode, uint32_t imm26) {
+  CHECK(IsUint<26>(imm26)) << imm26;
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift | imm26;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitFR(int opcode, int fmt, FpuRegister ft, FpuRegister fs, FpuRegister fd,
+                             int funct) {
+  CHECK_NE(ft, kNoFpuRegister);
+  CHECK_NE(fs, kNoFpuRegister);
+  CHECK_NE(fd, kNoFpuRegister);
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift |
+                      fmt << kFmtShift |
+                      static_cast<uint32_t>(ft) << kFtShift |
+                      static_cast<uint32_t>(fs) << kFsShift |
+                      static_cast<uint32_t>(fd) << kFdShift |
+                      funct;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitFI(int opcode, int fmt, FpuRegister ft, uint16_t imm) {
+  CHECK_NE(ft, kNoFpuRegister);
+  uint32_t encoding = static_cast<uint32_t>(opcode) << kOpcodeShift |
+                      fmt << kFmtShift |
+                      static_cast<uint32_t>(ft) << kFtShift |
+                      imm;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsa3R(int operation,
+                                int df,
+                                VectorRegister wt,
+                                VectorRegister ws,
+                                VectorRegister wd,
+                                int minor_opcode) {
+  CHECK_NE(wt, kNoVectorRegister);
+  CHECK_NE(ws, kNoVectorRegister);
+  CHECK_NE(wd, kNoVectorRegister);
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      operation << kMsaOperationShift |
+                      df << kDfShift |
+                      static_cast<uint32_t>(wt) << kWtShift |
+                      static_cast<uint32_t>(ws) << kWsShift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsaBIT(int operation,
+                                 int df_m,
+                                 VectorRegister ws,
+                                 VectorRegister wd,
+                                 int minor_opcode) {
+  CHECK_NE(ws, kNoVectorRegister);
+  CHECK_NE(wd, kNoVectorRegister);
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      operation << kMsaOperationShift |
+                      df_m << kDfMShift |
+                      static_cast<uint32_t>(ws) << kWsShift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsaELM(int operation,
+                                 int df_n,
+                                 VectorRegister ws,
+                                 VectorRegister wd,
+                                 int minor_opcode) {
+  CHECK_NE(ws, kNoVectorRegister);
+  CHECK_NE(wd, kNoVectorRegister);
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      operation << kMsaELMOperationShift |
+                      df_n << kDfNShift |
+                      static_cast<uint32_t>(ws) << kWsShift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsaMI10(int s10,
+                                  GpuRegister rs,
+                                  VectorRegister wd,
+                                  int minor_opcode,
+                                  int df) {
+  CHECK_NE(rs, kNoGpuRegister);
+  CHECK_NE(wd, kNoVectorRegister);
+  CHECK(IsUint<10>(s10)) << s10;
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      s10 << kS10Shift |
+                      static_cast<uint32_t>(rs) << kWsShift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode << kS10MinorShift |
+                      df;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsaI10(int operation,
+                                 int df,
+                                 int i10,
+                                 VectorRegister wd,
+                                 int minor_opcode) {
+  CHECK_NE(wd, kNoVectorRegister);
+  CHECK(IsUint<10>(i10)) << i10;
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      operation << kMsaOperationShift |
+                      df << kDfShift |
+                      i10 << kI10Shift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsa2R(int operation,
+                                int df,
+                                VectorRegister ws,
+                                VectorRegister wd,
+                                int minor_opcode) {
+  CHECK_NE(ws, kNoVectorRegister);
+  CHECK_NE(wd, kNoVectorRegister);
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      operation << kMsa2ROperationShift |
+                      df << kDf2RShift |
+                      static_cast<uint32_t>(ws) << kWsShift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitMsa2RF(int operation,
+                                 int df,
+                                 VectorRegister ws,
+                                 VectorRegister wd,
+                                 int minor_opcode) {
+  CHECK_NE(ws, kNoVectorRegister);
+  CHECK_NE(wd, kNoVectorRegister);
+  uint32_t encoding = static_cast<uint32_t>(kMsaMajorOpcode) << kOpcodeShift |
+                      operation << kMsa2RFOperationShift |
+                      df << kDf2RShift |
+                      static_cast<uint32_t>(ws) << kWsShift |
+                      static_cast<uint32_t>(wd) << kWdShift |
+                      minor_opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::Addu(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Addw(rd, rs, rt);
+}
+
+void Riscv64Assembler::Addiu(GpuRegister rd, GpuRegister rs, int16_t imm16) {
+  if (IsInt<12>(imm16)) {
+    Addiw(rd, rs, imm16);
+  } else {
+    int32_t l = imm16 & 0xFFF;
+    int32_t h = imm16 >> 12;
+    if ((l & 0x800) != 0) {
+      h += 1;
+    }
+    // rs and rd may be same or be TMP, use TMP2 here.
+    Lui(TMP2, h);
+    Addiw(TMP2, TMP2, l);
+    Addu(rd, TMP2, rs);
+  }
+}
+
+void Riscv64Assembler::Daddu(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Add(rd, rs, rt);;
+}
+
+void Riscv64Assembler::Daddiu(GpuRegister rd, GpuRegister rs, int16_t imm16) {
+  if (IsInt<12>(imm16)) {
+    Addi(rd, rs, imm16);
+  } else {
+    int32_t l = imm16 & 0xFFF;
+    int32_t h = imm16 >> 12;
+    if ((l & 0x800) != 0) {
+      h += 1;  // overflow ?
+    }
+    // rs and rd may be same or be TMP, use TMP2 here.
+    Lui(TMP2, h);
+    Addiw(TMP2, TMP2, l);
+    Daddu(rd, TMP2, rs);
+  }
+}
+
+void Riscv64Assembler::Subu(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Subw(rd, rs, rt);
+}
+
+void Riscv64Assembler::Dsubu(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Sub(rd, rs, rt);
+}
+
+void Riscv64Assembler::MulR6(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  // MulR6 --> Mulw in Riscv64
+  Mulw(rd, rs, rt);
+}
+
+void Riscv64Assembler::MuhR6(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  // There's no instruction in Riscv64 can get the high 32bit of 32-bit Multiplication.
+  // Shift left 32 for both of source operands
+  // Use TMP2 and T6 here
+  Slli(TMP2, rs, 32);
+  Slli(T6, rt, 32);
+  Mul(rd, TMP2, T6);   // rd <-- (rs x rt)'s 64-bit result
+  Srai(rd, rd, 32);  // get the high 32bit result and keep sign
+}
+
+void Riscv64Assembler::DivR6(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  // DivR6 --> Divw in Riscv64
+  Divw(rd, rs, rt);
+}
+
+void Riscv64Assembler::ModR6(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Remw(rd, rs, rt);
+}
+
+void Riscv64Assembler::DivuR6(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  // DivuR6 --> Divuw in Riscv64
+  Divuw(rd, rs, rt);
+}
+
+void Riscv64Assembler::ModuR6(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Remuw(rd, rs, rt);
+}
+
+void Riscv64Assembler::Dmul(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  // Dmul --> Mul in Riscv64
+  Mul(rd, rs, rt);
+}
+
+void Riscv64Assembler::Dmuh(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Mulh(rd, rs, rt);
+}
+
+void Riscv64Assembler::Ddiv(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Div(rd, rs, rt);
+}
+
+void Riscv64Assembler::Dmod(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Rem(rd, rs, rt);
+}
+
+void Riscv64Assembler::Ddivu(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Divu(rd, rs, rt);
+}
+
+void Riscv64Assembler::Dmodu(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Remu(rd, rs, rt);
+}
+
+void Riscv64Assembler::Bitswap(GpuRegister rd, GpuRegister rt) {
+  assert(0);
+  EmitRtd(0x1f, rt, rd, 0x0, 0x20);
+}
+
+void Riscv64Assembler::Dbitswap(GpuRegister rd, GpuRegister rt) {
+  assert(0);
+  EmitRtd(0x1f, rt, rd, 0x0, 0x24);
+}
+
+void Riscv64Assembler::Seb(GpuRegister rd, GpuRegister rt) {
+  Srliw(rd, rt, 24);  // Sign extend bit 7 to hight 32-bit
+  Srai(rd, rd, 24);
+}
+
+void Riscv64Assembler::Seh(GpuRegister rd, GpuRegister rt) {
+  Srliw(rd, rt, 16);  // Sign extend bit 15 to hight 32-bit
+  Srai(rd, rd, 16);
+}
+
+void Riscv64Assembler::Dsbh(GpuRegister rd, GpuRegister rt) {
+  assert(0);
+  EmitRtd(0x1f, rt, rd, 0x2, 0x24);
+}
+
+void Riscv64Assembler::Dshd(GpuRegister rd, GpuRegister rt) {
+  assert(0);
+  EmitRtd(0x1f, rt, rd, 0x5, 0x24);
+}
+
+void Riscv64Assembler::Dext(GpuRegister rt, GpuRegister rs, int pos, int size) {
+  CHECK(IsUint<5>(pos)) << pos;
+  CHECK(IsUint<5>(size - 1)) << size;
+  Srli(rt, rs, pos);
+  Slli(rt, rt, (64-size));
+  Srli(rt, rt, (64-size));
+}
+
+void Riscv64Assembler::Ins(GpuRegister rd, GpuRegister rt, int pos, int size) {
+  assert(0);
+  CHECK(IsUint<5>(pos)) << pos;
+  CHECK(IsUint<5>(size - 1)) << size;
+  CHECK(IsUint<5>(pos + size - 1)) << pos << " + " << size;
+  EmitR(0x1f, rt, rd, static_cast<GpuRegister>(pos + size - 1), pos, 0x04);
+}
+
+void Riscv64Assembler::Dinsm(GpuRegister rt, GpuRegister rs, int pos, int size) {
+  assert(0);
+  CHECK(IsUint<5>(pos)) << pos;
+  CHECK(2 <= size && size <= 64) << size;
+  CHECK(IsUint<5>(pos + size - 33)) << pos << " + " << size;
+  EmitR(0x1f, rs, rt, static_cast<GpuRegister>(pos + size - 33), pos, 0x5);
+}
+
+void Riscv64Assembler::Dinsu(GpuRegister rt, GpuRegister rs, int pos, int size) {
+  assert(0);
+  CHECK(IsUint<5>(pos - 32)) << pos;
+  CHECK(IsUint<5>(size - 1)) << size;
+  CHECK(IsUint<5>(pos + size - 33)) << pos << " + " << size;
+  EmitR(0x1f, rs, rt, static_cast<GpuRegister>(pos + size - 33), pos - 32, 0x6);
+}
+
+void Riscv64Assembler::Dins(GpuRegister rt, GpuRegister rs, int pos, int size) {
+  assert(0);
+  CHECK(IsUint<5>(pos)) << pos;
+  CHECK(IsUint<5>(size - 1)) << size;
+  CHECK(IsUint<5>(pos + size - 1)) << pos << " + " << size;
+  EmitR(0x1f, rs, rt, static_cast<GpuRegister>(pos + size - 1), pos, 0x7);
+}
+
+void Riscv64Assembler::DblIns(GpuRegister rt, GpuRegister rs, int pos, int size) {
+  assert(0);
+  if (pos >= 32) {
+    Dinsu(rt, rs, pos, size);
+  } else if ((static_cast<int64_t>(pos) + size - 1) >= 32) {
+    Dinsm(rt, rs, pos, size);
+  } else {
+    Dins(rt, rs, pos, size);
+  }
+}
+
+void Riscv64Assembler::Lsa(GpuRegister rd, GpuRegister rs, GpuRegister rt, int saPlusOne) {
+  CHECK(1 <= saPlusOne && saPlusOne <= 4) << saPlusOne;
+  Slli(TMP2, rs, saPlusOne);
+  Addw(rd, TMP2, rt);
+}
+
+void Riscv64Assembler::Dlsa(GpuRegister rd, GpuRegister rs, GpuRegister rt, int saPlusOne) {
+  CHECK(1 <= saPlusOne && saPlusOne <= 4) << saPlusOne;
+  Slli(TMP2, rs, saPlusOne);
+  Add(rd, TMP2, rt);
+}
+
+void Riscv64Assembler::Wsbh(GpuRegister rd, GpuRegister rt) {
+  assert(0);
+  EmitRtd(0x1f, rt, rd, 2, 0x20);
+}
+
+void Riscv64Assembler::Sc(GpuRegister rt, GpuRegister base, int16_t imm9) {
+  CHECK(IsInt<9>(imm9));
+  if (imm9 != 0) {
+    Addi(TMP2, base, imm9);
+    ScW(rt, rt, TMP2);   // todo: sucess, 0; fail, 1, which is opposite to mips;
+  } else {
+    ScW(rt, rt, base);
+  }
+}
+
+void Riscv64Assembler::Scd(GpuRegister rt, GpuRegister base, int16_t imm9) {
+  CHECK(IsInt<9>(imm9));
+  if (imm9 != 0) {
+    Addi(TMP2, base, imm9);
+    ScD(rt, rt, TMP2);   // todo: sucess, 0; fail, 1, which is opposite to mips;
+  } else {
+    ScD(rt, rt, base);
+  }
+}
+
+void Riscv64Assembler::Ll(GpuRegister rt, GpuRegister base, int16_t imm9) {
+  CHECK(IsInt<9>(imm9));
+  if (imm9 != 0) {
+    Addi(TMP2, base, imm9);
+    LrW(rt, TMP2);   // todo: sucess, 0; fail, 1, which is opposite to mips;
+  } else {
+    LrW(rt, base);
+  }
+}
+
+void Riscv64Assembler::Lld(GpuRegister rt, GpuRegister base, int16_t imm9) {
+  CHECK(IsInt<9>(imm9));
+  if (imm9 != 0) {
+    Addi(TMP2, base, imm9);
+    LrD(rt, TMP2);   // todo: sucess, 0; fail, 1, which is opposite to mips;
+  } else {
+    LrD(rt, base);
+  }
+}
+
+void Riscv64Assembler::Sll(GpuRegister rd, GpuRegister rt, int shamt) {
+  Slliw(rd, rt, shamt);
+}
+
+void Riscv64Assembler::Srl(GpuRegister rd, GpuRegister rt, int shamt) {
+  Srliw(rd, rt, shamt);
+}
+
+void Riscv64Assembler::Rotr(GpuRegister rd, GpuRegister rt, int shamt) {
+  CHECK(0 <= shamt < 32) << shamt;
+  // Riscv64 codegen don't use the blocked registers for rd, rt, rs till now.
+  // It's safe to use scratch registers here.
+  Srliw(TMP, rt, shamt);
+  Slliw(rd, rt, 32-shamt);  // logical shift left (32 -shamt)
+  Or(rd, rd, TMP);
+}
+
+void Riscv64Assembler::Sra(GpuRegister rd, GpuRegister rt, int shamt) {
+  Sraiw(rd, rt, shamt);
+}
+
+void Riscv64Assembler::Sllv(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  Sllw(rd, rt, rs);
+}
+
+void Riscv64Assembler::Rotrv(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  // Riscv64 codegen don't use the blocked registers for rd, rt, rs till now.
+  // It's safe to use TMP scratch registers here.
+  Srlw(TMP, rt, rs);
+  Subw(TMP2, ZERO, rs);  // TMP2 = -rs
+  Addiw(TMP2, TMP2, 32);   // TMP2 = 32 -rs
+  Andi(TMP2, TMP2, 0x1F);
+  Sllw(rd, rt, TMP2);
+  Or(rd, rd, TMP);
+}
+
+void Riscv64Assembler::Srlv(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  Srlw(rd, rt, rs);
+}
+
+void Riscv64Assembler::Srav(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  Sraw(rd, rt, rs);
+}
+
+void Riscv64Assembler::Dsll(GpuRegister rd, GpuRegister rt, int shamt) {
+  Slli(rd, rt, shamt);
+}
+
+void Riscv64Assembler::Dsrl(GpuRegister rd, GpuRegister rt, int shamt) {
+  Srli(rd, rt, shamt);
+}
+
+void Riscv64Assembler::Drotr(GpuRegister rd, GpuRegister rt, int shamt) {
+  CHECK(0 <= shamt < 32) << shamt;
+  // Riscv64 codegen don't use the blocked registers for rd, rt, rs till now.
+  // It's safe to use scratch registers here.
+  Srli(TMP, rt, shamt);
+  Slli(rd, rt, (64 - shamt));
+  Or(rd, rd, TMP);
+}
+
+void Riscv64Assembler::Dsra(GpuRegister rd, GpuRegister rt, int shamt) {
+  Srai(rd, rt, shamt);
+}
+
+void Riscv64Assembler::Dsll32(GpuRegister rd, GpuRegister rt, int shamt) {
+  CHECK(0 <= shamt < 32) << shamt;
+
+  Slli(rd, rt, shamt+32);
+}
+
+void Riscv64Assembler::Dsrl32(GpuRegister rd, GpuRegister rt, int shamt) {
+  CHECK(0 <= shamt < 32) << shamt;
+
+  Srli(rd, rt, shamt+32);
+}
+
+void Riscv64Assembler::Drotr32(GpuRegister rd, GpuRegister rt, int shamt) {
+  CHECK(0 <= shamt < 32) << shamt;
+  Drotr(rd, rt, 32+shamt);
+}
+
+void Riscv64Assembler::Dsra32(GpuRegister rd, GpuRegister rt, int shamt) {
+  CHECK(0 <= shamt < 32) << shamt;
+
+  Srai(rd, rt, shamt+32);
+}
+
+void Riscv64Assembler::Dsllv(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  Sll(rd, rt, rs);
+}
+
+void Riscv64Assembler::Dsrlv(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  Srl(rd, rt, rs);
+}
+
+void Riscv64Assembler::Drotrv(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  // Riscv64 codegen don't use the blocked registers for rd, rt, rs till now.
+  // It's safe to use scratch registers here.
+  Srl(TMP, rt, rs);
+  Sub(TMP2, ZERO, rs);  // TMP2 = -rs
+  Addi(TMP2, TMP2, 64);   // TMP2 = 64 -rs
+  Sll(rd, rt, TMP2);
+  Or(rd, rd, TMP);
+}
+
+void Riscv64Assembler::Dsrav(GpuRegister rd, GpuRegister rt, GpuRegister rs) {
+  Sra(rd, rt, rs);
+}
+
+void Riscv64Assembler::Lwpc(GpuRegister rs, uint32_t imm19) {
+  Auipc(rs, (imm19<<2)>>12);
+  Lw(rs, rs, (imm19<<2)&0xFFF);
+}
+
+void Riscv64Assembler::Lwupc(GpuRegister rs, uint32_t imm19) {
+  Auipc(rs, (imm19<<2)>>12);
+  Lwu(rs, rs, (imm19<<2)&0xFFF);
+}
+
+void Riscv64Assembler::Ldpc(GpuRegister rs, uint32_t imm18) {
+  Auipc(rs, (imm18<<2)>>12);
+  Ld(rs, rs, (imm18<<2)&0xFFF);
+}
+
+void Riscv64Assembler::Aui(GpuRegister rt, GpuRegister rs, uint16_t imm16) {
+  int32_t l = imm16 & 0xFFF;
+  int32_t h = imm16 >> 12;
+  if ((l & 0x800) != 0) {
+    h += 1;  // overflow ?
+  }
+
+  // rs and rd may be same or be TMP, use TMP2 here.
+  Lui(TMP2, h);
+  Addiw(TMP2, TMP2, l);
+  Slliw(TMP2, TMP2, 16);
+  Addw(rt, rs, TMP2);
+}
+
+void Riscv64Assembler::Daui(GpuRegister rt, GpuRegister rs, uint16_t imm16) {
+  int32_t l = imm16 & 0xFFF;
+  int32_t h = imm16 >> 12;
+  if ((l & 0x800) != 0) {
+    h += 1;  // overflow ?
+  }
+
+  // rs and rd may be same or be TMP, use TMP2 here.
+  Lui(TMP2, h);
+  Addi(TMP2, TMP2, l);
+  Slli(TMP2, TMP2, 16);
+  Add(rt, rs, TMP2);
+}
+
+void Riscv64Assembler::Dahi(GpuRegister rs, uint16_t imm16) {
+  int32_t l = imm16 & 0xFFF;
+  int32_t h = imm16 >> 12;
+  if ((l & 0x800) != 0) {
+    h += 1;  // overflow ?
+  }
+
+  // rs and rd may be same or be TMP, use TMP2 here.
+  Lui(TMP2, h);
+  Addi(TMP2, TMP2, l);
+  Slli(TMP2, TMP2, 32);
+  Add(rs, rs, TMP2);
+}
+
+void Riscv64Assembler::Dati(GpuRegister rs, uint16_t imm16) {
+  int32_t l = imm16 & 0xFFF;
+  int32_t h = imm16 >> 12;
+  if ((l & 0x800) != 0) {
+    h += 1;  // overflow ?
+  }
+
+  // rs and rd may be same or be TMP, use TMP2 here.
+  Lui(TMP2, h);
+  Addi(TMP2, TMP2, l);
+  Slli(TMP2, TMP2, 48);
+  Add(rs, rs, TMP2);
+}
+
+void Riscv64Assembler::Sync(uint32_t stype) {
+  // FIXME: T-HEAD, just for simplify, use normal fence here for all types.
+  Fence(0xf, 0xf);
+}
+
+void Riscv64Assembler::Seleqz(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Move(TMP2, rt);
+  Move(rd, rs);
+  Beq(TMP2, ZERO, 8);
+  Move(rd, ZERO);
+}
+
+void Riscv64Assembler::Selnez(GpuRegister rd, GpuRegister rs, GpuRegister rt) {
+  Move(TMP2, rt);
+  Move(rd, rs);
+  Bne(TMP2, ZERO, 8);
+  Move(rd, ZERO);
+}
+
+void Riscv64Assembler::Clz(GpuRegister rd, GpuRegister rs) {
+  assert(0);
+  EmitRsd(0, rs, rd, 0x01, 0x10);
+}
+
+void Riscv64Assembler::Clo(GpuRegister rd, GpuRegister rs) {
+  assert(0);
+  EmitRsd(0, rs, rd, 0x01, 0x11);
+}
+
+void Riscv64Assembler::Dclz(GpuRegister rd, GpuRegister rs) {
+  assert(0);
+  EmitRsd(0, rs, rd, 0x01, 0x12);
+}
+
+void Riscv64Assembler::Dclo(GpuRegister rd, GpuRegister rs) {
+  assert(0);
+  EmitRsd(0, rs, rd, 0x01, 0x13);
+}
+
+void Riscv64Assembler::Jalr(GpuRegister rd, GpuRegister rs) {
+  Jalr(rd, rs, 0);
+}
+
+void Riscv64Assembler::Jalr(GpuRegister rs) {
+  Jalr(RA, rs, 0);
+}
+
+void Riscv64Assembler::Jr(GpuRegister rs) {
+  Jalr(ZERO, rs, 0);
+}
+
+void Riscv64Assembler::Addiupc(GpuRegister rs, uint32_t imm19) {
+  CHECK(IsUint<19>(imm19)) << imm19;
+  Auipc(rs, (imm19<<2)>>12);
+  Addi(rs, rs, (imm19<<2)&0xFFF);
+}
+
+void Riscv64Assembler::Bc(uint32_t imm20) {
+  Jal(ZERO, imm20);
+}
+
+void Riscv64Assembler::Balc(uint32_t imm20) {
+  Jal(RA, imm20);
+}
+
+void Riscv64Assembler::Jic(GpuRegister rt, uint16_t imm16) {
+  Jalr(ZERO, rt, imm16);
+}
+
+void Riscv64Assembler::Jialc(GpuRegister rt, uint16_t imm16) {
+  Jalr(RA, rt, imm16);
+}
+
+void Riscv64Assembler::Bltc(GpuRegister rs, GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rs, ZERO);
+  CHECK_NE(rt, ZERO);
+  CHECK_NE(rs, rt);
+  Blt(rs, rt, imm12);
+}
+
+void Riscv64Assembler::Bltzc(GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rt, ZERO);
+  Blt(rt, ZERO, imm12);
+}
+
+void Riscv64Assembler::Bgtzc(GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rt, ZERO);
+  Blt(ZERO, rt, imm12);
+}
+
+void Riscv64Assembler::Bgec(GpuRegister rs, GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rs, ZERO);
+  CHECK_NE(rt, ZERO);
+  CHECK_NE(rs, rt);
+  Bge(rs, rt, imm12);
+}
+
+void Riscv64Assembler::Bgezc(GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rt, ZERO);
+  Bge(rt, ZERO, imm12);
+}
+
+void Riscv64Assembler::Blezc(GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rt, ZERO);
+  Bge(ZERO, rt, imm12);
+}
+
+void Riscv64Assembler::Bltuc(GpuRegister rs, GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rs, ZERO);
+  CHECK_NE(rt, ZERO);
+  CHECK_NE(rs, rt);
+  Bltu(rs, rt, imm12);
+}
+
+void Riscv64Assembler::Bgeuc(GpuRegister rs, GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rs, ZERO);
+  CHECK_NE(rt, ZERO);
+  CHECK_NE(rs, rt);
+  Bgeu(rs, rt, imm12);
+}
+
+void Riscv64Assembler::Beqc(GpuRegister rs, GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rs, ZERO);
+  CHECK_NE(rt, ZERO);
+  CHECK_NE(rs, rt);
+  Beq(rs, rt, imm12);
+}
+
+void Riscv64Assembler::Bnec(GpuRegister rs, GpuRegister rt, uint16_t imm12) {
+  CHECK_NE(rs, ZERO);
+  CHECK_NE(rt, ZERO);
+  CHECK_NE(rs, rt);
+  Bne(rs, rt, imm12);
+}
+
+void Riscv64Assembler::Beqzc(GpuRegister rs, uint32_t imm12) {
+  CHECK_NE(rs, ZERO);
+  Beq(rs, ZERO, imm12);
+}
+
+void Riscv64Assembler::Bnezc(GpuRegister rs, uint32_t imm12) {
+  CHECK_NE(rs, ZERO);
+  Bne(rs, ZERO, imm12);
+}
+
+void Riscv64Assembler::EmitBcond(BranchCondition cond,
+                                  GpuRegister rs,
+                                  GpuRegister rt,
+                                  uint32_t imm16_21) {
+  switch (cond) {
+    case kCondLT:
+      Bltc(rs, rt, imm16_21);
+      break;
+    case kCondGE:
+      Bgec(rs, rt, imm16_21);
+      break;
+    case kCondLE:
+      Bgec(rt, rs, imm16_21);
+      break;
+    case kCondGT:
+      Bltc(rt, rs, imm16_21);
+      break;
+    case kCondLTZ:
+      CHECK_EQ(rt, ZERO);
+      Bltzc(rs, imm16_21);
+      break;
+    case kCondGEZ:
+      CHECK_EQ(rt, ZERO);
+      Bgezc(rs, imm16_21);
+      break;
+    case kCondLEZ:
+      CHECK_EQ(rt, ZERO);
+      Blezc(rs, imm16_21);
+      break;
+    case kCondGTZ:
+      CHECK_EQ(rt, ZERO);
+      Bgtzc(rs, imm16_21);
+      break;
+    case kCondEQ:
+      Beqc(rs, rt, imm16_21);
+      break;
+    case kCondNE:
+      Bnec(rs, rt, imm16_21);
+      break;
+    case kCondEQZ:
+      CHECK_EQ(rt, ZERO);
+      Beqzc(rs, imm16_21);
+      break;
+    case kCondNEZ:
+      CHECK_EQ(rt, ZERO);
+      Bnezc(rs, imm16_21);
+      break;
+    case kCondLTU:
+      Bltuc(rs, rt, imm16_21);
+      break;
+    case kCondGEU:
+      Bgeuc(rs, rt, imm16_21);
+      break;
+    case kUncond:
+      // LOG(FATAL) << "Unexpected branch condition " << cond;
+      LOG(FATAL) << "Unexpected branch condition ";
+      UNREACHABLE();
+  }
+}
+
+void Riscv64Assembler::AddS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FAddS(fd, fs, ft);
+}
+
+void Riscv64Assembler::SubS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FSubS(fd, fs, ft);
+}
+
+void Riscv64Assembler::MulS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMulS(fd, fs, ft);
+}
+
+void Riscv64Assembler::DivS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FDivS(fd, fs, ft);
+}
+
+void Riscv64Assembler::AddD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FAddD(fd, fs, ft);
+}
+
+void Riscv64Assembler::SubD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FSubD(fd, fs, ft);
+}
+
+void Riscv64Assembler::MulD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMulD(fd, fs, ft);
+}
+
+void Riscv64Assembler::DivD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FDivD(fd, fs, ft);
+}
+
+void Riscv64Assembler::SqrtS(FpuRegister fd, FpuRegister fs) {
+  FSqrtS(fd, fs);
+}
+
+void Riscv64Assembler::SqrtD(FpuRegister fd, FpuRegister fs) {
+  FSqrtD(fd, fs);
+}
+
+void Riscv64Assembler::AbsS(FpuRegister fd, FpuRegister fs) {
+  FSgnjxS(fd, fs, fs);
+}
+
+void Riscv64Assembler::AbsD(FpuRegister fd, FpuRegister fs) {
+  FSgnjxD(fd, fs, fs);
+}
+
+void Riscv64Assembler::MovS(FpuRegister fd, FpuRegister fs) {
+  FSgnjS(fd, fs, fs);
+}
+
+void Riscv64Assembler::MovD(FpuRegister fd, FpuRegister fs) {
+  FSgnjD(fd, fs, fs);
+}
+
+void Riscv64Assembler::NegS(FpuRegister fd, FpuRegister fs) {
+  FSgnjnS(fd, fs, fs);
+}
+
+void Riscv64Assembler::NegD(FpuRegister fd, FpuRegister fs) {
+  FSgnjnD(fd, fs, fs);
+}
+
+void Riscv64Assembler::RoundLS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0x8);
+}
+
+void Riscv64Assembler::RoundLD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0x8);
+}
+
+void Riscv64Assembler::RoundWS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0xc);
+}
+
+void Riscv64Assembler::RoundWD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0xc);
+}
+
+void Riscv64Assembler::TruncLS(GpuRegister rd, FpuRegister fs) {
+  Xor(rd, rd, rd);
+  FEqS(TMP, fs, fs);
+  riscv64::Riscv64Label label;
+  Beqzc(TMP, &label);
+  FCvtLS(rd, fs, kFPRoundingModeRTZ);
+  Bind(&label);
+}
+
+void Riscv64Assembler::TruncLD(GpuRegister rd, FpuRegister fs) {
+  Xor(rd, rd, rd);
+  FEqD(TMP, fs, fs);
+  riscv64::Riscv64Label label;
+  Beqzc(TMP, &label);
+  FCvtLD(rd, fs, kFPRoundingModeRTZ);
+  Bind(&label);
+}
+
+void Riscv64Assembler::TruncWS(GpuRegister rd, FpuRegister fs) {
+  Xor(rd, rd, rd);
+  FEqS(TMP, fs, fs);
+  riscv64::Riscv64Label label;
+  Beqzc(TMP, &label);
+  FCvtWS(rd, fs, kFPRoundingModeRTZ);
+  Bind(&label);
+}
+
+void Riscv64Assembler::TruncWD(GpuRegister rd, FpuRegister fs) {
+  Xor(rd, rd, rd);
+  FEqD(TMP, fs, fs);
+  riscv64::Riscv64Label label;
+  Beqzc(TMP, &label);
+  FCvtWD(rd, fs, kFPRoundingModeRTZ);
+  Bind(&label);
+}
+
+void Riscv64Assembler::CeilLS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0xa);
+}
+
+void Riscv64Assembler::CeilLD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0xa);
+}
+
+void Riscv64Assembler::CeilWS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0xe);
+}
+
+void Riscv64Assembler::CeilWD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0xe);
+}
+
+void Riscv64Assembler::FloorLS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0xb);
+}
+
+void Riscv64Assembler::FloorLD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0xb);
+}
+
+void Riscv64Assembler::FloorWS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0xf);
+}
+
+void Riscv64Assembler::FloorWD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0xf);
+}
+
+void Riscv64Assembler::SelS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMvXW(TMP, fd);
+  Andi(TMP, TMP, 1);
+  Beq(TMP, ZERO, 12);
+  FSgnjS(fd, ft, ft);
+  Jal(ZERO, 8);
+
+  FSgnjS(fd, fs, fs);
+}
+
+void Riscv64Assembler::SelD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMvXD(TMP, fd);
+  Andi(TMP, TMP, 1);
+  Beq(TMP, ZERO, 12);
+  FSgnjD(fd, ft, ft);
+  Jal(ZERO, 8);
+
+  FSgnjD(fd, fs, fs);
+}
+
+void Riscv64Assembler::SeleqzS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMvXW(TMP, ft);
+  Andi(TMP, TMP, 1);
+  Beq(TMP, ZERO, 16);
+  Addiw(TMP, ZERO, 0);
+  FCvtSW(fd, TMP);
+  Jal(ZERO, 8);
+
+  FSgnjS(fd, fs, fs);
+}
+
+void Riscv64Assembler::SeleqzD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMvXD(TMP, ft);
+  Andi(TMP, TMP, 1);
+  Beq(TMP, ZERO, 16);
+  Addi(TMP, ZERO, 0);
+  FCvtDL(fd, TMP);
+  Jal(ZERO, 8);
+
+  FSgnjD(fd, fs, fs);
+}
+
+void Riscv64Assembler::SelnezS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMvXW(TMP, ft);
+  Andi(TMP, TMP, 1);
+  Bne(TMP, ZERO, 16);
+  Addiw(TMP, ZERO, 0);
+  FCvtSW(fd, TMP);
+  Jal(ZERO, 8);
+
+  FSgnjS(fd, fs, fs);
+}
+
+void Riscv64Assembler::SelnezD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMvXD(TMP, ft);
+  Andi(TMP, TMP, 1);
+  Bne(TMP, ZERO, 16);
+  Addi(TMP, ZERO, 0);
+  FCvtDL(fd, TMP);
+  Jal(ZERO, 8);
+
+  FSgnjD(fd, fs, fs);
+}
+
+void Riscv64Assembler::RintS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0x1a);
+}
+
+void Riscv64Assembler::RintD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0x1a);
+}
+
+void Riscv64Assembler::ClassS(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x10, static_cast<FpuRegister>(0), fs, fd, 0x1b);
+}
+
+void Riscv64Assembler::ClassD(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+  EmitFR(0x11, 0x11, static_cast<FpuRegister>(0), fs, fd, 0x1b);
+}
+
+void Riscv64Assembler::MinS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMinS(fd, fs, ft);
+}
+
+void Riscv64Assembler::MinD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMinD(fd, fs, ft);
+}
+
+void Riscv64Assembler::MaxS(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMaxS(fd, fs, ft);
+}
+
+void Riscv64Assembler::MaxD(FpuRegister fd, FpuRegister fs, FpuRegister ft) {
+  FMaxD(fd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUnS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassS(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassS(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  Addi(rd, ZERO, 0);  // unordered false;
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpEqS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FEqS(rd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUeqS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassS(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassS(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  FEqS(rd, fs, ft);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpLtS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FLtS(rd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUltS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassS(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassS(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  FLtS(rd, fs, ft);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpLeS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FLeS(rd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUleS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassS(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassS(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  FLeS(rd, fs, ft);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpOrS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  CmpUnS(rd, fs, ft);
+  Sub(rd, ZERO, rd);
+}
+
+void Riscv64Assembler::CmpUneS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassS(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 28);
+
+  FClassS(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 16);
+
+  FEqS(TMP, fs, ft);
+  Sltiu(rd, TMP, 1);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpNeS(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FEqS(rd, fs, ft);
+  Sltiu(rd, rd, 1);
+}
+
+void Riscv64Assembler::CmpUnD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassD(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassD(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  Addi(rd, ZERO, 0);  // unordered false;
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpEqD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FEqD(rd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUeqD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassD(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassD(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  FEqD(rd, fs, ft);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpLtD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FLtD(rd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUltD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassD(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassD(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  FLtD(rd, fs, ft);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpLeD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FLeD(rd, fs, ft);
+}
+
+void Riscv64Assembler::CmpUleD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassD(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 24);
+
+  FClassD(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 12);
+
+  FLeD(rd, fs, ft);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpOrD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  CmpUnD(rd, fs, ft);
+  Sltiu(rd, rd, 1);
+}
+
+void Riscv64Assembler::CmpUneD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FClassD(TMP, fs);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 28);
+
+  FClassD(TMP, ft);
+  Srli(TMP, TMP, 8);
+  Bne(TMP, ZERO, 16);
+
+  FEqD(TMP, fs, ft);
+  Sltiu(rd, rd, 1);
+  Jal(ZERO, 8);
+
+  Addi(rd, ZERO, 1);  // unordered true;
+}
+
+void Riscv64Assembler::CmpNeD(GpuRegister rd, FpuRegister fs, FpuRegister ft) {
+  FEqD(rd, fs, ft);
+  Sltiu(rd, rd, 1);
+}
+
+void Riscv64Assembler::Cvtsw(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+}
+
+void Riscv64Assembler::Cvtdw(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+}
+
+void Riscv64Assembler::Cvtsd(FpuRegister fd, FpuRegister fs) {
+  FCvtSD(fd, fs);
+}
+
+void Riscv64Assembler::Cvtds(FpuRegister fd, FpuRegister fs) {
+  FCvtDS(fd, fs);
+}
+
+void Riscv64Assembler::Cvtsl(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+}
+
+void Riscv64Assembler::Cvtdl(FpuRegister fd, FpuRegister fs) {
+  assert(0);
+}
+
+void Riscv64Assembler::Mfc1(GpuRegister rt, FpuRegister fs) {
+  // Move float32 in fs to rt
+  FMvXW(rt, fs);
+}
+
+void Riscv64Assembler::Mfhc1(GpuRegister rt, FpuRegister fs) {
+  FMvXD(rt, fs);
+  Srli(rt, rt, 32);
+}
+
+void Riscv64Assembler::Mtc1(GpuRegister rt, FpuRegister fs) {
+  // Move float32 in rt to fs
+  FMvWX(fs, rt);
+}
+
+void Riscv64Assembler::Mthc1(GpuRegister rt, FpuRegister fs) {
+  FMvXD(TMP, fs);
+  Slli(TMP, TMP, 32);
+  Srli(TMP, TMP, 32);
+  Slli(rt, rt, 32);
+  Or(rt, rt, TMP);
+  FMvDX(fs, rt);
+}
+
+void Riscv64Assembler::Dmfc1(GpuRegister rt, FpuRegister fs) {
+  // Move double in fs to rt
+  FMvXD(rt, fs);
+}
+
+void Riscv64Assembler::Dmtc1(GpuRegister rt, FpuRegister fs) {
+  // Move double in rt to fs
+  FMvDX(fs, rt);
+}
+
+void Riscv64Assembler::Lwc1(FpuRegister ft, GpuRegister rs, uint16_t imm12) {
+  FLw(ft, rs, imm12);  // warning: lw offset max is 12bits, not 16bits
+}
+
+void Riscv64Assembler::Ldc1(FpuRegister ft, GpuRegister rs, uint16_t imm12) {
+  FLd(ft, rs, imm12);  // warning: lw offset max is 12bits, not 16bits
+}
+
+void Riscv64Assembler::Swc1(FpuRegister ft, GpuRegister rs, uint16_t imm12) {
+  FSw(ft, rs, imm12);  // warning: lw offset max is 12bits, not 16bits
+}
+
+void Riscv64Assembler::Sdc1(FpuRegister ft, GpuRegister rs, uint16_t imm12) {
+  FSd(ft, rs, imm12);  // warning: lw offset max is 12bits, not 16bits
+}
+
+void Riscv64Assembler::Break() {
+  Ebreak();
+}
+
+void Riscv64Assembler::Nop() {
+  Addi(ZERO, ZERO, 0);
+}
+
+void Riscv64Assembler::Move(GpuRegister rd, GpuRegister rs) {
+  Or(rd, rs, ZERO);
+}
+
+void Riscv64Assembler::Clear(GpuRegister rd) {
+  Move(rd, ZERO);
+}
+
+void Riscv64Assembler::Not(GpuRegister rd, GpuRegister rs) {
+  Xori(rd, rs, -1);
+}
+
+void Riscv64Assembler::AndV(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x0, wt, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::OrV(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x1, wt, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::NorV(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x2, wt, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::XorV(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x3, wt, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::AddvB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x0, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::AddvH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x1, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::AddvW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x2, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::AddvD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x3, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::SubvB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x0, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::SubvH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x1, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::SubvW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x2, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::SubvD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x3, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Asub_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x0, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x1, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x2, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x3, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x0, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x1, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x2, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::Asub_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x3, wt, ws, wd, 0x11);
+}
+
+void Riscv64Assembler::MulvB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MulvH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MulvW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MulvD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Div_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Mod_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::Add_aB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x0, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Add_aH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x1, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Add_aW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x2, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Add_aD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x3, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x0, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x1, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x2, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x3, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x0, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x1, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x2, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Ave_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x3, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x0, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x1, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x2, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x3, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x0, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x1, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x2, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Aver_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x3, wt, ws, wd, 0x10);
+}
+
+void Riscv64Assembler::Max_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x0, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x1, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x2, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x3, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x3, 0x0, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x3, 0x1, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x3, 0x2, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Max_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x3, 0x3, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x0, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x1, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x2, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x3, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x0, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x1, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x2, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::Min_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x3, wt, ws, wd, 0xe);
+}
+
+void Riscv64Assembler::FaddW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x0, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FaddD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x1, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FsubW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x2, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FsubD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x3, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmulW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x0, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmulD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x1, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FdivW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x2, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FdivD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x3, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmaxW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x0, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmaxD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x1, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FminW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x0, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FminD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x1, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::Ffint_sW(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2RF(0x19e, 0x0, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::Ffint_sD(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2RF(0x19e, 0x1, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::Ftint_sW(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2RF(0x19c, 0x0, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::Ftint_sD(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2RF(0x19c, 0x1, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::SllB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x0, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SllH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x1, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SllW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x2, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SllD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x0, 0x3, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SraB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x0, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SraH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x1, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SraW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x2, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SraD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x3, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SrlB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x0, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SrlH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x1, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SrlW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x2, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SrlD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x3, wt, ws, wd, 0xd);
+}
+
+void Riscv64Assembler::SlliB(VectorRegister wd, VectorRegister ws, int shamt3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(shamt3)) << shamt3;
+  EmitMsaBIT(0x0, shamt3 | kMsaDfMByteMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SlliH(VectorRegister wd, VectorRegister ws, int shamt4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(shamt4)) << shamt4;
+  EmitMsaBIT(0x0, shamt4 | kMsaDfMHalfwordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SlliW(VectorRegister wd, VectorRegister ws, int shamt5) {
+  CHECK(HasMsa());
+  CHECK(IsUint<5>(shamt5)) << shamt5;
+  EmitMsaBIT(0x0, shamt5 | kMsaDfMWordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SlliD(VectorRegister wd, VectorRegister ws, int shamt6) {
+  CHECK(HasMsa());
+  CHECK(IsUint<6>(shamt6)) << shamt6;
+  EmitMsaBIT(0x0, shamt6 | kMsaDfMDoublewordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SraiB(VectorRegister wd, VectorRegister ws, int shamt3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(shamt3)) << shamt3;
+  EmitMsaBIT(0x1, shamt3 | kMsaDfMByteMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SraiH(VectorRegister wd, VectorRegister ws, int shamt4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(shamt4)) << shamt4;
+  EmitMsaBIT(0x1, shamt4 | kMsaDfMHalfwordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SraiW(VectorRegister wd, VectorRegister ws, int shamt5) {
+  CHECK(HasMsa());
+  CHECK(IsUint<5>(shamt5)) << shamt5;
+  EmitMsaBIT(0x1, shamt5 | kMsaDfMWordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SraiD(VectorRegister wd, VectorRegister ws, int shamt6) {
+  CHECK(HasMsa());
+  CHECK(IsUint<6>(shamt6)) << shamt6;
+  EmitMsaBIT(0x1, shamt6 | kMsaDfMDoublewordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SrliB(VectorRegister wd, VectorRegister ws, int shamt3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(shamt3)) << shamt3;
+  EmitMsaBIT(0x2, shamt3 | kMsaDfMByteMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SrliH(VectorRegister wd, VectorRegister ws, int shamt4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(shamt4)) << shamt4;
+  EmitMsaBIT(0x2, shamt4 | kMsaDfMHalfwordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SrliW(VectorRegister wd, VectorRegister ws, int shamt5) {
+  CHECK(HasMsa());
+  CHECK(IsUint<5>(shamt5)) << shamt5;
+  EmitMsaBIT(0x2, shamt5 | kMsaDfMWordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::SrliD(VectorRegister wd, VectorRegister ws, int shamt6) {
+  CHECK(HasMsa());
+  CHECK(IsUint<6>(shamt6)) << shamt6;
+  EmitMsaBIT(0x2, shamt6 | kMsaDfMDoublewordMask, ws, wd, 0x9);
+}
+
+void Riscv64Assembler::MoveV(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsaBIT(0x1, 0x3e, ws, wd, 0x19);
+}
+
+void Riscv64Assembler::SplatiB(VectorRegister wd, VectorRegister ws, int n4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(n4)) << n4;
+  EmitMsaELM(0x1, n4 | kMsaDfNByteMask, ws, wd, 0x19);
+}
+
+void Riscv64Assembler::SplatiH(VectorRegister wd, VectorRegister ws, int n3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(n3)) << n3;
+  EmitMsaELM(0x1, n3 | kMsaDfNHalfwordMask, ws, wd, 0x19);
+}
+
+void Riscv64Assembler::SplatiW(VectorRegister wd, VectorRegister ws, int n2) {
+  CHECK(HasMsa());
+  CHECK(IsUint<2>(n2)) << n2;
+  EmitMsaELM(0x1, n2 | kMsaDfNWordMask, ws, wd, 0x19);
+}
+
+void Riscv64Assembler::SplatiD(VectorRegister wd, VectorRegister ws, int n1) {
+  CHECK(HasMsa());
+  CHECK(IsUint<1>(n1)) << n1;
+  EmitMsaELM(0x1, n1 | kMsaDfNDoublewordMask, ws, wd, 0x19);
+}
+
+void Riscv64Assembler::Copy_sB(GpuRegister rd, VectorRegister ws, int n4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(n4)) << n4;
+  EmitMsaELM(0x2, n4 | kMsaDfNByteMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::Copy_sH(GpuRegister rd, VectorRegister ws, int n3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(n3)) << n3;
+  EmitMsaELM(0x2, n3 | kMsaDfNHalfwordMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::Copy_sW(GpuRegister rd, VectorRegister ws, int n2) {
+  CHECK(HasMsa());
+  CHECK(IsUint<2>(n2)) << n2;
+  EmitMsaELM(0x2, n2 | kMsaDfNWordMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::Copy_sD(GpuRegister rd, VectorRegister ws, int n1) {
+  CHECK(HasMsa());
+  CHECK(IsUint<1>(n1)) << n1;
+  EmitMsaELM(0x2, n1 | kMsaDfNDoublewordMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::Copy_uB(GpuRegister rd, VectorRegister ws, int n4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(n4)) << n4;
+  EmitMsaELM(0x3, n4 | kMsaDfNByteMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::Copy_uH(GpuRegister rd, VectorRegister ws, int n3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(n3)) << n3;
+  EmitMsaELM(0x3, n3 | kMsaDfNHalfwordMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::Copy_uW(GpuRegister rd, VectorRegister ws, int n2) {
+  CHECK(HasMsa());
+  CHECK(IsUint<2>(n2)) << n2;
+  EmitMsaELM(0x3, n2 | kMsaDfNWordMask, ws, static_cast<VectorRegister>(rd), 0x19);
+}
+
+void Riscv64Assembler::InsertB(VectorRegister wd, GpuRegister rs, int n4) {
+  CHECK(HasMsa());
+  CHECK(IsUint<4>(n4)) << n4;
+  EmitMsaELM(0x4, n4 | kMsaDfNByteMask, static_cast<VectorRegister>(rs), wd, 0x19);
+}
+
+void Riscv64Assembler::InsertH(VectorRegister wd, GpuRegister rs, int n3) {
+  CHECK(HasMsa());
+  CHECK(IsUint<3>(n3)) << n3;
+  EmitMsaELM(0x4, n3 | kMsaDfNHalfwordMask, static_cast<VectorRegister>(rs), wd, 0x19);
+}
+
+void Riscv64Assembler::InsertW(VectorRegister wd, GpuRegister rs, int n2) {
+  CHECK(HasMsa());
+  CHECK(IsUint<2>(n2)) << n2;
+  EmitMsaELM(0x4, n2 | kMsaDfNWordMask, static_cast<VectorRegister>(rs), wd, 0x19);
+}
+
+void Riscv64Assembler::InsertD(VectorRegister wd, GpuRegister rs, int n1) {
+  CHECK(HasMsa());
+  CHECK(IsUint<1>(n1)) << n1;
+  EmitMsaELM(0x4, n1 | kMsaDfNDoublewordMask, static_cast<VectorRegister>(rs), wd, 0x19);
+}
+
+void Riscv64Assembler::FillB(VectorRegister wd, GpuRegister rs) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc0, 0x0, static_cast<VectorRegister>(rs), wd, 0x1e);
+}
+
+void Riscv64Assembler::FillH(VectorRegister wd, GpuRegister rs) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc0, 0x1, static_cast<VectorRegister>(rs), wd, 0x1e);
+}
+
+void Riscv64Assembler::FillW(VectorRegister wd, GpuRegister rs) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc0, 0x2, static_cast<VectorRegister>(rs), wd, 0x1e);
+}
+
+void Riscv64Assembler::FillD(VectorRegister wd, GpuRegister rs) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc0, 0x3, static_cast<VectorRegister>(rs), wd, 0x1e);
+}
+
+void Riscv64Assembler::LdiB(VectorRegister wd, int imm8) {
+  CHECK(HasMsa());
+  CHECK(IsInt<8>(imm8)) << imm8;
+  EmitMsaI10(0x6, 0x0, imm8 & kMsaS10Mask, wd, 0x7);
+}
+
+void Riscv64Assembler::LdiH(VectorRegister wd, int imm10) {
+  CHECK(HasMsa());
+  CHECK(IsInt<10>(imm10)) << imm10;
+  EmitMsaI10(0x6, 0x1, imm10 & kMsaS10Mask, wd, 0x7);
+}
+
+void Riscv64Assembler::LdiW(VectorRegister wd, int imm10) {
+  CHECK(HasMsa());
+  CHECK(IsInt<10>(imm10)) << imm10;
+  EmitMsaI10(0x6, 0x2, imm10 & kMsaS10Mask, wd, 0x7);
+}
+
+void Riscv64Assembler::LdiD(VectorRegister wd, int imm10) {
+  CHECK(HasMsa());
+  CHECK(IsInt<10>(imm10)) << imm10;
+  EmitMsaI10(0x6, 0x3, imm10 & kMsaS10Mask, wd, 0x7);
+}
+
+void Riscv64Assembler::LdB(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<10>(offset)) << offset;
+  EmitMsaMI10(offset & kMsaS10Mask, rs, wd, 0x8, 0x0);
+}
+
+void Riscv64Assembler::LdH(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<11>(offset)) << offset;
+  CHECK_ALIGNED(offset, kRiscv64HalfwordSize);
+  EmitMsaMI10((offset >> TIMES_2) & kMsaS10Mask, rs, wd, 0x8, 0x1);
+}
+
+void Riscv64Assembler::LdW(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<12>(offset)) << offset;
+  CHECK_ALIGNED(offset, kRiscv64WordSize);
+  EmitMsaMI10((offset >> TIMES_4) & kMsaS10Mask, rs, wd, 0x8, 0x2);
+}
+
+void Riscv64Assembler::LdD(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<13>(offset)) << offset;
+  CHECK_ALIGNED(offset, kRiscv64DoublewordSize);
+  EmitMsaMI10((offset >> TIMES_8) & kMsaS10Mask, rs, wd, 0x8, 0x3);
+}
+
+void Riscv64Assembler::StB(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<10>(offset)) << offset;
+  EmitMsaMI10(offset & kMsaS10Mask, rs, wd, 0x9, 0x0);
+}
+
+void Riscv64Assembler::StH(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<11>(offset)) << offset;
+  CHECK_ALIGNED(offset, kRiscv64HalfwordSize);
+  EmitMsaMI10((offset >> TIMES_2) & kMsaS10Mask, rs, wd, 0x9, 0x1);
+}
+
+void Riscv64Assembler::StW(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<12>(offset)) << offset;
+  CHECK_ALIGNED(offset, kRiscv64WordSize);
+  EmitMsaMI10((offset >> TIMES_4) & kMsaS10Mask, rs, wd, 0x9, 0x2);
+}
+
+void Riscv64Assembler::StD(VectorRegister wd, GpuRegister rs, int offset) {
+  CHECK(HasMsa());
+  CHECK(IsInt<13>(offset)) << offset;
+  CHECK_ALIGNED(offset, kRiscv64DoublewordSize);
+  EmitMsaMI10((offset >> TIMES_8) & kMsaS10Mask, rs, wd, 0x9, 0x3);
+}
+
+void Riscv64Assembler::IlvlB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x0, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvlH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x1, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvlW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x2, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvlD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x3, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvrB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x0, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvrH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x1, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvrW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x2, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvrD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x3, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvevB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x0, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvevH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x1, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvevW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x2, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvevD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x6, 0x3, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvodB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x0, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvodH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x1, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvodW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x2, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::IlvodD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x7, 0x3, wt, ws, wd, 0x14);
+}
+
+void Riscv64Assembler::MaddvB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MaddvH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MaddvW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MaddvD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x1, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MsubvB(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x0, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MsubvH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x1, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MsubvW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x2, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::MsubvD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x3, wt, ws, wd, 0x12);
+}
+
+void Riscv64Assembler::FmaddW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x0, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmaddD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x1, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmsubW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x2, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::FmsubD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x2, 0x3, wt, ws, wd, 0x1b);
+}
+
+void Riscv64Assembler::Hadd_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x1, wt, ws, wd, 0x15);
+}
+
+void Riscv64Assembler::Hadd_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x2, wt, ws, wd, 0x15);
+}
+
+void Riscv64Assembler::Hadd_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x4, 0x3, wt, ws, wd, 0x15);
+}
+
+void Riscv64Assembler::Hadd_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x1, wt, ws, wd, 0x15);
+}
+
+void Riscv64Assembler::Hadd_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x2, wt, ws, wd, 0x15);
+}
+
+void Riscv64Assembler::Hadd_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt) {
+  CHECK(HasMsa());
+  EmitMsa3R(0x5, 0x3, wt, ws, wd, 0x15);
+}
+
+void Riscv64Assembler::PcntB(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc1, 0x0, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::PcntH(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc1, 0x1, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::PcntW(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc1, 0x2, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::PcntD(VectorRegister wd, VectorRegister ws) {
+  CHECK(HasMsa());
+  EmitMsa2R(0xc1, 0x3, ws, wd, 0x1e);
+}
+
+void Riscv64Assembler::ReplicateFPToVectorRegister(VectorRegister dst,
+                                                  FpuRegister src,
+                                                  bool is_double) {
+  // Float or double in FPU register Fx can be considered as 0th element in vector register Wx.
+  if (is_double) {
+    SplatiD(dst, static_cast<VectorRegister>(src), 0);
+  } else {
+    SplatiW(dst, static_cast<VectorRegister>(src), 0);
+  }
+}
+
+void Riscv64Assembler::LoadConst32(GpuRegister rd, int32_t value) {
+  // Use 11-bit check here for avoiding sign-extension.
+  if (IsInt<11>(value)) {
+    Addiw(rd, ZERO, value);
+  } else {
+    int32_t l = value & 0xFFF;
+    int32_t h = value >> 12;
+    if ((l & 0x800) != 0) {
+      h += 1;  // overflow ?
+    }
+    Lui(rd, h);
+    Addiw(rd, rd, l);
+  }
+}
+
+// This function is only used for testing purposes.
+void Riscv64Assembler::RecordLoadConst64Path(int value ATTRIBUTE_UNUSED) {
+}
+
+void Riscv64Assembler::LoadConst64(GpuRegister rd, int64_t value) {
+  // TemplateLoadConst64(this, rd, value);
+  if (IsInt<32>(value)) {
+    LoadConst32(rd, value);
+  } else {
+    // Need to optimize in the future.
+    int32_t hi = value >> 32;
+    int32_t lo = value;
+
+    GpuRegister scratch = TMP2;
+
+    LoadConst32(scratch, lo);
+    LoadConst32(rd, hi);
+    Slli(rd, rd, 32);
+    Slli(scratch, scratch, 32);
+    Srli(scratch, scratch, 32);
+    Or(rd, rd, scratch);
+  }
+}
+
+void Riscv64Assembler::Addiu32(GpuRegister rt, GpuRegister rs, int32_t value) {
+  if (IsInt<12>(value)) {
+    Addiw(rt, rs, value);
+  } else {
+    LoadConst32(TMP2, value);
+    Addw(rt, rs, TMP2);
+  }
+}
+
+// TODO: don't use rtmp, use daui, dahi, dati.
+void Riscv64Assembler::Daddiu64(GpuRegister rt, GpuRegister rs, int64_t value, GpuRegister rtmp) {
+  CHECK_NE(rs, rtmp);
+  if (IsInt<12>(value)) {
+    Addi(rt, rs, value);
+  } else {
+    LoadConst64(rtmp, value);
+    Add(rt, rs, rtmp);
+  }
+}
+
+void Riscv64Assembler::Branch::InitShortOrLong(Riscv64Assembler::Branch::OffsetBits offset_size,
+                                              Riscv64Assembler::Branch::Type short_type,
+                                              Riscv64Assembler::Branch::Type long_type) {
+  type_ = (offset_size <= branch_info_[short_type].offset_size) ? short_type : long_type;
+}
+
+void Riscv64Assembler::Branch::InitializeType(Type initial_type) {
+  OffsetBits offset_size_needed = GetOffsetSizeNeeded(location_, target_);
+
+    switch (initial_type) {
+      case kLabel:
+      case kLiteral:
+      case kLiteralUnsigned:
+      case kLiteralLong:
+        CHECK(!IsResolved());
+        type_ = initial_type;
+        break;
+      case kCall:
+        InitShortOrLong(offset_size_needed, kCall, kLongCall);
+        break;
+      case kCondBranch:
+        switch (condition_) {
+          case kUncond:
+            InitShortOrLong(offset_size_needed, kUncondBranch, kLongUncondBranch);
+            break;
+          default:
+            InitShortOrLong(offset_size_needed, kCondBranch, kLongCondBranch);
+            break;
+        }
+        break;
+      case kBareCall:
+        type_ = kBareCall;
+        CHECK_LE(offset_size_needed, GetOffsetSize());
+        break;
+      case kBareCondBranch:
+        type_ = (condition_ == kUncond) ? kBareUncondBranch : kBareCondBranch;
+        CHECK_LE(offset_size_needed, GetOffsetSize());
+        break;
+      default:
+        LOG(FATAL) << "Unexpected branch type " << initial_type;
+        UNREACHABLE();
+    }
+
+  old_type_ = type_;
+}
+
+bool Riscv64Assembler::Branch::IsNop(BranchCondition condition, GpuRegister lhs, GpuRegister rhs) {
+  switch (condition) {
+    case kCondLT:
+    case kCondGT:
+    case kCondNE:
+    case kCondLTU:
+      return lhs == rhs;
+    default:
+      return false;
+  }
+}
+
+bool Riscv64Assembler::Branch::IsUncond(BranchCondition condition,
+                                       GpuRegister lhs,
+                                       GpuRegister rhs) {
+  switch (condition) {
+    case kUncond:
+      return true;
+    case kCondGE:
+    case kCondLE:
+    case kCondEQ:
+    case kCondGEU:
+      return lhs == rhs;
+    default:
+      return false;
+  }
+}
+
+Riscv64Assembler::Branch::Branch(uint32_t location, uint32_t target, bool is_call, bool is_bare)
+    : old_location_(location),
+      location_(location),
+      target_(target),
+      lhs_reg_(ZERO),
+      rhs_reg_(ZERO),
+      condition_(kUncond) {
+  InitializeType(
+      (is_call ? (is_bare ? kBareCall : kCall) : (is_bare ? kBareCondBranch : kCondBranch)));
+}
+
+Riscv64Assembler::Branch::Branch(uint32_t location,
+                                uint32_t target,
+                                Riscv64Assembler::BranchCondition condition,
+                                GpuRegister lhs_reg,
+                                GpuRegister rhs_reg,
+                                bool is_bare)
+    : old_location_(location),
+      location_(location),
+      target_(target),
+      lhs_reg_(lhs_reg),
+      rhs_reg_(rhs_reg),
+      condition_(condition) {
+  // FIXME: T-HEAD
+  // CHECK_NE(condition, kUncond);
+  switch (condition) {
+    case kCondEQ:
+    case kCondNE:
+    case kCondLT:
+    case kCondGE:
+    case kCondLE:
+    case kCondGT:
+    case kCondLTU:
+    case kCondGEU:
+      CHECK_NE(lhs_reg, ZERO);
+      CHECK_NE(rhs_reg, ZERO);
+      break;
+    case kCondLTZ:
+    case kCondGEZ:
+    case kCondLEZ:
+    case kCondGTZ:
+    case kCondEQZ:
+    case kCondNEZ:
+      CHECK_NE(lhs_reg, ZERO);
+      CHECK_EQ(rhs_reg, ZERO);
+      break;
+    case kUncond:
+      UNREACHABLE();
+  }
+  CHECK(!IsNop(condition, lhs_reg, rhs_reg));
+  if (IsUncond(condition, lhs_reg, rhs_reg)) {
+    // Branch condition is always true, make the branch unconditional.
+    condition_ = kUncond;
+  }
+  InitializeType((is_bare ? kBareCondBranch : kCondBranch));
+}
+
+Riscv64Assembler::Branch::Branch(uint32_t location, GpuRegister dest_reg, Type label_or_literal_type)
+    : old_location_(location),
+      location_(location),
+      target_(kUnresolved),
+      lhs_reg_(dest_reg),
+      rhs_reg_(ZERO),
+      condition_(kUncond) {
+  CHECK_NE(dest_reg, ZERO);
+  InitializeType(label_or_literal_type);
+}
+
+Riscv64Assembler::BranchCondition Riscv64Assembler::Branch::OppositeCondition(
+    Riscv64Assembler::BranchCondition cond) {
+  switch (cond) {
+    case kCondLT:
+      return kCondGE;
+    case kCondGE:
+      return kCondLT;
+    case kCondLE:
+      return kCondGT;
+    case kCondGT:
+      return kCondLE;
+    case kCondLTZ:
+      return kCondGEZ;
+    case kCondGEZ:
+      return kCondLTZ;
+    case kCondLEZ:
+      return kCondGTZ;
+    case kCondGTZ:
+      return kCondLEZ;
+    case kCondEQ:
+      return kCondNE;
+    case kCondNE:
+      return kCondEQ;
+    case kCondEQZ:
+      return kCondNEZ;
+    case kCondNEZ:
+      return kCondEQZ;
+    case kCondLTU:
+      return kCondGEU;
+    case kCondGEU:
+      return kCondLTU;
+    case kUncond:
+      // LOG(FATAL) << "Unexpected branch condition " << cond;
+      LOG(FATAL) << "Unexpected branch condition ";
+  }
+  UNREACHABLE();
+}
+
+Riscv64Assembler::Branch::Type Riscv64Assembler::Branch::GetType() const {
+  return type_;
+}
+
+Riscv64Assembler::BranchCondition Riscv64Assembler::Branch::GetCondition() const {
+  return condition_;
+}
+
+GpuRegister Riscv64Assembler::Branch::GetLeftRegister() const {
+  return lhs_reg_;
+}
+
+GpuRegister Riscv64Assembler::Branch::GetRightRegister() const {
+  return rhs_reg_;
+}
+
+uint32_t Riscv64Assembler::Branch::GetTarget() const {
+  return target_;
+}
+
+uint32_t Riscv64Assembler::Branch::GetLocation() const {
+  return location_;
+}
+
+uint32_t Riscv64Assembler::Branch::GetOldLocation() const {
+  return old_location_;
+}
+
+uint32_t Riscv64Assembler::Branch::GetLength() const {
+  return branch_info_[type_].length;
+}
+
+uint32_t Riscv64Assembler::Branch::GetOldLength() const {
+  return branch_info_[old_type_].length;
+}
+
+uint32_t Riscv64Assembler::Branch::GetSize() const {
+  return GetLength() * sizeof(uint32_t);
+}
+
+uint32_t Riscv64Assembler::Branch::GetOldSize() const {
+  return GetOldLength() * sizeof(uint32_t);
+}
+
+uint32_t Riscv64Assembler::Branch::GetEndLocation() const {
+  return GetLocation() + GetSize();
+}
+
+uint32_t Riscv64Assembler::Branch::GetOldEndLocation() const {
+  return GetOldLocation() + GetOldSize();
+}
+
+bool Riscv64Assembler::Branch::IsBare() const {
+  switch (type_) {
+    // R6 short branches (can't be promoted to long), forbidden/delay slots filled manually.
+    case kBareUncondBranch:
+    case kBareCondBranch:
+    case kBareCall:
+      return true;
+    default:
+      return false;
+  }
+}
+
+bool Riscv64Assembler::Branch::IsLong() const {
+  switch (type_) {
+    // R6 short branches (can be promoted to long).
+    case kUncondBranch:
+    case kCondBranch:
+    case kCall:
+    // R6 short branches (can't be promoted to long), forbidden/delay slots filled manually.
+    case kBareUncondBranch:
+    case kBareCondBranch:
+    case kBareCall:
+      return false;
+    // Long branches.
+    case kLongUncondBranch:
+    case kLongCondBranch:
+    case kLongCall:
+    // label.
+    case kLabel:
+    // literals.
+    case kLiteral:
+    case kLiteralUnsigned:
+    case kLiteralLong:
+      return true;
+  }
+  UNREACHABLE();
+}
+
+bool Riscv64Assembler::Branch::IsResolved() const {
+  return target_ != kUnresolved;
+}
+
+Riscv64Assembler::Branch::OffsetBits Riscv64Assembler::Branch::GetOffsetSize() const {
+  return branch_info_[type_].offset_size;
+}
+
+Riscv64Assembler::Branch::OffsetBits Riscv64Assembler::Branch::GetOffsetSizeNeeded(uint32_t location,
+                                                                                 uint32_t target) {
+  // For unresolved targets assume the shortest encoding
+  // (later it will be made longer if needed).
+  if (target == kUnresolved)
+    return kOffset13;
+  int64_t distance = static_cast<int64_t>(target) - location;
+  // To simplify calculations in composite branches consisting of multiple instructions
+  // bump up the distance by a value larger than the max byte size of a composite branch.
+  distance += (distance >= 0) ? kMaxBranchSize : -kMaxBranchSize;
+  if (IsInt<kOffset13>(distance))
+    return kOffset13;
+  else if (IsInt<kOffset21>(distance))
+    return kOffset21;
+  return kOffset32;
+}
+
+void Riscv64Assembler::Branch::Resolve(uint32_t target) {
+  target_ = target;
+}
+
+void Riscv64Assembler::Branch::Relocate(uint32_t expand_location, uint32_t delta) {
+  if (location_ > expand_location) {
+    location_ += delta;
+  }
+  if (!IsResolved()) {
+    return;  // Don't know the target yet.
+  }
+  if (target_ > expand_location) {
+    target_ += delta;
+  }
+}
+
+void Riscv64Assembler::Branch::PromoteToLong() {
+  CHECK(!IsBare());  // Bare branches do not promote.
+  switch (type_) {
+    // R6 short branches (can be promoted to long).
+    case kUncondBranch:
+      type_ = kLongUncondBranch;
+      break;
+    case kCondBranch:
+      type_ = kLongCondBranch;
+      break;
+    case kCall:
+      type_ = kLongCall;
+      break;
+    default:
+      // Note: 'type_' is already long.
+      break;
+  }
+  CHECK(IsLong());
+}
+
+uint32_t Riscv64Assembler::Branch::PromoteIfNeeded(uint32_t max_short_distance) {
+  // If the branch is still unresolved or already long, nothing to do.
+  if (IsLong() || !IsResolved()) {
+    return 0;
+  }
+  // Promote the short branch to long if the offset size is too small
+  // to hold the distance between location_ and target_.
+  if (GetOffsetSizeNeeded(location_, target_) > GetOffsetSize()) {
+    PromoteToLong();
+    uint32_t old_size = GetOldSize();
+    uint32_t new_size = GetSize();
+    CHECK_GT(new_size, old_size);
+    return new_size - old_size;
+  }
+  // The following logic is for debugging/testing purposes.
+  // Promote some short branches to long when it's not really required.
+  if (UNLIKELY(max_short_distance != std::numeric_limits<uint32_t>::max() && !IsBare())) {
+    int64_t distance = static_cast<int64_t>(target_) - location_;
+    distance = (distance >= 0) ? distance : -distance;
+    if (distance >= max_short_distance) {
+      PromoteToLong();
+      uint32_t old_size = GetOldSize();
+      uint32_t new_size = GetSize();
+      CHECK_GT(new_size, old_size);
+      return new_size - old_size;
+    }
+  }
+  return 0;
+}
+
+uint32_t Riscv64Assembler::Branch::GetOffsetLocation() const {
+  return location_ + branch_info_[type_].instr_offset * sizeof(uint32_t);
+}
+
+uint32_t Riscv64Assembler::Branch::GetOffset() const {
+  CHECK(IsResolved());
+  uint32_t ofs_mask = 0xFFFFFFFF >> (32 - GetOffsetSize());
+  // Calculate the byte distance between instructions and also account for
+  // different PC-relative origins.
+  uint32_t offset_location = GetOffsetLocation();
+  uint32_t offset = target_ - offset_location - branch_info_[type_].pc_org * sizeof(uint32_t);
+  // Prepare the offset for encoding into the instruction(s).
+  offset = (offset & ofs_mask) >> branch_info_[type_].offset_shift;
+  return offset;
+}
+
+Riscv64Assembler::Branch* Riscv64Assembler::GetBranch(uint32_t branch_id) {
+  CHECK_LT(branch_id, branches_.size());
+  return &branches_[branch_id];
+}
+
+const Riscv64Assembler::Branch* Riscv64Assembler::GetBranch(uint32_t branch_id) const {
+  CHECK_LT(branch_id, branches_.size());
+  return &branches_[branch_id];
+}
+
+void Riscv64Assembler::Bind(Riscv64Label* label) {
+  CHECK(!label->IsBound());
+  uint32_t bound_pc = buffer_.Size();
+
+  // Walk the list of branches referring to and preceding this label.
+  // Store the previously unknown target addresses in them.
+  while (label->IsLinked()) {
+    uint32_t branch_id = label->Position();
+    Branch* branch = GetBranch(branch_id);
+    branch->Resolve(bound_pc);
+
+    uint32_t branch_location = branch->GetLocation();
+    // Extract the location of the previous branch in the list (walking the list backwards;
+    // the previous branch ID was stored in the space reserved for this branch).
+    uint32_t prev = buffer_.Load<uint32_t>(branch_location);
+
+    // On to the previous branch in the list...
+    label->position_ = prev;
+  }
+
+  // Now make the label object contain its own location (relative to the end of the preceding
+  // branch, if any; it will be used by the branches referring to and following this label).
+  label->prev_branch_id_plus_one_ = branches_.size();
+  if (label->prev_branch_id_plus_one_) {
+    uint32_t branch_id = label->prev_branch_id_plus_one_ - 1;
+    const Branch* branch = GetBranch(branch_id);
+    bound_pc -= branch->GetEndLocation();
+  }
+  label->BindTo(bound_pc);
+}
+
+uint32_t Riscv64Assembler::GetLabelLocation(const Riscv64Label* label) const {
+  CHECK(label->IsBound());
+  uint32_t target = label->Position();
+  if (label->prev_branch_id_plus_one_) {
+    // Get label location based on the branch preceding it.
+    uint32_t branch_id = label->prev_branch_id_plus_one_ - 1;
+    const Branch* branch = GetBranch(branch_id);
+    target += branch->GetEndLocation();
+  }
+  return target;
+}
+
+uint32_t Riscv64Assembler::GetAdjustedPosition(uint32_t old_position) {
+  // We can reconstruct the adjustment by going through all the branches from the beginning
+  // up to the old_position. Since we expect AdjustedPosition() to be called in a loop
+  // with increasing old_position, we can use the data from last AdjustedPosition() to
+  // continue where we left off and the whole loop should be O(m+n) where m is the number
+  // of positions to adjust and n is the number of branches.
+  if (old_position < last_old_position_) {
+    last_position_adjustment_ = 0;
+    last_old_position_ = 0;
+    last_branch_id_ = 0;
+  }
+  while (last_branch_id_ != branches_.size()) {
+    const Branch* branch = GetBranch(last_branch_id_);
+    if (branch->GetLocation() >= old_position + last_position_adjustment_) {
+      break;
+    }
+    last_position_adjustment_ += branch->GetSize() - branch->GetOldSize();
+    ++last_branch_id_;
+  }
+  last_old_position_ = old_position;
+  return old_position + last_position_adjustment_;
+}
+
+void Riscv64Assembler::FinalizeLabeledBranch(Riscv64Label* label) {
+  uint32_t length = branches_.back().GetLength();
+  if (!label->IsBound()) {
+    // Branch forward (to a following label), distance is unknown.
+    // The first branch forward will contain 0, serving as the terminator of
+    // the list of forward-reaching branches.
+    Emit(label->position_);
+    length--;
+    // Now make the label object point to this branch
+    // (this forms a linked list of branches preceding this label).
+    uint32_t branch_id = branches_.size() - 1;
+    label->LinkTo(branch_id);
+  }
+  // Reserve space for the branch.
+  for (; length != 0u; --length) {
+    Nop();
+  }
+}
+
+void Riscv64Assembler::Buncond(Riscv64Label* label, bool is_bare) {
+  uint32_t target = label->IsBound() ? GetLabelLocation(label) : Branch::kUnresolved;
+  branches_.emplace_back(buffer_.Size(), target, /* is_call= */ false, is_bare);
+  FinalizeLabeledBranch(label);
+}
+
+void Riscv64Assembler::Bcond(Riscv64Label* label,
+                            bool is_bare,
+                            BranchCondition condition,
+                            GpuRegister lhs,
+                            GpuRegister rhs) {
+  // If lhs = rhs, this can be a NOP.
+  if (Branch::IsNop(condition, lhs, rhs)) {
+    return;
+  }
+  uint32_t target = label->IsBound() ? GetLabelLocation(label) : Branch::kUnresolved;
+  branches_.emplace_back(buffer_.Size(), target, condition, lhs, rhs, is_bare);
+  FinalizeLabeledBranch(label);
+}
+
+void Riscv64Assembler::Call(Riscv64Label* label, bool is_bare) {
+  uint32_t target = label->IsBound() ? GetLabelLocation(label) : Branch::kUnresolved;
+  branches_.emplace_back(buffer_.Size(), target, /* is_call= */ true, is_bare);
+  FinalizeLabeledBranch(label);
+}
+
+void Riscv64Assembler::LoadLabelAddress(GpuRegister dest_reg, Riscv64Label* label) {
+  // Label address loads are treated as pseudo branches since they require very similar handling.
+  DCHECK(!label->IsBound());
+  branches_.emplace_back(buffer_.Size(), dest_reg, Branch::kLabel);
+  FinalizeLabeledBranch(label);
+}
+
+Literal* Riscv64Assembler::NewLiteral(size_t size, const uint8_t* data) {
+  // We don't support byte and half-word literals.
+  if (size == 4u) {
+    literals_.emplace_back(size, data);
+    return &literals_.back();
+  } else {
+    DCHECK_EQ(size, 8u);
+    long_literals_.emplace_back(size, data);
+    return &long_literals_.back();
+  }
+}
+
+void Riscv64Assembler::LoadLiteral(GpuRegister dest_reg,
+                                  LoadOperandType load_type,
+                                  Literal* literal) {
+  // Literal loads are treated as pseudo branches since they require very similar handling.
+  Branch::Type literal_type;
+  switch (load_type) {
+    case kLoadWord:
+      DCHECK_EQ(literal->GetSize(), 4u);
+      literal_type = Branch::kLiteral;
+      break;
+    case kLoadUnsignedWord:
+      DCHECK_EQ(literal->GetSize(), 4u);
+      literal_type = Branch::kLiteralUnsigned;
+      break;
+    case kLoadDoubleword:
+      DCHECK_EQ(literal->GetSize(), 8u);
+      literal_type = Branch::kLiteralLong;
+      break;
+    default:
+      LOG(FATAL) << "Unexpected literal load type " << load_type;
+      UNREACHABLE();
+  }
+  Riscv64Label* label = literal->GetLabel();
+  DCHECK(!label->IsBound());
+  branches_.emplace_back(buffer_.Size(), dest_reg, literal_type);
+  FinalizeLabeledBranch(label);
+}
+
+JumpTable* Riscv64Assembler::CreateJumpTable(std::vector<Riscv64Label*>&& labels) {
+  jump_tables_.emplace_back(std::move(labels));
+  JumpTable* table = &jump_tables_.back();
+  DCHECK(!table->GetLabel()->IsBound());
+  return table;
+}
+
+void Riscv64Assembler::ReserveJumpTableSpace() {
+  if (!jump_tables_.empty()) {
+    for (JumpTable& table : jump_tables_) {
+      Riscv64Label* label = table.GetLabel();
+      Bind(label);
+
+      // Bulk ensure capacity, as this may be large.
+      size_t orig_size = buffer_.Size();
+      size_t required_capacity = orig_size + table.GetSize();
+      if (required_capacity > buffer_.Capacity()) {
+        buffer_.ExtendCapacity(required_capacity);
+      }
+#ifndef NDEBUG
+      buffer_.has_ensured_capacity_ = true;
+#endif
+
+      // Fill the space with dummy data as the data is not final
+      // until the branches have been promoted. And we shouldn't
+      // be moving uninitialized data during branch promotion.
+      for (size_t cnt = table.GetData().size(), i = 0; i < cnt; i++) {
+        buffer_.Emit<uint32_t>(0x1abe1234u);
+      }
+
+#ifndef NDEBUG
+      buffer_.has_ensured_capacity_ = false;
+#endif
+    }
+  }
+}
+
+void Riscv64Assembler::EmitJumpTables() {
+  if (!jump_tables_.empty()) {
+    CHECK(!overwriting_);
+    // Switch from appending instructions at the end of the buffer to overwriting
+    // existing instructions (here, jump tables) in the buffer.
+    overwriting_ = true;
+
+    for (JumpTable& table : jump_tables_) {
+      Riscv64Label* table_label = table.GetLabel();
+      uint32_t start = GetLabelLocation(table_label);
+      overwrite_location_ = start;
+
+      for (Riscv64Label* target : table.GetData()) {
+        CHECK_EQ(buffer_.Load<uint32_t>(overwrite_location_), 0x1abe1234u);
+        // The table will contain target addresses relative to the table start.
+        uint32_t offset = GetLabelLocation(target) - start;
+        Emit(offset);
+      }
+    }
+
+    overwriting_ = false;
+  }
+}
+
+void Riscv64Assembler::EmitLiterals() {
+  if (!literals_.empty()) {
+    for (Literal& literal : literals_) {
+      Riscv64Label* label = literal.GetLabel();
+      Bind(label);
+      AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+      DCHECK_EQ(literal.GetSize(), 4u);
+      for (size_t i = 0, size = literal.GetSize(); i != size; ++i) {
+        buffer_.Emit<uint8_t>(literal.GetData()[i]);
+      }
+    }
+  }
+  if (!long_literals_.empty()) {
+    // Reserve 4 bytes for potential alignment. If after the branch promotion the 64-bit
+    // literals don't end up 8-byte-aligned, they will be moved down 4 bytes.
+    Emit(0);  // NOP.
+    for (Literal& literal : long_literals_) {
+      Riscv64Label* label = literal.GetLabel();
+      Bind(label);
+      AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+      DCHECK_EQ(literal.GetSize(), 8u);
+      for (size_t i = 0, size = literal.GetSize(); i != size; ++i) {
+        buffer_.Emit<uint8_t>(literal.GetData()[i]);
+      }
+    }
+  }
+}
+
+void Riscv64Assembler::PromoteBranches() {
+  // Promote short branches to long as necessary.
+  bool changed;
+  do {
+    changed = false;
+    for (auto& branch : branches_) {
+      CHECK(branch.IsResolved());
+      uint32_t delta = branch.PromoteIfNeeded();
+      // If this branch has been promoted and needs to expand in size,
+      // relocate all branches by the expansion size.
+      if (delta) {
+        changed = true;
+        uint32_t expand_location = branch.GetLocation();
+        for (auto& branch2 : branches_) {
+          branch2.Relocate(expand_location, delta);
+        }
+      }
+    }
+  } while (changed);
+
+  // Account for branch expansion by resizing the code buffer
+  // and moving the code in it to its final location.
+  size_t branch_count = branches_.size();
+  if (branch_count > 0) {
+    // Resize.
+    Branch& last_branch = branches_[branch_count - 1];
+    uint32_t size_delta = last_branch.GetEndLocation() - last_branch.GetOldEndLocation();
+    uint32_t old_size = buffer_.Size();
+    buffer_.Resize(old_size + size_delta);
+    // Move the code residing between branch placeholders.
+    uint32_t end = old_size;
+    for (size_t i = branch_count; i > 0; ) {
+      Branch& branch = branches_[--i];
+      uint32_t size = end - branch.GetOldEndLocation();
+      buffer_.Move(branch.GetEndLocation(), branch.GetOldEndLocation(), size);
+      end = branch.GetOldLocation();
+    }
+  }
+
+  // Align 64-bit literals by moving them down by 4 bytes if needed.
+  // This will reduce the PC-relative distance, which should be safe for both near and far literals.
+  if (!long_literals_.empty()) {
+    uint32_t first_literal_location = GetLabelLocation(long_literals_.front().GetLabel());
+    size_t lit_size = long_literals_.size() * sizeof(uint64_t);
+    size_t buf_size = buffer_.Size();
+    // 64-bit literals must be at the very end of the buffer.
+    CHECK_EQ(first_literal_location + lit_size, buf_size);
+    if (!IsAligned<sizeof(uint64_t)>(first_literal_location)) {
+      buffer_.Move(first_literal_location - sizeof(uint32_t), first_literal_location, lit_size);
+      // The 4 reserved bytes proved useless, reduce the buffer size.
+      buffer_.Resize(buf_size - sizeof(uint32_t));
+      // Reduce target addresses in literal and address loads by 4 bytes in order for correct
+      // offsets from PC to be generated.
+      for (auto& branch : branches_) {
+        uint32_t target = branch.GetTarget();
+        if (target >= first_literal_location) {
+          branch.Resolve(target - sizeof(uint32_t));
+        }
+      }
+      // If after this we ever call GetLabelLocation() to get the location of a 64-bit literal,
+      // we need to adjust the location of the literal's label as well.
+      for (Literal& literal : long_literals_) {
+        // Bound label's position is negative, hence incrementing it instead of decrementing.
+        literal.GetLabel()->position_ += sizeof(uint32_t);
+      }
+    }
+  }
+}
+
+// Note: make sure branch_info_[] and EmitBranch() are kept synchronized.
+const Riscv64Assembler::Branch::BranchInfo Riscv64Assembler::Branch::branch_info_[] = {
+  // short branches (can be promoted to long).
+  {  1, 0, 0, Riscv64Assembler::Branch::kOffset21, 0 },  // kUncondBranch
+  {  1, 0, 0, Riscv64Assembler::Branch::kOffset13, 0 },  // kCondBranch
+  {  1, 0, 0, Riscv64Assembler::Branch::kOffset21, 0 },  // kCall
+  // short branches (can't be promoted to long), forbidden/delay slots filled manually.
+  {  1, 0, 0, Riscv64Assembler::Branch::kOffset21, 0 },  // kBareUncondBranch
+  {  1, 0, 0, Riscv64Assembler::Branch::kOffset13, 0 },  // kBareCondBranch
+  {  1, 0, 0, Riscv64Assembler::Branch::kOffset21, 0 },  // kBareCall
+
+  // label.
+  {  2, 0, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLabel
+  // literals.
+  {  2, 0, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLiteral
+  {  2, 0, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLiteralUnsigned
+  {  2, 0, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLiteralLong
+
+  // Long branches.
+  {  2, 0, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLongUncondBranch
+  {  3, 1, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLongCondBranch
+  {  2, 0, 0, Riscv64Assembler::Branch::kOffset32, 0 },  // kLongCall
+};
+
+// Note: make sure branch_info_[] and EmitBranch() are kept synchronized.
+void Riscv64Assembler::EmitBranch(Riscv64Assembler::Branch* branch) {
+  CHECK(overwriting_);
+  overwrite_location_ = branch->GetLocation();
+  uint32_t offset = branch->GetOffset();
+  BranchCondition condition = branch->GetCondition();
+  GpuRegister lhs = branch->GetLeftRegister();
+  GpuRegister rhs = branch->GetRightRegister();
+  switch (branch->GetType()) {
+    // Short branches.
+    case Branch::kUncondBranch:
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Bc(offset);
+      break;
+    case Branch::kCondBranch:
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      EmitBcond(condition, lhs, rhs, offset);
+      break;
+    case Branch::kCall:
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Balc(offset);
+      break;
+    case Branch::kBareUncondBranch:
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Bc(offset);
+      break;
+    case Branch::kBareCondBranch:
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      EmitBcond(condition, lhs, rhs, offset);
+      break;
+    case Branch::kBareCall:
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Balc(offset);
+      break;
+
+    // label.
+    case Branch::kLabel:
+      offset += (offset & 0x800) << 1;  // Account for sign extension in daddiu.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Addi(lhs, AT, Low12Bits(offset));
+      break;
+    // literals.
+    case Branch::kLiteral:
+      offset += (offset & 0x800) << 1;  // Account for sign extension in lw.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Lw(lhs, AT, Low12Bits(offset));
+      break;
+    case Branch::kLiteralUnsigned:
+      offset += (offset & 0x800) << 1;  // Account for sign extension in lwu.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Lwu(lhs, AT, Low12Bits(offset));
+      break;
+    case Branch::kLiteralLong:
+      offset += (offset & 0x800) << 1;  // Account for sign extension in ld.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Ld(lhs, AT, Low12Bits(offset));
+      break;
+
+    // Long branches.
+    case Branch::kLongUncondBranch:
+      offset += (offset & 0x800) << 1;  // Account for sign extension in jic.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Jic(AT, Low12Bits(offset));
+      break;
+    case Branch::kLongCondBranch:
+      // Skip (2 + itself) instructions and continue if the Cond isn't taken.
+      EmitBcond(Branch::OppositeCondition(condition), lhs, rhs, 12);
+      offset += (offset & 0x800) << 1;  // Account for sign extension in jic.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Jic(AT, Low12Bits(offset));
+      break;
+    case Branch::kLongCall:
+      offset += (offset & 0x800) << 1;  // Account for sign extension in jialc.
+      CHECK_EQ(overwrite_location_, branch->GetOffsetLocation());
+      Auipc(AT, High20Bits(offset));
+      Jialc(AT, Low12Bits(offset));
+      break;
+  }
+  CHECK_EQ(overwrite_location_, branch->GetEndLocation());
+  CHECK_LT(branch->GetSize(), static_cast<uint32_t>(Branch::kMaxBranchSize));
+}
+
+void Riscv64Assembler::Bc(Riscv64Label* label, bool is_bare) {
+  Buncond(label, is_bare);
+}
+
+void Riscv64Assembler::Balc(Riscv64Label* label, bool is_bare) {
+  Call(label, is_bare);
+}
+
+void Riscv64Assembler::Jal(Riscv64Label* label, bool is_bare) {
+  Call(label, is_bare);
+}
+
+void Riscv64Assembler::Bltc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondLT, rs, rt);
+}
+
+void Riscv64Assembler::Bltzc(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondLTZ, rt);
+}
+
+void Riscv64Assembler::Bgtzc(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondGTZ, rt);
+}
+
+void Riscv64Assembler::Bgec(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondGE, rs, rt);
+}
+
+void Riscv64Assembler::Bgezc(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondGEZ, rt);
+}
+
+void Riscv64Assembler::Blezc(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondLEZ, rt);
+}
+
+void Riscv64Assembler::Bltuc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondLTU, rs, rt);
+}
+
+void Riscv64Assembler::Bgeuc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondGEU, rs, rt);
+}
+
+void Riscv64Assembler::Beqc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondEQ, rs, rt);
+}
+
+void Riscv64Assembler::Bnec(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondNE, rs, rt);
+}
+
+void Riscv64Assembler::Beqzc(GpuRegister rs, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondEQZ, rs);
+}
+
+void Riscv64Assembler::Bnezc(GpuRegister rs, Riscv64Label* label, bool is_bare) {
+  Bcond(label, is_bare, kCondNEZ, rs);
+}
+
+void Riscv64Assembler::Bltz(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondLTZ, rt);
+}
+
+void Riscv64Assembler::Bgtz(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondGTZ, rt);
+}
+
+void Riscv64Assembler::Bgez(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondGEZ, rt);
+}
+
+void Riscv64Assembler::Blez(GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondLEZ, rt);
+}
+
+void Riscv64Assembler::Beq(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondEQ, rs, rt);
+}
+
+void Riscv64Assembler::Bne(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondNE, rs, rt);
+}
+
+void Riscv64Assembler::Blt(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondLT, rs, rt);
+}
+
+void Riscv64Assembler::Bge(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondGE, rs, rt);
+}
+
+void Riscv64Assembler::Bltu(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondLTU, rs, rt);
+}
+
+void Riscv64Assembler::Bgeu(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare) {
+  CHECK(is_bare);
+  Bcond(label, is_bare, kCondGEU, rs, rt);
+}
+
+void Riscv64Assembler::Beqz(GpuRegister rs, Riscv64Label* label, bool is_bare) {
+//  CHECK(is_bare);
+  Bcond(label, is_bare, kCondEQZ, rs);
+}
+
+void Riscv64Assembler::Bnez(GpuRegister rs, Riscv64Label* label, bool is_bare) {
+//  CHECK(is_bare);
+  Bcond(label, is_bare, kCondNEZ, rs);
+}
+
+void Riscv64Assembler::AdjustBaseAndOffset(GpuRegister& base,
+                                          int32_t& offset,
+                                          bool is_doubleword) {
+  // This method is used to adjust the base register and offset pair
+  // for a load/store when the offset doesn't fit into int16_t.
+  // It is assumed that `base + offset` is sufficiently aligned for memory
+  // operands that are machine word in size or smaller. For doubleword-sized
+  // operands it's assumed that `base` is a multiple of 8, while `offset`
+  // may be a multiple of 4 (e.g. 4-byte-aligned long and double arguments
+  // and spilled variables on the stack accessed relative to the stack
+  // pointer register).
+  // We preserve the "alignment" of `offset` by adjusting it by a multiple of 8.
+  CHECK_NE(base, AT);  // Must not overwrite the register `base` while loading `offset`.
+
+  bool doubleword_aligned = IsAligned<kRiscv64DoublewordSize>(offset);
+  bool two_accesses = is_doubleword && !doubleword_aligned;
+
+  // IsInt<12> must be passed a signed value, hence the static cast below.
+  if (IsInt<12>(offset) &&
+      (!two_accesses || IsInt<12>(static_cast<int32_t>(offset + kRiscv64WordSize)))) {
+    // Nothing to do: `offset` (and, if needed, `offset + 4`) fits into int12_t.
+    return;
+  }
+
+  // Remember the "(mis)alignment" of `offset`, it will be checked at the end.
+  uint32_t misalignment = offset & (kRiscv64DoublewordSize - 1);
+
+  // First, see if `offset` can be represented as a sum of two 16-bit signed
+  // offsets. This can save an instruction.
+  // To simplify matters, only do this for a symmetric range of offsets from
+  // about -64KB to about +64KB, allowing further addition of 4 when accessing
+  // 64-bit variables with two 32-bit accesses.
+  constexpr int32_t kMinOffsetForSimpleAdjustment = 0x7f8;  // Max int12_t that's a multiple of 8.
+  constexpr int32_t kMaxOffsetForSimpleAdjustment = 2 * kMinOffsetForSimpleAdjustment;
+
+  if (0 <= offset && offset <= kMaxOffsetForSimpleAdjustment) {
+    Addi(AT, base, kMinOffsetForSimpleAdjustment);
+    offset -= kMinOffsetForSimpleAdjustment;
+  } else if (-kMaxOffsetForSimpleAdjustment <= offset && offset < 0) {
+    Addi(AT, base, -kMinOffsetForSimpleAdjustment);
+    offset += kMinOffsetForSimpleAdjustment;
+  } else {
+    // In more complex cases take advantage of the daui instruction, e.g.:
+    //    daui   AT, base, offset_high
+    //   [dahi   AT, 1]                       // When `offset` is close to +2GB.
+    //    lw     reg_lo, offset_low(AT)
+    //   [lw     reg_hi, (offset_low+4)(AT)]  // If misaligned 64-bit load.
+    // or when offset_low+4 overflows int16_t:
+    //    daui   AT, base, offset_high
+    //    daddiu AT, AT, 8
+    //    lw     reg_lo, (offset_low-8)(AT)
+    //    lw     reg_hi, (offset_low-4)(AT)
+    int32_t offset_low12 = 0xFFF & offset;
+    int32_t offset_high20 = offset >> 12;
+
+    if (offset_low12 & 0x800) {  // check int12_t sign bit
+      offset_high20 += 1;
+      offset_low12 |= 0xFFFFF000;  // sign extend offset_low12
+    }
+
+    Lui(AT, offset_high20);
+    Add(AT, base, AT);
+
+    if (two_accesses && !IsInt<12>(static_cast<int32_t>(offset_low12 + kRiscv64WordSize))) {
+      // Avoid overflow in the 12-bit offset of the load/store instruction when adding 4.
+      Addi(AT, AT, kRiscv64DoublewordSize);
+      offset_low12 -= kRiscv64DoublewordSize;
+    }
+
+    offset = offset_low12;
+  }
+  base = AT;
+
+  CHECK(IsInt<12>(offset));
+  if (two_accesses) {
+    CHECK(IsInt<12>(static_cast<int32_t>(offset + kRiscv64WordSize)));
+  }
+  CHECK_EQ(misalignment, offset & (kRiscv64DoublewordSize - 1));
+}
+
+void Riscv64Assembler::AdjustBaseOffsetAndElementSizeShift(GpuRegister& base,
+                                                          int32_t& offset,
+                                                          int& element_size_shift) {
+  // This method is used to adjust the base register, offset and element_size_shift
+  // for a vector load/store when the offset doesn't fit into allowed number of bits.
+  // MSA ld.df and st.df instructions take signed offsets as arguments, but maximum
+  // offset is dependant on the size of the data format df (10-bit offsets for ld.b,
+  // 11-bit for ld.h, 12-bit for ld.w and 13-bit for ld.d).
+  // If element_size_shift is non-negative at entry, it won't be changed, but offset
+  // will be checked for appropriate alignment. If negative at entry, it will be
+  // adjusted based on offset for maximum fit.
+  // It's assumed that `base` is a multiple of 8.
+
+  CHECK_NE(base, AT);  // Must not overwrite the register `base` while loading `offset`.
+
+  if (element_size_shift >= 0) {
+    CHECK_LE(element_size_shift, TIMES_8);
+    CHECK_GE(JAVASTYLE_CTZ(offset), element_size_shift);
+  } else if (IsAligned<kRiscv64DoublewordSize>(offset)) {
+    element_size_shift = TIMES_8;
+  } else if (IsAligned<kRiscv64WordSize>(offset)) {
+    element_size_shift = TIMES_4;
+  } else if (IsAligned<kRiscv64HalfwordSize>(offset)) {
+    element_size_shift = TIMES_2;
+  } else {
+    element_size_shift = TIMES_1;
+  }
+
+  const int low_len = 10 + element_size_shift;  // How many low bits of `offset` ld.df/st.df
+                                                // will take.
+  int16_t low = offset & ((1 << low_len) - 1);  // Isolate these bits.
+  low -= (low & (1 << (low_len - 1))) << 1;     // Sign-extend these bits.
+  if (low == offset) {
+    return;  // `offset` fits into ld.df/st.df.
+  }
+
+  // First, see if `offset` can be represented as a sum of two signed offsets.
+  // This can save an instruction.
+
+  // Max int16_t that's a multiple of element size.
+  const int32_t kMaxDeltaForSimpleAdjustment = 0x8000 - (1 << element_size_shift);
+  // Max ld.df/st.df offset that's a multiple of element size.
+  const int32_t kMaxLoadStoreOffset = 0x1ff << element_size_shift;
+  const int32_t kMaxOffsetForSimpleAdjustment = kMaxDeltaForSimpleAdjustment + kMaxLoadStoreOffset;
+
+  if (IsInt<16>(offset)) {
+    Daddiu(AT, base, offset);
+    offset = 0;
+  } else if (0 <= offset && offset <= kMaxOffsetForSimpleAdjustment) {
+    Daddiu(AT, base, kMaxDeltaForSimpleAdjustment);
+    offset -= kMaxDeltaForSimpleAdjustment;
+  } else if (-kMaxOffsetForSimpleAdjustment <= offset && offset < 0) {
+    Daddiu(AT, base, -kMaxDeltaForSimpleAdjustment);
+    offset += kMaxDeltaForSimpleAdjustment;
+  } else {
+    // Let's treat `offset` as 64-bit to simplify handling of sign
+    // extensions in the instructions that supply its smaller signed parts.
+    //
+    // 16-bit or smaller parts of `offset`:
+    // |63  top  48|47  hi  32|31  upper  16|15  mid  13-10|12-9  low  0|
+    //
+    // Instructions that supply each part as a signed integer addend:
+    // |dati       |dahi      |daui         |daddiu        |ld.df/st.df |
+    //
+    // `top` is always 0, so dati isn't used.
+    // `hi` is 1 when `offset` is close to +2GB and 0 otherwise.
+    uint64_t tmp = static_cast<uint64_t>(offset) - low;  // Exclude `low` from the rest of `offset`
+                                                         // (accounts for sign of `low`).
+    tmp += (tmp & (UINT64_C(1) << 15)) << 1;  // Account for sign extension in daddiu.
+    tmp += (tmp & (UINT64_C(1) << 31)) << 1;  // Account for sign extension in daui.
+    int16_t mid = Low16Bits(tmp);
+    int16_t upper = High16Bits(tmp);
+    int16_t hi = Low16Bits(High32Bits(tmp));
+    Daui(AT, base, upper);
+    if (hi != 0) {
+      CHECK_EQ(hi, 1);
+      Dahi(AT, hi);
+    }
+    if (mid != 0) {
+      Daddiu(AT, AT, mid);
+    }
+    offset = low;
+  }
+  base = AT;
+  CHECK_GE(JAVASTYLE_CTZ(offset), element_size_shift);
+  CHECK(IsInt<10>(offset >> element_size_shift));
+}
+
+void Riscv64Assembler::LoadFromOffset(LoadOperandType type,
+                                     GpuRegister reg,
+                                     GpuRegister base,
+                                     int32_t offset) {
+  LoadFromOffset<>(type, reg, base, offset);
+}
+
+void Riscv64Assembler::LoadFpuFromOffset(LoadOperandType type,
+                                        FpuRegister reg,
+                                        GpuRegister base,
+                                        int32_t offset) {
+  LoadFpuFromOffset<>(type, reg, base, offset);
+}
+
+void Riscv64Assembler::EmitLoad(ManagedRegister m_dst, GpuRegister src_register, int32_t src_offset,
+                               size_t size) {
+  Riscv64ManagedRegister dst = m_dst.AsRiscv64();
+  if (dst.IsNoRegister()) {
+    CHECK_EQ(0u, size) << dst;
+  } else if (dst.IsGpuRegister()) {
+    if (size == 4) {
+      LoadFromOffset(kLoadWord, dst.AsGpuRegister(), src_register, src_offset);
+    } else if (size == 8) {
+      CHECK_EQ(8u, size) << dst;
+      LoadFromOffset(kLoadDoubleword, dst.AsGpuRegister(), src_register, src_offset);
+    } else {
+      UNIMPLEMENTED(FATAL) << "We only support Load() of size 4 and 8";
+    }
+  } else if (dst.IsFpuRegister()) {
+    if (size == 4) {
+      CHECK_EQ(4u, size) << dst;
+      LoadFpuFromOffset(kLoadWord, dst.AsFpuRegister(), src_register, src_offset);
+    } else if (size == 8) {
+      CHECK_EQ(8u, size) << dst;
+      LoadFpuFromOffset(kLoadDoubleword, dst.AsFpuRegister(), src_register, src_offset);
+    } else {
+      UNIMPLEMENTED(FATAL) << "We only support Load() of size 4 and 8";
+    }
+  }
+}
+
+void Riscv64Assembler::StoreToOffset(StoreOperandType type,
+                                    GpuRegister reg,
+                                    GpuRegister base,
+                                    int32_t offset) {
+  StoreToOffset<>(type, reg, base, offset);
+}
+
+void Riscv64Assembler::StoreFpuToOffset(StoreOperandType type,
+                                       FpuRegister reg,
+                                       GpuRegister base,
+                                       int32_t offset) {
+  StoreFpuToOffset<>(type, reg, base, offset);
+}
+
+static dwarf::Reg DWARFReg(GpuRegister reg) {
+  return dwarf::Reg::Riscv64Core(static_cast<int>(reg));
+}
+
+constexpr size_t kFramePointerSize = 8;
+
+void Riscv64Assembler::BuildFrame(size_t frame_size,
+                                 ManagedRegister method_reg,
+                                 ArrayRef<const ManagedRegister> callee_save_regs,
+                                 const ManagedRegisterEntrySpills& entry_spills) {
+  CHECK_ALIGNED(frame_size, kStackAlignment);
+  DCHECK(!overwriting_);
+
+  // Increase frame to required size.
+  IncreaseFrameSize(frame_size);
+
+  // Push callee saves and return address
+  int stack_offset = frame_size - kFramePointerSize;
+  StoreToOffset(kStoreDoubleword, RA, SP, stack_offset);
+  cfi_.RelOffset(DWARFReg(RA), stack_offset);
+  for (int i = callee_save_regs.size() - 1; i >= 0; --i) {
+    stack_offset -= kFramePointerSize;
+    GpuRegister reg = callee_save_regs[i].AsRiscv64().AsGpuRegister();
+    StoreToOffset(kStoreDoubleword, reg, SP, stack_offset);
+    cfi_.RelOffset(DWARFReg(reg), stack_offset);
+  }
+
+  // Write out Method*.
+  StoreToOffset(kStoreDoubleword, method_reg.AsRiscv64().AsGpuRegister(), SP, 0);
+
+  // Write out entry spills.
+  int32_t offset = frame_size + kFramePointerSize;
+  for (const ManagedRegisterSpill& spill : entry_spills) {
+    Riscv64ManagedRegister reg = spill.AsRiscv64();
+    int32_t size = spill.getSize();
+    if (reg.IsNoRegister()) {
+      // only increment stack offset.
+      offset += size;
+    } else if (reg.IsFpuRegister()) {
+      StoreFpuToOffset((size == 4) ? kStoreWord : kStoreDoubleword,
+          reg.AsFpuRegister(), SP, offset);
+      offset += size;
+    } else if (reg.IsGpuRegister()) {
+      StoreToOffset((size == 4) ? kStoreWord : kStoreDoubleword,
+          reg.AsGpuRegister(), SP, offset);
+      offset += size;
+    }
+  }
+}
+
+void Riscv64Assembler::RemoveFrame(size_t frame_size,
+                                  ArrayRef<const ManagedRegister> callee_save_regs,
+                                  bool may_suspend ATTRIBUTE_UNUSED) {
+  CHECK_ALIGNED(frame_size, kStackAlignment);
+  DCHECK(!overwriting_);
+  cfi_.RememberState();
+
+  // Pop callee saves and return address
+  int stack_offset = frame_size - (callee_save_regs.size() * kFramePointerSize) - kFramePointerSize;
+  for (size_t i = 0; i < callee_save_regs.size(); ++i) {
+    GpuRegister reg = callee_save_regs[i].AsRiscv64().AsGpuRegister();
+    LoadFromOffset(kLoadDoubleword, reg, SP, stack_offset);
+    cfi_.Restore(DWARFReg(reg));
+    stack_offset += kFramePointerSize;
+  }
+  LoadFromOffset(kLoadDoubleword, RA, SP, stack_offset);
+  cfi_.Restore(DWARFReg(RA));
+
+  // Decrease frame to required size.
+  DecreaseFrameSize(frame_size);
+
+  // Then jump to the return address.
+  Jr(RA);
+  Nop();
+
+  // The CFI should be restored for any code that follows the exit block.
+  cfi_.RestoreState();
+  cfi_.DefCFAOffset(frame_size);
+}
+
+void Riscv64Assembler::IncreaseFrameSize(size_t adjust) {
+  CHECK_ALIGNED(adjust, kFramePointerSize);
+  DCHECK(!overwriting_);
+  Daddiu64(SP, SP, static_cast<int32_t>(-adjust));
+  cfi_.AdjustCFAOffset(adjust);
+}
+
+void Riscv64Assembler::DecreaseFrameSize(size_t adjust) {
+  CHECK_ALIGNED(adjust, kFramePointerSize);
+  DCHECK(!overwriting_);
+  Daddiu64(SP, SP, static_cast<int32_t>(adjust));
+  cfi_.AdjustCFAOffset(-adjust);
+}
+
+void Riscv64Assembler::Store(FrameOffset dest, ManagedRegister msrc, size_t size) {
+  Riscv64ManagedRegister src = msrc.AsRiscv64();
+  if (src.IsNoRegister()) {
+    CHECK_EQ(0u, size);
+  } else if (src.IsGpuRegister()) {
+    CHECK(size == 4 || size == 8) << size;
+    if (size == 8) {
+      StoreToOffset(kStoreDoubleword, src.AsGpuRegister(), SP, dest.Int32Value());
+    } else if (size == 4) {
+      StoreToOffset(kStoreWord, src.AsGpuRegister(), SP, dest.Int32Value());
+    } else {
+      UNIMPLEMENTED(FATAL) << "We only support Store() of size 4 and 8";
+    }
+  } else if (src.IsFpuRegister()) {
+    CHECK(size == 4 || size == 8) << size;
+    if (size == 8) {
+      StoreFpuToOffset(kStoreDoubleword, src.AsFpuRegister(), SP, dest.Int32Value());
+    } else if (size == 4) {
+      StoreFpuToOffset(kStoreWord, src.AsFpuRegister(), SP, dest.Int32Value());
+    } else {
+      UNIMPLEMENTED(FATAL) << "We only support Store() of size 4 and 8";
+    }
+  }
+}
+
+void Riscv64Assembler::StoreRef(FrameOffset dest, ManagedRegister msrc) {
+  Riscv64ManagedRegister src = msrc.AsRiscv64();
+  CHECK(src.IsGpuRegister());
+  StoreToOffset(kStoreWord, src.AsGpuRegister(), SP, dest.Int32Value());
+}
+
+void Riscv64Assembler::StoreRawPtr(FrameOffset dest, ManagedRegister msrc) {
+  Riscv64ManagedRegister src = msrc.AsRiscv64();
+  CHECK(src.IsGpuRegister());
+  StoreToOffset(kStoreDoubleword, src.AsGpuRegister(), SP, dest.Int32Value());
+}
+
+void Riscv64Assembler::StoreImmediateToFrame(FrameOffset dest, uint32_t imm,
+                                            ManagedRegister mscratch) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  LoadConst32(scratch.AsGpuRegister(), imm);
+  StoreToOffset(kStoreWord, scratch.AsGpuRegister(), SP, dest.Int32Value());
+}
+
+void Riscv64Assembler::StoreStackOffsetToThread(ThreadOffset64 thr_offs,
+                                               FrameOffset fr_offs,
+                                               ManagedRegister mscratch) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  Daddiu64(scratch.AsGpuRegister(), SP, fr_offs.Int32Value());
+  StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(), S1, thr_offs.Int32Value());
+}
+
+void Riscv64Assembler::StoreStackPointerToThread(ThreadOffset64 thr_offs) {
+  StoreToOffset(kStoreDoubleword, SP, S1, thr_offs.Int32Value());
+}
+
+void Riscv64Assembler::StoreSpanning(FrameOffset dest, ManagedRegister msrc,
+                                    FrameOffset in_off, ManagedRegister mscratch) {
+  Riscv64ManagedRegister src = msrc.AsRiscv64();
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  StoreToOffset(kStoreDoubleword, src.AsGpuRegister(), SP, dest.Int32Value());
+  LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(), SP, in_off.Int32Value());
+  StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(), SP, dest.Int32Value() + 8);
+}
+
+void Riscv64Assembler::Load(ManagedRegister mdest, FrameOffset src, size_t size) {
+  return EmitLoad(mdest, SP, src.Int32Value(), size);
+}
+
+void Riscv64Assembler::LoadFromThread(ManagedRegister mdest, ThreadOffset64 src, size_t size) {
+  return EmitLoad(mdest, S1, src.Int32Value(), size);
+}
+
+void Riscv64Assembler::LoadRef(ManagedRegister mdest, FrameOffset src) {
+  Riscv64ManagedRegister dest = mdest.AsRiscv64();
+  CHECK(dest.IsGpuRegister());
+  LoadFromOffset(kLoadUnsignedWord, dest.AsGpuRegister(), SP, src.Int32Value());
+}
+
+void Riscv64Assembler::LoadRef(ManagedRegister mdest, ManagedRegister base, MemberOffset offs,
+                              bool unpoison_reference) {
+  Riscv64ManagedRegister dest = mdest.AsRiscv64();
+  CHECK(dest.IsGpuRegister() && base.AsRiscv64().IsGpuRegister());
+  LoadFromOffset(kLoadUnsignedWord, dest.AsGpuRegister(),
+                 base.AsRiscv64().AsGpuRegister(), offs.Int32Value());
+  if (unpoison_reference) {
+    MaybeUnpoisonHeapReference(dest.AsGpuRegister());
+  }
+}
+
+void Riscv64Assembler::LoadRawPtr(ManagedRegister mdest, ManagedRegister base,
+                                 Offset offs) {
+  Riscv64ManagedRegister dest = mdest.AsRiscv64();
+  CHECK(dest.IsGpuRegister() && base.AsRiscv64().IsGpuRegister());
+  LoadFromOffset(kLoadDoubleword, dest.AsGpuRegister(),
+                 base.AsRiscv64().AsGpuRegister(), offs.Int32Value());
+}
+
+void Riscv64Assembler::LoadRawPtrFromThread(ManagedRegister mdest, ThreadOffset64 offs) {
+  Riscv64ManagedRegister dest = mdest.AsRiscv64();
+  CHECK(dest.IsGpuRegister());
+  LoadFromOffset(kLoadDoubleword, dest.AsGpuRegister(), S1, offs.Int32Value());
+}
+
+void Riscv64Assembler::SignExtend(ManagedRegister mreg ATTRIBUTE_UNUSED,
+                                 size_t size ATTRIBUTE_UNUSED) {
+  UNIMPLEMENTED(FATAL) << "No sign extension necessary for RISCV64";
+}
+
+void Riscv64Assembler::ZeroExtend(ManagedRegister mreg ATTRIBUTE_UNUSED,
+                                 size_t size ATTRIBUTE_UNUSED) {
+  UNIMPLEMENTED(FATAL) << "No zero extension necessary for RISCV64";
+}
+
+void Riscv64Assembler::Move(ManagedRegister mdest, ManagedRegister msrc, size_t size) {
+  Riscv64ManagedRegister dest = mdest.AsRiscv64();
+  Riscv64ManagedRegister src = msrc.AsRiscv64();
+  if (!dest.Equals(src)) {
+    if (dest.IsGpuRegister()) {
+      CHECK(src.IsGpuRegister()) << src;
+      Move(dest.AsGpuRegister(), src.AsGpuRegister());
+    } else if (dest.IsFpuRegister()) {
+      CHECK(src.IsFpuRegister()) << src;
+      if (size == 4) {
+        MovS(dest.AsFpuRegister(), src.AsFpuRegister());
+      } else if (size == 8) {
+        MovD(dest.AsFpuRegister(), src.AsFpuRegister());
+      } else {
+        UNIMPLEMENTED(FATAL) << "We only support Copy() of size 4 and 8";
+      }
+    }
+  }
+}
+
+void Riscv64Assembler::CopyRef(FrameOffset dest, FrameOffset src,
+                              ManagedRegister mscratch) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  LoadFromOffset(kLoadWord, scratch.AsGpuRegister(), SP, src.Int32Value());
+  StoreToOffset(kStoreWord, scratch.AsGpuRegister(), SP, dest.Int32Value());
+}
+
+void Riscv64Assembler::CopyRawPtrFromThread(FrameOffset fr_offs,
+                                           ThreadOffset64 thr_offs,
+                                           ManagedRegister mscratch) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(), S1, thr_offs.Int32Value());
+  StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(), SP, fr_offs.Int32Value());
+}
+
+void Riscv64Assembler::CopyRawPtrToThread(ThreadOffset64 thr_offs,
+                                         FrameOffset fr_offs,
+                                         ManagedRegister mscratch) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(),
+                 SP, fr_offs.Int32Value());
+  StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(),
+                S1, thr_offs.Int32Value());
+}
+
+void Riscv64Assembler::Copy(FrameOffset dest, FrameOffset src,
+                           ManagedRegister mscratch, size_t size) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  CHECK(size == 4 || size == 8) << size;
+  if (size == 4) {
+    LoadFromOffset(kLoadWord, scratch.AsGpuRegister(), SP, src.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(), SP, dest.Int32Value());
+  } else if (size == 8) {
+    LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(), SP, src.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(), SP, dest.Int32Value());
+  } else {
+    UNIMPLEMENTED(FATAL) << "We only support Copy() of size 4 and 8";
+  }
+}
+
+void Riscv64Assembler::Copy(FrameOffset dest, ManagedRegister src_base, Offset src_offset,
+                           ManagedRegister mscratch, size_t size) {
+  GpuRegister scratch = mscratch.AsRiscv64().AsGpuRegister();
+  CHECK(size == 4 || size == 8) << size;
+  if (size == 4) {
+    LoadFromOffset(kLoadWord, scratch, src_base.AsRiscv64().AsGpuRegister(),
+                   src_offset.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch, SP, dest.Int32Value());
+  } else if (size == 8) {
+    LoadFromOffset(kLoadDoubleword, scratch, src_base.AsRiscv64().AsGpuRegister(),
+                   src_offset.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch, SP, dest.Int32Value());
+  } else {
+    UNIMPLEMENTED(FATAL) << "We only support Copy() of size 4 and 8";
+  }
+}
+
+void Riscv64Assembler::Copy(ManagedRegister dest_base, Offset dest_offset, FrameOffset src,
+                           ManagedRegister mscratch, size_t size) {
+  GpuRegister scratch = mscratch.AsRiscv64().AsGpuRegister();
+  CHECK(size == 4 || size == 8) << size;
+  if (size == 4) {
+    LoadFromOffset(kLoadWord, scratch, SP, src.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch, dest_base.AsRiscv64().AsGpuRegister(),
+                  dest_offset.Int32Value());
+  } else if (size == 8) {
+    LoadFromOffset(kLoadDoubleword, scratch, SP, src.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch, dest_base.AsRiscv64().AsGpuRegister(),
+                  dest_offset.Int32Value());
+  } else {
+    UNIMPLEMENTED(FATAL) << "We only support Copy() of size 4 and 8";
+  }
+}
+
+void Riscv64Assembler::Copy(FrameOffset dest ATTRIBUTE_UNUSED,
+                           FrameOffset src_base ATTRIBUTE_UNUSED,
+                           Offset src_offset ATTRIBUTE_UNUSED,
+                           ManagedRegister mscratch ATTRIBUTE_UNUSED,
+                           size_t size ATTRIBUTE_UNUSED) {
+  UNIMPLEMENTED(FATAL) << "No RISCV64 implementation";
+}
+
+void Riscv64Assembler::Copy(ManagedRegister dest, Offset dest_offset,
+                           ManagedRegister src, Offset src_offset,
+                           ManagedRegister mscratch, size_t size) {
+  GpuRegister scratch = mscratch.AsRiscv64().AsGpuRegister();
+  CHECK(size == 4 || size == 8) << size;
+  if (size == 4) {
+    LoadFromOffset(kLoadWord, scratch, src.AsRiscv64().AsGpuRegister(), src_offset.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch, dest.AsRiscv64().AsGpuRegister(), dest_offset.Int32Value());
+  } else if (size == 8) {
+    LoadFromOffset(kLoadDoubleword, scratch, src.AsRiscv64().AsGpuRegister(),
+                   src_offset.Int32Value());
+    StoreToOffset(kStoreDoubleword, scratch, dest.AsRiscv64().AsGpuRegister(),
+                  dest_offset.Int32Value());
+  } else {
+    UNIMPLEMENTED(FATAL) << "We only support Copy() of size 4 and 8";
+  }
+}
+
+void Riscv64Assembler::Copy(FrameOffset dest ATTRIBUTE_UNUSED,
+                           Offset dest_offset ATTRIBUTE_UNUSED,
+                           FrameOffset src ATTRIBUTE_UNUSED,
+                           Offset src_offset ATTRIBUTE_UNUSED,
+                           ManagedRegister mscratch ATTRIBUTE_UNUSED,
+                           size_t size ATTRIBUTE_UNUSED) {
+  UNIMPLEMENTED(FATAL) << "No RISCV64 implementation";
+}
+
+void Riscv64Assembler::MemoryBarrier(ManagedRegister mreg ATTRIBUTE_UNUSED) {
+  // TODO: sync?
+  UNIMPLEMENTED(FATAL) << "No RISCV64 implementation";
+}
+
+void Riscv64Assembler::CreateHandleScopeEntry(ManagedRegister mout_reg,
+                                             FrameOffset handle_scope_offset,
+                                             ManagedRegister min_reg,
+                                             bool null_allowed) {
+  Riscv64ManagedRegister out_reg = mout_reg.AsRiscv64();
+  Riscv64ManagedRegister in_reg = min_reg.AsRiscv64();
+  CHECK(in_reg.IsNoRegister() || in_reg.IsGpuRegister()) << in_reg;
+  CHECK(out_reg.IsGpuRegister()) << out_reg;
+  if (null_allowed) {
+    Riscv64Label null_arg;
+    // Null values get a handle scope entry value of 0.  Otherwise, the handle scope entry is
+    // the address in the handle scope holding the reference.
+    // e.g. out_reg = (handle == 0) ? 0 : (SP+handle_offset)
+    if (in_reg.IsNoRegister()) {
+      LoadFromOffset(kLoadUnsignedWord, out_reg.AsGpuRegister(),
+                     SP, handle_scope_offset.Int32Value());
+      in_reg = out_reg;
+    }
+    if (!out_reg.Equals(in_reg)) {
+      LoadConst32(out_reg.AsGpuRegister(), 0);
+    }
+    Beqzc(in_reg.AsGpuRegister(), &null_arg);
+    Daddiu64(out_reg.AsGpuRegister(), SP, handle_scope_offset.Int32Value());
+    Bind(&null_arg);
+  } else {
+    Daddiu64(out_reg.AsGpuRegister(), SP, handle_scope_offset.Int32Value());
+  }
+}
+
+void Riscv64Assembler::CreateHandleScopeEntry(FrameOffset out_off,
+                                             FrameOffset handle_scope_offset,
+                                             ManagedRegister mscratch,
+                                             bool null_allowed) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  if (null_allowed) {
+    Riscv64Label null_arg;
+    LoadFromOffset(kLoadUnsignedWord, scratch.AsGpuRegister(), SP,
+                   handle_scope_offset.Int32Value());
+    // Null values get a handle scope entry value of 0.  Otherwise, the handle scope entry is
+    // the address in the handle scope holding the reference.
+    // e.g. scratch = (scratch == 0) ? 0 : (SP+handle_scope_offset)
+    Beqzc(scratch.AsGpuRegister(), &null_arg);
+    Daddiu64(scratch.AsGpuRegister(), SP, handle_scope_offset.Int32Value());
+    Bind(&null_arg);
+  } else {
+    Daddiu64(scratch.AsGpuRegister(), SP, handle_scope_offset.Int32Value());
+  }
+  StoreToOffset(kStoreDoubleword, scratch.AsGpuRegister(), SP, out_off.Int32Value());
+}
+
+// Given a handle scope entry, load the associated reference.
+void Riscv64Assembler::LoadReferenceFromHandleScope(ManagedRegister mout_reg,
+                                                   ManagedRegister min_reg) {
+  Riscv64ManagedRegister out_reg = mout_reg.AsRiscv64();
+  Riscv64ManagedRegister in_reg = min_reg.AsRiscv64();
+  CHECK(out_reg.IsGpuRegister()) << out_reg;
+  CHECK(in_reg.IsGpuRegister()) << in_reg;
+  Riscv64Label null_arg;
+  if (!out_reg.Equals(in_reg)) {
+    LoadConst32(out_reg.AsGpuRegister(), 0);
+  }
+  Beqzc(in_reg.AsGpuRegister(), &null_arg);
+  LoadFromOffset(kLoadDoubleword, out_reg.AsGpuRegister(),
+                 in_reg.AsGpuRegister(), 0);
+  Bind(&null_arg);
+}
+
+void Riscv64Assembler::VerifyObject(ManagedRegister src ATTRIBUTE_UNUSED,
+                                   bool could_be_null ATTRIBUTE_UNUSED) {
+  // TODO: not validating references
+}
+
+void Riscv64Assembler::VerifyObject(FrameOffset src ATTRIBUTE_UNUSED,
+                                   bool could_be_null ATTRIBUTE_UNUSED) {
+  // TODO: not validating references
+}
+
+void Riscv64Assembler::Call(ManagedRegister mbase, Offset offset, ManagedRegister mscratch) {
+  Riscv64ManagedRegister base = mbase.AsRiscv64();
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(base.IsGpuRegister()) << base;
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(),
+                 base.AsGpuRegister(), offset.Int32Value());
+  Jalr(scratch.AsGpuRegister());
+  Nop();
+  // TODO: place reference map on call
+}
+
+void Riscv64Assembler::Call(FrameOffset base, Offset offset, ManagedRegister mscratch) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  CHECK(scratch.IsGpuRegister()) << scratch;
+  // Call *(*(SP + base) + offset)
+  LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(),
+                 SP, base.Int32Value());
+  LoadFromOffset(kLoadDoubleword, scratch.AsGpuRegister(),
+                 scratch.AsGpuRegister(), offset.Int32Value());
+  Jalr(scratch.AsGpuRegister());
+  Nop();
+  // TODO: place reference map on call
+}
+
+void Riscv64Assembler::CallFromThread(ThreadOffset64 offset ATTRIBUTE_UNUSED,
+                                     ManagedRegister mscratch ATTRIBUTE_UNUSED) {
+  UNIMPLEMENTED(FATAL) << "No RISCV64 implementation";
+}
+
+void Riscv64Assembler::GetCurrentThread(ManagedRegister tr) {
+  Move(tr.AsRiscv64().AsGpuRegister(), S1);
+}
+
+void Riscv64Assembler::GetCurrentThread(FrameOffset offset,
+                                       ManagedRegister mscratch ATTRIBUTE_UNUSED) {
+  StoreToOffset(kStoreDoubleword, S1, SP, offset.Int32Value());
+}
+
+void Riscv64Assembler::ExceptionPoll(ManagedRegister mscratch, size_t stack_adjust) {
+  Riscv64ManagedRegister scratch = mscratch.AsRiscv64();
+  exception_blocks_.emplace_back(scratch, stack_adjust);
+  LoadFromOffset(kLoadDoubleword,
+                 scratch.AsGpuRegister(),
+                 S1,
+                 Thread::ExceptionOffset<kRiscv64PointerSize>().Int32Value());
+  Bnezc(scratch.AsGpuRegister(), exception_blocks_.back().Entry());
+}
+
+void Riscv64Assembler::EmitExceptionPoll(Riscv64ExceptionSlowPath* exception) {
+  Bind(exception->Entry());
+  if (exception->stack_adjust_ != 0) {  // Fix up the frame.
+    DecreaseFrameSize(exception->stack_adjust_);
+  }
+  // Pass exception object as argument.
+  // Don't care about preserving A0 as this call won't return.
+  CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
+  Move(A0, exception->scratch_.AsGpuRegister());
+  // Set up call to Thread::Current()->pDeliverException
+  LoadFromOffset(kLoadDoubleword,
+                 T9,
+                 S1,
+                 QUICK_ENTRYPOINT_OFFSET(kRiscv64PointerSize, pDeliverException).Int32Value());
+  Jr(T9);
+  Nop();
+
+  // Call never returns
+  Break();
+}
+
+// TODO dvt porting...
+void Riscv64Assembler::EmitI5(uint16_t funct7, uint16_t imm5, GpuRegister rs1, int funct3, GpuRegister rd, int opcode) {
+  uint32_t encoding = static_cast<uint32_t>(funct7) << 25 |
+                      (static_cast<uint32_t>(imm5) & 0x1F) << 20 |
+                      static_cast<uint32_t>(rs1) << 15 |
+                      static_cast<uint32_t>(funct3) << 12 |
+                      static_cast<uint32_t>(rd) << 7 |
+                      opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitI6(uint16_t funct6, uint16_t imm6, GpuRegister rs1, int funct3, GpuRegister rd, int opcode) {
+  uint32_t encoding = static_cast<uint32_t>(funct6) << 25 |
+                      (static_cast<uint32_t>(imm6) & 0x3F) << 20 |
+                      static_cast<uint32_t>(rs1) << 15 |
+                      static_cast<uint32_t>(funct3) << 12 |
+                      static_cast<uint32_t>(rd) << 7 |
+                      opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitB(uint16_t imm, GpuRegister rs2, GpuRegister rs1, int funct3, int opcode) {
+  CHECK(IsUint<13>(imm)) << imm;
+  uint32_t encoding = (static_cast<uint32_t>(imm)&0x1000) >> 12 << 31 |
+                      (static_cast<uint32_t>(imm)&0x07E0) >> 5 << 25 |
+                      static_cast<uint32_t>(rs2) << 20 |
+                      static_cast<uint32_t>(rs1) << 15 |
+                      static_cast<uint32_t>(funct3) << 12 |
+                      (static_cast<uint32_t>(imm)&0x1E) >> 1 << 8 |
+                      (static_cast<uint32_t>(imm)&0x0800) >> 11 << 7|
+                      opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitU(uint32_t imm, GpuRegister rd, int opcode) {
+  uint32_t encoding = static_cast<uint32_t>(imm) << 12 |
+                      static_cast<uint32_t>(rd) << 7 |
+                      opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::EmitJ(uint32_t imm20, GpuRegister rd, int opcode) {
+  CHECK(IsUint<21>(imm20)) << imm20;
+  // Riscv JAL: J-Imm = (offset x 2), encode (imm20>>1) into instruction.
+  uint32_t encoding = (static_cast<uint32_t>(imm20)&0x100000) >>20<< 31 |
+                      (static_cast<uint32_t>(imm20)&0x07FE) >> 1 << 21 |
+                      (static_cast<uint32_t>(imm20)&0x800) >> 11 << 20 |
+                      (static_cast<uint32_t>(imm20)&0xFF000) >> 12 << 12 |
+                      static_cast<uint32_t>(rd) << 7 |
+                      opcode;
+  Emit(encoding);
+}
+
+void Riscv64Assembler::Add(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x0, rs2, rs1, 0x0, rd, 0x33);
+}
+
+void Riscv64Assembler::Sub(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x20, rs2, rs1, 0x0, rd, 0x33);
+}
+
+void Riscv64Assembler::Sll(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x01, rd, 0x33);
+}
+
+void Riscv64Assembler::Slt(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x02, rd, 0x33);
+}
+
+void Riscv64Assembler::Sltu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x03, rd, 0x33);
+}
+
+void Riscv64Assembler::Xor(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x04, rd, 0x33);
+}
+
+void Riscv64Assembler::Srl(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x05, rd, 0x33);
+}
+
+void Riscv64Assembler::Sra(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x20, rs2, rs1, 0x05, rd, 0x33);
+}
+
+void Riscv64Assembler::Or(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x06, rd, 0x33);
+}
+
+void Riscv64Assembler::And(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x00, rs2, rs1, 0x07, rd, 0x33);
+}
+
+// RV32I-I
+void Riscv64Assembler::Jalr(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x0, rd, 0x67);
+}
+
+void Riscv64Assembler::Lb(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x0, rd, 0x03);
+}
+
+void Riscv64Assembler::Lh(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x1, rd, 0x03);
+}
+
+void Riscv64Assembler::Lw(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x2, rd, 0x03);
+}
+
+void Riscv64Assembler::Lbu(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x4, rd, 0x03);
+}
+
+void Riscv64Assembler::Lhu(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x5, rd, 0x03);
+}
+
+void Riscv64Assembler::Addi(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x0, rd, 0x13);
+}
+
+void Riscv64Assembler::Slti(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x2, rd, 0x13);
+}
+
+void Riscv64Assembler::Sltiu(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x3, rd, 0x13);
+}
+
+void Riscv64Assembler::Xori(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x4, rd, 0x13);
+}
+
+void Riscv64Assembler::Ori(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x6, rd, 0x13);
+}
+
+void Riscv64Assembler::Andi(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x7, rd, 0x13);
+}
+
+void Riscv64Assembler::Slli(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI6(0x0, offset, rs1, 0x1, rd, 0x13);
+}
+
+void Riscv64Assembler::Srli(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI6(0x0, offset, rs1, 0x5, rd, 0x13);
+}
+
+void Riscv64Assembler::Srai(GpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI6(0x20, offset, rs1, 0x5, rd, 0x13);
+}
+
+void Riscv64Assembler::Fence(uint8_t pred, uint8_t succ) {
+  EmitI(0x0 << 8 | pred << 4 | succ, 0x0, 0x0, 0x0, 0xf);
+}
+
+void Riscv64Assembler::FenceI() {
+  EmitI(0x0 << 6| 0x0 << 4 | 0x0, 0x0, 0x1, 0x0, 0xf);
+}
+
+void Riscv64Assembler::Ecall() {
+  EmitI(0x0, 0x0, 0x0, 0x0, 0x73);
+}
+
+void Riscv64Assembler::Ebreak() {
+  EmitI(0x1, 0x0, 0x0, 0x0, 0x73);
+}
+
+void Riscv64Assembler::Csrrw(GpuRegister rd, GpuRegister rs1, uint16_t csr) {
+  EmitI(csr, rs1, 0x1, rd, 0x73);
+}
+
+void Riscv64Assembler::Csrrs(GpuRegister rd, GpuRegister rs1, uint16_t csr) {
+  EmitI(csr, rs1, 0x2, rd, 0x73);
+}
+
+void Riscv64Assembler::Csrrc(GpuRegister rd, GpuRegister rs1, uint16_t csr) {
+  EmitI(csr, rs1, 0x3, rd, 0x73);
+}
+
+void Riscv64Assembler::Csrrwi(GpuRegister rd, uint16_t csr, uint8_t zimm) {
+  EmitI(csr, zimm, 0x5, rd, 0x73);
+}
+
+void Riscv64Assembler::Csrrsi(GpuRegister rd, uint16_t csr, uint8_t zimm) {
+  EmitI(csr, zimm, 0x6, rd, 0x73);
+}
+
+void Riscv64Assembler::Csrrci(GpuRegister rd, uint16_t csr, uint8_t zimm) {
+  EmitI(csr, zimm, 0x7, rd, 0x73);
+}
+
+// RV32I-S
+void Riscv64Assembler::Sb(GpuRegister rs2, GpuRegister rs1, uint16_t offset) {
+  EmitS(offset, rs2, rs1, 0x0, 0x23);
+}
+
+void Riscv64Assembler::Sh(GpuRegister rs2, GpuRegister rs1, uint16_t offset) {
+  EmitS(offset, rs2, rs1, 0x1, 0x23);
+}
+
+void Riscv64Assembler::Sw(GpuRegister rs2, GpuRegister rs1, uint16_t offset) {
+  EmitS(offset, rs2, rs1, 0x2, 0x23);
+}
+
+// RV32I-B
+void Riscv64Assembler::Beq(GpuRegister rs1, GpuRegister rs2, uint16_t offset) {
+  EmitB(offset, rs2, rs1, 0x0, 0x63);
+}
+
+void Riscv64Assembler::Bne(GpuRegister rs1, GpuRegister rs2, uint16_t offset) {
+  EmitB(offset, rs2, rs1, 0x1, 0x63);
+}
+
+void Riscv64Assembler::Blt(GpuRegister rs1, GpuRegister rs2, uint16_t offset) {
+  EmitB(offset, rs2, rs1, 0x4, 0x63);
+}
+
+void Riscv64Assembler::Bge(GpuRegister rs1, GpuRegister rs2, uint16_t offset) {
+  EmitB(offset, rs2, rs1, 0x5, 0x63);
+}
+
+void Riscv64Assembler::Bltu(GpuRegister rs1, GpuRegister rs2, uint16_t offset) {
+  EmitB(offset, rs2, rs1, 0x6, 0x63);
+}
+
+void Riscv64Assembler::Bgeu(GpuRegister rs1, GpuRegister rs2, uint16_t offset) {
+  EmitB(offset, rs2, rs1, 0x7, 0x63);
+}
+
+// RV32I-U
+void Riscv64Assembler::Lui(GpuRegister rd, uint32_t imm20) {
+  EmitU(imm20, rd, 0x37);
+}
+
+void Riscv64Assembler::Auipc(GpuRegister rd, uint32_t imm20) {
+  EmitU(imm20, rd, 0x17);
+}
+
+// RV32I-J
+void Riscv64Assembler::Jal(GpuRegister rd, uint32_t imm20) {
+  EmitJ(imm20, rd, 0x6F);
+}
+
+// RV64I-R
+void Riscv64Assembler::Addw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x0, rs2, rs1, 0x0, rd, 0x3b);
+}
+
+void Riscv64Assembler::Subw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x20, rs2, rs1, 0x0, rd, 0x3b);
+}
+
+void Riscv64Assembler::Sllw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x0, rs2, rs1, 0x1, rd, 0x3b);
+}
+
+void Riscv64Assembler::Srlw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x0, rs2, rs1, 0x5, rd, 0x3b);
+}
+
+void Riscv64Assembler::Sraw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x20, rs2, rs1, 0x5, rd, 0x3b);
+}
+
+// RV64I-I
+void Riscv64Assembler::Lwu(GpuRegister rd, GpuRegister rs1, uint16_t imm12) {
+  EmitI(imm12, rs1, 0x6, rd, 0x3);
+}
+
+void Riscv64Assembler::Ld(GpuRegister rd, GpuRegister rs1, uint16_t imm12) {
+  EmitI(imm12, rs1, 0x3, rd, 0x3);
+}
+
+void Riscv64Assembler::Addiw(GpuRegister rd, GpuRegister rs1, int16_t imm12) {
+  CHECK(imm12 >= -2048) << imm12;
+  CHECK(imm12 < 4096) << imm12;
+  EmitI(imm12, rs1, 0x0, rd, 0x1b);
+}
+
+void Riscv64Assembler::Slliw(GpuRegister rd, GpuRegister rs1, int16_t shamt) {
+  CHECK(static_cast<uint16_t>(shamt) < 32) << shamt;
+  EmitR(0x0, shamt, rs1, 0x1, rd, 0x1b);  // borrow EmitR to implement this function
+}
+
+void Riscv64Assembler::Srliw(GpuRegister rd, GpuRegister rs1, int16_t shamt) {
+  CHECK(static_cast<uint16_t>(shamt) < 32) << shamt;
+  EmitR(0x0, shamt, rs1, 0x5, rd, 0x1b);  // borrow EmitR to implement this function
+}
+
+void Riscv64Assembler::Sraiw(GpuRegister rd, GpuRegister rs1, int16_t shamt) {
+  CHECK(static_cast<uint16_t>(shamt) < 32) << shamt;
+  EmitR(0x20, shamt, rs1, 0x5, rd, 0x1b);  // borrow EmitR to implement this function
+}
+
+// RV64I-S
+void Riscv64Assembler::Sd(GpuRegister rs2, GpuRegister rs1, uint16_t imm12) {
+  EmitS(imm12, rs2, rs1, 0x3, 0x23);
+}
+
+// RV32M-R
+void Riscv64Assembler::Mul(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x0, rd, 0x33);
+}
+
+void Riscv64Assembler::Mulh(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x1, rd, 0x33);
+}
+
+void Riscv64Assembler::Mulhsu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x2, rd, 0x33);
+}
+
+void Riscv64Assembler::Mulhu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x3, rd, 0x33);
+}
+
+void Riscv64Assembler::Div(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x4, rd, 0x33);
+}
+
+void Riscv64Assembler::Divu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x5, rd, 0x33);
+}
+
+void Riscv64Assembler::Rem(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x6, rd, 0x33);
+}
+
+void Riscv64Assembler::Remu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x7, rd, 0x33);
+}
+
+// RV64M-R
+void Riscv64Assembler::Mulw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x0, rd, 0x3b);
+}
+
+void Riscv64Assembler::Divw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x4, rd, 0x3b);
+}
+
+void Riscv64Assembler::Divuw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x5, rd, 0x3b);
+}
+
+void Riscv64Assembler::Remw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x6, rd, 0x3b);
+}
+
+void Riscv64Assembler::Remuw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, 0x7, rd, 0x3b);
+}
+
+// RV32A
+void Riscv64Assembler::LrW(GpuRegister rd, GpuRegister rs1) {
+  EmitR4(0x2, 0x0, 0x0, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::ScW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x3, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoSwapW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x1, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoAddW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x0, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoXorW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x4, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoAndW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0xc, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoOrW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x8, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMinW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x10, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMaxW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x14, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMinuW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x18, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMaxuW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x1c, 0x0, rs2, rs1, 0x2, rd, 0x2f);
+}
+
+// RV64A
+void Riscv64Assembler::LrD(GpuRegister rd, GpuRegister rs1) {
+  EmitR4(0x2, 0x0, 0x0, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::ScD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x3, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoSwapD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x1, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoAddD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x0, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoXorD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x4, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoAndD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0xc, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoOrD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x8, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMinD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x10, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMaxD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x14, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMinuD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x18, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+void Riscv64Assembler::AmoMaxuD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1) {
+  EmitR4(0x1c, 0x0, rs2, rs1, 0x3, rd, 0x2f);
+}
+
+// RV32F-I
+void Riscv64Assembler::FLw(FpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x2, rd, 0x7);
+}
+
+// RV32F-S
+void Riscv64Assembler::FSw(FpuRegister rs2, GpuRegister rs1, uint16_t offset) {
+  EmitS(offset, rs2, rs1, 0x2, 0x27);
+}
+
+// RV32F-R
+void Riscv64Assembler::FMAddS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x0, rs2, rs1, FRM, rd, 0x43);
+}
+
+void Riscv64Assembler::FMSubS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x0, rs2, rs1, FRM, rd, 0x47);
+}
+
+void Riscv64Assembler::FNMSubS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x0, rs2, rs1, FRM, rd, 0x4b);
+}
+
+void Riscv64Assembler::FNMAddS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x0, rs2, rs1, FRM, rd, 0x4f);
+}
+
+void Riscv64Assembler::FAddS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x0, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FSubS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x4, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FMulS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x8, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FDivS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0xc, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FSqrtS(FpuRegister rd, FpuRegister rs1) {
+  EmitR(0x2c, 0x0, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FSgnjS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x10, rs2, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FSgnjnS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x10, rs2, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FSgnjxS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x10, rs2, rs1, 0x2, rd, 0x53);
+}
+
+void Riscv64Assembler::FMinS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x14, rs2, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FMaxS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x14, rs2, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtWS(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm) {
+  EmitR(0x60, 0x0, rs1, frm, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtWuS(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x60, 0x1, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FMvXW(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x70, 0x0, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FEqS(GpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x50, rs2, rs1, 0x2, rd, 0x53);
+}
+
+void Riscv64Assembler::FLtS(GpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x50, rs2, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FLeS(GpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x50, rs2, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FClassS(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x70, 0x0, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtSW(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x68, 0x0, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtSWu(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x68, 0x1, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FMvWX(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x78, 0x0, rs1, 0x0, rd, 0x53);
+}
+
+// RV64F-R
+void Riscv64Assembler::FCvtLS(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm) {
+  EmitR(0x60, 0x2, rs1, frm, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtLuS(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x60, 0x3, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtSL(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x68, 0x2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtSLu(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x68, 0x3, rs1, FRM, rd, 0x53);
+}
+
+// RV32D-I
+void Riscv64Assembler::FLd(FpuRegister rd, GpuRegister rs1, uint16_t offset) {
+  EmitI(offset, rs1, 0x3, rd, 0x7);
+}
+
+// RV32D-S
+void Riscv64Assembler::FSd(FpuRegister rs2, GpuRegister rs1, uint16_t offset) {
+  EmitS(offset, rs2, rs1, 0x3, 0x27);
+}
+
+// RV32D-R
+void Riscv64Assembler::FMAddD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x1, rs2, rs1, FRM, rd, 0x43);
+}
+
+void Riscv64Assembler::FMSubD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x1, rs2, rs1, FRM, rd, 0x47);
+}
+
+void Riscv64Assembler::FNMSubD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x1, rs2, rs1, FRM, rd, 0x4b);
+}
+
+void Riscv64Assembler::FNMAddD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3) {
+  EmitR4(rs3, 0x1, rs2, rs1, FRM, rd, 0x4f);
+}
+
+void Riscv64Assembler::FAddD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x1, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FSubD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x5, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FMulD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x9, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FDivD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0xd, rs2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FSqrtD(FpuRegister rd, FpuRegister rs1) {
+  EmitR(0x2d, 0x0, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FSgnjD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x11, rs2, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FSgnjnD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x11, rs2, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FSgnjxD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x11, rs2, rs1, 0x2, rd, 0x53);
+}
+
+void Riscv64Assembler::FMinD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x15, rs2, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FMaxD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x15, rs2, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtSD(FpuRegister rd, FpuRegister rs1) {
+  EmitR(0x20, 0x1, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtDS(FpuRegister rd, FpuRegister rs1) {
+  // EmitR(0x21, 0x0, rs1, FRM, rd, 0x53);
+  EmitR(0x21, 0x0, rs1, 0x0, rd, 0x53);  // TODO need confirm:FRM=0x0 gived by gcc compiler, why?
+}
+
+void Riscv64Assembler::FEqD(GpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x51, rs2, rs1, 0x2, rd, 0x53);
+}
+
+void Riscv64Assembler::FLtD(GpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x51, rs2, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FLeD(GpuRegister rd, FpuRegister rs1, FpuRegister rs2) {
+  EmitR(0x51, rs2, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FClassD(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x71, 0x0, rs1, 0x1, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtWD(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm) {
+  EmitR(0x61, 0x0, rs1, frm, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtWuD(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x61, 0x1, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtDW(FpuRegister rd, GpuRegister rs1) {
+  // EmitR(0x69, 0x0, rs1, FRM, rd, 0x53);// TODO confirm FRM='000' or '111'
+  EmitR(0x69, 0x0, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtDWu(FpuRegister rd, GpuRegister rs1) {
+  // EmitR(0x69, 0x1, rs1, FRM, rd, 0x53);// TODO confirm FRM='000' or '111'
+  EmitR(0x69, 0x1, rs1, 0x0, rd, 0x53);
+}
+
+// RV64D-R
+void Riscv64Assembler::FCvtLD(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm) {
+  EmitR(0x61, 0x2, rs1, frm, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtLuD(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x61, 0x3, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FMvXD(GpuRegister rd, FpuRegister rs1) {
+  EmitR(0x71, 0x0, rs1, 0x0, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtDL(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x69, 0x2, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FCvtDLu(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x69, 0x3, rs1, FRM, rd, 0x53);
+}
+
+void Riscv64Assembler::FMvDX(FpuRegister rd, GpuRegister rs1) {
+  EmitR(0x79, 0x0, rs1, 0x0, rd, 0x53);
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/utils/riscv64/assembler_riscv64.h b/compiler/utils/riscv64/assembler_riscv64.h
new file mode 100644
index 0000000000..26deda9747
--- /dev/null
+++ b/compiler/utils/riscv64/assembler_riscv64.h
@@ -0,0 +1,1928 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_UTILS_RISCV64_ASSEMBLER_RISCV64_H_
+#define ART_COMPILER_UTILS_RISCV64_ASSEMBLER_RISCV64_H_
+
+#include <deque>
+#include <utility>
+#include <vector>
+
+#include "arch/riscv64/instruction_set_features_riscv64.h"
+#include "base/arena_containers.h"
+#include "base/enums.h"
+#include "base/globals.h"
+#include "base/macros.h"
+#include "base/stl_util_identity.h"
+#include "constants_riscv64.h"
+#include "heap_poisoning.h"
+#include "managed_register_riscv64.h"
+#include "offsets.h"
+#include "utils/assembler.h"
+#include "utils/jni_macro_assembler.h"
+#include "utils/label.h"
+
+namespace art {
+namespace riscv64 {
+
+enum FPRoundingMode {
+  kFPRoundingModeRNE             = 0x0,  // Round to Nearest, ties to Even
+  kFPRoundingModeRTZ             = 0x1,  // Round towards Zero
+  kFPRoundingModeRDN             = 0x2,  // Round Down (towards )
+  kFPRoundingModeRUP             = 0x3,  // Round Up (towards +)
+  kFPRoundingModeRMM             = 0x4,  // Round to Nearest, ties to Max Magnitude
+  kFPRoundingModeDYN             = 0x7,  // Dynamic rounding mode
+};
+
+#define FRM                     (kFPRoundingModeDYN)
+
+enum LoadConst64Path {
+  kLoadConst64PathZero           = 0x0,
+  kLoadConst64PathOri            = 0x1,
+  kLoadConst64PathDaddiu         = 0x2,
+  kLoadConst64PathLui            = 0x4,
+  kLoadConst64PathLuiOri         = 0x8,
+  kLoadConst64PathOriDahi        = 0x10,
+  kLoadConst64PathOriDati        = 0x20,
+  kLoadConst64PathLuiDahi        = 0x40,
+  kLoadConst64PathLuiDati        = 0x80,
+  kLoadConst64PathDaddiuDsrlX    = 0x100,
+  kLoadConst64PathOriDsllX       = 0x200,
+  kLoadConst64PathDaddiuDsllX    = 0x400,
+  kLoadConst64PathLuiOriDsllX    = 0x800,
+  kLoadConst64PathOriDsllXOri    = 0x1000,
+  kLoadConst64PathDaddiuDsllXOri = 0x2000,
+  kLoadConst64PathDaddiuDahi     = 0x4000,
+  kLoadConst64PathDaddiuDati     = 0x8000,
+  kLoadConst64PathDinsu1         = 0x10000,
+  kLoadConst64PathDinsu2         = 0x20000,
+  kLoadConst64PathCatchAll       = 0x40000,
+  kLoadConst64PathAllPaths       = 0x7ffff,
+};
+
+inline uint16_t Low12Bits(uint32_t value) {
+  return static_cast<uint16_t>(value & 0xFFF);
+}
+
+inline uint32_t High20Bits(uint32_t value) {
+  return static_cast<uint32_t>(value >> 12);
+}
+
+template <typename Asm>
+void TemplateLoadConst32(Asm* a, GpuRegister rd, int32_t value) {
+  if (IsUint<16>(value)) {
+    // Use OR with (unsigned) immediate to encode 16b unsigned int.
+    a->Ori(rd, ZERO, value);
+  } else if (IsInt<16>(value)) {
+    // Use ADD with (signed) immediate to encode 16b signed int.
+    a->Addiw(rd, ZERO, value);
+  } else {
+    // Set 16 most significant bits of value. The "lui" instruction
+    // also clears the 16 least significant bits to zero.
+    a->Lui(rd, value >> 16);
+    if (value & 0xFFFF) {
+      // If the 16 least significant bits are non-zero, set them
+      // here.
+      a->Ori(rd, rd, value);
+    }
+  }
+}
+
+static inline int InstrCountForLoadReplicatedConst32(int64_t value) {
+  int32_t x = Low32Bits(value);
+  int32_t y = High32Bits(value);
+
+  if (x == y) {
+    return (IsUint<16>(x) || IsInt<16>(x) || ((x & 0xFFFF) == 0)) ? 2 : 3;
+  }
+
+  return INT_MAX;
+}
+
+template <typename Asm, typename Rtype, typename Vtype>
+void TemplateLoadConst64(Asm* a, Rtype rd, Vtype value) {
+  int bit31 = (value & UINT64_C(0x80000000)) != 0;
+  int rep32_count = InstrCountForLoadReplicatedConst32(value);
+
+  // Loads with 1 instruction.
+  if (IsUint<16>(value)) {
+    // 64-bit value can be loaded as an unsigned 16-bit number.
+    a->RecordLoadConst64Path(kLoadConst64PathOri);
+    a->Ori(rd, ZERO, value);
+  } else if (IsInt<16>(value)) {
+    // 64-bit value can be loaded as an signed 16-bit number.
+    a->RecordLoadConst64Path(kLoadConst64PathDaddiu);
+    a->Daddiu(rd, ZERO, value);
+  } else if ((value & 0xFFFF) == 0 && IsInt<16>(value >> 16)) {
+    // 64-bit value can be loaded as an signed 32-bit number which has all
+    // of its 16 least significant bits set to zero.
+    a->RecordLoadConst64Path(kLoadConst64PathLui);
+    a->Lui(rd, value >> 16);
+  } else if (IsInt<32>(value)) {
+    // Loads with 2 instructions.
+    // 64-bit value can be loaded as an signed 32-bit number which has some
+    // or all of its 16 least significant bits set to one.
+    a->RecordLoadConst64Path(kLoadConst64PathLuiOri);
+    a->Lui(rd, value >> 16);
+    a->Ori(rd, rd, value);
+  } else if ((value & 0xFFFF0000) == 0 && IsInt<16>(value >> 32)) {
+    // 64-bit value which consists of an unsigned 16-bit value in its
+    // least significant 32-bits, and a signed 16-bit value in its
+    // most significant 32-bits.
+    a->RecordLoadConst64Path(kLoadConst64PathOriDahi);
+    a->Ori(rd, ZERO, value);
+    a->Dahi(rd, value >> 32);
+  } else if ((value & UINT64_C(0xFFFFFFFF0000)) == 0) {
+    // 64-bit value which consists of an unsigned 16-bit value in its
+    // least significant 48-bits, and a signed 16-bit value in its
+    // most significant 16-bits.
+    a->RecordLoadConst64Path(kLoadConst64PathOriDati);
+    a->Ori(rd, ZERO, value);
+    a->Dati(rd, value >> 48);
+  } else if ((value & 0xFFFF) == 0 &&
+             (-32768 - bit31) <= (value >> 32) && (value >> 32) <= (32767 - bit31)) {
+    // 16 LSBs (Least Significant Bits) all set to zero.
+    // 48 MSBs (Most Significant Bits) hold a signed 32-bit value.
+    a->RecordLoadConst64Path(kLoadConst64PathLuiDahi);
+    a->Lui(rd, value >> 16);
+    a->Dahi(rd, (value >> 32) + bit31);
+  } else if ((value & 0xFFFF) == 0 && ((value >> 31) & 0x1FFFF) == ((0x20000 - bit31) & 0x1FFFF)) {
+    // 16 LSBs all set to zero.
+    // 48 MSBs hold a signed value which can't be represented by signed
+    // 32-bit number, and the middle 16 bits are all zero, or all one.
+    a->RecordLoadConst64Path(kLoadConst64PathLuiDati);
+    a->Lui(rd, value >> 16);
+    a->Dati(rd, (value >> 48) + bit31);
+  } else if (IsInt<16>(static_cast<int32_t>(value)) &&
+             (-32768 - bit31) <= (value >> 32) && (value >> 32) <= (32767 - bit31)) {
+    // 32 LSBs contain an unsigned 16-bit number.
+    // 32 MSBs contain a signed 16-bit number.
+    a->RecordLoadConst64Path(kLoadConst64PathDaddiuDahi);
+    a->Daddiu(rd, ZERO, value);
+    a->Dahi(rd, (value >> 32) + bit31);
+  } else if (IsInt<16>(static_cast<int32_t>(value)) &&
+             ((value >> 31) & 0x1FFFF) == ((0x20000 - bit31) & 0x1FFFF)) {
+    // 48 LSBs contain an unsigned 16-bit number.
+    // 16 MSBs contain a signed 16-bit number.
+    a->RecordLoadConst64Path(kLoadConst64PathDaddiuDati);
+    a->Daddiu(rd, ZERO, value);
+    a->Dati(rd, (value >> 48) + bit31);
+  } else if (IsPowerOfTwo(value + UINT64_C(1))) {
+    // 64-bit values which have their "n" MSBs set to one, and their
+    // "64-n" LSBs set to zero. "n" must meet the restrictions 0 < n < 64.
+    int shift_cnt = 64 - CTZ(value + UINT64_C(1));
+    a->RecordLoadConst64Path(kLoadConst64PathDaddiuDsrlX);
+    a->Daddiu(rd, ZERO, -1);
+    if (shift_cnt < 32) {
+      a->Dsrl(rd, rd, shift_cnt);
+    } else {
+      a->Dsrl32(rd, rd, shift_cnt & 31);
+    }
+  } else {
+    int shift_cnt = CTZ(value);
+    int64_t tmp = value >> shift_cnt;
+    a->RecordLoadConst64Path(kLoadConst64PathOriDsllX);
+    if (IsUint<16>(tmp)) {
+      // Value can be computed by loading a 16-bit unsigned value, and
+      // then shifting left.
+      a->Ori(rd, ZERO, tmp);
+      if (shift_cnt < 32) {
+        a->Dsll(rd, rd, shift_cnt);
+      } else {
+        a->Dsll32(rd, rd, shift_cnt & 31);
+      }
+    } else if (IsInt<16>(tmp)) {
+      // Value can be computed by loading a 16-bit signed value, and
+      // then shifting left.
+      a->RecordLoadConst64Path(kLoadConst64PathDaddiuDsllX);
+      a->Daddiu(rd, ZERO, tmp);
+      if (shift_cnt < 32) {
+        a->Dsll(rd, rd, shift_cnt);
+      } else {
+        a->Dsll32(rd, rd, shift_cnt & 31);
+      }
+    } else if (rep32_count < 3) {
+      // Value being loaded has 32 LSBs equal to the 32 MSBs, and the
+      // value loaded into the 32 LSBs can be loaded with a single
+      // MIPS instruction.
+      a->LoadConst32(rd, value);
+      a->Dinsu(rd, rd, 32, 32);
+      a->RecordLoadConst64Path(kLoadConst64PathDinsu1);
+    } else if (IsInt<32>(tmp)) {
+      // Loads with 3 instructions.
+      // Value can be computed by loading a 32-bit signed value, and
+      // then shifting left.
+      a->RecordLoadConst64Path(kLoadConst64PathLuiOriDsllX);
+      a->Lui(rd, tmp >> 16);
+      a->Ori(rd, rd, tmp);
+      if (shift_cnt < 32) {
+        a->Dsll(rd, rd, shift_cnt);
+      } else {
+        a->Dsll32(rd, rd, shift_cnt & 31);
+      }
+    } else {
+      shift_cnt = 16 + CTZ(value >> 16);
+      tmp = value >> shift_cnt;
+      if (IsUint<16>(tmp)) {
+        // Value can be computed by loading a 16-bit unsigned value,
+        // shifting left, and "or"ing in another 16-bit unsigned value.
+        a->RecordLoadConst64Path(kLoadConst64PathOriDsllXOri);
+        a->Ori(rd, ZERO, tmp);
+        if (shift_cnt < 32) {
+          a->Dsll(rd, rd, shift_cnt);
+        } else {
+          a->Dsll32(rd, rd, shift_cnt & 31);
+        }
+        a->Ori(rd, rd, value);
+      } else if (IsInt<16>(tmp)) {
+        // Value can be computed by loading a 16-bit signed value,
+        // shifting left, and "or"ing in a 16-bit unsigned value.
+        a->RecordLoadConst64Path(kLoadConst64PathDaddiuDsllXOri);
+        a->Daddiu(rd, ZERO, tmp);
+        if (shift_cnt < 32) {
+          a->Dsll(rd, rd, shift_cnt);
+        } else {
+          a->Dsll32(rd, rd, shift_cnt & 31);
+        }
+        a->Ori(rd, rd, value);
+      } else if (rep32_count < 4) {
+        // Value being loaded has 32 LSBs equal to the 32 MSBs, and the
+        // value in the 32 LSBs requires 2 MIPS instructions to load.
+        a->LoadConst32(rd, value);
+        a->Dinsu(rd, rd, 32, 32);
+        a->RecordLoadConst64Path(kLoadConst64PathDinsu2);
+      } else {
+        // Loads with 3-4 instructions.
+        // Catch-all case to get any other 64-bit values which aren't
+        // handled by special cases above.
+        uint64_t tmp2 = value;
+        a->RecordLoadConst64Path(kLoadConst64PathCatchAll);
+        a->LoadConst32(rd, value);
+        if (bit31) {
+          tmp2 += UINT64_C(0x100000000);
+        }
+        if (((tmp2 >> 32) & 0xFFFF) != 0) {
+          a->Dahi(rd, tmp2 >> 32);
+        }
+        if (tmp2 & UINT64_C(0x800000000000)) {
+          tmp2 += UINT64_C(0x1000000000000);
+        }
+        if ((tmp2 >> 48) != 0) {
+          a->Dati(rd, tmp2 >> 48);
+        }
+      }
+    }
+  }
+}
+
+static constexpr size_t kRiscv64HalfwordSize = 2;
+static constexpr size_t kRiscv64WordSize = 4;
+static constexpr size_t kRiscv64DoublewordSize = 8;
+
+enum LoadOperandType {
+  kLoadSignedByte,
+  kLoadUnsignedByte,
+  kLoadSignedHalfword,
+  kLoadUnsignedHalfword,
+  kLoadWord,
+  kLoadUnsignedWord,
+  kLoadDoubleword,
+  kLoadQuadword
+};
+
+enum StoreOperandType {
+  kStoreByte,
+  kStoreHalfword,
+  kStoreWord,
+  kStoreDoubleword,
+  kStoreQuadword
+};
+
+// Used to test the values returned by ClassS/ClassD.
+enum FPClassMaskType {
+  kSignalingNaN      = 0x001,
+  kQuietNaN          = 0x002,
+  kNegativeInfinity  = 0x004,
+  kNegativeNormal    = 0x008,
+  kNegativeSubnormal = 0x010,
+  kNegativeZero      = 0x020,
+  kPositiveInfinity  = 0x040,
+  kPositiveNormal    = 0x080,
+  kPositiveSubnormal = 0x100,
+  kPositiveZero      = 0x200,
+};
+
+class Riscv64Label : public Label {
+ public:
+  Riscv64Label() : prev_branch_id_plus_one_(0) {}
+
+  Riscv64Label(Riscv64Label&& src)
+      : Label(std::move(src)), prev_branch_id_plus_one_(src.prev_branch_id_plus_one_) {}
+
+ private:
+  uint32_t prev_branch_id_plus_one_;  // To get distance from preceding branch, if any.
+
+  friend class Riscv64Assembler;
+  DISALLOW_COPY_AND_ASSIGN(Riscv64Label);
+};
+
+// Assembler literal is a value embedded in code, retrieved using a PC-relative load.
+class Literal {
+ public:
+  static constexpr size_t kMaxSize = 8;
+
+  Literal(uint32_t size, const uint8_t* data)
+      : label_(), size_(size) {
+    DCHECK_LE(size, Literal::kMaxSize);
+    memcpy(data_, data, size);
+  }
+
+  template <typename T>
+  T GetValue() const {
+    DCHECK_EQ(size_, sizeof(T));
+    T value;
+    memcpy(&value, data_, sizeof(T));
+    return value;
+  }
+
+  uint32_t GetSize() const {
+    return size_;
+  }
+
+  const uint8_t* GetData() const {
+    return data_;
+  }
+
+  Riscv64Label* GetLabel() {
+    return &label_;
+  }
+
+  const Riscv64Label* GetLabel() const {
+    return &label_;
+  }
+
+ private:
+  Riscv64Label label_;
+  const uint32_t size_;
+  uint8_t data_[kMaxSize];
+
+  DISALLOW_COPY_AND_ASSIGN(Literal);
+};
+
+// Jump table: table of labels emitted after the code and before the literals. Similar to literals.
+class JumpTable {
+ public:
+  explicit JumpTable(std::vector<Riscv64Label*>&& labels)
+      : label_(), labels_(std::move(labels)) {
+  }
+
+  size_t GetSize() const {
+    return labels_.size() * sizeof(uint32_t);
+  }
+
+  const std::vector<Riscv64Label*>& GetData() const {
+    return labels_;
+  }
+
+  Riscv64Label* GetLabel() {
+    return &label_;
+  }
+
+  const Riscv64Label* GetLabel() const {
+    return &label_;
+  }
+
+ private:
+  Riscv64Label label_;
+  std::vector<Riscv64Label*> labels_;
+
+  DISALLOW_COPY_AND_ASSIGN(JumpTable);
+};
+
+// Slowpath entered when Thread::Current()->_exception is non-null.
+class Riscv64ExceptionSlowPath {
+ public:
+  explicit Riscv64ExceptionSlowPath(Riscv64ManagedRegister scratch, size_t stack_adjust)
+      : scratch_(scratch), stack_adjust_(stack_adjust) {}
+
+  Riscv64ExceptionSlowPath(Riscv64ExceptionSlowPath&& src)
+      : scratch_(src.scratch_),
+        stack_adjust_(src.stack_adjust_),
+        exception_entry_(std::move(src.exception_entry_)) {}
+
+ private:
+  Riscv64Label* Entry() { return &exception_entry_; }
+  const Riscv64ManagedRegister scratch_;
+  const size_t stack_adjust_;
+  Riscv64Label exception_entry_;
+
+  friend class Riscv64Assembler;
+  DISALLOW_COPY_AND_ASSIGN(Riscv64ExceptionSlowPath);
+};
+
+class Riscv64Assembler final : public Assembler, public JNIMacroAssembler<PointerSize::k64> {
+ public:
+  using JNIBase = JNIMacroAssembler<PointerSize::k64>;
+
+  explicit Riscv64Assembler(ArenaAllocator* allocator,
+                           const Riscv64InstructionSetFeatures* instruction_set_features = nullptr)
+      : Assembler(allocator),
+        overwriting_(false),
+        overwrite_location_(0),
+        literals_(allocator->Adapter(kArenaAllocAssembler)),
+        long_literals_(allocator->Adapter(kArenaAllocAssembler)),
+        jump_tables_(allocator->Adapter(kArenaAllocAssembler)),
+        last_position_adjustment_(0),
+        last_old_position_(0),
+        last_branch_id_(0),
+        has_msa_(false) {
+    (void) instruction_set_features;
+    cfi().DelayEmittingAdvancePCs();
+  }
+
+  virtual ~Riscv64Assembler() {
+    for (auto& branch : branches_) {
+      CHECK(branch.IsResolved());
+    }
+  }
+
+  size_t CodeSize() const override { return Assembler::CodeSize(); }
+  DebugFrameOpCodeWriterForAssembler& cfi() override { return Assembler::cfi(); }
+
+  // Emit Machine Instructions.
+  void Addu(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void Addiu(GpuRegister rt, GpuRegister rs, int16_t imm16);
+  void Daddu(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+  void Daddiu(GpuRegister rt, GpuRegister rs, int16_t imm16);  // RISCV64
+  void Subu(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void Dsubu(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+
+  void MulR6(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void MuhR6(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void DivR6(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void ModR6(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void DivuR6(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void ModuR6(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void Dmul(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+  void Dmuh(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+  void Ddiv(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+  void Dmod(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+  void Ddivu(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+  void Dmodu(GpuRegister rd, GpuRegister rs, GpuRegister rt);  // RISCV64
+
+  void Bitswap(GpuRegister rd, GpuRegister rt);
+  void Dbitswap(GpuRegister rd, GpuRegister rt);  // RISCV64
+  void Seb(GpuRegister rd, GpuRegister rt);
+  void Seh(GpuRegister rd, GpuRegister rt);
+  void Dsbh(GpuRegister rd, GpuRegister rt);  // RISCV64
+  void Dshd(GpuRegister rd, GpuRegister rt);  // RISCV64
+  void Dext(GpuRegister rs, GpuRegister rt, int pos, int size);  // RISCV64
+  void Ins(GpuRegister rt, GpuRegister rs, int pos, int size);
+  void Dins(GpuRegister rt, GpuRegister rs, int pos, int size);  // RISCV64
+  void Dinsm(GpuRegister rt, GpuRegister rs, int pos, int size);  // RISCV64
+  void Dinsu(GpuRegister rt, GpuRegister rs, int pos, int size);  // RISCV64
+  void DblIns(GpuRegister rt, GpuRegister rs, int pos, int size);  // RISCV64
+  void Lsa(GpuRegister rd, GpuRegister rs, GpuRegister rt, int saPlusOne);
+  void Dlsa(GpuRegister rd, GpuRegister rs, GpuRegister rt, int saPlusOne);  // RISCV64
+  void Wsbh(GpuRegister rd, GpuRegister rt);
+  void Sc(GpuRegister rt, GpuRegister base, int16_t imm9 = 0);
+  void Scd(GpuRegister rt, GpuRegister base, int16_t imm9 = 0);  // RISCV64
+  void Ll(GpuRegister rt, GpuRegister base, int16_t imm9 = 0);
+  void Lld(GpuRegister rt, GpuRegister base, int16_t imm9 = 0);  // RISCV64
+
+  void Sll(GpuRegister rd, GpuRegister rt, int shamt);
+  void Srl(GpuRegister rd, GpuRegister rt, int shamt);
+  void Rotr(GpuRegister rd, GpuRegister rt, int shamt);
+  void Sra(GpuRegister rd, GpuRegister rt, int shamt);
+  void Sllv(GpuRegister rd, GpuRegister rt, GpuRegister rs);
+  void Srlv(GpuRegister rd, GpuRegister rt, GpuRegister rs);
+  void Rotrv(GpuRegister rd, GpuRegister rt, GpuRegister rs);
+  void Srav(GpuRegister rd, GpuRegister rt, GpuRegister rs);
+  void Dsll(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Dsrl(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Drotr(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Dsra(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Dsll32(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Dsrl32(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Drotr32(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Dsra32(GpuRegister rd, GpuRegister rt, int shamt);  // RISCV64
+  void Dsllv(GpuRegister rd, GpuRegister rt, GpuRegister rs);  // RISCV64
+  void Dsrlv(GpuRegister rd, GpuRegister rt, GpuRegister rs);  // RISCV64
+  void Drotrv(GpuRegister rd, GpuRegister rt, GpuRegister rs);  // RISCV64
+  void Dsrav(GpuRegister rd, GpuRegister rt, GpuRegister rs);  // RISCV64
+
+  void Lwpc(GpuRegister rs, uint32_t imm19);
+  void Lwupc(GpuRegister rs, uint32_t imm19);  // RISCV64
+  void Ldpc(GpuRegister rs, uint32_t imm18);  // RISCV64
+  /*
+  void Lui(GpuRegister rt, uint16_t imm16);
+  */
+  void Aui(GpuRegister rt, GpuRegister rs, uint16_t imm16);
+  void Daui(GpuRegister rt, GpuRegister rs, uint16_t imm16);  // RISCV64
+  void Dahi(GpuRegister rs, uint16_t imm16);  // RISCV64
+  void Dati(GpuRegister rs, uint16_t imm16);  // RISCV64
+  void Sync(uint32_t stype);
+
+  void Seleqz(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void Selnez(GpuRegister rd, GpuRegister rs, GpuRegister rt);
+  void Clz(GpuRegister rd, GpuRegister rs);
+  void Clo(GpuRegister rd, GpuRegister rs);
+  void Dclz(GpuRegister rd, GpuRegister rs);  // RISCV64
+  void Dclo(GpuRegister rd, GpuRegister rs);  // RISCV64
+
+  void Jalr(GpuRegister rd, GpuRegister rs);
+  void Jalr(GpuRegister rs);
+  void Jr(GpuRegister rs);
+  void Addiupc(GpuRegister rs, uint32_t imm19);
+  void Bc(uint32_t imm20);
+  void Balc(uint32_t imm20);
+  void Jic(GpuRegister rt, uint16_t imm16);
+  void Jialc(GpuRegister rt, uint16_t imm16);
+  void Bltc(GpuRegister rs, GpuRegister rt, uint16_t imm12);
+  void Bltzc(GpuRegister rt, uint16_t imm12);
+  void Bgtzc(GpuRegister rt, uint16_t imm12);
+  void Bgec(GpuRegister rs, GpuRegister rt, uint16_t imm12);
+  void Bgezc(GpuRegister rt, uint16_t imm12);
+  void Blezc(GpuRegister rt, uint16_t imm12);
+  void Bltuc(GpuRegister rs, GpuRegister rt, uint16_t imm12);
+  void Bgeuc(GpuRegister rs, GpuRegister rt, uint16_t imm12);
+  void Beqc(GpuRegister rs, GpuRegister rt, uint16_t imm12);
+  void Bnec(GpuRegister rs, GpuRegister rt, uint16_t imm12);
+  void Beqzc(GpuRegister rs, uint32_t imm12);
+  void Bnezc(GpuRegister rs, uint32_t imm12);
+
+  void AddS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SubS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void MulS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void DivS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void AddD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SubD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void MulD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void DivD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SqrtS(FpuRegister fd, FpuRegister fs);
+  void SqrtD(FpuRegister fd, FpuRegister fs);
+  void AbsS(FpuRegister fd, FpuRegister fs);
+  void AbsD(FpuRegister fd, FpuRegister fs);
+  void MovS(FpuRegister fd, FpuRegister fs);
+  void MovD(FpuRegister fd, FpuRegister fs);
+  void NegS(FpuRegister fd, FpuRegister fs);
+  void NegD(FpuRegister fd, FpuRegister fs);
+  void RoundLS(FpuRegister fd, FpuRegister fs);
+  void RoundLD(FpuRegister fd, FpuRegister fs);
+  void RoundWS(FpuRegister fd, FpuRegister fs);
+  void RoundWD(FpuRegister fd, FpuRegister fs);
+  void TruncLS(GpuRegister rd, FpuRegister fs);
+  void TruncLD(GpuRegister rd, FpuRegister fs);
+  void TruncWS(GpuRegister rd, FpuRegister fs);
+  void TruncWD(GpuRegister rd, FpuRegister fs);
+  void CeilLS(FpuRegister fd, FpuRegister fs);
+  void CeilLD(FpuRegister fd, FpuRegister fs);
+  void CeilWS(FpuRegister fd, FpuRegister fs);
+  void CeilWD(FpuRegister fd, FpuRegister fs);
+  void FloorLS(FpuRegister fd, FpuRegister fs);
+  void FloorLD(FpuRegister fd, FpuRegister fs);
+  void FloorWS(FpuRegister fd, FpuRegister fs);
+  void FloorWD(FpuRegister fd, FpuRegister fs);
+  void SelS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SelD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SeleqzS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SeleqzD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SelnezS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void SelnezD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void RintS(FpuRegister fd, FpuRegister fs);
+  void RintD(FpuRegister fd, FpuRegister fs);
+  void ClassS(FpuRegister fd, FpuRegister fs);
+  void ClassD(FpuRegister fd, FpuRegister fs);
+  void MinS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void MinD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void MaxS(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void MaxD(FpuRegister fd, FpuRegister fs, FpuRegister ft);
+  void CmpUnS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpEqS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUeqS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpLtS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUltS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpLeS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUleS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpOrS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUneS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpNeS(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUnD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpEqD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUeqD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpLtD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUltD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpLeD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUleD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpOrD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpUneD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+  void CmpNeD(GpuRegister rd, FpuRegister fs, FpuRegister ft);
+
+  void Cvtsw(FpuRegister fd, FpuRegister fs);
+  void Cvtdw(FpuRegister fd, FpuRegister fs);
+  void Cvtsd(FpuRegister fd, FpuRegister fs);
+  void Cvtds(FpuRegister fd, FpuRegister fs);
+  void Cvtsl(FpuRegister fd, FpuRegister fs);
+  void Cvtdl(FpuRegister fd, FpuRegister fs);
+
+  void Mfc1(GpuRegister rt, FpuRegister fs);
+  void Mfhc1(GpuRegister rt, FpuRegister fs);
+  void Mtc1(GpuRegister rt, FpuRegister fs);
+  void Mthc1(GpuRegister rt, FpuRegister fs);
+  void Dmfc1(GpuRegister rt, FpuRegister fs);  // RISCV64
+  void Dmtc1(GpuRegister rt, FpuRegister fs);  // RISCV64
+  void Lwc1(FpuRegister ft, GpuRegister rs, uint16_t imm12);
+  void Ldc1(FpuRegister ft, GpuRegister rs, uint16_t imm12);
+  void Swc1(FpuRegister ft, GpuRegister rs, uint16_t imm12);
+  void Sdc1(FpuRegister ft, GpuRegister rs, uint16_t imm12);
+
+  void Break();
+  void Nop();
+  void Move(GpuRegister rd, GpuRegister rs);
+  void Clear(GpuRegister rd);
+  void Not(GpuRegister rd, GpuRegister rs);
+
+  // MSA instructions.
+  void AndV(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void OrV(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void NorV(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void XorV(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  void AddvB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void AddvH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void AddvW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void AddvD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SubvB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SubvH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SubvW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SubvD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Asub_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MulvB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MulvH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MulvW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MulvD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Div_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Mod_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Add_aB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Add_aH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Add_aW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Add_aD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Ave_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Aver_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Max_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_sB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_uB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Min_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  void FaddW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FaddD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FsubW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FsubD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmulW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmulD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FdivW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FdivD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmaxW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmaxD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FminW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FminD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  void Ffint_sW(VectorRegister wd, VectorRegister ws);
+  void Ffint_sD(VectorRegister wd, VectorRegister ws);
+  void Ftint_sW(VectorRegister wd, VectorRegister ws);
+  void Ftint_sD(VectorRegister wd, VectorRegister ws);
+
+  void SllB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SllH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SllW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SllD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SraB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SraH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SraW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SraD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SrlB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SrlH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SrlW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void SrlD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  // Immediate shift instructions, where shamtN denotes shift amount (must be between 0 and 2^N-1).
+  void SlliB(VectorRegister wd, VectorRegister ws, int shamt3);
+  void SlliH(VectorRegister wd, VectorRegister ws, int shamt4);
+  void SlliW(VectorRegister wd, VectorRegister ws, int shamt5);
+  void SlliD(VectorRegister wd, VectorRegister ws, int shamt6);
+  void SraiB(VectorRegister wd, VectorRegister ws, int shamt3);
+  void SraiH(VectorRegister wd, VectorRegister ws, int shamt4);
+  void SraiW(VectorRegister wd, VectorRegister ws, int shamt5);
+  void SraiD(VectorRegister wd, VectorRegister ws, int shamt6);
+  void SrliB(VectorRegister wd, VectorRegister ws, int shamt3);
+  void SrliH(VectorRegister wd, VectorRegister ws, int shamt4);
+  void SrliW(VectorRegister wd, VectorRegister ws, int shamt5);
+  void SrliD(VectorRegister wd, VectorRegister ws, int shamt6);
+
+  void MoveV(VectorRegister wd, VectorRegister ws);
+  void SplatiB(VectorRegister wd, VectorRegister ws, int n4);
+  void SplatiH(VectorRegister wd, VectorRegister ws, int n3);
+  void SplatiW(VectorRegister wd, VectorRegister ws, int n2);
+  void SplatiD(VectorRegister wd, VectorRegister ws, int n1);
+  void Copy_sB(GpuRegister rd, VectorRegister ws, int n4);
+  void Copy_sH(GpuRegister rd, VectorRegister ws, int n3);
+  void Copy_sW(GpuRegister rd, VectorRegister ws, int n2);
+  void Copy_sD(GpuRegister rd, VectorRegister ws, int n1);
+  void Copy_uB(GpuRegister rd, VectorRegister ws, int n4);
+  void Copy_uH(GpuRegister rd, VectorRegister ws, int n3);
+  void Copy_uW(GpuRegister rd, VectorRegister ws, int n2);
+  void InsertB(VectorRegister wd, GpuRegister rs, int n4);
+  void InsertH(VectorRegister wd, GpuRegister rs, int n3);
+  void InsertW(VectorRegister wd, GpuRegister rs, int n2);
+  void InsertD(VectorRegister wd, GpuRegister rs, int n1);
+  void FillB(VectorRegister wd, GpuRegister rs);
+  void FillH(VectorRegister wd, GpuRegister rs);
+  void FillW(VectorRegister wd, GpuRegister rs);
+  void FillD(VectorRegister wd, GpuRegister rs);
+
+  void LdiB(VectorRegister wd, int imm8);
+  void LdiH(VectorRegister wd, int imm10);
+  void LdiW(VectorRegister wd, int imm10);
+  void LdiD(VectorRegister wd, int imm10);
+  void LdB(VectorRegister wd, GpuRegister rs, int offset);
+  void LdH(VectorRegister wd, GpuRegister rs, int offset);
+  void LdW(VectorRegister wd, GpuRegister rs, int offset);
+  void LdD(VectorRegister wd, GpuRegister rs, int offset);
+  void StB(VectorRegister wd, GpuRegister rs, int offset);
+  void StH(VectorRegister wd, GpuRegister rs, int offset);
+  void StW(VectorRegister wd, GpuRegister rs, int offset);
+  void StD(VectorRegister wd, GpuRegister rs, int offset);
+
+  void IlvlB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvlH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvlW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvlD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvrB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvrH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvrW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvrD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvevB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvevH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvevW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvevD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvodB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvodH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvodW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void IlvodD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  void MaddvB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MaddvH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MaddvW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MaddvD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MsubvB(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MsubvH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MsubvW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void MsubvD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmaddW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmaddD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmsubW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void FmsubD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  void Hadd_sH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Hadd_sW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Hadd_sD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Hadd_uH(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Hadd_uW(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+  void Hadd_uD(VectorRegister wd, VectorRegister ws, VectorRegister wt);
+
+  void PcntB(VectorRegister wd, VectorRegister ws);
+  void PcntH(VectorRegister wd, VectorRegister ws);
+  void PcntW(VectorRegister wd, VectorRegister ws);
+  void PcntD(VectorRegister wd, VectorRegister ws);
+
+
+  // TODO dvt porting...
+  /////////////////////////////// RV32I ///////////////////////////////
+  // RV32I-R
+  void Add(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Sub(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Sll(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Slt(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Sltu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Xor(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Srl(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Sra(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Or(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void And(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  // RV32I-I
+  void Jalr(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Lb(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Lh(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Lw(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Lbu(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Lhu(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Addi(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Slti(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Sltiu(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Xori(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Ori(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Andi(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Slli(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Srli(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Srai(GpuRegister rd, GpuRegister rs1, uint16_t offset);
+  void Fence(uint8_t pred, uint8_t succ);
+  void FenceI();
+  void Ecall();
+  void Ebreak();
+  void Csrrw(GpuRegister rd, GpuRegister rs1, uint16_t csr);  // the order is not consitence with instruction
+  void Csrrs(GpuRegister rd, GpuRegister rs1, uint16_t csr);  // the order is not consitence with instruction
+  void Csrrc(GpuRegister rd, GpuRegister rs1, uint16_t csr);  // the order is not consitence with instruction
+  void Csrrwi(GpuRegister rd, uint16_t csr, uint8_t zimm /*imm5*/);
+  void Csrrsi(GpuRegister rd, uint16_t csr, uint8_t zimm /*imm5*/);
+  void Csrrci(GpuRegister rd, uint16_t csr, uint8_t zimm /*imm5*/);
+
+  // RV32I-S
+  void Sb(GpuRegister rs2, GpuRegister rs1, uint16_t offset);
+  void Sh(GpuRegister rs2, GpuRegister rs1, uint16_t offset);
+  void Sw(GpuRegister rs2, GpuRegister rs1, uint16_t offset);
+  // RV32I-B
+  void Beq(GpuRegister rs1, GpuRegister rs2, uint16_t offset);
+  void Bne(GpuRegister rs1, GpuRegister rs2, uint16_t offset);
+  void Blt(GpuRegister rs1, GpuRegister rs2, uint16_t offset);
+  void Bge(GpuRegister rs1, GpuRegister rs2, uint16_t offset);
+  void Bltu(GpuRegister rs1, GpuRegister rs2, uint16_t offset);
+  void Bgeu(GpuRegister rs1, GpuRegister rs2, uint16_t offset);
+  // RV32I-U
+  void Lui(GpuRegister rd, uint32_t imm20);
+  void Auipc(GpuRegister rd, uint32_t imm20);
+  // RV32I-J
+  void Jal(GpuRegister rd, uint32_t imm20);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV64I ///////////////////////////////
+  // RV64I-R
+  void Addw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Subw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Sllw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Srlw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Sraw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  // RV64I-I
+  void Lwu(GpuRegister rd, GpuRegister rs1, uint16_t imm12);
+  void Ld(GpuRegister rd, GpuRegister rs1, uint16_t imm12);
+  // void Slli(GpuRegister rd, GpuRegister rs1, uint16_t shamt); // Duplicated with RV32I, why?
+  // void Srli(GpuRegister rd, GpuRegister rs1, uint16_t shamt); // Duplicated with RV32I, why?
+  // void Srai(GpuRegister rd, GpuRegister rs1, uint16_t shamt); // Duplicated with RV32I, why?
+  void Addiw(GpuRegister rd, GpuRegister rs1, int16_t imm12);
+  void Slliw(GpuRegister rd, GpuRegister rs1, int16_t shamt);
+  void Srliw(GpuRegister rd, GpuRegister rs1, int16_t shamt);
+  void Sraiw(GpuRegister rd, GpuRegister rs1, int16_t shamt);
+  // RV64I-S
+  void Sd(GpuRegister rs2, GpuRegister rs1, uint16_t imm12);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV32M ///////////////////////////////
+  // RV32M-R
+  void Mul(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Mulh(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Mulhsu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Mulhu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Div(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Divu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Rem(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Remu(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV32A ///////////////////////////////
+  // TODO confirm aq=? rl=?
+  void LrW(GpuRegister rd, GpuRegister rs1);
+  void ScW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoSwapW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoAddW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoXorW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoAndW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoOrW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMinW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMaxW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMinuW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMaxuW(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV64A ///////////////////////////////
+  // TODO confirm aq=? rl=?
+  void LrD(GpuRegister rd, GpuRegister rs1);
+  void ScD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoSwapD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoAddD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoXorD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoAndD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoOrD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMinD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMaxD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMinuD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  void AmoMaxuD(GpuRegister rd, GpuRegister rs2, GpuRegister rs1);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV64M ///////////////////////////////
+  // RV64M-R
+  void Mulw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Divw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Divuw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Remw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+  void Remuw(GpuRegister rd, GpuRegister rs1, GpuRegister rs2);
+
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV32F ///////////////////////////////
+  // RV32F-I
+  void FLw(FpuRegister rd, GpuRegister rs1, uint16_t offset);
+  // RV32F-S
+  void FSw(FpuRegister rs2, GpuRegister rs1, uint16_t offset);
+  // RV32F-R
+  void FMAddS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FMSubS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FNMSubS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FNMAddS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FAddS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSubS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FMulS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FDivS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSqrtS(FpuRegister rd, FpuRegister rs1);
+  void FSgnjS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSgnjnS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSgnjxS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FMinS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FMaxS(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FCvtWS(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm = FRM);
+  void FCvtWuS(GpuRegister rd, FpuRegister rs1);
+  void FMvXW(GpuRegister rd, FpuRegister rs1);
+  void FEqS(GpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FLtS(GpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FLeS(GpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FClassS(GpuRegister rd, FpuRegister rs1);
+  void FCvtSW(FpuRegister rd, GpuRegister rs1);
+  void FCvtSWu(FpuRegister rd, GpuRegister rs1);
+  void FMvWX(FpuRegister rd, GpuRegister rs1);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV64F ///////////////////////////////
+  // RV64F-R
+  void FCvtLS(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm = FRM);
+  void FCvtLuS(GpuRegister rd, FpuRegister rs1);
+  void FCvtSL(FpuRegister rd, GpuRegister rs1);
+  void FCvtSLu(FpuRegister rd, GpuRegister rs1);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV32D ///////////////////////////////
+  // RV32D-I
+  void FLd(FpuRegister rd, GpuRegister rs1, uint16_t offset);
+  // RV32D-S
+  void FSd(FpuRegister rs2, GpuRegister rs1, uint16_t offset);
+  // RV32D-R
+  void FMAddD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FMSubD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FNMSubD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FNMAddD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2, FpuRegister rs3);
+  void FAddD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSubD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FMulD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FDivD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSqrtD(FpuRegister rd, FpuRegister rs1);
+  void FSgnjD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSgnjnD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FSgnjxD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FMinD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FMaxD(FpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FCvtSD(FpuRegister rd, FpuRegister rs1);
+  void FCvtDS(FpuRegister rd, FpuRegister rs1);
+  void FEqD(GpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FLtD(GpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FLeD(GpuRegister rd, FpuRegister rs1, FpuRegister rs2);
+  void FClassD(GpuRegister rd, FpuRegister rs1);
+  void FCvtWD(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm = FRM);
+  void FCvtWuD(GpuRegister rd, FpuRegister rs1);
+  void FCvtDW(FpuRegister rd, GpuRegister rs1);
+  void FCvtDWu(FpuRegister rd, GpuRegister rs1);
+  ///////////////////////////////////////////////////////////////////
+
+  /////////////////////////////// RV64D ///////////////////////////////
+  void FCvtLD(GpuRegister rd, FpuRegister rs1, FPRoundingMode frm = FRM);
+  void FCvtLuD(GpuRegister rd, FpuRegister rs1);
+  void FMvXD(GpuRegister rd, FpuRegister rs1);
+  void FCvtDL(FpuRegister rd, GpuRegister rs1);
+  void FCvtDLu(FpuRegister rd, GpuRegister rs1);
+  void FMvDX(FpuRegister rd, GpuRegister rs1);
+  ///////////////////////////////////////////////////////////////////
+
+  // Helper for replicating floating point value in all destination elements.
+  void ReplicateFPToVectorRegister(VectorRegister dst, FpuRegister src, bool is_double);
+
+  // Higher level composite instructions.
+  int InstrCountForLoadReplicatedConst32(int64_t);
+  void LoadConst32(GpuRegister rd, int32_t value);
+  void LoadConst64(GpuRegister rd, int64_t value);  // RISCV64
+
+  // This function is only used for testing purposes.
+  void RecordLoadConst64Path(int value);
+
+  void Addiu32(GpuRegister rt, GpuRegister rs, int32_t value);
+  void Daddiu64(GpuRegister rt, GpuRegister rs, int64_t value, GpuRegister rtmp = AT);  // RISCV64
+
+  //
+  // Heap poisoning.
+  //
+
+  // Poison a heap reference contained in `src` and store it in `dst`.
+  void PoisonHeapReference(GpuRegister dst, GpuRegister src) {
+    // dst = -src.
+    // Negate the 32-bit ref.
+    Dsubu(dst, ZERO, src);
+    // And constrain it to 32 bits (zero-extend into bits 32 through 63) as on Arm64 and x86/64.
+    Dext(dst, dst, 0, 32);
+  }
+  // Poison a heap reference contained in `reg`.
+  void PoisonHeapReference(GpuRegister reg) {
+    // reg = -reg.
+    PoisonHeapReference(reg, reg);
+  }
+  // Unpoison a heap reference contained in `reg`.
+  void UnpoisonHeapReference(GpuRegister reg) {
+    // reg = -reg.
+    // Negate the 32-bit ref.
+    Sub(reg, ZERO, reg);
+    // And constrain it to 32 bits (zero-extend into bits 32 through 63) as on Arm64 and x86/64.
+    Addiw(reg, reg, 0);
+  }
+  // Poison a heap reference contained in `reg` if heap poisoning is enabled.
+  void MaybePoisonHeapReference(GpuRegister reg) {
+    if (kPoisonHeapReferences) {
+      PoisonHeapReference(reg);
+    }
+  }
+  // Unpoison a heap reference contained in `reg` if heap poisoning is enabled.
+  void MaybeUnpoisonHeapReference(GpuRegister reg) {
+    if (kPoisonHeapReferences) {
+      UnpoisonHeapReference(reg);
+    }
+  }
+
+  void Bind(Label* label) override {
+    Bind(down_cast<Riscv64Label*>(label));
+  }
+  void Jump(Label* label ATTRIBUTE_UNUSED) override {
+    UNIMPLEMENTED(FATAL) << "Do not use Jump for RISCV64";
+  }
+
+  void Bind(Riscv64Label* label);
+
+  // Don't warn about a different virtual Bind/Jump in the base class.
+  using JNIBase::Bind;
+  using JNIBase::Jump;
+
+  // Create a new label that can be used with Jump/Bind calls.
+  std::unique_ptr<JNIMacroLabel> CreateLabel() override {
+    LOG(FATAL) << "Not implemented on RISCV64";
+    UNREACHABLE();
+  }
+  // Emit an unconditional jump to the label.
+  void Jump(JNIMacroLabel* label ATTRIBUTE_UNUSED) override {
+    LOG(FATAL) << "Not implemented on RISCV64";
+    UNREACHABLE();
+  }
+  // Emit a conditional jump to the label by applying a unary condition test to the register.
+  void Jump(JNIMacroLabel* label ATTRIBUTE_UNUSED,
+            JNIMacroUnaryCondition cond ATTRIBUTE_UNUSED,
+            ManagedRegister test ATTRIBUTE_UNUSED) override {
+    LOG(FATAL) << "Not implemented on RISCV64";
+    UNREACHABLE();
+  }
+
+  // Code at this offset will serve as the target for the Jump call.
+  void Bind(JNIMacroLabel* label ATTRIBUTE_UNUSED) override {
+    LOG(FATAL) << "Not implemented on RISCV64";
+    UNREACHABLE();
+  }
+
+  // Create a new literal with a given value.
+  // NOTE: Force the template parameter to be explicitly specified.
+  template <typename T>
+  Literal* NewLiteral(typename Identity<T>::type value) {
+    static_assert(std::is_integral<T>::value, "T must be an integral type.");
+    return NewLiteral(sizeof(value), reinterpret_cast<const uint8_t*>(&value));
+  }
+
+  // Load label address using PC-relative loads. To be used with data labels in the literal /
+  // jump table area only and not with regular code labels.
+  void LoadLabelAddress(GpuRegister dest_reg, Riscv64Label* label);
+
+  // Create a new literal with the given data.
+  Literal* NewLiteral(size_t size, const uint8_t* data);
+
+  // Load literal using PC-relative loads.
+  void LoadLiteral(GpuRegister dest_reg, LoadOperandType load_type, Literal* literal);
+
+  // Create a jump table for the given labels that will be emitted when finalizing.
+  // When the table is emitted, offsets will be relative to the location of the table.
+  // The table location is determined by the location of its label (the label precedes
+  // the table data) and should be loaded using LoadLabelAddress().
+  JumpTable* CreateJumpTable(std::vector<Riscv64Label*>&& labels);
+
+  // When `is_bare` is false, the branches will promote to long (if the range
+  // of the individual branch instruction is insufficient) and the delay/
+  // forbidden slots will be taken care of.
+  // Use `is_bare = false` when the branch target may be out of reach of the
+  // individual branch instruction. IOW, this is for general purpose use.
+  //
+  // When `is_bare` is true, just the branch instructions will be generated
+  // leaving delay/forbidden slot filling up to the caller and the branches
+  // won't promote to long if the range is insufficient (you'll get a
+  // compilation error when the range is exceeded).
+  // Use `is_bare = true` when the branch target is known to be within reach
+  // of the individual branch instruction. This is intended for small local
+  // optimizations around delay/forbidden slots.
+  // Also prefer using `is_bare = true` if the code near the branch is to be
+  // patched or analyzed at run time (e.g. introspection) to
+  // - show the intent and
+  // - fail during compilation rather than during patching/execution if the
+  //   bare branch range is insufficent but the code size and layout are
+  //   expected to remain unchanged
+  //
+  // R6 compact branches without delay/forbidden slots.
+  void Bc(Riscv64Label* label, bool is_bare = false);
+  void Balc(Riscv64Label* label, bool is_bare = false);
+  void Jal(Riscv64Label* label, bool is_bare = false);
+  // R6 compact branches with forbidden slots.
+  void Bltc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bltzc(GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bgtzc(GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bgec(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bgezc(GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Blezc(GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bltuc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bgeuc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Beqc(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Bnec(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);
+  void Beqzc(GpuRegister rs, Riscv64Label* label, bool is_bare = false);
+  void Bnezc(GpuRegister rs, Riscv64Label* label, bool is_bare = false);
+
+  void Bltz(GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bgtz(GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bgez(GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Blez(GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Jal(GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Beq(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bne(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Blt(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bge(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bltu(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bgeu(GpuRegister rs, GpuRegister rt, Riscv64Label* label, bool is_bare = false);  // R2
+  void Beqz(GpuRegister rs, Riscv64Label* label, bool is_bare = false);  // R2
+  void Bnez(GpuRegister rs, Riscv64Label* label, bool is_bare = false);  // R2
+
+  void EmitLoad(ManagedRegister m_dst, GpuRegister src_register, int32_t src_offset, size_t size);
+  void AdjustBaseAndOffset(GpuRegister& base, int32_t& offset, bool is_doubleword);
+  // If element_size_shift is negative at entry, its value will be calculated based on the offset.
+  void AdjustBaseOffsetAndElementSizeShift(GpuRegister& base,
+                                           int32_t& offset,
+                                           int& element_size_shift);
+
+ private:
+  // This will be used as an argument for loads/stores
+  // when there is no need for implicit null checks.
+  struct NoImplicitNullChecker {
+    void operator()() const {}
+  };
+
+ public:
+  template <typename ImplicitNullChecker = NoImplicitNullChecker>
+  void StoreConstToOffset(StoreOperandType type,
+                          int64_t value,
+                          GpuRegister base,
+                          int32_t offset,
+                          GpuRegister temp,
+                          ImplicitNullChecker null_checker = NoImplicitNullChecker()) {
+    // We permit `base` and `temp` to coincide (however, we check that neither is AT),
+    // in which case the `base` register may be overwritten in the process.
+    CHECK_NE(temp, AT);  // Must not use AT as temp, so as not to overwrite the adjusted base.
+    AdjustBaseAndOffset(base, offset, /* is_doubleword= */ (type == kStoreDoubleword));
+    GpuRegister reg;
+    // If the adjustment left `base` unchanged and equal to `temp`, we can't use `temp`
+    // to load and hold the value but we can use AT instead as AT hasn't been used yet.
+    // Otherwise, `temp` can be used for the value. And if `temp` is the same as the
+    // original `base` (that is, `base` prior to the adjustment), the original `base`
+    // register will be overwritten.
+    if (base == temp) {
+      temp = AT;
+    }
+
+    if (type == kStoreDoubleword && IsAligned<kRiscv64DoublewordSize>(offset)) {
+      if (value == 0) {
+        reg = ZERO;
+      } else {
+        reg = temp;
+        LoadConst64(reg, value);
+      }
+      Sd(reg, base, offset);
+      null_checker();
+    } else {
+      uint32_t low = Low32Bits(value);
+      uint32_t high = High32Bits(value);
+      if (low == 0) {
+        reg = ZERO;
+      } else {
+        reg = temp;
+        LoadConst32(reg, low);
+      }
+      switch (type) {
+        case kStoreByte:
+          Sb(reg, base, offset);
+          break;
+        case kStoreHalfword:
+          Sh(reg, base, offset);
+          break;
+        case kStoreWord:
+          Sw(reg, base, offset);
+          break;
+        case kStoreDoubleword:
+          // not aligned to kRiscv64DoublewordSize
+          CHECK_ALIGNED(offset, kRiscv64WordSize);
+          Sw(reg, base, offset);
+          null_checker();
+          if (high == 0) {
+            reg = ZERO;
+          } else {
+            reg = temp;
+            if (high != low) {
+              LoadConst32(reg, high);
+            }
+          }
+          Sw(reg, base, offset + kRiscv64WordSize);
+          break;
+        default:
+          LOG(FATAL) << "UNREACHABLE";
+      }
+      if (type != kStoreDoubleword) {
+        null_checker();
+      }
+    }
+  }
+
+  template <typename ImplicitNullChecker = NoImplicitNullChecker>
+  void LoadFromOffset(LoadOperandType type,
+                      GpuRegister reg,
+                      GpuRegister base,
+                      int32_t offset,
+                      ImplicitNullChecker null_checker = NoImplicitNullChecker()) {
+    AdjustBaseAndOffset(base, offset, /* is_doubleword= */ (type == kLoadDoubleword));
+
+    switch (type) {
+      case kLoadSignedByte:
+        Lb(reg, base, offset);
+        break;
+      case kLoadUnsignedByte:
+        Lbu(reg, base, offset);
+        break;
+      case kLoadSignedHalfword:
+        Lh(reg, base, offset);
+        break;
+      case kLoadUnsignedHalfword:
+        Lhu(reg, base, offset);
+        break;
+      case kLoadWord:
+        CHECK_ALIGNED(offset, kRiscv64WordSize);
+        Lw(reg, base, offset);
+        break;
+      case kLoadUnsignedWord:
+        CHECK_ALIGNED(offset, kRiscv64WordSize);
+        Lwu(reg, base, offset);
+        break;
+      case kLoadDoubleword:
+        Ld(reg, base, offset);
+        null_checker();
+        break;
+      default:
+        LOG(FATAL) << "UNREACHABLE";
+    }
+    if (type != kLoadDoubleword) {
+      null_checker();
+    }
+  }
+
+  template <typename ImplicitNullChecker = NoImplicitNullChecker>
+  void LoadFpuFromOffset(LoadOperandType type,
+                         FpuRegister reg,
+                         GpuRegister base,
+                         int32_t offset,
+                         ImplicitNullChecker null_checker = NoImplicitNullChecker()) {
+    // int element_size_shift = -1;
+    if (type != kLoadQuadword) {
+      AdjustBaseAndOffset(base, offset, /* is_doubleword= */ (type == kLoadDoubleword));
+    } else {
+      // AdjustBaseOffsetAndElementSizeShift(base, offset, element_size_shift);
+    }
+
+    switch (type) {
+      case kLoadWord:
+        CHECK_ALIGNED(offset, kRiscv64WordSize);
+        FLw(reg, base, offset);
+        null_checker();
+        break;
+      case kLoadDoubleword:
+        FLd(reg, base, offset);
+        null_checker();
+        break;
+      case kLoadQuadword:
+        UNIMPLEMENTED(FATAL) << "store kStoreQuadword not implemented";
+        break;
+      default:
+        LOG(FATAL) << "UNREACHABLE";
+    }
+  }
+
+  template <typename ImplicitNullChecker = NoImplicitNullChecker>
+  void StoreToOffset(StoreOperandType type,
+                     GpuRegister reg,
+                     GpuRegister base,
+                     int32_t offset,
+                     ImplicitNullChecker null_checker = NoImplicitNullChecker()) {
+    // Must not use AT as `reg`, so as not to overwrite the value being stored
+    // with the adjusted `base`.
+    CHECK_NE(reg, AT);
+    AdjustBaseAndOffset(base, offset, /* is_doubleword= */ (type == kStoreDoubleword));
+
+    switch (type) {
+      case kStoreByte:
+        Sb(reg, base, offset);
+        break;
+      case kStoreHalfword:
+        Sh(reg, base, offset);
+        break;
+      case kStoreWord:
+        CHECK_ALIGNED(offset, kRiscv64WordSize);
+        Sw(reg, base, offset);
+        break;
+      case kStoreDoubleword:
+        Sd(reg, base, offset);
+        null_checker();
+        break;
+      default:
+        LOG(FATAL) << "UNREACHABLE";
+    }
+    if (type != kStoreDoubleword) {
+      null_checker();
+    }
+  }
+
+  template <typename ImplicitNullChecker = NoImplicitNullChecker>
+  void StoreFpuToOffset(StoreOperandType type,
+                        FpuRegister reg,
+                        GpuRegister base,
+                        int32_t offset,
+                        ImplicitNullChecker null_checker = NoImplicitNullChecker()) {
+    // int element_size_shift = -1;
+    if (type != kStoreQuadword) {
+      AdjustBaseAndOffset(base, offset, /* is_doubleword= */ (type == kStoreDoubleword));
+    } else {
+      // AdjustBaseOffsetAndElementSizeShift(base, offset, element_size_shift);
+    }
+
+    switch (type) {
+      case kStoreWord:
+        CHECK_ALIGNED(offset, kRiscv64WordSize);
+        FSw(reg, base, offset);
+        null_checker();
+        break;
+      case kStoreDoubleword:
+        FSd(reg, base, offset);
+        null_checker();
+        break;
+      case kStoreQuadword:
+        UNIMPLEMENTED(FATAL) << "store kStoreQuadword not implemented";
+        null_checker();
+        break;
+      default:
+        LOG(FATAL) << "UNREACHABLE";
+    }
+  }
+
+  void LoadFromOffset(LoadOperandType type, GpuRegister reg, GpuRegister base, int32_t offset);
+  void LoadFpuFromOffset(LoadOperandType type, FpuRegister reg, GpuRegister base, int32_t offset);
+  void StoreToOffset(StoreOperandType type, GpuRegister reg, GpuRegister base, int32_t offset);
+  void StoreFpuToOffset(StoreOperandType type, FpuRegister reg, GpuRegister base, int32_t offset);
+
+  // Emit data (e.g. encoded instruction or immediate) to the instruction stream.
+  void Emit(uint32_t value);
+
+  //
+  // Overridden common assembler high-level functionality.
+  //
+
+  // Emit code that will create an activation on the stack.
+  void BuildFrame(size_t frame_size,
+                  ManagedRegister method_reg,
+                  ArrayRef<const ManagedRegister> callee_save_regs,
+                  const ManagedRegisterEntrySpills& entry_spills) override;
+
+  // Emit code that will remove an activation from the stack.
+  void RemoveFrame(size_t frame_size,
+                   ArrayRef<const ManagedRegister> callee_save_regs,
+                   bool may_suspend) override;
+
+  void IncreaseFrameSize(size_t adjust) override;
+  void DecreaseFrameSize(size_t adjust) override;
+
+  // Store routines.
+  void Store(FrameOffset offs, ManagedRegister msrc, size_t size) override;
+  void StoreRef(FrameOffset dest, ManagedRegister msrc) override;
+  void StoreRawPtr(FrameOffset dest, ManagedRegister msrc) override;
+
+  void StoreImmediateToFrame(FrameOffset dest, uint32_t imm, ManagedRegister mscratch) override;
+
+  void StoreStackOffsetToThread(ThreadOffset64 thr_offs,
+                                FrameOffset fr_offs,
+                                ManagedRegister mscratch) override;
+
+  void StoreStackPointerToThread(ThreadOffset64 thr_offs) override;
+
+  void StoreSpanning(FrameOffset dest, ManagedRegister msrc, FrameOffset in_off,
+                     ManagedRegister mscratch) override;
+
+  // Load routines.
+  void Load(ManagedRegister mdest, FrameOffset src, size_t size) override;
+
+  void LoadFromThread(ManagedRegister mdest, ThreadOffset64 src, size_t size) override;
+
+  void LoadRef(ManagedRegister dest, FrameOffset src) override;
+
+  void LoadRef(ManagedRegister mdest, ManagedRegister base, MemberOffset offs,
+               bool unpoison_reference) override;
+
+  void LoadRawPtr(ManagedRegister mdest, ManagedRegister base, Offset offs) override;
+
+  void LoadRawPtrFromThread(ManagedRegister mdest, ThreadOffset64 offs) override;
+
+  // Copying routines.
+  void Move(ManagedRegister mdest, ManagedRegister msrc, size_t size) override;
+
+  void CopyRawPtrFromThread(FrameOffset fr_offs,
+                            ThreadOffset64 thr_offs,
+                            ManagedRegister mscratch) override;
+
+  void CopyRawPtrToThread(ThreadOffset64 thr_offs,
+                          FrameOffset fr_offs,
+                          ManagedRegister mscratch) override;
+
+  void CopyRef(FrameOffset dest, FrameOffset src, ManagedRegister mscratch) override;
+
+  void Copy(FrameOffset dest, FrameOffset src, ManagedRegister mscratch, size_t size) override;
+
+  void Copy(FrameOffset dest, ManagedRegister src_base, Offset src_offset, ManagedRegister mscratch,
+            size_t size) override;
+
+  void Copy(ManagedRegister dest_base, Offset dest_offset, FrameOffset src,
+            ManagedRegister mscratch, size_t size) override;
+
+  void Copy(FrameOffset dest, FrameOffset src_base, Offset src_offset, ManagedRegister mscratch,
+            size_t size) override;
+
+  void Copy(ManagedRegister dest, Offset dest_offset, ManagedRegister src, Offset src_offset,
+            ManagedRegister mscratch, size_t size) override;
+
+  void Copy(FrameOffset dest, Offset dest_offset, FrameOffset src, Offset src_offset,
+            ManagedRegister mscratch, size_t size) override;
+
+  void MemoryBarrier(ManagedRegister) override;
+
+  // Sign extension.
+  void SignExtend(ManagedRegister mreg, size_t size) override;
+
+  // Zero extension.
+  void ZeroExtend(ManagedRegister mreg, size_t size) override;
+
+  // Exploit fast access in managed code to Thread::Current().
+  void GetCurrentThread(ManagedRegister tr) override;
+  void GetCurrentThread(FrameOffset dest_offset, ManagedRegister mscratch) override;
+
+  // Set up out_reg to hold a Object** into the handle scope, or to be null if the
+  // value is null and null_allowed. in_reg holds a possibly stale reference
+  // that can be used to avoid loading the handle scope entry to see if the value is
+  // null.
+  void CreateHandleScopeEntry(ManagedRegister out_reg, FrameOffset handlescope_offset,
+                              ManagedRegister in_reg, bool null_allowed) override;
+
+  // Set up out_off to hold a Object** into the handle scope, or to be null if the
+  // value is null and null_allowed.
+  void CreateHandleScopeEntry(FrameOffset out_off, FrameOffset handlescope_offset, ManagedRegister
+                              mscratch, bool null_allowed) override;
+
+  // src holds a handle scope entry (Object**) load this into dst.
+  void LoadReferenceFromHandleScope(ManagedRegister dst, ManagedRegister src) override;
+
+  // Heap::VerifyObject on src. In some cases (such as a reference to this) we
+  // know that src may not be null.
+  void VerifyObject(ManagedRegister src, bool could_be_null) override;
+  void VerifyObject(FrameOffset src, bool could_be_null) override;
+
+  // Call to address held at [base+offset].
+  void Call(ManagedRegister base, Offset offset, ManagedRegister mscratch) override;
+  void Call(FrameOffset base, Offset offset, ManagedRegister mscratch) override;
+  void CallFromThread(ThreadOffset64 offset, ManagedRegister mscratch) override;
+
+  // Generate code to check if Thread::Current()->exception_ is non-null
+  // and branch to a ExceptionSlowPath if it is.
+  void ExceptionPoll(ManagedRegister mscratch, size_t stack_adjust) override;
+
+  // Emit slow paths queued during assembly and promote short branches to long if needed.
+  void FinalizeCode() override;
+
+  // Emit branches and finalize all instructions.
+  void FinalizeInstructions(const MemoryRegion& region) override;
+
+  // Returns the (always-)current location of a label (can be used in class CodeGeneratorRISCV64,
+  // must be used instead of Riscv64Label::GetPosition()).
+  uint32_t GetLabelLocation(const Riscv64Label* label) const;
+
+  // Get the final position of a label after local fixup based on the old position
+  // recorded before FinalizeCode().
+  uint32_t GetAdjustedPosition(uint32_t old_position);
+
+  // Note that PC-relative literal loads are handled as pseudo branches because they need very
+  // similar relocation and may similarly expand in size to accomodate for larger offsets relative
+  // to PC.
+  enum BranchCondition {
+    kCondLT,
+    kCondGE,
+    kCondLE,
+    kCondGT,
+    kCondLTZ,
+    kCondGEZ,
+    kCondLEZ,
+    kCondGTZ,
+    kCondEQ,
+    kCondNE,
+    kCondEQZ,
+    kCondNEZ,
+    kCondLTU,
+    kCondGEU,
+    kUncond,
+  };
+  friend std::ostream& operator<<(std::ostream& os, const BranchCondition& rhs);
+
+ private:
+  class Branch {
+   public:
+    enum Type {
+      // R6 short branches (can be promoted to long).
+      kUncondBranch,
+      kCondBranch,
+      kCall,
+      // R6 short branches (can't be promoted to long), forbidden/delay slots filled manually.
+      kBareUncondBranch,
+      kBareCondBranch,
+      kBareCall,
+      // label.
+      kLabel,
+      // literals.
+      kLiteral,
+      kLiteralUnsigned,
+      kLiteralLong,
+      // Long branches.
+      kLongUncondBranch,
+      kLongCondBranch,
+      kLongCall,
+    };
+
+    // Bit sizes of offsets defined as enums to minimize chance of typos.
+    enum OffsetBits {
+      kOffset12 = 12,  // reserved for jalr
+      kOffset13 = 13,
+      kOffset21 = 21,
+      kOffset32 = 32,
+    };
+
+    static constexpr uint32_t kUnresolved = 0xffffffff;  // Unresolved target_
+    static constexpr int32_t kMaxBranchLength = 32;
+    static constexpr int32_t kMaxBranchSize = kMaxBranchLength * sizeof(uint32_t);
+
+    struct BranchInfo {
+      // Branch length as a number of 4-byte-long instructions.
+      uint32_t length;
+      // Ordinal number (0-based) of the first (or the only) instruction that contains the branch's
+      // PC-relative offset (or its most significant 16-bit half, which goes first).
+      uint32_t instr_offset;
+      // Different MIPS instructions with PC-relative offsets apply said offsets to slightly
+      // different origins, e.g. to PC or PC+4. Encode the origin distance (as a number of 4-byte
+      // instructions) from the instruction containing the offset.
+      uint32_t pc_org;
+      // How large (in bits) a PC-relative offset can be for a given type of branch (kCondBranch
+      // and kBareCondBranch are an exception: use kOffset23 for beqzc/bnezc).
+      OffsetBits offset_size;
+      // Some MIPS instructions with PC-relative offsets shift the offset by 2. Encode the shift
+      // count.
+      int offset_shift;
+    };
+    static const BranchInfo branch_info_[/* Type */];
+
+    // Unconditional branch or call.
+    Branch(uint32_t location, uint32_t target, bool is_call, bool is_bare);
+    // Conditional branch.
+    Branch(uint32_t location,
+           uint32_t target,
+           BranchCondition condition,
+           GpuRegister lhs_reg,
+           GpuRegister rhs_reg,
+           bool is_bare);
+    // Label address (in literal area) or literal.
+    Branch(uint32_t location, GpuRegister dest_reg, Type label_or_literal_type);
+
+    // Some conditional branches with lhs = rhs are effectively NOPs, while some
+    // others are effectively unconditional. MIPSR6 conditional branches require lhs != rhs.
+    // So, we need a way to identify such branches in order to emit no instructions for them
+    // or change them to unconditional.
+    static bool IsNop(BranchCondition condition, GpuRegister lhs, GpuRegister rhs);
+    static bool IsUncond(BranchCondition condition, GpuRegister lhs, GpuRegister rhs);
+
+    static BranchCondition OppositeCondition(BranchCondition cond);
+
+    Type GetType() const;
+    BranchCondition GetCondition() const;
+    GpuRegister GetLeftRegister() const;
+    GpuRegister GetRightRegister() const;
+    uint32_t GetTarget() const;
+    uint32_t GetLocation() const;
+    uint32_t GetOldLocation() const;
+    uint32_t GetLength() const;
+    uint32_t GetOldLength() const;
+    uint32_t GetSize() const;
+    uint32_t GetOldSize() const;
+    uint32_t GetEndLocation() const;
+    uint32_t GetOldEndLocation() const;
+    bool IsBare() const;
+    bool IsLong() const;
+    bool IsResolved() const;
+
+    // Returns the bit size of the signed offset that the branch instruction can handle.
+    OffsetBits GetOffsetSize() const;
+
+    // Calculates the distance between two byte locations in the assembler buffer and
+    // returns the number of bits needed to represent the distance as a signed integer.
+    //
+    // Branch instructions have signed offsets of 16, 19 (addiupc), 21 (beqzc/bnezc),
+    // and 26 (bc) bits, which are additionally shifted left 2 positions at run time.
+    //
+    // Composite branches (made of several instructions) with longer reach have 32-bit
+    // offsets encoded as 2 16-bit "halves" in two instructions (high half goes first).
+    // The composite branches cover the range of PC + ~+/-2GB. The range is not end-to-end,
+    // however. Consider the following implementation of a long unconditional branch, for
+    // example:
+    //
+    //   auipc at, offset_31_16  // at = pc + sign_extend(offset_31_16) << 16
+    //   jic   at, offset_15_0   // pc = at + sign_extend(offset_15_0)
+    //
+    // Both of the above instructions take 16-bit signed offsets as immediate operands.
+    // When bit 15 of offset_15_0 is 1, it effectively causes subtraction of 0x10000
+    // due to sign extension. This must be compensated for by incrementing offset_31_16
+    // by 1. offset_31_16 can only be incremented by 1 if it's not 0x7FFF. If it is
+    // 0x7FFF, adding 1 will overflow the positive offset into the negative range.
+    // Therefore, the long branch range is something like from PC - 0x80000000 to
+    // PC + 0x7FFF7FFF, IOW, shorter by 32KB on one side.
+    //
+    // The returned values are therefore: 18, 21, 23, 28 and 32. There's also a special
+    // case with the addiu instruction and a 16 bit offset.
+    static OffsetBits GetOffsetSizeNeeded(uint32_t location, uint32_t target);
+
+    // Resolve a branch when the target is known.
+    void Resolve(uint32_t target);
+
+    // Relocate a branch by a given delta if needed due to expansion of this or another
+    // branch at a given location by this delta (just changes location_ and target_).
+    void Relocate(uint32_t expand_location, uint32_t delta);
+
+    // If the branch is short, changes its type to long.
+    void PromoteToLong();
+
+    // If necessary, updates the type by promoting a short branch to a long branch
+    // based on the branch location and target. Returns the amount (in bytes) by
+    // which the branch size has increased.
+    // max_short_distance caps the maximum distance between location_ and target_
+    // that is allowed for short branches. This is for debugging/testing purposes.
+    // max_short_distance = 0 forces all short branches to become long.
+    // Use the implicit default argument when not debugging/testing.
+    uint32_t PromoteIfNeeded(uint32_t max_short_distance = std::numeric_limits<uint32_t>::max());
+
+    // Returns the location of the instruction(s) containing the offset.
+    uint32_t GetOffsetLocation() const;
+
+    // Calculates and returns the offset ready for encoding in the branch instruction(s).
+    uint32_t GetOffset() const;
+
+   private:
+    // Completes branch construction by determining and recording its type.
+    void InitializeType(Type initial_type);
+    // Helper for the above.
+    void InitShortOrLong(OffsetBits ofs_size, Type short_type, Type long_type);
+
+    uint32_t old_location_;      // Offset into assembler buffer in bytes.
+    uint32_t location_;          // Offset into assembler buffer in bytes.
+    uint32_t target_;            // Offset into assembler buffer in bytes.
+
+    GpuRegister lhs_reg_;        // Left-hand side register in conditional branches or
+                                 // destination register in literals.
+    GpuRegister rhs_reg_;        // Right-hand side register in conditional branches.
+    BranchCondition condition_;  // Condition for conditional branches.
+
+    Type type_;                  // Current type of the branch.
+    Type old_type_;              // Initial type of the branch.
+  };
+  friend std::ostream& operator<<(std::ostream& os, const Branch::Type& rhs);
+  friend std::ostream& operator<<(std::ostream& os, const Branch::OffsetBits& rhs);
+
+  void EmitRsd(int opcode, GpuRegister rs, GpuRegister rd, int shamt, int funct);
+  void EmitRtd(int opcode, GpuRegister rt, GpuRegister rd, int shamt, int funct);
+  void EmitI(int opcode, GpuRegister rs, GpuRegister rt, uint16_t imm);
+  void EmitI21(int opcode, GpuRegister rs, uint32_t imm21);
+  void EmitI26(int opcode, uint32_t imm26);
+  void EmitFR(int opcode, int fmt, FpuRegister ft, FpuRegister fs, FpuRegister fd, int funct);
+  void EmitFI(int opcode, int fmt, FpuRegister rt, uint16_t imm);
+  void EmitBcond(BranchCondition cond, GpuRegister rs, GpuRegister rt, uint32_t imm16_21);
+  void EmitMsa3R(int operation,
+                 int df,
+                 VectorRegister wt,
+                 VectorRegister ws,
+                 VectorRegister wd,
+                 int minor_opcode);
+  void EmitMsaBIT(int operation, int df_m, VectorRegister ws, VectorRegister wd, int minor_opcode);
+  void EmitMsaELM(int operation, int df_n, VectorRegister ws, VectorRegister wd, int minor_opcode);
+  void EmitMsaMI10(int s10, GpuRegister rs, VectorRegister wd, int minor_opcode, int df);
+  void EmitMsaI10(int operation, int df, int i10, VectorRegister wd, int minor_opcode);
+  void EmitMsa2R(int operation, int df, VectorRegister ws, VectorRegister wd, int minor_opcode);
+  void EmitMsa2RF(int operation, int df, VectorRegister ws, VectorRegister wd, int minor_opcode);
+
+  // TODO dvt porting...
+  template<typename Reg1, typename Reg2, typename Reg3>
+  void EmitR(int funct7, Reg1 rs2, Reg2 rs1, int funct3, Reg3 rd, int opcode) {
+    // TODO validate params
+    uint32_t encoding = static_cast<uint32_t>(funct7) << 25 |
+                        static_cast<uint32_t>(rs2) << 20 |
+                        static_cast<uint32_t>(rs1) << 15 |
+                        static_cast<uint32_t>(funct3) << 12 |
+                        static_cast<uint32_t>(rd) << 7 |
+                        opcode;
+    Emit(encoding);
+  }
+
+  template<typename Reg1, typename Reg2, typename Reg3, typename Reg4>
+  void EmitR4(Reg1 rs3, int funct2, Reg2 rs2, Reg3 rs1, int funct3, Reg4 rd, int opcode) {
+    // TODO validate params
+    uint32_t encoding = static_cast<uint32_t>(rs3) << 27 |
+                        static_cast<uint32_t>(funct2) << 25 |
+                        static_cast<uint32_t>(rs2) << 20 |
+                        static_cast<uint32_t>(rs1) << 15 |
+                        static_cast<uint32_t>(funct3) << 12 |
+                        static_cast<uint32_t>(rd) << 7 |
+                        opcode;
+    Emit(encoding);
+  }
+
+  template<typename Reg1, typename Reg2>
+  void EmitI(uint16_t imm, Reg1 rs1, int funct3, Reg2 rd, int opcode) {
+    uint32_t encoding = static_cast<uint32_t>(imm) << 20 |
+                        static_cast<uint32_t>(rs1) << 15 |
+                        static_cast<uint32_t>(funct3) << 12 |
+                        static_cast<uint32_t>(rd) << 7 |
+                        opcode;
+    Emit(encoding);
+  }
+
+  void EmitI5(uint16_t funct7, uint16_t imm5, GpuRegister rs1, int funct3, GpuRegister rd, int opcode);
+  void EmitI6(uint16_t funct6, uint16_t imm6, GpuRegister rs1, int funct3, GpuRegister rd, int opcode);
+
+  template<typename Reg1, typename Reg2>
+  void EmitS(uint16_t imm, Reg1 rs2, Reg2 rs1, int funct3, int opcode) {
+    // TODO validate params
+    uint32_t encoding = (static_cast<uint32_t>(imm)&0xFE0) << 20 |
+                        static_cast<uint32_t>(rs2) << 20 |
+                        static_cast<uint32_t>(rs1) << 15 |
+                        static_cast<uint32_t>(funct3) << 12 |
+                        (static_cast<uint32_t>(imm)&0x1F) << 7 |
+                      opcode;
+    Emit(encoding);
+  }
+
+  void EmitB(uint16_t imm, GpuRegister rs2, GpuRegister rs1, int funct3, int opcode);
+  void EmitU(uint32_t imm, GpuRegister rd, int opcode);
+  void EmitJ(uint32_t imm, GpuRegister rd, int opcode);
+
+  void Buncond(Riscv64Label* label, bool is_bare);
+  void Bcond(Riscv64Label* label,
+             bool is_bare,
+             BranchCondition condition,
+             GpuRegister lhs,
+             GpuRegister rhs = ZERO);
+  void Call(Riscv64Label* label, bool is_bare);
+  void FinalizeLabeledBranch(Riscv64Label* label);
+
+  Branch* GetBranch(uint32_t branch_id);
+  const Branch* GetBranch(uint32_t branch_id) const;
+
+  void EmitLiterals();
+  void ReserveJumpTableSpace();
+  void EmitJumpTables();
+  void PromoteBranches();
+  void EmitBranch(Branch* branch);
+  void EmitBranches();
+  void PatchCFI();
+
+  // Emits exception block.
+  void EmitExceptionPoll(Riscv64ExceptionSlowPath* exception);
+
+  bool HasMsa() const {
+    return has_msa_;
+  }
+
+  // List of exception blocks to generate at the end of the code cache.
+  std::vector<Riscv64ExceptionSlowPath> exception_blocks_;
+
+  std::vector<Branch> branches_;
+
+  // Whether appending instructions at the end of the buffer or overwriting the existing ones.
+  bool overwriting_;
+  // The current overwrite location.
+  uint32_t overwrite_location_;
+
+  // Use std::deque<> for literal labels to allow insertions at the end
+  // without invalidating pointers and references to existing elements.
+  ArenaDeque<Literal> literals_;
+  ArenaDeque<Literal> long_literals_;  // 64-bit literals separated for alignment reasons.
+
+  // Jump table list.
+  ArenaDeque<JumpTable> jump_tables_;
+
+  // Data for AdjustedPosition(), see the description there.
+  uint32_t last_position_adjustment_;
+  uint32_t last_old_position_;
+  uint32_t last_branch_id_;
+
+  const bool has_msa_;
+
+  DISALLOW_COPY_AND_ASSIGN(Riscv64Assembler);
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_COMPILER_UTILS_RISCV64_ASSEMBLER_RISCV64_H_
diff --git a/compiler/utils/riscv64/assembler_riscv64_test.cc b/compiler/utils/riscv64/assembler_riscv64_test.cc
new file mode 100644
index 0000000000..34f5b308e8
--- /dev/null
+++ b/compiler/utils/riscv64/assembler_riscv64_test.cc
@@ -0,0 +1,4552 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "assembler_riscv64.h"
+
+#include <inttypes.h>
+#include <map>
+#include <random>
+
+#include "base/bit_utils.h"
+#include "base/stl_util.h"
+#include "utils/assembler_test.h"
+
+#define TEST_RV_ALL           1
+#define TEST_RV32I_R          TEST_RV_ALL  // passed
+#define TEST_RV32I_I          TEST_RV_ALL  // passed
+#define TEST_RV32I_S          TEST_RV_ALL
+#define TEST_RV32I_B          1  // passed
+#define TEST_RV32I_J          1  // not passed
+#define TEST_RV64I_R          TEST_RV_ALL  // passed
+#define TEST_RV64I_I          TEST_RV_ALL  // passed
+#define TEST_RV64I_S          TEST_RV_ALL  // passed
+
+#define TEST_RV32M_R          TEST_RV_ALL  // passed
+#define TEST_RV64M_R          TEST_RV_ALL  // passed
+
+#define TEST_RV32A_R          TEST_RV_ALL  // passed
+#define TEST_RV64A_R          TEST_RV_ALL  // passed
+
+#define TEST_RV32F_R          TEST_RV_ALL  // passed
+#define TEST_RV32F_I          TEST_RV_ALL  // passed
+#define TEST_RV32F_S          TEST_RV_ALL  // passed
+#define TEST_RV64F_R          TEST_RV_ALL  // passed
+
+#define TEST_RV32D_R          TEST_RV_ALL  // passed
+#define TEST_RV32D_I          TEST_RV_ALL  // passed
+#define TEST_RV32D_S          TEST_RV_ALL  // passed
+#define TEST_RV64D_R          TEST_RV_ALL  // passed
+
+#define __ GetAssembler()->
+
+namespace art {
+
+struct RISCV64CpuRegisterCompare {
+  bool operator()(const riscv64::GpuRegister& a, const riscv64::GpuRegister& b) const {
+    return a < b;
+  }
+};
+
+class AssemblerRISCV64Test : public AssemblerTest<riscv64::Riscv64Assembler,
+                                                 riscv64::Riscv64Label,
+                                                 riscv64::GpuRegister,
+                                                 riscv64::FpuRegister,
+                                                 uint32_t,
+                                                 riscv64::VectorRegister> {
+ public:
+  using Base = AssemblerTest<riscv64::Riscv64Assembler,
+                             riscv64::Riscv64Label,
+                             riscv64::GpuRegister,
+                             riscv64::FpuRegister,
+                             uint32_t,
+                             riscv64::VectorRegister>;
+/*
+  // These tests were taking too long, so we hide the DriverStr() from AssemblerTest<>
+  // and reimplement it without the verification against `assembly_string`. b/73903608
+  void DriverStr(const std::string& assembly_string ATTRIBUTE_UNUSED,
+                 const std::string& test_name ATTRIBUTE_UNUSED) {
+    GetAssembler()->FinalizeCode();
+    std::vector<uint8_t> data(GetAssembler()->CodeSize());
+    MemoryRegion code(data.data(), data.size());
+    GetAssembler()->FinalizeInstructions(code);
+  }*/
+
+  AssemblerRISCV64Test()
+      : instruction_set_features_(Riscv64InstructionSetFeatures::FromVariant("default", nullptr)) {}
+
+ protected:
+  // Get the typically used name for this architecture, e.g., aarch64, x86-64, ...
+  std::string GetArchitectureString() override {
+    return "riscv64";
+  }
+
+  std::string GetAssemblerCmdName() override {
+    // We assemble and link for RISCV64R6. See GetAssemblerParameters() for details.
+    return "gcc";
+  }
+
+  std::string GetAssemblerParameters() override {
+    // We assemble and link for RISCV64R6. The reason is that object files produced for RISCV64R6
+    // (and MIPS32R6) with the GNU assembler don't have correct final offsets in PC-relative
+    // branches in the .text section and so they require a relocation pass (there's a relocation
+    // section, .rela.text, that has the needed info to fix up the branches).
+    // return " -march=mips64r6 -mmsa -Wa,--no-warn -Wl,-Ttext=0 -Wl,-e0 -nostdlib";
+    return " -march=rv64imafd -mabi=lp64 -Wa,--no-warn -Wl,-Ttext=0 -Wl,-e0 -nostdlib";
+  }
+
+  void Pad(std::vector<uint8_t>& data ATTRIBUTE_UNUSED) override {
+    // The GNU linker unconditionally pads the code segment with NOPs to a size that is a multiple
+    // of 16 and there doesn't appear to be a way to suppress this padding. Our assembler doesn't
+    // pad, so, in order for two assembler outputs to match, we need to match the padding as well.
+    // NOP is encoded as four zero bytes on MIPS.
+    // size_t pad_size = RoundUp(data.size(), 16u) - data.size();
+    // data.insert(data.end(), pad_size, 0);
+  }
+
+  std::string GetDisassembleParameters() override {
+    return " -D -bbinary -mriscv:rv64";
+  }
+
+  riscv64::Riscv64Assembler* CreateAssembler(ArenaAllocator* allocator) override {
+    return new (allocator) riscv64::Riscv64Assembler(allocator, instruction_set_features_.get());
+  }
+
+  void SetUpHelpers() override {
+    if (registers_.size() == 0) {
+      registers_.push_back(new riscv64::GpuRegister(riscv64::ZERO));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::RA));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::SP));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::GP));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::TP));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T0));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T1));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T2));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S0));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S1));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A0));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A1));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A2));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A3));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A4));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A5));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A6));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::A7));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S2));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S3));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S4));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S5));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S6));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S7));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S8));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S9));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S10));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::S11));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T3));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T4));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T5));
+      registers_.push_back(new riscv64::GpuRegister(riscv64::T6));
+
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::ZERO), "zero");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::RA), "ra");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::SP), "sp");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::GP), "gp");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::TP), "tp");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T0), "t0");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T1), "t1");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T2), "t2");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S0), "s0");  // s0/fp
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S1), "s1");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A0), "a0");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A1), "a1");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A2), "a2");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A3), "a3");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A4), "a4");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A5), "a5");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A6), "a6");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::A7), "a7");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S2), "s2");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S3), "s3");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S4), "s4");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S5), "s5");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S6), "s6");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S7), "s7");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S8), "s8");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S9), "s9");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S10), "s10");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::S11), "s11");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T3), "t3");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T4), "t4");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T5), "t5");
+      secondary_register_names_.emplace(riscv64::GpuRegister(riscv64::T6), "t6");
+
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT0));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT1));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT2));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT3));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT4));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT5));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT6));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT7));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS0));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS1));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA0));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA1));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA2));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA3));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA4));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA5));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA6));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FA7));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS2));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS3));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS4));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS5));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS6));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS7));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS8));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS9));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS10));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FS11));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT8));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT9));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT10));
+      fp_registers_.push_back(new riscv64::FpuRegister(riscv64::FT11));
+
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W0));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W1));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W2));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W3));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W4));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W5));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W6));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W7));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W8));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W9));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W10));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W11));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W12));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W13));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W14));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W15));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W16));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W17));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W18));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W19));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W20));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W21));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W22));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W23));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W24));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W25));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W26));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W27));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W28));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W29));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W30));
+      vec_registers_.push_back(new riscv64::VectorRegister(riscv64::W31));
+    }
+  }
+
+  void TearDown() override {
+    AssemblerTest::TearDown();
+    STLDeleteElements(&registers_);
+    STLDeleteElements(&fp_registers_);
+    STLDeleteElements(&vec_registers_);
+  }
+
+  std::vector<riscv64::Riscv64Label> GetAddresses() override {
+    UNIMPLEMENTED(FATAL) << "Feature not implemented yet";
+    UNREACHABLE();
+  }
+
+  std::vector<riscv64::GpuRegister*> GetRegisters() override {
+    return registers_;
+  }
+
+  std::vector<riscv64::FpuRegister*> GetFPRegisters() override {
+    return fp_registers_;
+  }
+
+  std::vector<riscv64::VectorRegister*> GetVectorRegisters() override {
+    return vec_registers_;
+  }
+
+  uint32_t CreateImmediate(int64_t imm_value) override {
+    return imm_value;
+  }
+
+  std::string GetSecondaryRegisterName(const riscv64::GpuRegister& reg) override {
+    CHECK(secondary_register_names_.find(reg) != secondary_register_names_.end());
+    return secondary_register_names_[reg];
+  }
+
+  std::string RepeatInsn(size_t count, const std::string& insn) {
+    std::string result;
+    for (; count != 0u; --count) {
+      result += insn;
+    }
+    return result;
+  }
+
+  void BranchHelper(void (riscv64::Riscv64Assembler::*f)(riscv64::Riscv64Label*,
+                                                       bool),
+                    const std::string& instr_name,
+                    bool is_bare = false) {
+    riscv64::Riscv64Label label1, label2;
+    (Base::GetAssembler()->*f)(&label1, is_bare);
+    constexpr size_t kAdduCount1 = 63;
+    for (size_t i = 0; i != kAdduCount1; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label1);
+    (Base::GetAssembler()->*f)(&label2, is_bare);
+    constexpr size_t kAdduCount2 = 64;
+    for (size_t i = 0; i != kAdduCount2; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label2);
+    (Base::GetAssembler()->*f)(&label1, is_bare);
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+
+    std::string expected =
+        ".set noreorder\n" +
+        instr_name + " 1f\n" +
+        RepeatInsn(kAdduCount1, "addu $zero, $zero, $zero\n") +
+        "1:\n" +
+        instr_name + " 2f\n" +
+        RepeatInsn(kAdduCount2, "addu $zero, $zero, $zero\n") +
+        "2:\n" +
+        instr_name + " 1b\n" +
+        "addu $zero, $zero, $zero\n";
+    DriverStr(expected, instr_name);
+  }
+
+  void BranchHelper1(void (riscv64::Riscv64Assembler::*f)(riscv64::Riscv64Label*,
+                                                       bool),
+                    const std::string& instr_name,
+                    bool is_bare = false) {
+    riscv64::Riscv64Label label1, label2;
+    (Base::GetAssembler()->*f)(&label1, is_bare);
+    constexpr size_t kAdduCount1 = 63;
+    for (size_t i = 0; i != kAdduCount1; ++i) {
+      __ Add(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label1);
+    (Base::GetAssembler()->*f)(&label2, is_bare);
+    constexpr size_t kAdduCount2 = 64;
+    for (size_t i = 0; i != kAdduCount2; ++i) {
+      __ Add(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label2);
+    (Base::GetAssembler()->*f)(&label1, is_bare);
+    __ Add(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+
+    std::string expected =
+        instr_name + " 1f\n" +
+        RepeatInsn(kAdduCount1, "add zero, zero, zero\n") +
+        "1:\n" +
+        instr_name + " 2f\n" +
+        RepeatInsn(kAdduCount2, "add zero, zero, zero\n") +
+        "2:\n" +
+        instr_name + " 1b\n" +
+        "add zero, zero, zero\n";
+    DriverStr(expected, instr_name);
+  }
+
+  void BranchCondOneRegHelper(void (riscv64::Riscv64Assembler::*f)(riscv64::GpuRegister,
+                                                                 riscv64::Riscv64Label*,
+                                                                 bool),
+                              const std::string& instr_name,
+                              bool is_bare = false) {
+    riscv64::Riscv64Label label;
+    (Base::GetAssembler()->*f)(riscv64::A0, &label, is_bare);
+    constexpr size_t kAdduCount1 = 63;
+    for (size_t i = 0; i != kAdduCount1; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label);
+    constexpr size_t kAdduCount2 = 64;
+    for (size_t i = 0; i != kAdduCount2; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    (Base::GetAssembler()->*f)(riscv64::A1, &label, is_bare);
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+
+    std::string expected =
+        ".set noreorder\n" +
+        instr_name + " $a0, 1f\n" +
+        (is_bare ? "" : "nop\n") +
+        RepeatInsn(kAdduCount1, "addu $zero, $zero, $zero\n") +
+        "1:\n" +
+        RepeatInsn(kAdduCount2, "addu $zero, $zero, $zero\n") +
+        instr_name + " $a1, 1b\n" +
+        (is_bare ? "" : "nop\n") +
+        "addu $zero, $zero, $zero\n";
+    DriverStr(expected, instr_name);
+  }
+
+  void BranchCondTwoRegsHelper(void (riscv64::Riscv64Assembler::*f)(riscv64::GpuRegister,
+                                                                  riscv64::GpuRegister,
+                                                                  riscv64::Riscv64Label*,
+                                                                  bool),
+                               const std::string& instr_name,
+                               bool is_bare = false) {
+    riscv64::Riscv64Label label;
+    (Base::GetAssembler()->*f)(riscv64::A0, riscv64::A1, &label, is_bare);
+    constexpr size_t kAdduCount1 = 63;
+    for (size_t i = 0; i != kAdduCount1; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label);
+    constexpr size_t kAdduCount2 = 64;
+    for (size_t i = 0; i != kAdduCount2; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    (Base::GetAssembler()->*f)(riscv64::A2, riscv64::A3, &label, is_bare);
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+
+    std::string expected =
+        ".set noreorder\n" +
+        instr_name + " $a0, $a1, 1f\n" +
+        (is_bare ? "" : "nop\n") +
+        RepeatInsn(kAdduCount1, "addu $zero, $zero, $zero\n") +
+        "1:\n" +
+        RepeatInsn(kAdduCount2, "addu $zero, $zero, $zero\n") +
+        instr_name + " $a2, $a3, 1b\n" +
+        (is_bare ? "" : "nop\n") +
+        "addu $zero, $zero, $zero\n";
+    DriverStr(expected, instr_name);
+  }
+
+  void BranchCondTwoRegsHelper1(void (riscv64::Riscv64Assembler::*f)(riscv64::GpuRegister,
+                                                                  riscv64::GpuRegister,
+                                                                  riscv64::Riscv64Label*,
+                                                                  bool),
+                               const std::string& instr_name,
+                               bool is_bare = false) {
+    riscv64::Riscv64Label label;
+    (Base::GetAssembler()->*f)(riscv64::A0, riscv64::A1, &label, is_bare);
+    constexpr size_t kAdduCount1 = 63;
+    for (size_t i = 0; i != kAdduCount1; ++i) {
+      __ Add(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label);
+    constexpr size_t kAdduCount2 = 64;
+    for (size_t i = 0; i != kAdduCount2; ++i) {
+      __ Add(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    (Base::GetAssembler()->*f)(riscv64::A2, riscv64::A3, &label, is_bare);
+    __ Add(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+
+    std::string expected =
+        instr_name + " a0, a1, 1f\n" +
+        (is_bare ? "" : "nop\n") +
+        RepeatInsn(kAdduCount1, "add zero, zero, zero\n") +
+        "1:\n" +
+        RepeatInsn(kAdduCount2, "add zero, zero, zero\n") +
+        instr_name + " a2, a3, 1b\n" +
+        (is_bare ? "" : "nop\n") +
+        "add zero, zero, zero\n";
+    DriverStr(expected, instr_name);
+  }
+
+  void BranchFpuCondHelper(void (riscv64::Riscv64Assembler::*f)(riscv64::FpuRegister,
+                                                              riscv64::Riscv64Label*,
+                                                              bool),
+                           const std::string& instr_name,
+                           bool is_bare = false) {
+    riscv64::Riscv64Label label;
+    (Base::GetAssembler()->*f)(riscv64::F0, &label, is_bare);
+    constexpr size_t kAdduCount1 = 63;
+    for (size_t i = 0; i != kAdduCount1; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    __ Bind(&label);
+    constexpr size_t kAdduCount2 = 64;
+    for (size_t i = 0; i != kAdduCount2; ++i) {
+      __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+    }
+    (Base::GetAssembler()->*f)(riscv64::FT11, &label, is_bare);
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+
+    std::string expected =
+        ".set noreorder\n" +
+        instr_name + " $f0, 1f\n" +
+        (is_bare ? "" : "nop\n") +
+        RepeatInsn(kAdduCount1, "addu $zero, $zero, $zero\n") +
+        "1:\n" +
+        RepeatInsn(kAdduCount2, "addu $zero, $zero, $zero\n") +
+        instr_name + " $f31, 1b\n" +
+        (is_bare ? "" : "nop\n") +
+        "addu $zero, $zero, $zero\n";
+    DriverStr(expected, instr_name);
+  }
+
+ private:
+  std::vector<riscv64::GpuRegister*> registers_;
+  std::map<riscv64::GpuRegister, std::string, RISCV64CpuRegisterCompare> secondary_register_names_;
+
+  std::vector<riscv64::FpuRegister*> fp_registers_;
+  std::vector<riscv64::VectorRegister*> vec_registers_;
+
+  std::unique_ptr<const Riscv64InstructionSetFeatures> instruction_set_features_;
+};
+
+TEST_F(AssemblerRISCV64Test, Toolchain) {
+  EXPECT_TRUE(CheckTools());
+}
+#if 0
+///////////////////
+// FP Operations //
+///////////////////
+
+TEST_F(AssemblerRISCV64Test, AddS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::AddS, "add.s ${reg1}, ${reg2}, ${reg3}"), "add.s");
+}
+
+TEST_F(AssemblerRISCV64Test, AddD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::AddD, "add.d ${reg1}, ${reg2}, ${reg3}"), "add.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SubS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SubS, "sub.s ${reg1}, ${reg2}, ${reg3}"), "sub.s");
+}
+
+TEST_F(AssemblerRISCV64Test, SubD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SubD, "sub.d ${reg1}, ${reg2}, ${reg3}"), "sub.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MulS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::MulS, "mul.s ${reg1}, ${reg2}, ${reg3}"), "mul.s");
+}
+
+TEST_F(AssemblerRISCV64Test, MulD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::MulD, "mul.d ${reg1}, ${reg2}, ${reg3}"), "mul.d");
+}
+
+TEST_F(AssemblerRISCV64Test, DivS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::DivS, "div.s ${reg1}, ${reg2}, ${reg3}"), "div.s");
+}
+
+TEST_F(AssemblerRISCV64Test, DivD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::DivD, "div.d ${reg1}, ${reg2}, ${reg3}"), "div.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SqrtS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::SqrtS, "sqrt.s ${reg1}, ${reg2}"), "sqrt.s");
+}
+
+TEST_F(AssemblerRISCV64Test, SqrtD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::SqrtD, "sqrt.d ${reg1}, ${reg2}"), "sqrt.d");
+}
+
+TEST_F(AssemblerRISCV64Test, AbsS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::AbsS, "abs.s ${reg1}, ${reg2}"), "abs.s");
+}
+
+TEST_F(AssemblerRISCV64Test, AbsD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::AbsD, "abs.d ${reg1}, ${reg2}"), "abs.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MovS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::MovS, "mov.s ${reg1}, ${reg2}"), "mov.s");
+}
+
+TEST_F(AssemblerRISCV64Test, MovD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::MovD, "mov.d ${reg1}, ${reg2}"), "mov.d");
+}
+
+TEST_F(AssemblerRISCV64Test, NegS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::NegS, "neg.s ${reg1}, ${reg2}"), "neg.s");
+}
+
+TEST_F(AssemblerRISCV64Test, NegD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::NegD, "neg.d ${reg1}, ${reg2}"), "neg.d");
+}
+
+TEST_F(AssemblerRISCV64Test, RoundLS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::RoundLS, "round.l.s ${reg1}, ${reg2}"), "round.l.s");
+}
+
+TEST_F(AssemblerRISCV64Test, RoundLD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::RoundLD, "round.l.d ${reg1}, ${reg2}"), "round.l.d");
+}
+
+TEST_F(AssemblerRISCV64Test, RoundWS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::RoundWS, "round.w.s ${reg1}, ${reg2}"), "round.w.s");
+}
+
+TEST_F(AssemblerRISCV64Test, RoundWD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::RoundWD, "round.w.d ${reg1}, ${reg2}"), "round.w.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CeilLS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::CeilLS, "ceil.l.s ${reg1}, ${reg2}"), "ceil.l.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CeilLD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::CeilLD, "ceil.l.d ${reg1}, ${reg2}"), "ceil.l.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CeilWS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::CeilWS, "ceil.w.s ${reg1}, ${reg2}"), "ceil.w.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CeilWD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::CeilWD, "ceil.w.d ${reg1}, ${reg2}"), "ceil.w.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FloorLS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FloorLS, "floor.l.s ${reg1}, ${reg2}"), "floor.l.s");
+}
+
+TEST_F(AssemblerRISCV64Test, FloorLD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FloorLD, "floor.l.d ${reg1}, ${reg2}"), "floor.l.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FloorWS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FloorWS, "floor.w.s ${reg1}, ${reg2}"), "floor.w.s");
+}
+
+TEST_F(AssemblerRISCV64Test, FloorWD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FloorWD, "floor.w.d ${reg1}, ${reg2}"), "floor.w.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SelS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SelS, "sel.s ${reg1}, ${reg2}, ${reg3}"), "sel.s");
+}
+
+TEST_F(AssemblerRISCV64Test, SelD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SelD, "sel.d ${reg1}, ${reg2}, ${reg3}"), "sel.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SeleqzS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SeleqzS, "seleqz.s ${reg1}, ${reg2}, ${reg3}"),
+            "seleqz.s");
+}
+
+TEST_F(AssemblerRISCV64Test, SeleqzD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SeleqzD, "seleqz.d ${reg1}, ${reg2}, ${reg3}"),
+            "seleqz.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SelnezS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SelnezS, "selnez.s ${reg1}, ${reg2}, ${reg3}"),
+            "selnez.s");
+}
+
+TEST_F(AssemblerRISCV64Test, SelnezD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::SelnezD, "selnez.d ${reg1}, ${reg2}, ${reg3}"),
+            "selnez.d");
+}
+
+TEST_F(AssemblerRISCV64Test, RintS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::RintS, "rint.s ${reg1}, ${reg2}"), "rint.s");
+}
+
+TEST_F(AssemblerRISCV64Test, RintD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::RintD, "rint.d ${reg1}, ${reg2}"), "rint.d");
+}
+
+TEST_F(AssemblerRISCV64Test, ClassS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::ClassS, "class.s ${reg1}, ${reg2}"), "class.s");
+}
+
+TEST_F(AssemblerRISCV64Test, ClassD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::ClassD, "class.d ${reg1}, ${reg2}"), "class.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MinS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::MinS, "min.s ${reg1}, ${reg2}, ${reg3}"), "min.s");
+}
+
+TEST_F(AssemblerRISCV64Test, MinD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::MinD, "min.d ${reg1}, ${reg2}, ${reg3}"), "min.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MaxS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::MaxS, "max.s ${reg1}, ${reg2}, ${reg3}"), "max.s");
+}
+
+TEST_F(AssemblerRISCV64Test, MaxD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::MaxD, "max.d ${reg1}, ${reg2}, ${reg3}"), "max.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUnS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUnS, "cmp.un.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.un.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpEqS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpEqS, "cmp.eq.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.eq.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUeqS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUeqS, "cmp.ueq.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ueq.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpLtS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpLtS, "cmp.lt.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.lt.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUltS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUltS, "cmp.ult.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ult.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpLeS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpLeS, "cmp.le.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.le.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUleS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUleS, "cmp.ule.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ule.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpOrS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpOrS, "cmp.or.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.or.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUneS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUneS, "cmp.une.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.une.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpNeS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpNeS, "cmp.ne.s ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ne.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUnD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUnD, "cmp.un.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.un.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpEqD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpEqD, "cmp.eq.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.eq.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUeqD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUeqD, "cmp.ueq.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ueq.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpLtD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpLtD, "cmp.lt.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.lt.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUltD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUltD, "cmp.ult.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ult.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpLeD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpLeD, "cmp.le.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.le.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUleD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUleD, "cmp.ule.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ule.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpOrD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpOrD, "cmp.or.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.or.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpUneD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpUneD, "cmp.une.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.une.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CmpNeD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::CmpNeD, "cmp.ne.d ${reg1}, ${reg2}, ${reg3}"),
+            "cmp.ne.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CvtDL) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::Cvtdl, "cvt.d.l ${reg1}, ${reg2}"), "cvt.d.l");
+}
+
+TEST_F(AssemblerRISCV64Test, CvtDS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::Cvtds, "cvt.d.s ${reg1}, ${reg2}"), "cvt.d.s");
+}
+
+TEST_F(AssemblerRISCV64Test, CvtDW) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::Cvtdw, "cvt.d.w ${reg1}, ${reg2}"), "cvt.d.w");
+}
+
+TEST_F(AssemblerRISCV64Test, CvtSL) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::Cvtsl, "cvt.s.l ${reg1}, ${reg2}"), "cvt.s.l");
+}
+
+TEST_F(AssemblerRISCV64Test, CvtSD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::Cvtsd, "cvt.s.d ${reg1}, ${reg2}"), "cvt.s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, CvtSW) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::Cvtsw, "cvt.s.w ${reg1}, ${reg2}"), "cvt.s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, TruncWS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::TruncWS, "trunc.w.s ${reg1}, ${reg2}"), "trunc.w.s");
+}
+
+TEST_F(AssemblerRISCV64Test, TruncWD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::TruncWD, "trunc.w.d ${reg1}, ${reg2}"), "trunc.w.d");
+}
+
+TEST_F(AssemblerRISCV64Test, TruncLS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::TruncLS, "trunc.l.s ${reg1}, ${reg2}"), "trunc.l.s");
+}
+
+TEST_F(AssemblerRISCV64Test, TruncLD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::TruncLD, "trunc.l.d ${reg1}, ${reg2}"), "trunc.l.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Mfc1) {
+  DriverStr(RepeatRF(&riscv64::Riscv64Assembler::Mfc1, "mfc1 ${reg1}, ${reg2}"), "Mfc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Mfhc1) {
+  DriverStr(RepeatRF(&riscv64::Riscv64Assembler::Mfhc1, "mfhc1 ${reg1}, ${reg2}"), "Mfhc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Mtc1) {
+  DriverStr(RepeatRF(&riscv64::Riscv64Assembler::Mtc1, "mtc1 ${reg1}, ${reg2}"), "Mtc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Mthc1) {
+  DriverStr(RepeatRF(&riscv64::Riscv64Assembler::Mthc1, "mthc1 ${reg1}, ${reg2}"), "Mthc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Dmfc1) {
+  DriverStr(RepeatRF(&riscv64::Riscv64Assembler::Dmfc1, "dmfc1 ${reg1}, ${reg2}"), "Dmfc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Dmtc1) {
+  DriverStr(RepeatRF(&riscv64::Riscv64Assembler::Dmtc1, "dmtc1 ${reg1}, ${reg2}"), "Dmtc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Lwc1) {
+  DriverStr(RepeatFRIb(&riscv64::Riscv64Assembler::Lwc1, -16, "lwc1 ${reg1}, {imm}(${reg2})"),
+            "lwc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Ldc1) {
+  DriverStr(RepeatFRIb(&riscv64::Riscv64Assembler::Ldc1, -16, "ldc1 ${reg1}, {imm}(${reg2})"),
+            "ldc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Swc1) {
+  DriverStr(RepeatFRIb(&riscv64::Riscv64Assembler::Swc1, -16, "swc1 ${reg1}, {imm}(${reg2})"),
+            "swc1");
+}
+
+TEST_F(AssemblerRISCV64Test, Sdc1) {
+  DriverStr(RepeatFRIb(&riscv64::Riscv64Assembler::Sdc1, -16, "sdc1 ${reg1}, {imm}(${reg2})"),
+            "sdc1");
+}
+
+//////////////
+// BRANCHES //
+//////////////
+
+TEST_F(AssemblerRISCV64Test, Jalr) {
+  DriverStr(".set noreorder\n" +
+            RepeatRRNoDupes(&riscv64::Riscv64Assembler::Jalr, "jalr ${reg1}, ${reg2}"), "jalr");
+}
+
+TEST_F(AssemblerRISCV64Test, Bc) {
+  BranchHelper(&riscv64::Riscv64Assembler::Bc, "Bc");
+}
+
+TEST_F(AssemblerRISCV64Test, Balc) {
+  BranchHelper(&riscv64::Riscv64Assembler::Balc, "Balc");
+}
+
+TEST_F(AssemblerRISCV64Test, Beqzc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Beqzc, "Beqzc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bnezc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bnezc, "Bnezc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bltzc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bltzc, "Bltzc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bgezc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bgezc, "Bgezc");
+}
+
+TEST_F(AssemblerRISCV64Test, Blezc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Blezc, "Blezc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bgtzc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bgtzc, "Bgtzc");
+}
+
+TEST_F(AssemblerRISCV64Test, Beqc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Beqc, "Beqc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bnec) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bnec, "Bnec");
+}
+
+TEST_F(AssemblerRISCV64Test, Bltc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bltc, "Bltc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bgec) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bgec, "Bgec");
+}
+
+TEST_F(AssemblerRISCV64Test, Bltuc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bltuc, "Bltuc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bgeuc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bgeuc, "Bgeuc");
+}
+
+TEST_F(AssemblerRISCV64Test, Bc1eqz) {
+  BranchFpuCondHelper(&riscv64::Riscv64Assembler::Bc1eqz, "Bc1eqz");
+}
+
+TEST_F(AssemblerRISCV64Test, Bc1nez) {
+  BranchFpuCondHelper(&riscv64::Riscv64Assembler::Bc1nez, "Bc1nez");
+}
+
+TEST_F(AssemblerRISCV64Test, BareBc) {
+  BranchHelper(&riscv64::Riscv64Assembler::Bc, "Bc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBalc) {
+  BranchHelper(&riscv64::Riscv64Assembler::Balc, "Balc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBeqzc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Beqzc, "Beqzc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBnezc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bnezc, "Bnezc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBltzc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bltzc, "Bltzc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBgezc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bgezc, "Bgezc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBlezc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Blezc, "Blezc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBgtzc) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bgtzc, "Bgtzc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBeqc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Beqc, "Beqc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBnec) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bnec, "Bnec", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBltc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bltc, "Bltc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBgec) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bgec, "Bgec", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBltuc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bltuc, "Bltuc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBgeuc) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bgeuc, "Bgeuc", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBc1eqz) {
+  BranchFpuCondHelper(&riscv64::Riscv64Assembler::Bc1eqz, "Bc1eqz", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBc1nez) {
+  BranchFpuCondHelper(&riscv64::Riscv64Assembler::Bc1nez, "Bc1nez", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBeqz) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Beqz, "Beqz", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBnez) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bnez, "Bnez", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBltz) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bltz, "Bltz", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBgez) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bgez, "Bgez", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBlez) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Blez, "Blez", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBgtz) {
+  BranchCondOneRegHelper(&riscv64::Riscv64Assembler::Bgtz, "Bgtz", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBeq) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Beq, "Beq", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, BareBne) {
+  BranchCondTwoRegsHelper(&riscv64::Riscv64Assembler::Bne, "Bne", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, LongBeqc) {
+  riscv64::Riscv64Label label;
+  __ Beqc(riscv64::A0, riscv64::A1, &label);
+  constexpr uint32_t kAdduCount1 = (1u << 15) + 1;
+  for (uint32_t i = 0; i != kAdduCount1; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+  __ Bind(&label);
+  constexpr uint32_t kAdduCount2 = (1u << 15) + 1;
+  for (uint32_t i = 0; i != kAdduCount2; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+  __ Beqc(riscv64::A2, riscv64::A3, &label);
+
+  uint32_t offset_forward = 2 + kAdduCount1;  // 2: account for auipc and jic.
+  offset_forward <<= 2;
+  offset_forward += (offset_forward & 0x8000) << 1;  // Account for sign extension in jic.
+
+  uint32_t offset_back = -(kAdduCount2 + 1);  // 1: account for bnec.
+  offset_back <<= 2;
+  offset_back += (offset_back & 0x8000) << 1;  // Account for sign extension in jic.
+
+  std::ostringstream oss;
+  oss <<
+      ".set noreorder\n"
+      "bnec $a0, $a1, 1f\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_forward) << "\n"
+      "jic $at, 0x" << std::hex << Low16Bits(offset_forward) << "\n"
+      "1:\n" <<
+      RepeatInsn(kAdduCount1, "addu $zero, $zero, $zero\n") <<
+      "2:\n" <<
+      RepeatInsn(kAdduCount2, "addu $zero, $zero, $zero\n") <<
+      "bnec $a2, $a3, 3f\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_back) << "\n"
+      "jic $at, 0x" << std::hex << Low16Bits(offset_back) << "\n"
+      "3:\n";
+  std::string expected = oss.str();
+  DriverStr(expected, "LongBeqc");
+}
+
+TEST_F(AssemblerRISCV64Test, LongBeqzc) {
+  constexpr uint32_t kNopCount1 = (1u << 20) + 1;
+  constexpr uint32_t kNopCount2 = (1u << 20) + 1;
+  constexpr uint32_t kRequiredCapacity = (kNopCount1 + kNopCount2 + 6u) * 4u;
+  ASSERT_LT(__ GetBuffer()->Capacity(), kRequiredCapacity);
+  __ GetBuffer()->ExtendCapacity(kRequiredCapacity);
+  riscv64::Riscv64Label label;
+  __ Beqzc(riscv64::A0, &label);
+  for (uint32_t i = 0; i != kNopCount1; ++i) {
+    __ Nop();
+  }
+  __ Bind(&label);
+  for (uint32_t i = 0; i != kNopCount2; ++i) {
+    __ Nop();
+  }
+  __ Beqzc(riscv64::A2, &label);
+
+  uint32_t offset_forward = 2 + kNopCount1;  // 2: account for auipc and jic.
+  offset_forward <<= 2;
+  offset_forward += (offset_forward & 0x8000) << 1;  // Account for sign extension in jic.
+
+  uint32_t offset_back = -(kNopCount2 + 1);  // 1: account for bnezc.
+  offset_back <<= 2;
+  offset_back += (offset_back & 0x8000) << 1;  // Account for sign extension in jic.
+
+  // Note, we're using the ".fill" directive to tell the assembler to generate many NOPs
+  // instead of generating them ourselves in the source code. This saves test time.
+  std::ostringstream oss;
+  oss <<
+      ".set noreorder\n"
+      "bnezc $a0, 1f\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_forward) << "\n"
+      "jic $at, 0x" << std::hex << Low16Bits(offset_forward) << "\n"
+      "1:\n" <<
+      ".fill 0x" << std::hex << kNopCount1 << " , 4, 0\n"
+      "2:\n" <<
+      ".fill 0x" << std::hex << kNopCount2 << " , 4, 0\n"
+      "bnezc $a2, 3f\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_back) << "\n"
+      "jic $at, 0x" << std::hex << Low16Bits(offset_back) << "\n"
+      "3:\n";
+  std::string expected = oss.str();
+  DriverStr(expected, "LongBeqzc");
+}
+
+TEST_F(AssemblerRISCV64Test, LongBalc) {
+  constexpr uint32_t kNopCount1 = (1u << 25) + 1;
+  constexpr uint32_t kNopCount2 = (1u << 25) + 1;
+  constexpr uint32_t kRequiredCapacity = (kNopCount1 + kNopCount2 + 6u) * 4u;
+  ASSERT_LT(__ GetBuffer()->Capacity(), kRequiredCapacity);
+  __ GetBuffer()->ExtendCapacity(kRequiredCapacity);
+  riscv64::Riscv64Label label1, label2;
+  __ Balc(&label1);
+  for (uint32_t i = 0; i != kNopCount1; ++i) {
+    __ Nop();
+  }
+  __ Bind(&label1);
+  __ Balc(&label2);
+  for (uint32_t i = 0; i != kNopCount2; ++i) {
+    __ Nop();
+  }
+  __ Bind(&label2);
+  __ Balc(&label1);
+
+  uint32_t offset_forward1 = 2 + kNopCount1;  // 2: account for auipc and jialc.
+  offset_forward1 <<= 2;
+  offset_forward1 += (offset_forward1 & 0x8000) << 1;  // Account for sign extension in jialc.
+
+  uint32_t offset_forward2 = 2 + kNopCount2;  // 2: account for auipc and jialc.
+  offset_forward2 <<= 2;
+  offset_forward2 += (offset_forward2 & 0x8000) << 1;  // Account for sign extension in jialc.
+
+  uint32_t offset_back = -(2 + kNopCount2);  // 2: account for auipc and jialc.
+  offset_back <<= 2;
+  offset_back += (offset_back & 0x8000) << 1;  // Account for sign extension in jialc.
+
+  // Note, we're using the ".fill" directive to tell the assembler to generate many NOPs
+  // instead of generating them ourselves in the source code. This saves a few minutes
+  // of test time.
+  std::ostringstream oss;
+  oss <<
+      ".set noreorder\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_forward1) << "\n"
+      "jialc $at, 0x" << std::hex << Low16Bits(offset_forward1) << "\n"
+      ".fill 0x" << std::hex << kNopCount1 << " , 4, 0\n"
+      "1:\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_forward2) << "\n"
+      "jialc $at, 0x" << std::hex << Low16Bits(offset_forward2) << "\n"
+      ".fill 0x" << std::hex << kNopCount2 << " , 4, 0\n"
+      "2:\n"
+      "auipc $at, 0x" << std::hex << High16Bits(offset_back) << "\n"
+      "jialc $at, 0x" << std::hex << Low16Bits(offset_back) << "\n";
+  std::string expected = oss.str();
+  DriverStr(expected, "LongBalc");
+}
+
+//////////
+// MISC //
+//////////
+
+TEST_F(AssemblerRISCV64Test, Lwpc) {
+  // Lwpc() takes an unsigned 19-bit immediate, while the GNU assembler needs a signed offset,
+  // hence the sign extension from bit 18 with `imm - ((imm & 0x40000) << 1)`.
+  // The GNU assembler also wants the offset to be a multiple of 4, which it will shift right
+  // by 2 positions when encoding, hence `<< 2` to compensate for that shift.
+  // We capture the value of the immediate with `.set imm, {imm}` because the value is needed
+  // twice for the sign extension, but `{imm}` is substituted only once.
+  const char* code = ".set imm, {imm}\nlw ${reg}, ((imm - ((imm & 0x40000) << 1)) << 2)($pc)";
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Lwpc, 19, code), "Lwpc");
+}
+
+TEST_F(AssemblerRISCV64Test, Lwupc) {
+  // The comment for the Lwpc test applies here as well.
+  const char* code = ".set imm, {imm}\nlwu ${reg}, ((imm - ((imm & 0x40000) << 1)) << 2)($pc)";
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Lwupc, 19, code), "Lwupc");
+}
+
+TEST_F(AssemblerRISCV64Test, Ldpc) {
+  // The comment for the Lwpc test applies here as well.
+  const char* code = ".set imm, {imm}\nld ${reg}, ((imm - ((imm & 0x20000) << 1)) << 3)($pc)";
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Ldpc, 18, code), "Ldpc");
+}
+
+TEST_F(AssemblerRISCV64Test, Auipc) {
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Auipc, 16, "auipc ${reg}, {imm}"), "Auipc");
+}
+
+TEST_F(AssemblerRISCV64Test, Addiupc) {
+  // The comment from the Lwpc() test applies to this Addiupc() test as well.
+  const char* code = ".set imm, {imm}\naddiupc ${reg}, (imm - ((imm & 0x40000) << 1)) << 2";
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Addiupc, 19, code), "Addiupc");
+}
+
+TEST_F(AssemblerRISCV64Test, Addu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Addu, "addu ${reg1}, ${reg2}, ${reg3}"), "addu");
+}
+
+TEST_F(AssemblerRISCV64Test, Addiu) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Addiu, -16, "addiu ${reg1}, ${reg2}, {imm}"),
+            "addiu");
+}
+
+TEST_F(AssemblerRISCV64Test, Daddu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Daddu, "daddu ${reg1}, ${reg2}, ${reg3}"), "daddu");
+}
+
+TEST_F(AssemblerRISCV64Test, Daddiu) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Daddiu, -16, "daddiu ${reg1}, ${reg2}, {imm}"),
+            "daddiu");
+}
+
+TEST_F(AssemblerRISCV64Test, Subu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Subu, "subu ${reg1}, ${reg2}, ${reg3}"), "subu");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsubu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dsubu, "dsubu ${reg1}, ${reg2}, ${reg3}"), "dsubu");
+}
+
+TEST_F(AssemblerRISCV64Test, MulR6) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::MulR6, "mul ${reg1}, ${reg2}, ${reg3}"), "mulR6");
+}
+
+TEST_F(AssemblerRISCV64Test, DivR6) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::DivR6, "div ${reg1}, ${reg2}, ${reg3}"), "divR6");
+}
+
+TEST_F(AssemblerRISCV64Test, ModR6) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::ModR6, "mod ${reg1}, ${reg2}, ${reg3}"), "modR6");
+}
+
+TEST_F(AssemblerRISCV64Test, DivuR6) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::DivuR6, "divu ${reg1}, ${reg2}, ${reg3}"),
+            "divuR6");
+}
+
+TEST_F(AssemblerRISCV64Test, ModuR6) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::ModuR6, "modu ${reg1}, ${reg2}, ${reg3}"),
+            "moduR6");
+}
+
+TEST_F(AssemblerRISCV64Test, Dmul) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dmul, "dmul ${reg1}, ${reg2}, ${reg3}"), "dmul");
+}
+
+TEST_F(AssemblerRISCV64Test, Ddiv) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Ddiv, "ddiv ${reg1}, ${reg2}, ${reg3}"), "ddiv");
+}
+
+TEST_F(AssemblerRISCV64Test, Dmod) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dmod, "dmod ${reg1}, ${reg2}, ${reg3}"), "dmod");
+}
+
+TEST_F(AssemblerRISCV64Test, Ddivu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Ddivu, "ddivu ${reg1}, ${reg2}, ${reg3}"), "ddivu");
+}
+
+TEST_F(AssemblerRISCV64Test, Dmodu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dmodu, "dmodu ${reg1}, ${reg2}, ${reg3}"), "dmodu");
+}
+
+TEST_F(AssemblerRISCV64Test, And) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::And, "and ${reg1}, ${reg2}, ${reg3}"), "and");
+}
+
+TEST_F(AssemblerRISCV64Test, Andi) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Andi, 16, "andi ${reg1}, ${reg2}, {imm}"), "andi");
+}
+
+TEST_F(AssemblerRISCV64Test, Or) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Or, "or ${reg1}, ${reg2}, ${reg3}"), "or");
+}
+
+TEST_F(AssemblerRISCV64Test, Ori) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Ori, 16, "ori ${reg1}, ${reg2}, {imm}"), "ori");
+}
+
+TEST_F(AssemblerRISCV64Test, Xor) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Xor, "xor ${reg1}, ${reg2}, ${reg3}"), "xor");
+}
+
+TEST_F(AssemblerRISCV64Test, Xori) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Xori, 16, "xori ${reg1}, ${reg2}, {imm}"), "xori");
+}
+
+TEST_F(AssemblerRISCV64Test, Nor) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Nor, "nor ${reg1}, ${reg2}, ${reg3}"), "nor");
+}
+
+TEST_F(AssemblerRISCV64Test, Lb) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lb, -16, "lb ${reg1}, {imm}(${reg2})"), "lb");
+}
+
+TEST_F(AssemblerRISCV64Test, Lh) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lh, -16, "lh ${reg1}, {imm}(${reg2})"), "lh");
+}
+
+TEST_F(AssemblerRISCV64Test, Lw) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lw, -16, "lw ${reg1}, {imm}(${reg2})"), "lw");
+}
+
+TEST_F(AssemblerRISCV64Test, Ld) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Ld, -16, "ld ${reg1}, {imm}(${reg2})"), "ld");
+}
+
+TEST_F(AssemblerRISCV64Test, Lbu) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lbu, -16, "lbu ${reg1}, {imm}(${reg2})"), "lbu");
+}
+
+TEST_F(AssemblerRISCV64Test, Lhu) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lhu, -16, "lhu ${reg1}, {imm}(${reg2})"), "lhu");
+}
+
+TEST_F(AssemblerRISCV64Test, Lwu) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lwu, -16, "lwu ${reg1}, {imm}(${reg2})"), "lwu");
+}
+
+TEST_F(AssemblerRISCV64Test, Lui) {
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Lui, 16, "lui ${reg}, {imm}"), "lui");
+}
+
+TEST_F(AssemblerRISCV64Test, Daui) {
+  std::vector<riscv64::GpuRegister*> reg1_registers = GetRegisters();
+  std::vector<riscv64::GpuRegister*> reg2_registers = GetRegisters();
+  reg2_registers.erase(reg2_registers.begin());  // reg2 can't be ZERO, remove it.
+  std::vector<int64_t> imms = CreateImmediateValuesBits(/* imm_bits= */ 16, /* as_uint= */ true);
+  WarnOnCombinations(reg1_registers.size() * reg2_registers.size() * imms.size());
+  std::ostringstream expected;
+  for (riscv64::GpuRegister* reg1 : reg1_registers) {
+    for (riscv64::GpuRegister* reg2 : reg2_registers) {
+      for (int64_t imm : imms) {
+        __ Daui(*reg1, *reg2, imm);
+        expected << "daui $" << *reg1 << ", $" << *reg2 << ", " << imm << "\n";
+      }
+    }
+  }
+  DriverStr(expected.str(), "daui");
+}
+
+TEST_F(AssemblerRISCV64Test, Dahi) {
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Dahi, 16, "dahi ${reg}, ${reg}, {imm}"), "dahi");
+}
+
+TEST_F(AssemblerRISCV64Test, Dati) {
+  DriverStr(RepeatRIb(&riscv64::Riscv64Assembler::Dati, 16, "dati ${reg}, ${reg}, {imm}"), "dati");
+}
+
+TEST_F(AssemblerRISCV64Test, Sb) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sb, -16, "sb ${reg1}, {imm}(${reg2})"), "sb");
+}
+
+TEST_F(AssemblerRISCV64Test, Sh) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sh, -16, "sh ${reg1}, {imm}(${reg2})"), "sh");
+}
+
+TEST_F(AssemblerRISCV64Test, Sw) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sw, -16, "sw ${reg1}, {imm}(${reg2})"), "sw");
+}
+
+TEST_F(AssemblerRISCV64Test, Sd) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sd, -16, "sd ${reg1}, {imm}(${reg2})"), "sd");
+}
+
+TEST_F(AssemblerRISCV64Test, Slt) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Slt, "slt ${reg1}, ${reg2}, ${reg3}"), "slt");
+}
+
+TEST_F(AssemblerRISCV64Test, Sltu) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Sltu, "sltu ${reg1}, ${reg2}, ${reg3}"), "sltu");
+}
+
+TEST_F(AssemblerRISCV64Test, Slti) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Slti, -16, "slti ${reg1}, ${reg2}, {imm}"),
+            "slti");
+}
+
+TEST_F(AssemblerRISCV64Test, Sltiu) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sltiu, -16, "sltiu ${reg1}, ${reg2}, {imm}"),
+            "sltiu");
+}
+
+TEST_F(AssemblerRISCV64Test, Move) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Move, "or ${reg1}, ${reg2}, $zero"), "move");
+}
+
+TEST_F(AssemblerRISCV64Test, Clear) {
+  DriverStr(RepeatR(&riscv64::Riscv64Assembler::Clear, "or ${reg}, $zero, $zero"), "clear");
+}
+
+TEST_F(AssemblerRISCV64Test, Not) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Not, "nor ${reg1}, ${reg2}, $zero"), "not");
+}
+
+TEST_F(AssemblerRISCV64Test, Bitswap) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Bitswap, "bitswap ${reg1}, ${reg2}"), "bitswap");
+}
+
+TEST_F(AssemblerRISCV64Test, Dbitswap) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Dbitswap, "dbitswap ${reg1}, ${reg2}"), "dbitswap");
+}
+
+TEST_F(AssemblerRISCV64Test, Seb) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Seb, "seb ${reg1}, ${reg2}"), "seb");
+}
+
+TEST_F(AssemblerRISCV64Test, Seh) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Seh, "seh ${reg1}, ${reg2}"), "seh");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsbh) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Dsbh, "dsbh ${reg1}, ${reg2}"), "dsbh");
+}
+
+TEST_F(AssemblerRISCV64Test, Dshd) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Dshd, "dshd ${reg1}, ${reg2}"), "dshd");
+}
+
+TEST_F(AssemblerRISCV64Test, Dext) {
+  std::vector<riscv64::GpuRegister*> reg1_registers = GetRegisters();
+  std::vector<riscv64::GpuRegister*> reg2_registers = GetRegisters();
+  WarnOnCombinations(reg1_registers.size() * reg2_registers.size() * 33 * 16);
+  std::ostringstream expected;
+  for (riscv64::GpuRegister* reg1 : reg1_registers) {
+    for (riscv64::GpuRegister* reg2 : reg2_registers) {
+      for (int32_t pos = 0; pos < 32; pos++) {
+        for (int32_t size = 1; size <= 32; size++) {
+          __ Dext(*reg1, *reg2, pos, size);
+          expected << "dext $" << *reg1 << ", $" << *reg2 << ", " << pos << ", " << size << "\n";
+        }
+      }
+    }
+  }
+
+  DriverStr(expected.str(), "Dext");
+}
+
+TEST_F(AssemblerRISCV64Test, Ins) {
+  std::vector<riscv64::GpuRegister*> regs = GetRegisters();
+  WarnOnCombinations(regs.size() * regs.size() * 33 * 16);
+  std::string expected;
+  for (riscv64::GpuRegister* reg1 : regs) {
+    for (riscv64::GpuRegister* reg2 : regs) {
+      for (int32_t pos = 0; pos < 32; pos++) {
+        for (int32_t size = 1; pos + size <= 32; size++) {
+          __ Ins(*reg1, *reg2, pos, size);
+          std::ostringstream instr;
+          instr << "ins $" << *reg1 << ", $" << *reg2 << ", " << pos << ", " << size << "\n";
+          expected += instr.str();
+        }
+      }
+    }
+  }
+  DriverStr(expected, "Ins");
+}
+
+TEST_F(AssemblerRISCV64Test, DblIns) {
+  std::vector<riscv64::GpuRegister*> reg1_registers = GetRegisters();
+  std::vector<riscv64::GpuRegister*> reg2_registers = GetRegisters();
+  WarnOnCombinations(reg1_registers.size() * reg2_registers.size() * 65 * 32);
+  std::ostringstream expected;
+  for (riscv64::GpuRegister* reg1 : reg1_registers) {
+    for (riscv64::GpuRegister* reg2 : reg2_registers) {
+      for (int32_t pos = 0; pos < 64; pos++) {
+        for (int32_t size = 1; pos + size <= 64; size++) {
+          __ DblIns(*reg1, *reg2, pos, size);
+          expected << "dins $" << *reg1 << ", $" << *reg2 << ", " << pos << ", " << size << "\n";
+        }
+      }
+    }
+  }
+
+  DriverStr(expected.str(), "DblIns");
+}
+
+TEST_F(AssemblerRISCV64Test, Lsa) {
+  DriverStr(RepeatRRRIb(&riscv64::Riscv64Assembler::Lsa,
+                        2,
+                        "lsa ${reg1}, ${reg2}, ${reg3}, {imm}",
+                        1),
+            "lsa");
+}
+
+TEST_F(AssemblerRISCV64Test, Dlsa) {
+  DriverStr(RepeatRRRIb(&riscv64::Riscv64Assembler::Dlsa,
+                        2,
+                        "dlsa ${reg1}, ${reg2}, ${reg3}, {imm}",
+                        1),
+            "dlsa");
+}
+
+TEST_F(AssemblerRISCV64Test, Wsbh) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Wsbh, "wsbh ${reg1}, ${reg2}"), "wsbh");
+}
+
+TEST_F(AssemblerRISCV64Test, Sll) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sll, 5, "sll ${reg1}, ${reg2}, {imm}"), "sll");
+}
+
+TEST_F(AssemblerRISCV64Test, Srl) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Srl, 5, "srl ${reg1}, ${reg2}, {imm}"), "srl");
+}
+
+TEST_F(AssemblerRISCV64Test, Rotr) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Rotr, 5, "rotr ${reg1}, ${reg2}, {imm}"), "rotr");
+}
+
+TEST_F(AssemblerRISCV64Test, Sra) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sra, 5, "sra ${reg1}, ${reg2}, {imm}"), "sra");
+}
+
+TEST_F(AssemblerRISCV64Test, Sllv) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Sllv, "sllv ${reg1}, ${reg2}, ${reg3}"), "sllv");
+}
+
+TEST_F(AssemblerRISCV64Test, Srlv) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Srlv, "srlv ${reg1}, ${reg2}, ${reg3}"), "srlv");
+}
+
+TEST_F(AssemblerRISCV64Test, Rotrv) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Rotrv, "rotrv ${reg1}, ${reg2}, ${reg3}"), "rotrv");
+}
+
+TEST_F(AssemblerRISCV64Test, Srav) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Srav, "srav ${reg1}, ${reg2}, ${reg3}"), "srav");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsll) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Dsll, 5, "dsll ${reg1}, ${reg2}, {imm}"), "dsll");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsrl) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Dsrl, 5, "dsrl ${reg1}, ${reg2}, {imm}"), "dsrl");
+}
+
+TEST_F(AssemblerRISCV64Test, Drotr) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Drotr, 5, "drotr ${reg1}, ${reg2}, {imm}"),
+            "drotr");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsra) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Dsra, 5, "dsra ${reg1}, ${reg2}, {imm}"), "dsra");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsll32) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Dsll32, 5, "dsll32 ${reg1}, ${reg2}, {imm}"),
+            "dsll32");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsrl32) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Dsrl32, 5, "dsrl32 ${reg1}, ${reg2}, {imm}"),
+            "dsrl32");
+}
+
+TEST_F(AssemblerRISCV64Test, Drotr32) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Drotr32, 5, "drotr32 ${reg1}, ${reg2}, {imm}"),
+            "drotr32");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsra32) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Dsra32, 5, "dsra32 ${reg1}, ${reg2}, {imm}"),
+            "dsra32");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsllv) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dsllv, "dsllv ${reg1}, ${reg2}, ${reg3}"), "dsllv");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsrlv) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dsrlv, "dsrlv ${reg1}, ${reg2}, ${reg3}"), "dsrlv");
+}
+
+TEST_F(AssemblerRISCV64Test, Dsrav) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Dsrav, "dsrav ${reg1}, ${reg2}, ${reg3}"), "dsrav");
+}
+
+TEST_F(AssemblerRISCV64Test, Sc) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Sc, -9, "sc ${reg1}, {imm}(${reg2})"), "sc");
+}
+
+TEST_F(AssemblerRISCV64Test, Scd) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Scd, -9, "scd ${reg1}, {imm}(${reg2})"), "scd");
+}
+
+TEST_F(AssemblerRISCV64Test, Ll) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Ll, -9, "ll ${reg1}, {imm}(${reg2})"), "ll");
+}
+
+TEST_F(AssemblerRISCV64Test, Lld) {
+  DriverStr(RepeatRRIb(&riscv64::Riscv64Assembler::Lld, -9, "lld ${reg1}, {imm}(${reg2})"), "lld");
+}
+
+TEST_F(AssemblerRISCV64Test, Seleqz) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Seleqz, "seleqz ${reg1}, ${reg2}, ${reg3}"),
+            "seleqz");
+}
+
+TEST_F(AssemblerRISCV64Test, Selnez) {
+  DriverStr(RepeatRRR(&riscv64::Riscv64Assembler::Selnez, "selnez ${reg1}, ${reg2}, ${reg3}"),
+            "selnez");
+}
+
+TEST_F(AssemblerRISCV64Test, Clz) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Clz, "clz ${reg1}, ${reg2}"), "clz");
+}
+
+TEST_F(AssemblerRISCV64Test, Clo) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Clo, "clo ${reg1}, ${reg2}"), "clo");
+}
+
+TEST_F(AssemblerRISCV64Test, Dclz) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Dclz, "dclz ${reg1}, ${reg2}"), "dclz");
+}
+
+TEST_F(AssemblerRISCV64Test, Dclo) {
+  DriverStr(RepeatRR(&riscv64::Riscv64Assembler::Dclo, "dclo ${reg1}, ${reg2}"), "dclo");
+}
+
+TEST_F(AssemblerRISCV64Test, LoadFromOffset) {
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 1);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x7FFF);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x8001);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x7FFFFFFE);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x7FFFFFFF);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadSignedByte, riscv64::A0, riscv64::A1, 0x80000001);
+
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 1);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x7FFF);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x8001);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x7FFFFFFE);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x7FFFFFFF);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedByte, riscv64::A0, riscv64::A1, 0x80000001);
+
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 2);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x7FFE);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x8002);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x7FFFFFFC);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x7FFFFFFE);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadSignedHalfword, riscv64::A0, riscv64::A1, 0x80000002);
+
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 2);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x7FFE);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x8002);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x7FFFFFFC);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x7FFFFFFE);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedHalfword, riscv64::A0, riscv64::A1, 0x80000002);
+
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 4);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x7FFC);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x8004);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x7FFFFFF8);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x7FFFFFFC);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadWord, riscv64::A0, riscv64::A1, 0x80000004);
+
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 4);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x7FFC);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x8004);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x7FFFFFF8);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x7FFFFFFC);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadUnsignedWord, riscv64::A0, riscv64::A1, 0x80000004);
+
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A0, 0);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 4);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 256);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 1000);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x7FFC);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x8000);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x8004);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x10000);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x27FFC);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x12345678);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, -256);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, -32768);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x7FFFFFF8);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x7FFFFFFC);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x80000000);
+  __ LoadFromOffset(riscv64::kLoadDoubleword, riscv64::A0, riscv64::A1, 0x80000004);
+
+  const char* expected =
+      "lb $a0, 0($a0)\n"
+      "lb $a0, 0($a1)\n"
+      "lb $a0, 1($a1)\n"
+      "lb $a0, 256($a1)\n"
+      "lb $a0, 1000($a1)\n"
+      "lb $a0, 0x7FFF($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "lb $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lb $a0, 9($at)\n"
+      "daui $at, $a1, 1\n"
+      "lb $a0, 0($at)\n"
+      "daui $at, $a1, 0x1234\n"
+      "lb $a0, 0x5678($at)\n"
+      "lb $a0, -256($a1)\n"
+      "lb $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "lb $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lb $a0, -2($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lb $a0, -1($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lb $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lb $a0, 1($at)\n"
+
+      "lbu $a0, 0($a0)\n"
+      "lbu $a0, 0($a1)\n"
+      "lbu $a0, 1($a1)\n"
+      "lbu $a0, 256($a1)\n"
+      "lbu $a0, 1000($a1)\n"
+      "lbu $a0, 0x7FFF($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "lbu $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lbu $a0, 9($at)\n"
+      "daui $at, $a1, 1\n"
+      "lbu $a0, 0($at)\n"
+      "daui $at, $a1, 0x1234\n"
+      "lbu $a0, 0x5678($at)\n"
+      "lbu $a0, -256($a1)\n"
+      "lbu $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "lbu $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lbu $a0, -2($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lbu $a0, -1($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lbu $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lbu $a0, 1($at)\n"
+
+      "lh $a0, 0($a0)\n"
+      "lh $a0, 0($a1)\n"
+      "lh $a0, 2($a1)\n"
+      "lh $a0, 256($a1)\n"
+      "lh $a0, 1000($a1)\n"
+      "lh $a0, 0x7FFE($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "lh $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lh $a0, 10($at)\n"
+      "daui $at, $a1, 1\n"
+      "lh $a0, 0($at)\n"
+      "daui $at, $a1, 0x1234\n"
+      "lh $a0, 0x5678($at)\n"
+      "lh $a0, -256($a1)\n"
+      "lh $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "lh $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lh $a0, -4($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lh $a0, -2($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lh $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lh $a0, 2($at)\n"
+
+      "lhu $a0, 0($a0)\n"
+      "lhu $a0, 0($a1)\n"
+      "lhu $a0, 2($a1)\n"
+      "lhu $a0, 256($a1)\n"
+      "lhu $a0, 1000($a1)\n"
+      "lhu $a0, 0x7FFE($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "lhu $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lhu $a0, 10($at)\n"
+      "daui $at, $a1, 1\n"
+      "lhu $a0, 0($at)\n"
+      "daui $at, $a1, 0x1234\n"
+      "lhu $a0, 0x5678($at)\n"
+      "lhu $a0, -256($a1)\n"
+      "lhu $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "lhu $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lhu $a0, -4($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lhu $a0, -2($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lhu $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lhu $a0, 2($at)\n"
+
+      "lw $a0, 0($a0)\n"
+      "lw $a0, 0($a1)\n"
+      "lw $a0, 4($a1)\n"
+      "lw $a0, 256($a1)\n"
+      "lw $a0, 1000($a1)\n"
+      "lw $a0, 0x7FFC($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "lw $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lw $a0, 12($at)\n"
+      "daui $at, $a1, 1\n"
+      "lw $a0, 0($at)\n"
+      "daui $at, $a1, 0x1234\n"
+      "lw $a0, 0x5678($at)\n"
+      "lw $a0, -256($a1)\n"
+      "lw $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "lw $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lw $a0, -8($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lw $a0, -4($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lw $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lw $a0, 4($at)\n"
+
+      "lwu $a0, 0($a0)\n"
+      "lwu $a0, 0($a1)\n"
+      "lwu $a0, 4($a1)\n"
+      "lwu $a0, 256($a1)\n"
+      "lwu $a0, 1000($a1)\n"
+      "lwu $a0, 0x7FFC($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "lwu $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lwu $a0, 12($at)\n"
+      "daui $at, $a1, 1\n"
+      "lwu $a0, 0($at)\n"
+      "daui $at, $a1, 0x1234\n"
+      "lwu $a0, 0x5678($at)\n"
+      "lwu $a0, -256($a1)\n"
+      "lwu $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "lwu $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lwu $a0, -8($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lwu $a0, -4($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lwu $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lwu $a0, 4($at)\n"
+
+      "ld $a0, 0($a0)\n"
+      "ld $a0, 0($a1)\n"
+      "lwu $a0, 4($a1)\n"
+      "lwu $t3, 8($a1)\n"
+      "dinsu $a0, $t3, 32, 32\n"
+      "ld $a0, 256($a1)\n"
+      "ld $a0, 1000($a1)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lwu $a0, 4($at)\n"
+      "lwu $t3, 8($at)\n"
+      "dinsu $a0, $t3, 32, 32\n"
+      "daddiu $at, $a1, 32760\n"
+      "ld $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760\n"
+      "lwu $a0, 12($at)\n"
+      "lwu $t3, 16($at)\n"
+      "dinsu $a0, $t3, 32, 32\n"
+      "daui $at, $a1, 1\n"
+      "ld $a0, 0($at)\n"
+      "daui $at, $a1, 2\n"
+      "daddiu $at, $at, 8\n"
+      "lwu $a0, 0x7ff4($at)\n"
+      "lwu $t3, 0x7ff8($at)\n"
+      "dinsu $a0, $t3, 32, 32\n"
+      "daui $at, $a1, 0x1234\n"
+      "ld $a0, 0x5678($at)\n"
+      "ld $a0, -256($a1)\n"
+      "ld $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "ld $a0, -4352($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "ld $a0, -8($at)\n"
+      "daui $at, $a1, 32768\n"
+      "dahi $at, $at, 1\n"
+      "lwu $a0, -4($at)\n"
+      "lwu $t3, 0($at)\n"
+      "dinsu $a0, $t3, 32, 32\n"
+      "daui $at, $a1, 32768\n"
+      "ld $a0, 0($at)\n"
+      "daui $at, $a1, 32768\n"
+      "lwu $a0, 4($at)\n"
+      "lwu $t3, 8($at)\n"
+      "dinsu $a0, $t3, 32, 32\n";
+  DriverStr(expected, "LoadFromOffset");
+}
+
+TEST_F(AssemblerRISCV64Test, LoadFpuFromOffset) {
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 4);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 256);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0x7FFC);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0x8000);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0x8004);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0x10000);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0x12345678);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, -256);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, -32768);
+  __ LoadFpuFromOffset(riscv64::kLoadWord, riscv64::F0, riscv64::A0, 0xABCDEF00);
+
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 4);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 256);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0x7FFC);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0x8000);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0x8004);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0x10000);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0x12345678);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, -256);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, -32768);
+  __ LoadFpuFromOffset(riscv64::kLoadDoubleword, riscv64::F0, riscv64::A0, 0xABCDEF00);
+
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 1);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 2);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 8);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 511);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 512);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 513);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 514);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 516);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 1022);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 1024);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 1025);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 1026);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 1028);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 2044);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 2048);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 2049);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 2050);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 2052);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4088);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4096);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4097);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4098);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4100);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 4104);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0x7FFC);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0x8000);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0x10000);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0x12345678);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0x12350078);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -256);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -511);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -513);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -1022);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -1026);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -2044);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -2052);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -4096);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -4104);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, -32768);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0xABCDEF00);
+  __ LoadFpuFromOffset(riscv64::kLoadQuadword, riscv64::F0, riscv64::A0, 0x7FFFABCD);
+
+  const char* expected =
+      "lwc1 $f0, 0($a0)\n"
+      "lwc1 $f0, 4($a0)\n"
+      "lwc1 $f0, 256($a0)\n"
+      "lwc1 $f0, 0x7FFC($a0)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "lwc1 $f0, 8($at)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "lwc1 $f0, 12($at)\n"
+      "daui $at, $a0, 1\n"
+      "lwc1 $f0, 0($at)\n"
+      "daui $at, $a0, 4660 # 0x1234\n"
+      "lwc1 $f0, 22136($at) # 0x5678\n"
+      "lwc1 $f0, -256($a0)\n"
+      "lwc1 $f0, -32768($a0)\n"
+      "daui $at, $a0, 0xABCE\n"
+      "lwc1 $f0, -0x1100($at) # 0xEF00\n"
+
+      "ldc1 $f0, 0($a0)\n"
+      "lwc1 $f0, 4($a0)\n"
+      "lw $t3, 8($a0)\n"
+      "mthc1 $t3, $f0\n"
+      "ldc1 $f0, 256($a0)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "lwc1 $f0, 4($at)\n"
+      "lw $t3, 8($at)\n"
+      "mthc1 $t3, $f0\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "ldc1 $f0, 8($at)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "lwc1 $f0, 12($at)\n"
+      "lw $t3, 16($at)\n"
+      "mthc1 $t3, $f0\n"
+      "daui $at, $a0, 1\n"
+      "ldc1 $f0, 0($at)\n"
+      "daui $at, $a0, 4660 # 0x1234\n"
+      "ldc1 $f0, 22136($at) # 0x5678\n"
+      "ldc1 $f0, -256($a0)\n"
+      "ldc1 $f0, -32768($a0)\n"
+      "daui $at, $a0, 0xABCE\n"
+      "ldc1 $f0, -0x1100($at) # 0xEF00\n"
+
+      "ld.d $w0, 0($a0)\n"
+      "ld.b $w0, 1($a0)\n"
+      "ld.h $w0, 2($a0)\n"
+      "ld.w $w0, 4($a0)\n"
+      "ld.d $w0, 8($a0)\n"
+      "ld.b $w0, 511($a0)\n"
+      "ld.d $w0, 512($a0)\n"
+      "daddiu $at, $a0, 513\n"
+      "ld.b $w0, 0($at)\n"
+      "ld.h $w0, 514($a0)\n"
+      "ld.w $w0, 516($a0)\n"
+      "ld.h $w0, 1022($a0)\n"
+      "ld.d $w0, 1024($a0)\n"
+      "daddiu $at, $a0, 1025\n"
+      "ld.b $w0, 0($at)\n"
+      "daddiu $at, $a0, 1026\n"
+      "ld.h $w0, 0($at)\n"
+      "ld.w $w0, 1028($a0)\n"
+      "ld.w $w0, 2044($a0)\n"
+      "ld.d $w0, 2048($a0)\n"
+      "daddiu $at, $a0, 2049\n"
+      "ld.b $w0, 0($at)\n"
+      "daddiu $at, $a0, 2050\n"
+      "ld.h $w0, 0($at)\n"
+      "daddiu $at, $a0, 2052\n"
+      "ld.w $w0, 0($at)\n"
+      "ld.d $w0, 4088($a0)\n"
+      "daddiu $at, $a0, 4096\n"
+      "ld.d $w0, 0($at)\n"
+      "daddiu $at, $a0, 4097\n"
+      "ld.b $w0, 0($at)\n"
+      "daddiu $at, $a0, 4098\n"
+      "ld.h $w0, 0($at)\n"
+      "daddiu $at, $a0, 4100\n"
+      "ld.w $w0, 0($at)\n"
+      "daddiu $at, $a0, 4104\n"
+      "ld.d $w0, 0($at)\n"
+      "daddiu $at, $a0, 0x7FFC\n"
+      "ld.w $w0, 0($at)\n"
+      "daddiu $at, $a0, 0x7FF8\n"
+      "ld.d $w0, 8($at)\n"
+      "daui $at, $a0, 0x1\n"
+      "ld.d $w0, 0($at)\n"
+      "daui $at, $a0, 0x1234\n"
+      "daddiu $at, $at, 0x6000\n"
+      "ld.d $w0, -2440($at) # 0xF678\n"
+      "daui $at, $a0, 0x1235\n"
+      "ld.d $w0, 0x78($at)\n"
+      "ld.d $w0, -256($a0)\n"
+      "ld.b $w0, -511($a0)\n"
+      "daddiu $at, $a0, -513\n"
+      "ld.b $w0, 0($at)\n"
+      "ld.h $w0, -1022($a0)\n"
+      "daddiu $at, $a0, -1026\n"
+      "ld.h $w0, 0($at)\n"
+      "ld.w $w0, -2044($a0)\n"
+      "daddiu $at, $a0, -2052\n"
+      "ld.w $w0, 0($at)\n"
+      "ld.d $w0, -4096($a0)\n"
+      "daddiu $at, $a0, -4104\n"
+      "ld.d $w0, 0($at)\n"
+      "daddiu $at, $a0, -32768\n"
+      "ld.d $w0, 0($at)\n"
+      "daui $at, $a0, 0xABCE\n"
+      "daddiu $at, $at, -8192 # 0xE000\n"
+      "ld.d $w0, 0xF00($at)\n"
+      "daui $at, $a0, 0x8000\n"
+      "dahi $at, $at, 1\n"
+      "daddiu $at, $at, -21504 # 0xAC00\n"
+      "ld.b $w0, -51($at) # 0xFFCD\n";
+  DriverStr(expected, "LoadFpuFromOffset");
+}
+
+TEST_F(AssemblerRISCV64Test, StoreToOffset) {
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A0, 0);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 1);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 256);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 1000);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0x7FFF);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0x8000);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0x8001);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0x10000);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0x12345678);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, -256);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, -32768);
+  __ StoreToOffset(riscv64::kStoreByte, riscv64::A0, riscv64::A1, 0xABCDEF00);
+
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A0, 0);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 2);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 256);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 1000);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0x7FFE);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0x8000);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0x8002);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0x10000);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0x12345678);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, -256);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, -32768);
+  __ StoreToOffset(riscv64::kStoreHalfword, riscv64::A0, riscv64::A1, 0xABCDEF00);
+
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A0, 0);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 4);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 256);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 1000);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0x7FFC);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0x8000);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0x8004);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0x10000);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0x12345678);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, -256);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, -32768);
+  __ StoreToOffset(riscv64::kStoreWord, riscv64::A0, riscv64::A1, 0xABCDEF00);
+
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A0, 0);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 4);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 256);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 1000);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x7FFC);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x8000);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x8004);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x10000);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x12345678);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, -256);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, -32768);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0xABCDEF00);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x7FFFFFF8);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x7FFFFFFC);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x80000000);
+  __ StoreToOffset(riscv64::kStoreDoubleword, riscv64::A0, riscv64::A1, 0x80000004);
+
+  const char* expected =
+      "sb $a0, 0($a0)\n"
+      "sb $a0, 0($a1)\n"
+      "sb $a0, 1($a1)\n"
+      "sb $a0, 256($a1)\n"
+      "sb $a0, 1000($a1)\n"
+      "sb $a0, 0x7FFF($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sb $a0, 8($at)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sb $a0, 9($at)\n"
+      "daui $at, $a1, 1\n"
+      "sb $a0, 0($at)\n"
+      "daui $at, $a1, 4660 # 0x1234\n"
+      "sb $a0, 22136($at) # 0x5678\n"
+      "sb $a0, -256($a1)\n"
+      "sb $a0, -32768($a1)\n"
+      "daui $at, $a1, 43982 # 0xABCE\n"
+      "sb $a0, -4352($at) # 0xEF00\n"
+
+      "sh $a0, 0($a0)\n"
+      "sh $a0, 0($a1)\n"
+      "sh $a0, 2($a1)\n"
+      "sh $a0, 256($a1)\n"
+      "sh $a0, 1000($a1)\n"
+      "sh $a0, 0x7FFE($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sh $a0, 8($at)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sh $a0, 10($at)\n"
+      "daui $at, $a1, 1\n"
+      "sh $a0, 0($at)\n"
+      "daui $at, $a1, 4660 # 0x1234\n"
+      "sh $a0, 22136($at) # 0x5678\n"
+      "sh $a0, -256($a1)\n"
+      "sh $a0, -32768($a1)\n"
+      "daui $at, $a1, 43982 # 0xABCE\n"
+      "sh $a0, -4352($at) # 0xEF00\n"
+
+      "sw $a0, 0($a0)\n"
+      "sw $a0, 0($a1)\n"
+      "sw $a0, 4($a1)\n"
+      "sw $a0, 256($a1)\n"
+      "sw $a0, 1000($a1)\n"
+      "sw $a0, 0x7FFC($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sw $a0, 8($at)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sw $a0, 12($at)\n"
+      "daui $at, $a1, 1\n"
+      "sw $a0, 0($at)\n"
+      "daui $at, $a1, 4660 # 0x1234\n"
+      "sw $a0, 22136($at) # 0x5678\n"
+      "sw $a0, -256($a1)\n"
+      "sw $a0, -32768($a1)\n"
+      "daui $at, $a1, 43982 # 0xABCE\n"
+      "sw $a0, -4352($at) # 0xEF00\n"
+
+      "sd $a0, 0($a0)\n"
+      "sd $a0, 0($a1)\n"
+      "sw $a0, 4($a1)\n"
+      "dsrl32 $t3, $a0, 0\n"
+      "sw $t3, 8($a1)\n"
+      "sd $a0, 256($a1)\n"
+      "sd $a0, 1000($a1)\n"
+      "daddiu $at, $a1, 0x7FF8\n"
+      "sw $a0, 4($at)\n"
+      "dsrl32 $t3, $a0, 0\n"
+      "sw $t3, 8($at)\n"
+      "daddiu $at, $a1, 32760 # 0x7FF8\n"
+      "sd $a0, 8($at)\n"
+      "daddiu $at, $a1, 32760 # 0x7FF8\n"
+      "sw $a0, 12($at)\n"
+      "dsrl32 $t3, $a0, 0\n"
+      "sw $t3, 16($at)\n"
+      "daui $at, $a1, 1\n"
+      "sd $a0, 0($at)\n"
+      "daui $at, $a1, 4660 # 0x1234\n"
+      "sd $a0, 22136($at) # 0x5678\n"
+      "sd $a0, -256($a1)\n"
+      "sd $a0, -32768($a1)\n"
+      "daui $at, $a1, 0xABCE\n"
+      "sd $a0, -0x1100($at)\n"
+      "daui $at, $a1, 0x8000\n"
+      "dahi $at, $at, 1\n"
+      "sd $a0, -8($at)\n"
+      "daui $at, $a1, 0x8000\n"
+      "dahi $at, $at, 1\n"
+      "sw $a0, -4($at) # 0xFFFC\n"
+      "dsrl32 $t3, $a0, 0\n"
+      "sw $t3, 0($at) # 0x0\n"
+      "daui $at, $a1, 0x8000\n"
+      "sd $a0, 0($at) # 0x0\n"
+      "daui $at, $a1, 0x8000\n"
+      "sw $a0, 4($at) # 0x4\n"
+      "dsrl32 $t3, $a0, 0\n"
+      "sw $t3, 8($at) # 0x8\n";
+  DriverStr(expected, "StoreToOffset");
+}
+
+TEST_F(AssemblerRISCV64Test, StoreFpuToOffset) {
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 4);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 256);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0x7FFC);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0x8000);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0x8004);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0x10000);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0x12345678);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, -256);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, -32768);
+  __ StoreFpuToOffset(riscv64::kStoreWord, riscv64::F0, riscv64::A0, 0xABCDEF00);
+
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 4);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 256);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0x7FFC);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0x8000);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0x8004);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0x10000);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0x12345678);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, -256);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, -32768);
+  __ StoreFpuToOffset(riscv64::kStoreDoubleword, riscv64::F0, riscv64::A0, 0xABCDEF00);
+
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 1);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 2);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 8);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 511);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 512);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 513);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 514);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 516);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 1022);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 1024);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 1025);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 1026);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 1028);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 2044);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 2048);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 2049);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 2050);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 2052);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4088);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4096);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4097);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4098);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4100);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 4104);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0x7FFC);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0x8000);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0x10000);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0x12345678);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0x12350078);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -256);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -511);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -513);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -1022);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -1026);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -2044);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -2052);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -4096);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -4104);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, -32768);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0xABCDEF00);
+  __ StoreFpuToOffset(riscv64::kStoreQuadword, riscv64::F0, riscv64::A0, 0x7FFFABCD);
+
+  const char* expected =
+      "swc1 $f0, 0($a0)\n"
+      "swc1 $f0, 4($a0)\n"
+      "swc1 $f0, 256($a0)\n"
+      "swc1 $f0, 0x7FFC($a0)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "swc1 $f0, 8($at)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "swc1 $f0, 12($at)\n"
+      "daui $at, $a0, 1\n"
+      "swc1 $f0, 0($at)\n"
+      "daui $at, $a0, 4660 # 0x1234\n"
+      "swc1 $f0, 22136($at) # 0x5678\n"
+      "swc1 $f0, -256($a0)\n"
+      "swc1 $f0, -32768($a0)\n"
+      "daui $at, $a0, 0xABCE\n"
+      "swc1 $f0, -0x1100($at)\n"
+
+      "sdc1 $f0, 0($a0)\n"
+      "mfhc1 $t3, $f0\n"
+      "swc1 $f0, 4($a0)\n"
+      "sw $t3, 8($a0)\n"
+      "sdc1 $f0, 256($a0)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "mfhc1 $t3, $f0\n"
+      "swc1 $f0, 4($at)\n"
+      "sw $t3, 8($at)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "sdc1 $f0, 8($at)\n"
+      "daddiu $at, $a0, 32760 # 0x7FF8\n"
+      "mfhc1 $t3, $f0\n"
+      "swc1 $f0, 12($at)\n"
+      "sw $t3, 16($at)\n"
+      "daui $at, $a0, 1\n"
+      "sdc1 $f0, 0($at)\n"
+      "daui $at, $a0, 4660 # 0x1234\n"
+      "sdc1 $f0, 22136($at) # 0x5678\n"
+      "sdc1 $f0, -256($a0)\n"
+      "sdc1 $f0, -32768($a0)\n"
+      "daui $at, $a0, 0xABCE\n"
+      "sdc1 $f0, -0x1100($at)\n"
+
+      "st.d $w0, 0($a0)\n"
+      "st.b $w0, 1($a0)\n"
+      "st.h $w0, 2($a0)\n"
+      "st.w $w0, 4($a0)\n"
+      "st.d $w0, 8($a0)\n"
+      "st.b $w0, 511($a0)\n"
+      "st.d $w0, 512($a0)\n"
+      "daddiu $at, $a0, 513\n"
+      "st.b $w0, 0($at)\n"
+      "st.h $w0, 514($a0)\n"
+      "st.w $w0, 516($a0)\n"
+      "st.h $w0, 1022($a0)\n"
+      "st.d $w0, 1024($a0)\n"
+      "daddiu $at, $a0, 1025\n"
+      "st.b $w0, 0($at)\n"
+      "daddiu $at, $a0, 1026\n"
+      "st.h $w0, 0($at)\n"
+      "st.w $w0, 1028($a0)\n"
+      "st.w $w0, 2044($a0)\n"
+      "st.d $w0, 2048($a0)\n"
+      "daddiu $at, $a0, 2049\n"
+      "st.b $w0, 0($at)\n"
+      "daddiu $at, $a0, 2050\n"
+      "st.h $w0, 0($at)\n"
+      "daddiu $at, $a0, 2052\n"
+      "st.w $w0, 0($at)\n"
+      "st.d $w0, 4088($a0)\n"
+      "daddiu $at, $a0, 4096\n"
+      "st.d $w0, 0($at)\n"
+      "daddiu $at, $a0, 4097\n"
+      "st.b $w0, 0($at)\n"
+      "daddiu $at, $a0, 4098\n"
+      "st.h $w0, 0($at)\n"
+      "daddiu $at, $a0, 4100\n"
+      "st.w $w0, 0($at)\n"
+      "daddiu $at, $a0, 4104\n"
+      "st.d $w0, 0($at)\n"
+      "daddiu $at, $a0, 0x7FFC\n"
+      "st.w $w0, 0($at)\n"
+      "daddiu $at, $a0, 0x7FF8\n"
+      "st.d $w0, 8($at)\n"
+      "daui $at, $a0, 0x1\n"
+      "st.d $w0, 0($at)\n"
+      "daui $at, $a0, 0x1234\n"
+      "daddiu $at, $at, 0x6000\n"
+      "st.d $w0, -2440($at) # 0xF678\n"
+      "daui $at, $a0, 0x1235\n"
+      "st.d $w0, 0x78($at)\n"
+      "st.d $w0, -256($a0)\n"
+      "st.b $w0, -511($a0)\n"
+      "daddiu $at, $a0, -513\n"
+      "st.b $w0, 0($at)\n"
+      "st.h $w0, -1022($a0)\n"
+      "daddiu $at, $a0, -1026\n"
+      "st.h $w0, 0($at)\n"
+      "st.w $w0, -2044($a0)\n"
+      "daddiu $at, $a0, -2052\n"
+      "st.w $w0, 0($at)\n"
+      "st.d $w0, -4096($a0)\n"
+      "daddiu $at, $a0, -4104\n"
+      "st.d $w0, 0($at)\n"
+      "daddiu $at, $a0, -32768\n"
+      "st.d $w0, 0($at)\n"
+      "daui $at, $a0, 0xABCE\n"
+      "daddiu $at, $at, -8192 # 0xE000\n"
+      "st.d $w0, 0xF00($at)\n"
+      "daui $at, $a0, 0x8000\n"
+      "dahi $at, $at, 1\n"
+      "daddiu $at, $at, -21504 # 0xAC00\n"
+      "st.b $w0, -51($at) # 0xFFCD\n";
+  DriverStr(expected, "StoreFpuToOffset");
+}
+
+TEST_F(AssemblerRISCV64Test, StoreConstToOffset) {
+  __ StoreConstToOffset(riscv64::kStoreByte, 0xFF, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreHalfword, 0xFFFF, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreWord, 0x12345678, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreDoubleword, 0x123456789ABCDEF0, riscv64::A1, +0, riscv64::T8);
+
+  __ StoreConstToOffset(riscv64::kStoreByte, 0, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreHalfword, 0, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreWord, 0, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreDoubleword, 0, riscv64::A1, +0, riscv64::T8);
+
+  __ StoreConstToOffset(riscv64::kStoreDoubleword, 0x1234567812345678, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreDoubleword, 0x1234567800000000, riscv64::A1, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreDoubleword, 0x0000000012345678, riscv64::A1, +0, riscv64::T8);
+
+  __ StoreConstToOffset(riscv64::kStoreWord, 0, riscv64::T8, +0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreWord, 0x12345678, riscv64::T8, +0, riscv64::T8);
+
+  __ StoreConstToOffset(riscv64::kStoreWord, 0, riscv64::A1, -0xFFF0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreWord, 0x12345678, riscv64::A1, +0xFFF0, riscv64::T8);
+
+  __ StoreConstToOffset(riscv64::kStoreWord, 0, riscv64::T8, -0xFFF0, riscv64::T8);
+  __ StoreConstToOffset(riscv64::kStoreWord, 0x12345678, riscv64::T8, +0xFFF0, riscv64::T8);
+
+  const char* expected =
+      "ori $t8, $zero, 0xFF\n"
+      "sb $t8, 0($a1)\n"
+      "ori $t8, $zero, 0xFFFF\n"
+      "sh $t8, 0($a1)\n"
+      "lui $t8, 0x1234\n"
+      "ori $t8, $t8,0x5678\n"
+      "sw $t8, 0($a1)\n"
+      "lui $t8, 0x9abc\n"
+      "ori $t8, $t8,0xdef0\n"
+      "dahi $t8, $t8, 0x5679\n"
+      "dati $t8, $t8, 0x1234\n"
+      "sd $t8, 0($a1)\n"
+      "sb $zero, 0($a1)\n"
+      "sh $zero, 0($a1)\n"
+      "sw $zero, 0($a1)\n"
+      "sd $zero, 0($a1)\n"
+      "lui $t8, 0x1234\n"
+      "ori $t8, $t8,0x5678\n"
+      "dins $t8, $t8, 0x20, 0x20\n"
+      "sd $t8, 0($a1)\n"
+      "lui $t8, 0x246\n"
+      "ori $t8, $t8, 0x8acf\n"
+      "dsll32 $t8, $t8, 0x3\n"
+      "sd $t8, 0($a1)\n"
+      "lui $t8, 0x1234\n"
+      "ori $t8, $t8, 0x5678\n"
+      "sd $t8, 0($a1)\n"
+      "sw $zero, 0($t8)\n"
+      "lui $at,0x1234\n"
+      "ori $at, $at, 0x5678\n"
+      "sw  $at, 0($t8)\n"
+      "daddiu $at, $a1, -32760 # 0x8008\n"
+      "sw $zero, -32760($at) # 0x8008\n"
+      "daddiu $at, $a1, 32760 # 0x7FF8\n"
+      "lui $t8, 4660 # 0x1234\n"
+      "ori $t8, $t8, 22136 # 0x5678\n"
+      "sw $t8, 32760($at) # 0x7FF8\n"
+      "daddiu $at, $t8, -32760 # 0x8008\n"
+      "sw $zero, -32760($at) # 0x8008\n"
+      "daddiu $at, $t8, 32760 # 0x7FF8\n"
+      "lui $t8, 4660 # 0x1234\n"
+      "ori $t8, $t8, 22136 # 0x5678\n"
+      "sw $t8, 32760($at) # 0x7FF8\n";
+  DriverStr(expected, "StoreConstToOffset");
+}
+//////////////////////////////
+// Loading/adding Constants //
+//////////////////////////////
+
+TEST_F(AssemblerRISCV64Test, LoadConst32) {
+  // IsUint<16>(value)
+  __ LoadConst32(riscv64::V0, 0);
+  __ LoadConst32(riscv64::V0, 65535);
+  // IsInt<16>(value)
+  __ LoadConst32(riscv64::V0, -1);
+  __ LoadConst32(riscv64::V0, -32768);
+  // Everything else
+  __ LoadConst32(riscv64::V0, 65536);
+  __ LoadConst32(riscv64::V0, 65537);
+  __ LoadConst32(riscv64::V0, 2147483647);
+  __ LoadConst32(riscv64::V0, -32769);
+  __ LoadConst32(riscv64::V0, -65536);
+  __ LoadConst32(riscv64::V0, -65537);
+  __ LoadConst32(riscv64::V0, -2147483647);
+  __ LoadConst32(riscv64::V0, -2147483648);
+
+  const char* expected =
+      // IsUint<16>(value)
+      "ori $v0, $zero, 0\n"         // __ LoadConst32(riscv64::V0, 0);
+      "ori $v0, $zero, 65535\n"     // __ LoadConst32(riscv64::V0, 65535);
+      // IsInt<16>(value)
+      "addiu $v0, $zero, -1\n"      // __ LoadConst32(riscv64::V0, -1);
+      "addiu $v0, $zero, -32768\n"  // __ LoadConst32(riscv64::V0, -32768);
+      // Everything else
+      "lui $v0, 1\n"                // __ LoadConst32(riscv64::V0, 65536);
+      "lui $v0, 1\n"                // __ LoadConst32(riscv64::V0, 65537);
+      "ori $v0, 1\n"                //                 "
+      "lui $v0, 32767\n"            // __ LoadConst32(riscv64::V0, 2147483647);
+      "ori $v0, 65535\n"            //                 "
+      "lui $v0, 65535\n"            // __ LoadConst32(riscv64::V0, -32769);
+      "ori $v0, 32767\n"            //                 "
+      "lui $v0, 65535\n"            // __ LoadConst32(riscv64::V0, -65536);
+      "lui $v0, 65534\n"            // __ LoadConst32(riscv64::V0, -65537);
+      "ori $v0, 65535\n"            //                 "
+      "lui $v0, 32768\n"            // __ LoadConst32(riscv64::V0, -2147483647);
+      "ori $v0, 1\n"                //                 "
+      "lui $v0, 32768\n";           // __ LoadConst32(riscv64::V0, -2147483648);
+  DriverStr(expected, "LoadConst32");
+}
+
+TEST_F(AssemblerRISCV64Test, Addiu32) {
+  __ Addiu32(riscv64::A1, riscv64::A2, -0x8000);
+  __ Addiu32(riscv64::A1, riscv64::A2, +0);
+  __ Addiu32(riscv64::A1, riscv64::A2, +0x7FFF);
+  __ Addiu32(riscv64::A1, riscv64::A2, -0x8001);
+  __ Addiu32(riscv64::A1, riscv64::A2, +0x8000);
+  __ Addiu32(riscv64::A1, riscv64::A2, -0x10000);
+  __ Addiu32(riscv64::A1, riscv64::A2, +0x10000);
+  __ Addiu32(riscv64::A1, riscv64::A2, +0x12345678);
+
+  const char* expected =
+      "addiu $a1, $a2, -0x8000\n"
+      "addiu $a1, $a2, 0\n"
+      "addiu $a1, $a2, 0x7FFF\n"
+      "aui $a1, $a2, 0xFFFF\n"
+      "addiu $a1, $a1, 0x7FFF\n"
+      "aui $a1, $a2, 1\n"
+      "addiu $a1, $a1, -0x8000\n"
+      "aui $a1, $a2, 0xFFFF\n"
+      "aui $a1, $a2, 1\n"
+      "aui $a1, $a2, 0x1234\n"
+      "addiu $a1, $a1, 0x5678\n";
+  DriverStr(expected, "Addiu32");
+}
+
+static uint64_t SignExtend16To64(uint16_t n) {
+  return static_cast<int16_t>(n);
+}
+
+// The art::riscv64::Riscv64Assembler::LoadConst64() method uses a template
+// to minimize the number of instructions needed to load a 64-bit constant
+// value into a register. The template calls various methods which emit
+// MIPS machine instructions. This struct (class) uses the same template
+// but overrides the definitions of the methods which emit MIPS instructions
+// to use methods which simulate the operation of the corresponding MIPS
+// instructions. After invoking LoadConst64() the target register should
+// contain the same 64-bit value as was input to LoadConst64(). If the
+// simulated register doesn't contain the correct value then there is probably
+// an error in the template function.
+struct LoadConst64Tester {
+  LoadConst64Tester() {
+    // Initialize all of the registers for simulation to zero.
+    for (int r = 0; r < 32; r++) {
+      regs_[r] = 0;
+    }
+    // Clear all of the path flags.
+    loadconst64_paths_ = art::riscv64::kLoadConst64PathZero;
+  }
+  void Addiu(riscv64::GpuRegister rd, riscv64::GpuRegister rs, uint16_t c) {
+    regs_[rd] = static_cast<int32_t>(regs_[rs] + SignExtend16To64(c));
+  }
+  void Daddiu(riscv64::GpuRegister rd, riscv64::GpuRegister rs, uint16_t c) {
+    regs_[rd] = regs_[rs] + SignExtend16To64(c);
+  }
+  void Dahi(riscv64::GpuRegister rd, uint16_t c) {
+    regs_[rd] += SignExtend16To64(c) << 32;
+  }
+  void Dati(riscv64::GpuRegister rd, uint16_t c) {
+    regs_[rd] += SignExtend16To64(c) << 48;
+  }
+  void Dinsu(riscv64::GpuRegister rt, riscv64::GpuRegister rs, int pos, int size) {
+    CHECK(IsUint<5>(pos - 32)) << pos;
+    CHECK(IsUint<5>(size - 1)) << size;
+    CHECK(IsUint<5>(pos + size - 33)) << pos << " + " << size;
+    uint64_t src_mask = (UINT64_C(1) << size) - 1;
+    uint64_t dsk_mask = ~(src_mask << pos);
+
+    regs_[rt] = (regs_[rt] & dsk_mask) | ((regs_[rs] & src_mask) << pos);
+  }
+  void Dsll(riscv64::GpuRegister rd, riscv64::GpuRegister rt, int shamt) {
+    regs_[rd] = regs_[rt] << (shamt & 0x1f);
+  }
+  void Dsll32(riscv64::GpuRegister rd, riscv64::GpuRegister rt, int shamt) {
+    regs_[rd] = regs_[rt] << (32 + (shamt & 0x1f));
+  }
+  void Dsrl(riscv64::GpuRegister rd, riscv64::GpuRegister rt, int shamt) {
+    regs_[rd] = regs_[rt] >> (shamt & 0x1f);
+  }
+  void Dsrl32(riscv64::GpuRegister rd, riscv64::GpuRegister rt, int shamt) {
+    regs_[rd] = regs_[rt] >> (32 + (shamt & 0x1f));
+  }
+  void Lui(riscv64::GpuRegister rd, uint16_t c) {
+    regs_[rd] = SignExtend16To64(c) << 16;
+  }
+  void Ori(riscv64::GpuRegister rd, riscv64::GpuRegister rs, uint16_t c) {
+    regs_[rd] = regs_[rs] | c;
+  }
+  void LoadConst32(riscv64::GpuRegister rd, int32_t c) {
+    CHECK_NE(rd, 0);
+    riscv64::TemplateLoadConst32<LoadConst64Tester>(this, rd, c);
+    CHECK_EQ(regs_[rd], static_cast<uint64_t>(c));
+  }
+  void LoadConst64(riscv64::GpuRegister rd, int64_t c) {
+    CHECK_NE(rd, 0);
+    riscv64::TemplateLoadConst64<LoadConst64Tester>(this, rd, c);
+    CHECK_EQ(regs_[rd], static_cast<uint64_t>(c));
+  }
+  uint64_t regs_[32];
+
+  // Getter function for loadconst64_paths_.
+  int GetPathsCovered() {
+    return loadconst64_paths_;
+  }
+
+  void RecordLoadConst64Path(int value) {
+    loadconst64_paths_ |= value;
+  }
+
+ private:
+  // This variable holds a bitmask to tell us which paths were taken
+  // through the template function which loads 64-bit values.
+  int loadconst64_paths_;
+};
+
+TEST_F(AssemblerRISCV64Test, LoadConst64) {
+  const uint16_t imms[] = {
+      0, 1, 2, 3, 4, 0x33, 0x66, 0x55, 0x99, 0xaa, 0xcc, 0xff, 0x5500, 0x5555,
+      0x7ffc, 0x7ffd, 0x7ffe, 0x7fff, 0x8000, 0x8001, 0x8002, 0x8003, 0x8004,
+      0xaaaa, 0xfffc, 0xfffd, 0xfffe, 0xffff
+  };
+  unsigned d0, d1, d2, d3;
+  LoadConst64Tester tester;
+
+  union {
+    int64_t v64;
+    uint16_t v16[4];
+  } u;
+
+  for (d3 = 0; d3 < sizeof imms / sizeof imms[0]; d3++) {
+    u.v16[3] = imms[d3];
+
+    for (d2 = 0; d2 < sizeof imms / sizeof imms[0]; d2++) {
+      u.v16[2] = imms[d2];
+
+      for (d1 = 0; d1 < sizeof imms / sizeof imms[0]; d1++) {
+        u.v16[1] = imms[d1];
+
+        for (d0 = 0; d0 < sizeof imms / sizeof imms[0]; d0++) {
+          u.v16[0] = imms[d0];
+
+          tester.LoadConst64(riscv64::V0, u.v64);
+        }
+      }
+    }
+  }
+
+  // Verify that we tested all paths through the "load 64-bit value"
+  // function template.
+  EXPECT_EQ(tester.GetPathsCovered(), art::riscv64::kLoadConst64PathAllPaths);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadFarthestNearLabelAddress) {
+  riscv64::Riscv64Label label;
+  __ LoadLabelAddress(riscv64::V0, &label);
+  constexpr uint32_t kAdduCount = 0x3FFDE;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+  __ Bind(&label);
+
+  std::string expected =
+      "lapc $v0, 1f\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "1:\n";
+  DriverStr(expected, "LoadFarthestNearLabelAddress");
+  EXPECT_EQ(__ GetLabelLocation(&label), (1 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadNearestFarLabelAddress) {
+  riscv64::Riscv64Label label;
+  __ LoadLabelAddress(riscv64::V0, &label);
+  constexpr uint32_t kAdduCount = 0x3FFDF;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+  __ Bind(&label);
+
+  std::string expected =
+      "1:\n"
+      "auipc $at, %hi(2f - 1b)\n"
+      "daddiu $v0, $at, %lo(2f - 1b)\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "2:\n";
+  DriverStr(expected, "LoadNearestFarLabelAddress");
+  EXPECT_EQ(__ GetLabelLocation(&label), (2 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadFarthestNearLiteral) {
+  riscv64::Literal* literal = __ NewLiteral<uint32_t>(0x12345678);
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadWord, literal);
+  constexpr uint32_t kAdduCount = 0x3FFDE;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+
+  std::string expected =
+      "lwpc $v0, 1f\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "1:\n"
+      ".word 0x12345678\n";
+  DriverStr(expected, "LoadFarthestNearLiteral");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (1 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadNearestFarLiteral) {
+  riscv64::Literal* literal = __ NewLiteral<uint32_t>(0x12345678);
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadWord, literal);
+  constexpr uint32_t kAdduCount = 0x3FFDF;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+
+  std::string expected =
+      "1:\n"
+      "auipc $at, %hi(2f - 1b)\n"
+      "lw $v0, %lo(2f - 1b)($at)\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "2:\n"
+      ".word 0x12345678\n";
+  DriverStr(expected, "LoadNearestFarLiteral");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (2 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadFarthestNearLiteralUnsigned) {
+  riscv64::Literal* literal = __ NewLiteral<uint32_t>(0x12345678);
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadUnsignedWord, literal);
+  constexpr uint32_t kAdduCount = 0x3FFDE;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+
+  std::string expected =
+      "lwupc $v0, 1f\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "1:\n"
+      ".word 0x12345678\n";
+  DriverStr(expected, "LoadFarthestNearLiteralUnsigned");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (1 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadNearestFarLiteralUnsigned) {
+  riscv64::Literal* literal = __ NewLiteral<uint32_t>(0x12345678);
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadUnsignedWord, literal);
+  constexpr uint32_t kAdduCount = 0x3FFDF;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+
+  std::string expected =
+      "1:\n"
+      "auipc $at, %hi(2f - 1b)\n"
+      "lwu $v0, %lo(2f - 1b)($at)\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "2:\n"
+      ".word 0x12345678\n";
+  DriverStr(expected, "LoadNearestFarLiteralUnsigned");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (2 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadFarthestNearLiteralLong) {
+  riscv64::Literal* literal = __ NewLiteral<uint64_t>(UINT64_C(0x0123456789ABCDEF));
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadDoubleword, literal);
+  constexpr uint32_t kAdduCount = 0x3FFDD;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+
+  std::string expected =
+      "ldpc $v0, 1f\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "1:\n"
+      ".dword 0x0123456789ABCDEF\n";
+  DriverStr(expected, "LoadFarthestNearLiteralLong");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (1 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LoadNearestFarLiteralLong) {
+  riscv64::Literal* literal = __ NewLiteral<uint64_t>(UINT64_C(0x0123456789ABCDEF));
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadDoubleword, literal);
+  constexpr uint32_t kAdduCount = 0x3FFDE;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+
+  std::string expected =
+      "1:\n"
+      "auipc $at, %hi(2f - 1b)\n"
+      "ld $v0, %lo(2f - 1b)($at)\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "2:\n"
+      ".dword 0x0123456789ABCDEF\n";
+  DriverStr(expected, "LoadNearestFarLiteralLong");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (2 + kAdduCount) * 4);
+}
+
+TEST_F(AssemblerRISCV64Test, LongLiteralAlignmentNop) {
+  riscv64::Literal* literal1 = __ NewLiteral<uint64_t>(UINT64_C(0x0123456789ABCDEF));
+  riscv64::Literal* literal2 = __ NewLiteral<uint64_t>(UINT64_C(0x5555555555555555));
+  riscv64::Literal* literal3 = __ NewLiteral<uint64_t>(UINT64_C(0xAAAAAAAAAAAAAAAA));
+  __ LoadLiteral(riscv64::A1, riscv64::kLoadDoubleword, literal1);
+  __ LoadLiteral(riscv64::A2, riscv64::kLoadDoubleword, literal2);
+  __ LoadLiteral(riscv64::A3, riscv64::kLoadDoubleword, literal3);
+  __ LoadLabelAddress(riscv64::V0, literal1->GetLabel());
+  __ LoadLabelAddress(riscv64::V1, literal2->GetLabel());
+  // A nop will be inserted here before the 64-bit literals.
+
+  std::string expected =
+      "ldpc $a1, 1f\n"
+      // The GNU assembler incorrectly requires the ldpc instruction to be located
+      // at an address that's a multiple of 8. TODO: Remove this workaround if/when
+      // the assembler is fixed.
+      // "ldpc $a2, 2f\n"
+      ".word 0xECD80004\n"
+      "ldpc $a3, 3f\n"
+      "lapc $v0, 1f\n"
+      "lapc $v1, 2f\n"
+      "nop\n"
+      "1:\n"
+      ".dword 0x0123456789ABCDEF\n"
+      "2:\n"
+      ".dword 0x5555555555555555\n"
+      "3:\n"
+      ".dword 0xAAAAAAAAAAAAAAAA\n";
+  DriverStr(expected, "LongLiteralAlignmentNop");
+  EXPECT_EQ(__ GetLabelLocation(literal1->GetLabel()), 6 * 4u);
+  EXPECT_EQ(__ GetLabelLocation(literal2->GetLabel()), 8 * 4u);
+  EXPECT_EQ(__ GetLabelLocation(literal3->GetLabel()), 10 * 4u);
+}
+
+TEST_F(AssemblerRISCV64Test, LongLiteralAlignmentNoNop) {
+  riscv64::Literal* literal1 = __ NewLiteral<uint64_t>(UINT64_C(0x0123456789ABCDEF));
+  riscv64::Literal* literal2 = __ NewLiteral<uint64_t>(UINT64_C(0x5555555555555555));
+  __ LoadLiteral(riscv64::A1, riscv64::kLoadDoubleword, literal1);
+  __ LoadLiteral(riscv64::A2, riscv64::kLoadDoubleword, literal2);
+  __ LoadLabelAddress(riscv64::V0, literal1->GetLabel());
+  __ LoadLabelAddress(riscv64::V1, literal2->GetLabel());
+
+  std::string expected =
+      "ldpc $a1, 1f\n"
+      // The GNU assembler incorrectly requires the ldpc instruction to be located
+      // at an address that's a multiple of 8. TODO: Remove this workaround if/when
+      // the assembler is fixed.
+      // "ldpc $a2, 2f\n"
+      ".word 0xECD80003\n"
+      "lapc $v0, 1f\n"
+      "lapc $v1, 2f\n"
+      "1:\n"
+      ".dword 0x0123456789ABCDEF\n"
+      "2:\n"
+      ".dword 0x5555555555555555\n";
+  DriverStr(expected, "LongLiteralAlignmentNoNop");
+  EXPECT_EQ(__ GetLabelLocation(literal1->GetLabel()), 4 * 4u);
+  EXPECT_EQ(__ GetLabelLocation(literal2->GetLabel()), 6 * 4u);
+}
+
+TEST_F(AssemblerRISCV64Test, FarLongLiteralAlignmentNop) {
+  riscv64::Literal* literal = __ NewLiteral<uint64_t>(UINT64_C(0x0123456789ABCDEF));
+  __ LoadLiteral(riscv64::V0, riscv64::kLoadDoubleword, literal);
+  __ LoadLabelAddress(riscv64::V1, literal->GetLabel());
+  constexpr uint32_t kAdduCount = 0x3FFDF;
+  for (uint32_t i = 0; i != kAdduCount; ++i) {
+    __ Addu(riscv64::ZERO, riscv64::ZERO, riscv64::ZERO);
+  }
+  // A nop will be inserted here before the 64-bit literal.
+
+  std::string expected =
+      "1:\n"
+      "auipc $at, %hi(3f - 1b)\n"
+      "ld $v0, %lo(3f - 1b)($at)\n"
+      "2:\n"
+      "auipc $at, %hi(3f - 2b)\n"
+      "daddiu $v1, $at, %lo(3f - 2b)\n" +
+      RepeatInsn(kAdduCount, "addu $zero, $zero, $zero\n") +
+      "nop\n"
+      "3:\n"
+      ".dword 0x0123456789ABCDEF\n";
+  DriverStr(expected, "FarLongLiteralAlignmentNop");
+  EXPECT_EQ(__ GetLabelLocation(literal->GetLabel()), (5 + kAdduCount) * 4);
+}
+
+// MSA instructions.
+
+TEST_F(AssemblerRISCV64Test, AndV) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::AndV, "and.v ${reg1}, ${reg2}, ${reg3}"), "and.v");
+}
+
+TEST_F(AssemblerRISCV64Test, OrV) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::OrV, "or.v ${reg1}, ${reg2}, ${reg3}"), "or.v");
+}
+
+TEST_F(AssemblerRISCV64Test, NorV) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::NorV, "nor.v ${reg1}, ${reg2}, ${reg3}"), "nor.v");
+}
+
+TEST_F(AssemblerRISCV64Test, XorV) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::XorV, "xor.v ${reg1}, ${reg2}, ${reg3}"), "xor.v");
+}
+
+TEST_F(AssemblerRISCV64Test, AddvB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::AddvB, "addv.b ${reg1}, ${reg2}, ${reg3}"),
+            "addv.b");
+}
+
+TEST_F(AssemblerRISCV64Test, AddvH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::AddvH, "addv.h ${reg1}, ${reg2}, ${reg3}"),
+            "addv.h");
+}
+
+TEST_F(AssemblerRISCV64Test, AddvW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::AddvW, "addv.w ${reg1}, ${reg2}, ${reg3}"),
+            "addv.w");
+}
+
+TEST_F(AssemblerRISCV64Test, AddvD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::AddvD, "addv.d ${reg1}, ${reg2}, ${reg3}"),
+            "addv.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SubvB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SubvB, "subv.b ${reg1}, ${reg2}, ${reg3}"),
+            "subv.b");
+}
+
+TEST_F(AssemblerRISCV64Test, SubvH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SubvH, "subv.h ${reg1}, ${reg2}, ${reg3}"),
+            "subv.h");
+}
+
+TEST_F(AssemblerRISCV64Test, SubvW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SubvW, "subv.w ${reg1}, ${reg2}, ${reg3}"),
+            "subv.w");
+}
+
+TEST_F(AssemblerRISCV64Test, SubvD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SubvD, "subv.d ${reg1}, ${reg2}, ${reg3}"),
+            "subv.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_sB, "asub_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "asub_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_sH, "asub_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "asub_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_sW, "asub_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "asub_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_sD, "asub_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "asub_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_uB, "asub_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "asub_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_uH, "asub_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "asub_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_uW, "asub_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "asub_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Asub_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Asub_uD, "asub_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "asub_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MulvB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MulvB, "mulv.b ${reg1}, ${reg2}, ${reg3}"),
+            "mulv.b");
+}
+
+TEST_F(AssemblerRISCV64Test, MulvH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MulvH, "mulv.h ${reg1}, ${reg2}, ${reg3}"),
+            "mulv.h");
+}
+
+TEST_F(AssemblerRISCV64Test, MulvW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MulvW, "mulv.w ${reg1}, ${reg2}, ${reg3}"),
+            "mulv.w");
+}
+
+TEST_F(AssemblerRISCV64Test, MulvD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MulvD, "mulv.d ${reg1}, ${reg2}, ${reg3}"),
+            "mulv.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_sB, "div_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "div_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_sH, "div_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "div_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_sW, "div_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "div_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_sD, "div_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "div_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_uB, "div_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "div_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_uH, "div_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "div_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_uW, "div_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "div_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Div_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Div_uD, "div_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "div_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_sB, "mod_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "mod_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_sH, "mod_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "mod_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_sW, "mod_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "mod_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_sD, "mod_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "mod_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_uB, "mod_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "mod_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_uH, "mod_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "mod_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_uW, "mod_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "mod_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Mod_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Mod_uD, "mod_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "mod_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Add_aB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Add_aB, "add_a.b ${reg1}, ${reg2}, ${reg3}"),
+            "add_a.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Add_aH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Add_aH, "add_a.h ${reg1}, ${reg2}, ${reg3}"),
+            "add_a.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Add_aW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Add_aW, "add_a.w ${reg1}, ${reg2}, ${reg3}"),
+            "add_a.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Add_aD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Add_aD, "add_a.d ${reg1}, ${reg2}, ${reg3}"),
+            "add_a.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_sB, "ave_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "ave_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_sH, "ave_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "ave_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_sW, "ave_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "ave_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_sD, "ave_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "ave_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_uB, "ave_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "ave_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_uH, "ave_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "ave_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_uW, "ave_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "ave_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Ave_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Ave_uD, "ave_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "ave_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_sB, "aver_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "aver_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_sH, "aver_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "aver_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_sW, "aver_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "aver_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_sD, "aver_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "aver_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_uB, "aver_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "aver_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_uH, "aver_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "aver_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_uW, "aver_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "aver_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Aver_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Aver_uD, "aver_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "aver_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_sB, "max_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "max_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_sH, "max_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "max_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_sW, "max_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "max_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_sD, "max_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "max_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_uB, "max_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "max_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_uH, "max_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "max_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_uW, "max_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "max_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Max_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Max_uD, "max_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "max_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_sB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_sB, "min_s.b ${reg1}, ${reg2}, ${reg3}"),
+            "min_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_sH, "min_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "min_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_sW, "min_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "min_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_sD, "min_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "min_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_uB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_uB, "min_u.b ${reg1}, ${reg2}, ${reg3}"),
+            "min_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_uH, "min_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "min_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_uW, "min_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "min_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Min_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Min_uD, "min_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "min_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FaddW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FaddW, "fadd.w ${reg1}, ${reg2}, ${reg3}"),
+            "fadd.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FaddD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FaddD, "fadd.d ${reg1}, ${reg2}, ${reg3}"),
+            "fadd.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FsubW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FsubW, "fsub.w ${reg1}, ${reg2}, ${reg3}"),
+            "fsub.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FsubD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FsubD, "fsub.d ${reg1}, ${reg2}, ${reg3}"),
+            "fsub.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FmulW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmulW, "fmul.w ${reg1}, ${reg2}, ${reg3}"),
+            "fmul.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FmulD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmulD, "fmul.d ${reg1}, ${reg2}, ${reg3}"),
+            "fmul.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FdivW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FdivW, "fdiv.w ${reg1}, ${reg2}, ${reg3}"),
+            "fdiv.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FdivD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FdivD, "fdiv.d ${reg1}, ${reg2}, ${reg3}"),
+            "fdiv.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FmaxW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmaxW, "fmax.w ${reg1}, ${reg2}, ${reg3}"),
+            "fmax.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FmaxD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmaxD, "fmax.d ${reg1}, ${reg2}, ${reg3}"),
+            "fmax.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FminW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FminW, "fmin.w ${reg1}, ${reg2}, ${reg3}"),
+            "fmin.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FminD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FminD, "fmin.d ${reg1}, ${reg2}, ${reg3}"),
+            "fmin.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Ffint_sW) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::Ffint_sW, "ffint_s.w ${reg1}, ${reg2}"),
+            "ffint_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Ffint_sD) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::Ffint_sD, "ffint_s.d ${reg1}, ${reg2}"),
+            "ffint_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Ftint_sW) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::Ftint_sW, "ftint_s.w ${reg1}, ${reg2}"),
+            "ftint_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Ftint_sD) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::Ftint_sD, "ftint_s.d ${reg1}, ${reg2}"),
+            "ftint_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SllB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SllB, "sll.b ${reg1}, ${reg2}, ${reg3}"), "sll.b");
+}
+
+TEST_F(AssemblerRISCV64Test, SllH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SllH, "sll.h ${reg1}, ${reg2}, ${reg3}"), "sll.h");
+}
+
+TEST_F(AssemblerRISCV64Test, SllW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SllW, "sll.w ${reg1}, ${reg2}, ${reg3}"), "sll.w");
+}
+
+TEST_F(AssemblerRISCV64Test, SllD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SllD, "sll.d ${reg1}, ${reg2}, ${reg3}"), "sll.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SraB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SraB, "sra.b ${reg1}, ${reg2}, ${reg3}"), "sra.b");
+}
+
+TEST_F(AssemblerRISCV64Test, SraH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SraH, "sra.h ${reg1}, ${reg2}, ${reg3}"), "sra.h");
+}
+
+TEST_F(AssemblerRISCV64Test, SraW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SraW, "sra.w ${reg1}, ${reg2}, ${reg3}"), "sra.w");
+}
+
+TEST_F(AssemblerRISCV64Test, SraD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SraD, "sra.d ${reg1}, ${reg2}, ${reg3}"), "sra.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SrlB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SrlB, "srl.b ${reg1}, ${reg2}, ${reg3}"), "srl.b");
+}
+
+TEST_F(AssemblerRISCV64Test, SrlH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SrlH, "srl.h ${reg1}, ${reg2}, ${reg3}"), "srl.h");
+}
+
+TEST_F(AssemblerRISCV64Test, SrlW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SrlW, "srl.w ${reg1}, ${reg2}, ${reg3}"), "srl.w");
+}
+
+TEST_F(AssemblerRISCV64Test, SrlD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::SrlD, "srl.d ${reg1}, ${reg2}, ${reg3}"), "srl.d");
+}
+
+TEST_F(AssemblerRISCV64Test, SlliB) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SlliB, 3, "slli.b ${reg1}, ${reg2}, {imm}"),
+            "slli.b");
+}
+
+TEST_F(AssemblerRISCV64Test, SlliH) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SlliH, 4, "slli.h ${reg1}, ${reg2}, {imm}"),
+            "slli.h");
+}
+
+TEST_F(AssemblerRISCV64Test, SlliW) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SlliW, 5, "slli.w ${reg1}, ${reg2}, {imm}"),
+            "slli.w");
+}
+
+TEST_F(AssemblerRISCV64Test, SlliD) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SlliD, 6, "slli.d ${reg1}, ${reg2}, {imm}"),
+            "slli.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MoveV) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::MoveV, "move.v ${reg1}, ${reg2}"), "move.v");
+}
+
+TEST_F(AssemblerRISCV64Test, SplatiB) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SplatiB, 4, "splati.b ${reg1}, ${reg2}[{imm}]"),
+            "splati.b");
+}
+
+TEST_F(AssemblerRISCV64Test, SplatiH) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SplatiH, 3, "splati.h ${reg1}, ${reg2}[{imm}]"),
+            "splati.h");
+}
+
+TEST_F(AssemblerRISCV64Test, SplatiW) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SplatiW, 2, "splati.w ${reg1}, ${reg2}[{imm}]"),
+            "splati.w");
+}
+
+TEST_F(AssemblerRISCV64Test, SplatiD) {
+  DriverStr(RepeatVVIb(&riscv64::Riscv64Assembler::SplatiD, 1, "splati.d ${reg1}, ${reg2}[{imm}]"),
+            "splati.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_sB) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_sB, 4, "copy_s.b ${reg1}, ${reg2}[{imm}]"),
+            "copy_s.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_sH) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_sH, 3, "copy_s.h ${reg1}, ${reg2}[{imm}]"),
+            "copy_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_sW) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_sW, 2, "copy_s.w ${reg1}, ${reg2}[{imm}]"),
+            "copy_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_sD) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_sD, 1, "copy_s.d ${reg1}, ${reg2}[{imm}]"),
+            "copy_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_uB) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_uB, 4, "copy_u.b ${reg1}, ${reg2}[{imm}]"),
+            "copy_u.b");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_uH) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_uH, 3, "copy_u.h ${reg1}, ${reg2}[{imm}]"),
+            "copy_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Copy_uW) {
+  DriverStr(RepeatRVIb(&riscv64::Riscv64Assembler::Copy_uW, 2, "copy_u.w ${reg1}, ${reg2}[{imm}]"),
+            "copy_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, InsertB) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::InsertB, 4, "insert.b ${reg1}[{imm}], ${reg2}"),
+            "insert.b");
+}
+
+TEST_F(AssemblerRISCV64Test, InsertH) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::InsertH, 3, "insert.h ${reg1}[{imm}], ${reg2}"),
+            "insert.h");
+}
+
+TEST_F(AssemblerRISCV64Test, InsertW) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::InsertW, 2, "insert.w ${reg1}[{imm}], ${reg2}"),
+            "insert.w");
+}
+
+TEST_F(AssemblerRISCV64Test, InsertD) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::InsertD, 1, "insert.d ${reg1}[{imm}], ${reg2}"),
+            "insert.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FillB) {
+  DriverStr(RepeatVR(&riscv64::Riscv64Assembler::FillB, "fill.b ${reg1}, ${reg2}"), "fill.b");
+}
+
+TEST_F(AssemblerRISCV64Test, FillH) {
+  DriverStr(RepeatVR(&riscv64::Riscv64Assembler::FillH, "fill.h ${reg1}, ${reg2}"), "fill.h");
+}
+
+TEST_F(AssemblerRISCV64Test, FillW) {
+  DriverStr(RepeatVR(&riscv64::Riscv64Assembler::FillW, "fill.w ${reg1}, ${reg2}"), "fill.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FillD) {
+  DriverStr(RepeatVR(&riscv64::Riscv64Assembler::FillD, "fill.d ${reg1}, ${reg2}"), "fill.d");
+}
+
+TEST_F(AssemblerRISCV64Test, PcntB) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::PcntB, "pcnt.b ${reg1}, ${reg2}"), "pcnt.b");
+}
+
+TEST_F(AssemblerRISCV64Test, PcntH) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::PcntH, "pcnt.h ${reg1}, ${reg2}"), "pcnt.h");
+}
+
+TEST_F(AssemblerRISCV64Test, PcntW) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::PcntW, "pcnt.w ${reg1}, ${reg2}"), "pcnt.w");
+}
+
+TEST_F(AssemblerRISCV64Test, PcntD) {
+  DriverStr(RepeatVV(&riscv64::Riscv64Assembler::PcntD, "pcnt.d ${reg1}, ${reg2}"), "pcnt.d");
+}
+
+TEST_F(AssemblerRISCV64Test, LdiB) {
+  DriverStr(RepeatVIb(&riscv64::Riscv64Assembler::LdiB, -8, "ldi.b ${reg}, {imm}"), "ldi.b");
+}
+
+TEST_F(AssemblerRISCV64Test, LdiH) {
+  DriverStr(RepeatVIb(&riscv64::Riscv64Assembler::LdiH, -10, "ldi.h ${reg}, {imm}"), "ldi.h");
+}
+
+TEST_F(AssemblerRISCV64Test, LdiW) {
+  DriverStr(RepeatVIb(&riscv64::Riscv64Assembler::LdiW, -10, "ldi.w ${reg}, {imm}"), "ldi.w");
+}
+
+TEST_F(AssemblerRISCV64Test, LdiD) {
+  DriverStr(RepeatVIb(&riscv64::Riscv64Assembler::LdiD, -10, "ldi.d ${reg}, {imm}"), "ldi.d");
+}
+
+TEST_F(AssemblerRISCV64Test, LdB) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::LdB, -10, "ld.b ${reg1}, {imm}(${reg2})"), "ld.b");
+}
+
+TEST_F(AssemblerRISCV64Test, LdH) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::LdH, -10, "ld.h ${reg1}, {imm}(${reg2})", 0, 2),
+            "ld.h");
+}
+
+TEST_F(AssemblerRISCV64Test, LdW) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::LdW, -10, "ld.w ${reg1}, {imm}(${reg2})", 0, 4),
+            "ld.w");
+}
+
+TEST_F(AssemblerRISCV64Test, LdD) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::LdD, -10, "ld.d ${reg1}, {imm}(${reg2})", 0, 8),
+            "ld.d");
+}
+
+TEST_F(AssemblerRISCV64Test, StB) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::StB, -10, "st.b ${reg1}, {imm}(${reg2})"), "st.b");
+}
+
+TEST_F(AssemblerRISCV64Test, StH) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::StH, -10, "st.h ${reg1}, {imm}(${reg2})", 0, 2),
+            "st.h");
+}
+
+TEST_F(AssemblerRISCV64Test, StW) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::StW, -10, "st.w ${reg1}, {imm}(${reg2})", 0, 4),
+            "st.w");
+}
+
+TEST_F(AssemblerRISCV64Test, StD) {
+  DriverStr(RepeatVRIb(&riscv64::Riscv64Assembler::StD, -10, "st.d ${reg1}, {imm}(${reg2})", 0, 8),
+            "st.d");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvlB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvlB, "ilvl.b ${reg1}, ${reg2}, ${reg3}"),
+            "ilvl.b");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvlH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvlH, "ilvl.h ${reg1}, ${reg2}, ${reg3}"),
+            "ilvl.h");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvlW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvlW, "ilvl.w ${reg1}, ${reg2}, ${reg3}"),
+            "ilvl.w");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvlD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvlD, "ilvl.d ${reg1}, ${reg2}, ${reg3}"),
+            "ilvl.d");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvrB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvrB, "ilvr.b ${reg1}, ${reg2}, ${reg3}"),
+            "ilvr.b");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvrH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvrH, "ilvr.h ${reg1}, ${reg2}, ${reg3}"),
+            "ilvr.h");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvrW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvrW, "ilvr.w ${reg1}, ${reg2}, ${reg3}"),
+            "ilvr.w");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvrD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvrD, "ilvr.d ${reg1}, ${reg2}, ${reg3}"),
+            "ilvr.d");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvevB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvevB, "ilvev.b ${reg1}, ${reg2}, ${reg3}"),
+            "ilvev.b");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvevH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvevH, "ilvev.h ${reg1}, ${reg2}, ${reg3}"),
+            "ilvev.h");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvevW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvevW, "ilvev.w ${reg1}, ${reg2}, ${reg3}"),
+            "ilvev.w");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvevD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvevD, "ilvev.d ${reg1}, ${reg2}, ${reg3}"),
+            "ilvev.d");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvodB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvodB, "ilvod.b ${reg1}, ${reg2}, ${reg3}"),
+            "ilvod.b");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvodH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvodH, "ilvod.h ${reg1}, ${reg2}, ${reg3}"),
+            "ilvod.h");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvodW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvodW, "ilvod.w ${reg1}, ${reg2}, ${reg3}"),
+            "ilvod.w");
+}
+
+TEST_F(AssemblerRISCV64Test, IlvodD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::IlvodD, "ilvod.d ${reg1}, ${reg2}, ${reg3}"),
+            "ilvod.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MaddvB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MaddvB, "maddv.b ${reg1}, ${reg2}, ${reg3}"),
+            "maddv.b");
+}
+
+TEST_F(AssemblerRISCV64Test, MaddvH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MaddvH, "maddv.h ${reg1}, ${reg2}, ${reg3}"),
+            "maddv.h");
+}
+
+TEST_F(AssemblerRISCV64Test, MaddvW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MaddvW, "maddv.w ${reg1}, ${reg2}, ${reg3}"),
+            "maddv.w");
+}
+
+TEST_F(AssemblerRISCV64Test, MaddvD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MaddvD, "maddv.d ${reg1}, ${reg2}, ${reg3}"),
+            "maddv.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Hadd_sH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Hadd_sH, "hadd_s.h ${reg1}, ${reg2}, ${reg3}"),
+            "hadd_s.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Hadd_sW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Hadd_sW, "hadd_s.w ${reg1}, ${reg2}, ${reg3}"),
+            "hadd_s.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Hadd_sD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Hadd_sD, "hadd_s.d ${reg1}, ${reg2}, ${reg3}"),
+            "hadd_s.d");
+}
+
+TEST_F(AssemblerRISCV64Test, Hadd_uH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Hadd_uH, "hadd_u.h ${reg1}, ${reg2}, ${reg3}"),
+            "hadd_u.h");
+}
+
+TEST_F(AssemblerRISCV64Test, Hadd_uW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Hadd_uW, "hadd_u.w ${reg1}, ${reg2}, ${reg3}"),
+            "hadd_u.w");
+}
+
+TEST_F(AssemblerRISCV64Test, Hadd_uD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::Hadd_uD, "hadd_u.d ${reg1}, ${reg2}, ${reg3}"),
+            "hadd_u.d");
+}
+
+TEST_F(AssemblerRISCV64Test, MsubvB) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MsubvB, "msubv.b ${reg1}, ${reg2}, ${reg3}"),
+            "msubv.b");
+}
+
+TEST_F(AssemblerRISCV64Test, MsubvH) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MsubvH, "msubv.h ${reg1}, ${reg2}, ${reg3}"),
+            "msubv.h");
+}
+
+TEST_F(AssemblerRISCV64Test, MsubvW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MsubvW, "msubv.w ${reg1}, ${reg2}, ${reg3}"),
+            "msubv.w");
+}
+
+TEST_F(AssemblerRISCV64Test, MsubvD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::MsubvD, "msubv.d ${reg1}, ${reg2}, ${reg3}"),
+            "msubv.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FmaddW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmaddW, "fmadd.w ${reg1}, ${reg2}, ${reg3}"),
+            "fmadd.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FmaddD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmaddD, "fmadd.d ${reg1}, ${reg2}, ${reg3}"),
+            "fmadd.d");
+}
+
+TEST_F(AssemblerRISCV64Test, FmsubW) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmsubW, "fmsub.w ${reg1}, ${reg2}, ${reg3}"),
+            "fmsub.w");
+}
+
+TEST_F(AssemblerRISCV64Test, FmsubD) {
+  DriverStr(RepeatVVV(&riscv64::Riscv64Assembler::FmsubD, "fmsub.d ${reg1}, ${reg2}, ${reg3}"),
+            "fmsub.d");
+}
+#endif
+#if 1
+#if  TEST_RV32I_R
+TEST_F(AssemblerRISCV64Test, Add) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Add, "add {reg1}, {reg2}, {reg3}"), "Add");
+}
+
+TEST_F(AssemblerRISCV64Test, Sub) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Sub, "sub {reg1}, {reg2}, {reg3}"), "Sub");
+}
+
+TEST_F(AssemblerRISCV64Test, Sll) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Sll, "sll {reg1}, {reg2}, {reg3}"), "Sll");
+}
+
+TEST_F(AssemblerRISCV64Test, Slt) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Slt, "slt {reg1}, {reg2}, {reg3}"), "Slt");
+}
+
+TEST_F(AssemblerRISCV64Test, Sltu) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Sltu, "sltu {reg1}, {reg2}, {reg3}"), "Sltu");
+}
+
+TEST_F(AssemblerRISCV64Test, Xor) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Xor, "xor {reg1}, {reg2}, {reg3}"), "Xor");
+}
+
+TEST_F(AssemblerRISCV64Test, Srl) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Srl, "srl {reg1}, {reg2}, {reg3}"), "Srl");
+}
+
+TEST_F(AssemblerRISCV64Test, Sra) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Sra, "sra {reg1}, {reg2}, {reg3}"), "Sra");
+}
+
+TEST_F(AssemblerRISCV64Test, Or) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Or, "or {reg1}, {reg2}, {reg3}"), "Or");
+}
+
+TEST_F(AssemblerRISCV64Test, And) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::And, "and {reg1}, {reg2}, {reg3}"), "And");
+}
+#endif
+
+#if TEST_RV32I_I
+TEST_F(AssemblerRISCV64Test, Jalr) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Jalr, -11, "jalr {reg1}, {imm}({reg2})"), "Jalr");
+}
+TEST_F(AssemblerRISCV64Test, Lb) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Lb, -11, "lb {reg1}, {imm}({reg2})"), "Lb");
+}
+
+TEST_F(AssemblerRISCV64Test, Lh) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Lh, -11, "lh {reg1}, {imm}({reg2})"), "Lh");
+}
+
+TEST_F(AssemblerRISCV64Test, Lw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Lw, -11, "lw {reg1}, {imm}({reg2})"), "Lw");
+}
+
+TEST_F(AssemblerRISCV64Test, Lbu) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Lbu, -11, "lbu {reg1}, {imm}({reg2})"), "Lbu");
+}
+
+TEST_F(AssemblerRISCV64Test, Lhu) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Lhu, -11, "lhu {reg1}, {imm}({reg2})"), "Lhu");
+}
+
+TEST_F(AssemblerRISCV64Test, Addi) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Addi, -11, "addi {reg1}, {reg2}, {imm}"), "Addi");
+}
+
+TEST_F(AssemblerRISCV64Test, Slti) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Slti, -11, "slti {reg1}, {reg2}, {imm}"), "Slti");
+}
+
+TEST_F(AssemblerRISCV64Test, Sltiu) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Sltiu, -11, "sltiu {reg1}, {reg2}, {imm}"), "Sltiu");
+}
+
+TEST_F(AssemblerRISCV64Test, Xori) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Xori, -11, "xori {reg1}, {reg2}, {imm}"), "Xori");
+}
+
+TEST_F(AssemblerRISCV64Test, Ori) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Ori, -11, "ori {reg1}, {reg2}, {imm}"), "Ori");
+}
+
+TEST_F(AssemblerRISCV64Test, Andi) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Andi, -11, "andi {reg1}, {reg2}, {imm}"), "Andi");
+}
+
+
+TEST_F(AssemblerRISCV64Test, Fence) {
+  __ Fence(0x02, 0x03);
+  DriverStr("fence r, rw", "Fence");
+}
+
+TEST_F(AssemblerRISCV64Test, FenceI) {
+  __ FenceI();
+  DriverStr("fence.i", "FenceI");
+}
+
+TEST_F(AssemblerRISCV64Test, Ecall) {
+  __ Ecall();
+  DriverStr("ecall", "Ecall");
+}
+
+TEST_F(AssemblerRISCV64Test, Ebreak) {
+  __ Ebreak();
+  DriverStr("ebreak", "Ebreak");
+}
+
+TEST_F(AssemblerRISCV64Test, Csrrw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Csrrw, 12, "csrrw {reg1}, {imm}, {reg2}"), "Csrrw");
+}
+
+TEST_F(AssemblerRISCV64Test, Csrrs) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Csrrs, 12, "csrrs {reg1}, {imm}, {reg2}"), "Csrrs");
+}
+
+TEST_F(AssemblerRISCV64Test, Csrrc) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Csrrc, 12, "csrrc {reg1}, {imm}, {reg2}"), "Csrrc");
+}
+
+TEST_F(AssemblerRISCV64Test, Csrrwi) {
+  __ Csrrwi(riscv64::A0, 0, 1);
+  DriverStr("csrrwi a0, ustatus, 1\n"
+    , "Csrrwi");
+}
+
+TEST_F(AssemblerRISCV64Test, Csrrsi) {
+  __ Csrrsi(riscv64::A0, 0, 1);
+  DriverStr("csrrsi a0, ustatus, 1\n"
+    , "Csrrsi");
+}
+
+TEST_F(AssemblerRISCV64Test, Csrrci) {
+  __ Csrrci(riscv64::A0, 0, 1);
+  DriverStr("csrrci a0, ustatus, 1\n"
+    , "Csrrci");
+}
+#endif
+
+#if TEST_RV32I_S
+TEST_F(AssemblerRISCV64Test, Sb) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Sb, -11, "sb {reg1}, {imm}({reg2})"), "Sb");
+}
+
+TEST_F(AssemblerRISCV64Test, Sh) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Sh, -11, "sh {reg1}, {imm}({reg2})"), "Sh");
+}
+
+TEST_F(AssemblerRISCV64Test, Sw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Sw, -11, "sw {reg1}, {imm}({reg2})"), "Sw");
+}
+#endif
+
+#if TEST_RV32I_B
+TEST_F(AssemblerRISCV64Test, Beq) {
+  BranchCondTwoRegsHelper1(&riscv64::Riscv64Assembler::Beq, "Beq", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, Bne) {
+  BranchCondTwoRegsHelper1(&riscv64::Riscv64Assembler::Bne, "Bne", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, Blt) {
+  BranchCondTwoRegsHelper1(&riscv64::Riscv64Assembler::Blt, "Blt", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, Bge) {
+  BranchCondTwoRegsHelper1(&riscv64::Riscv64Assembler::Bge, "Bge", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, Bltu) {
+  BranchCondTwoRegsHelper1(&riscv64::Riscv64Assembler::Bltu, "Bltu", /* is_bare= */ true);
+}
+
+TEST_F(AssemblerRISCV64Test, Bgeu) {
+  BranchCondTwoRegsHelper1(&riscv64::Riscv64Assembler::Bgeu, "Bgeu", /* is_bare= */ true);
+}
+#endif
+
+#if TEST_RV32I_U
+TEST_F(AssemblerRISCV64Test, Lui) {
+  DriverStr(RepeatrIb(&riscv64::Riscv64Assembler::Lui, 20, "lui {reg}, {imm}"), "Lui");
+}
+
+TEST_F(AssemblerRISCV64Test, Auipc) {
+  DriverStr(RepeatrIb(&riscv64::Riscv64Assembler::Auipc, 20, "auipc {reg}, {imm}"), "Auipc");
+}
+#endif
+
+#if TEST_RV32I_J
+TEST_F(AssemblerRISCV64Test, Jal) {
+  BranchHelper1(&riscv64::Riscv64Assembler::Jal, "Jal");
+}
+#endif
+
+#if TEST_RV64I_R
+TEST_F(AssemblerRISCV64Test, Addw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Addw, "addw {reg1}, {reg2}, {reg3}"), "Addw");
+}
+
+TEST_F(AssemblerRISCV64Test, Subw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Subw, "subw {reg1}, {reg2}, {reg3}"), "Subw");
+}
+
+TEST_F(AssemblerRISCV64Test, Sllw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Sllw, "sllw {reg1}, {reg2}, {reg3}"), "Sllw");
+}
+
+TEST_F(AssemblerRISCV64Test, Srlw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Srlw, "srlw {reg1}, {reg2}, {reg3}"), "Srlw");
+}
+
+TEST_F(AssemblerRISCV64Test, Sraw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Sraw, "sraw {reg1}, {reg2}, {reg3}"), "Sraw");
+}
+#endif
+
+#if TEST_RV64I_I
+TEST_F(AssemblerRISCV64Test, Lwu) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Lwu, -11, "lwu {reg1}, {imm}({reg2})"), "Lwu");
+}
+
+TEST_F(AssemblerRISCV64Test, Ld) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Ld, -11, "ld {reg1}, {imm}({reg2})"), "Ld");
+}
+
+TEST_F(AssemblerRISCV64Test, Slli) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Slli, 6, "slli {reg1}, {reg2}, {imm}"), "Slli");
+}
+
+TEST_F(AssemblerRISCV64Test, Srli) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Srli, 6, "srli {reg1}, {reg2}, {imm}"), "Srli");
+}
+
+TEST_F(AssemblerRISCV64Test, Srai) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Srai, 6, "srai {reg1}, {reg2}, {imm}"), "Srai");
+}
+
+TEST_F(AssemblerRISCV64Test, Addiw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Addiw, -11, "addiw {reg1}, {reg2}, {imm}"), "Addiw");
+}
+
+TEST_F(AssemblerRISCV64Test, Slliw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Slliw, 5, "slliw {reg1}, {reg2}, {imm}"), "Slliw");
+}
+
+TEST_F(AssemblerRISCV64Test, Srliw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Srliw, 5, "srliw {reg1}, {reg2}, {imm}"), "Srliw");
+}
+
+TEST_F(AssemblerRISCV64Test, Sraiw) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Sraiw, 5, "sraiw {reg1}, {reg2}, {imm}"), "Sraiw");
+}
+#endif
+
+#if TEST_RV64I_S
+TEST_F(AssemblerRISCV64Test, Sd) {
+  DriverStr(RepeatrrIb(&riscv64::Riscv64Assembler::Sd, -11, "sd {reg1}, {imm}({reg2})"), "Sd");
+}
+#endif
+
+#if TEST_RV32M_R
+TEST_F(AssemblerRISCV64Test, Mul) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Mul, "mul {reg1}, {reg2}, {reg3}"), "Mul");
+}
+
+TEST_F(AssemblerRISCV64Test, Mulh) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Mulh, "mulh {reg1}, {reg2}, {reg3}"), "Mulh");
+}
+
+TEST_F(AssemblerRISCV64Test, Mulhsu) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Mulhsu, "mulhsu {reg1}, {reg2}, {reg3}"), "Mulhsu");
+}
+
+TEST_F(AssemblerRISCV64Test, Mulhu) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Mulhu, "mulhu {reg1}, {reg2}, {reg3}"), "Mulhu");
+}
+
+TEST_F(AssemblerRISCV64Test, Div) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Div, "div {reg1}, {reg2}, {reg3}"), "Div");
+}
+
+TEST_F(AssemblerRISCV64Test, Divu) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Divu, "divu {reg1}, {reg2}, {reg3}"), "Divu");
+}
+
+TEST_F(AssemblerRISCV64Test, Rem) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Rem, "rem {reg1}, {reg2}, {reg3}"), "Rem");
+}
+
+TEST_F(AssemblerRISCV64Test, Remu) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Remu, "remu {reg1}, {reg2}, {reg3}"), "Remu");
+}
+#endif
+
+#if TEST_RV64M_R
+TEST_F(AssemblerRISCV64Test, Mulw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Mulw, "mulw {reg1}, {reg2}, {reg3}"), "Mulw");
+}
+
+TEST_F(AssemblerRISCV64Test, Divw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Divw, "divw {reg1}, {reg2}, {reg3}"), "Divw");
+}
+
+TEST_F(AssemblerRISCV64Test, Divuw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Divuw, "divuw {reg1}, {reg2}, {reg3}"), "Divuw");
+}
+
+TEST_F(AssemblerRISCV64Test, Remw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Remw, "remw {reg1}, {reg2}, {reg3}"), "Remw");
+}
+
+TEST_F(AssemblerRISCV64Test, Remuw) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::Remuw, "remuw {reg1}, {reg2}, {reg3}"), "Remuw");
+}
+#endif
+
+#if TEST_RV32A_R
+TEST_F(AssemblerRISCV64Test, LrW) {
+  DriverStr(Repeatrr(&riscv64::Riscv64Assembler::LrW, "lr.w {reg1}, ({reg2})"), "LrW");
+}
+
+TEST_F(AssemblerRISCV64Test, ScW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::ScW, "sc.w {reg1}, {reg2}, ({reg3})"), "ScW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoSwapW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoSwapW, "amoswap.w {reg1}, {reg2}, ({reg3})"), "AmoSwapW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoAddW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoAddW, "amoadd.w {reg1}, {reg2}, ({reg3})"), "AmoAddW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoXorW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoXorW, "amoxor.w {reg1}, {reg2}, ({reg3})"), "AmoXorW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoAndW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoAndW, "amoand.w {reg1}, {reg2}, ({reg3})"), "AmoAndW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoOrW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoOrW, "amoor.w {reg1}, {reg2}, ({reg3})"), "AmoOrW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMinW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMinW, "amomin.w {reg1}, {reg2}, ({reg3})"), "AmoMinW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMaxW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMaxW, "amomax.w {reg1}, {reg2}, ({reg3})"), "AmoMaxW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMinuW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMinuW, "amominu.w {reg1}, {reg2}, ({reg3})"), "AmoMinuW");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMaxuW) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMaxuW, "amomaxu.w {reg1}, {reg2}, ({reg3})"), "AmoMaxuW");
+}
+#endif
+
+#if TEST_RV64A_R
+TEST_F(AssemblerRISCV64Test, LrD) {
+  DriverStr(Repeatrr(&riscv64::Riscv64Assembler::LrD, "lr.d {reg1}, ({reg2})"), "LrD");
+}
+
+TEST_F(AssemblerRISCV64Test, ScD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::ScD, "sc.d {reg1}, {reg2}, ({reg3})"), "ScD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoSwapD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoSwapD, "amoswap.d {reg1}, {reg2}, ({reg3})"), "AmoSwapD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoAddD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoAddD, "amoadd.d {reg1}, {reg2}, ({reg3})"), "AmoAddD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoXorD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoXorD, "amoxor.d {reg1}, {reg2}, ({reg3})"), "AmoXorD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoAndD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoAndD, "amoand.d {reg1}, {reg2}, ({reg3})"), "AmoAndD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoOrD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoOrD, "amoor.d {reg1}, {reg2}, ({reg3})"), "AmoOrD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMinD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMinD, "amomin.d {reg1}, {reg2}, ({reg3})"), "AmoMinD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMaxD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMaxD, "amomax.d {reg1}, {reg2}, ({reg3})"), "AmoMaxD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMinuD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMinuD, "amominu.d {reg1}, {reg2}, ({reg3})"), "AmoMinuD");
+}
+
+TEST_F(AssemblerRISCV64Test, AmoMaxuD) {
+  DriverStr(Repeatrrr(&riscv64::Riscv64Assembler::AmoMaxuD, "amomaxu.d {reg1}, {reg2}, ({reg3})"), "AmoMaxuD");
+}
+#endif
+
+#if TEST_RV32F_I
+TEST_F(AssemblerRISCV64Test, FLw) {
+  DriverStr(RepeatFrIb(&riscv64::Riscv64Assembler::FLw, -11, "flw {reg1}, {imm}({reg2})"), "FLw");
+}
+#endif
+
+#if TEST_RV32F_S
+TEST_F(AssemblerRISCV64Test, FSw) {
+  DriverStr(RepeatFrIb(&riscv64::Riscv64Assembler::FSw, 2, "fsw {reg1}, {imm}({reg2})"), "FSw");
+}
+#endif
+
+#if TEST_RV32F_R
+# if 1
+// takes too long time, close it for quick testing
+TEST_F(AssemblerRISCV64Test, FMAddS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMAddS, "fmadd.s {reg1}, {reg2}, {reg3}, {reg4}"), "FMAddS");
+}
+
+TEST_F(AssemblerRISCV64Test, FMSubS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMSubS, "fmsub.s {reg1}, {reg2}, {reg3}, {reg4}"), "FMSubS");
+}
+
+TEST_F(AssemblerRISCV64Test, FNMSubS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FNMSubS, "fnmsub.s {reg1}, {reg2}, {reg3}, {reg4}"), "FNMSubS");
+}
+
+TEST_F(AssemblerRISCV64Test, FNMAddS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FNMAddS, "fnmadd.s {reg1}, {reg2}, {reg3}, {reg4}"), "FNMAddS");
+}
+#endif
+
+TEST_F(AssemblerRISCV64Test, FAddS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FAddS, "fadd.s {reg1}, {reg2}, {reg3}"), "FAddS");
+}
+
+TEST_F(AssemblerRISCV64Test, FSubS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSubS, "fsub.s {reg1}, {reg2}, {reg3}"), "FSubS");
+}
+
+TEST_F(AssemblerRISCV64Test, FMulS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMulS, "fmul.s {reg1}, {reg2}, {reg3}"), "FMulS");
+}
+
+TEST_F(AssemblerRISCV64Test, FDivS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FDivS, "fdiv.s {reg1}, {reg2}, {reg3}"), "FDivS");
+}
+
+TEST_F(AssemblerRISCV64Test, FSqrtS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FSqrtS, "fsqrt.s {reg1}, {reg2}"), "FSqrtS");
+}
+
+TEST_F(AssemblerRISCV64Test, FSgnjS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSgnjS, "fsgnj.s {reg1}, {reg2}, {reg3}"), "FSgnjS");
+}
+
+TEST_F(AssemblerRISCV64Test, FSgnjnS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSgnjnS, "fsgnjn.s {reg1}, {reg2}, {reg3}"), "FSgnjnS");
+}
+
+TEST_F(AssemblerRISCV64Test, FSgnjxS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSgnjxS, "fsgnjx.s {reg1}, {reg2}, {reg3}"), "FSgnjxS");
+}
+
+TEST_F(AssemblerRISCV64Test, FMinS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMinS, "fmin.s {reg1}, {reg2}, {reg3}"), "FMinS");
+}
+
+TEST_F(AssemblerRISCV64Test, FMaxS) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMaxS, "fmax.s {reg1}, {reg2}, {reg3}"), "FMaxS");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtWS) {
+  // DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtWS, "fcvt.w.s {reg1}, {reg2}, {reg3}"), "FCvtWS");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtWuS) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtWuS, "fcvt.wu.s {reg1}, {reg2}"), "FCvtWuS");
+}
+
+TEST_F(AssemblerRISCV64Test, FMvXW) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FMvXW, "fmv.x.w {reg1}, {reg2}"), "FMvXW");
+}
+
+TEST_F(AssemblerRISCV64Test, FEqS) {
+  DriverStr(RepeatrFF(&riscv64::Riscv64Assembler::FEqS, "feq.s {reg1}, {reg2}, {reg3}"), "FEqS");
+}
+
+TEST_F(AssemblerRISCV64Test, FLtS) {
+  DriverStr(RepeatrFF(&riscv64::Riscv64Assembler::FLtS, "flt.s {reg1}, {reg2}, {reg3}"), "FLtS");
+}
+
+TEST_F(AssemblerRISCV64Test, FLeS) {
+  DriverStr(RepeatrFF(&riscv64::Riscv64Assembler::FLeS, "fle.s {reg1}, {reg2}, {reg3}"), "FLeS");
+}
+
+TEST_F(AssemblerRISCV64Test, FClassS) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FClassS, "fclass.s {reg1}, {reg2}"), "FClassS");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtSW) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtSW, "fcvt.s.w {reg1}, {reg2}"), "FCvtSW");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtSWu) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtSWu, "fcvt.s.wu {reg1}, {reg2}"), "FCvtSWu");
+}
+
+TEST_F(AssemblerRISCV64Test, FMvWX) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FMvWX, "fmv.w.x {reg1}, {reg2}"), "FMvWX");
+}
+#endif
+
+#if TEST_RV64F_R
+TEST_F(AssemblerRISCV64Test, FCvtLS) {
+  // DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtLS, "fcvt.l.s {reg1}, {reg2}, {reg3}"), "FCvtLS");
+}
+TEST_F(AssemblerRISCV64Test, FCvtLuS) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtLuS, "fcvt.lu.s {reg1}, {reg2}"), "FCvtLuS");
+}
+TEST_F(AssemblerRISCV64Test, FCvtSL) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtSL, "fcvt.s.l {reg1}, {reg2}"), "FCvtSL");
+}
+TEST_F(AssemblerRISCV64Test, FCvtSLu) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtSLu, "fcvt.s.lu {reg1}, {reg2}"), "FCvtSLu");
+}
+#endif
+
+#if TEST_RV32D_I
+TEST_F(AssemblerRISCV64Test, FLd) {
+  DriverStr(RepeatFrIb(&riscv64::Riscv64Assembler::FLd, -11, "fld {reg1}, {imm}({reg2})"), "FLw");
+}
+#endif
+
+#if TEST_RV32D_S
+TEST_F(AssemblerRISCV64Test, FSd) {
+  DriverStr(RepeatFrIb(&riscv64::Riscv64Assembler::FSd, 2, "fsd {reg1}, {imm}({reg2})"), "FSw");
+}
+#endif
+
+#if TEST_RV32D_R
+# if 1
+// takes too long time, close it for quick testing
+TEST_F(AssemblerRISCV64Test, FMAddD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMAddD, "fmadd.d {reg1}, {reg2}, {reg3}, {reg4}"), "FMAddD");
+}
+
+TEST_F(AssemblerRISCV64Test, FMSubD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMSubD, "fmsub.d {reg1}, {reg2}, {reg3}, {reg4}"), "FMSubD");
+}
+
+TEST_F(AssemblerRISCV64Test, FNMSubD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FNMSubD, "fnmsub.d {reg1}, {reg2}, {reg3}, {reg4}"), "FNMSubD");
+}
+
+TEST_F(AssemblerRISCV64Test, FNMAddD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FNMAddD, "fnmadd.d {reg1}, {reg2}, {reg3}, {reg4}"), "FNMAddD");
+}
+#endif
+
+TEST_F(AssemblerRISCV64Test, FAddD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FAddD, "fadd.d {reg1}, {reg2}, {reg3}"), "FAddD");
+}
+
+TEST_F(AssemblerRISCV64Test, FSubD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSubD, "fsub.d {reg1}, {reg2}, {reg3}"), "FSubD");
+}
+
+TEST_F(AssemblerRISCV64Test, FMulD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMulD, "fmul.d {reg1}, {reg2}, {reg3}"), "FMulD");
+}
+
+TEST_F(AssemblerRISCV64Test, FDivD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FDivD, "fdiv.d {reg1}, {reg2}, {reg3}"), "FDivD");
+}
+
+TEST_F(AssemblerRISCV64Test, FSqrtD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FSqrtD, "fsqrt.d {reg1}, {reg2}"), "FSqrtD");
+}
+
+TEST_F(AssemblerRISCV64Test, FSgnjD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSgnjD, "fsgnj.d {reg1}, {reg2}, {reg3}"), "FSgnjD");
+}
+
+TEST_F(AssemblerRISCV64Test, FSgnjnD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSgnjnD, "fsgnjn.d {reg1}, {reg2}, {reg3}"), "FSgnjnD");
+}
+
+TEST_F(AssemblerRISCV64Test, FSgnjxD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FSgnjxD, "fsgnjx.d {reg1}, {reg2}, {reg3}"), "FSgnjxD");
+}
+
+TEST_F(AssemblerRISCV64Test, FMinD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMinD, "fmin.d {reg1}, {reg2}, {reg3}"), "FMinD");
+}
+
+TEST_F(AssemblerRISCV64Test, FMaxD) {
+  DriverStr(RepeatFFF(&riscv64::Riscv64Assembler::FMaxD, "fmax.d {reg1}, {reg2}, {reg3}"), "FMaxD");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtSD) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FCvtSD, "fcvt.s.d {reg1}, {reg2}"), "FCvtSD");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtDS) {
+  DriverStr(RepeatFF(&riscv64::Riscv64Assembler::FCvtDS, "fcvt.d.s {reg1}, {reg2}"), "FCvtDS");
+}
+
+TEST_F(AssemblerRISCV64Test, FEqD) {
+  DriverStr(RepeatrFF(&riscv64::Riscv64Assembler::FEqD, "feq.d {reg1}, {reg2}, {reg3}"), "FEqD");
+}
+
+TEST_F(AssemblerRISCV64Test, FLtD) {
+  DriverStr(RepeatrFF(&riscv64::Riscv64Assembler::FLtD, "flt.d {reg1}, {reg2}, {reg3}"), "FLtD");
+}
+
+TEST_F(AssemblerRISCV64Test, FLeD) {
+  DriverStr(RepeatrFF(&riscv64::Riscv64Assembler::FLeD, "fle.d {reg1}, {reg2}, {reg3}"), "FLeD");
+}
+
+TEST_F(AssemblerRISCV64Test, FClassD) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FClassD, "fclass.d {reg1}, {reg2}"), "FClassD");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtWD) {
+  // DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtWD, "fcvt.w.d {reg1}, {reg2}, {reg3}"), "FCvtWD");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtWuD) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtWuD, "fcvt.wu.d {reg1}, {reg2}"), "FCvtWuD");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtDW) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtDW, "fcvt.d.w {reg1}, {reg2}"), "FCvtDW");
+}
+
+TEST_F(AssemblerRISCV64Test, FCvtDWu) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtDWu, "fcvt.d.wu {reg1}, {reg2}"), "FCvtDWu");
+}
+#endif
+
+#if TEST_RV64D_R
+TEST_F(AssemblerRISCV64Test, FCvtLD) {
+  // DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtLD, "fcvt.l.d {reg1}, {reg2}, {reg3}"), "FCvtLD");
+}
+TEST_F(AssemblerRISCV64Test, FCvtLuD) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FCvtLuD, "fcvt.lu.d {reg1}, {reg2}"), "FCvtLuD");
+}
+TEST_F(AssemblerRISCV64Test, FMvXD) {
+  DriverStr(RepeatrF(&riscv64::Riscv64Assembler::FMvXD, "fmv.x.d {reg1}, {reg2}"), "FMvXD");
+}
+TEST_F(AssemblerRISCV64Test, FCvtDL) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtDL, "fcvt.d.l {reg1}, {reg2}"), "FCvtDL");
+}
+TEST_F(AssemblerRISCV64Test, FCvtDLu) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FCvtDLu, "fcvt.d.lu {reg1}, {reg2}"), "FCvtDLu");
+}
+TEST_F(AssemblerRISCV64Test, FMvDX) {
+  DriverStr(RepeatFr(&riscv64::Riscv64Assembler::FMvDX, "fmv.d.x {reg1}, {reg2}"), "FMvDX");
+}
+#endif
+
+#endif
+
+#undef __
+
+}  // namespace art
diff --git a/compiler/utils/riscv64/constants_riscv64.h b/compiler/utils/riscv64/constants_riscv64.h
new file mode 100644
index 0000000000..54b867ddb2
--- /dev/null
+++ b/compiler/utils/riscv64/constants_riscv64.h
@@ -0,0 +1,115 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_UTILS_RISCV64_CONSTANTS_RISCV64_H_
+#define ART_COMPILER_UTILS_RISCV64_CONSTANTS_RISCV64_H_
+
+#include <iosfwd>
+
+#include <android-base/logging.h>
+
+#include "arch/riscv64/registers_riscv64.h"
+#include "base/globals.h"
+#include "base/macros.h"
+
+namespace art {
+namespace riscv64 {
+
+// Constants used for the decoding or encoding of the individual fields of instructions.
+enum InstructionFields {
+  kOpcodeShift = 26,
+  kOpcodeBits = 6,
+  kRsShift = 21,
+  kRsBits = 5,
+  kRtShift = 16,
+  kRtBits = 5,
+  kRdShift = 11,
+  kRdBits = 5,
+  kShamtShift = 6,
+  kShamtBits = 5,
+  kFunctShift = 0,
+  kFunctBits = 6,
+
+  kFmtShift = 21,
+  kFmtBits = 5,
+  kFtShift = 16,
+  kFtBits = 5,
+  kFsShift = 11,
+  kFsBits = 5,
+  kFdShift = 6,
+  kFdBits = 5,
+
+  kMsaOperationShift = 23,
+  kMsaELMOperationShift = 22,
+  kMsa2ROperationShift = 18,
+  kMsa2RFOperationShift = 17,
+  kDfShift = 21,
+  kDfMShift = 16,
+  kDf2RShift = 16,
+  kDfNShift = 16,
+  kWtShift = 16,
+  kWtBits = 5,
+  kWsShift = 11,
+  kWsBits = 5,
+  kWdShift = 6,
+  kWdBits = 5,
+  kS10Shift = 16,
+  kI10Shift = 11,
+  kS10MinorShift = 2,
+
+  kBranchOffsetMask = 0x0000ffff,
+  kJumpOffsetMask = 0x03ffffff,
+  kMsaMajorOpcode = 0x1e,
+  kMsaDfMByteMask = 0x70,
+  kMsaDfMHalfwordMask = 0x60,
+  kMsaDfMWordMask = 0x40,
+  kMsaDfMDoublewordMask = 0x00,
+  kMsaDfNByteMask = 0x00,
+  kMsaDfNHalfwordMask = 0x20,
+  kMsaDfNWordMask = 0x30,
+  kMsaDfNDoublewordMask = 0x38,
+  kMsaS10Mask = 0x3ff,
+};
+
+enum ScaleFactor {
+  TIMES_1 = 0,
+  TIMES_2 = 1,
+  TIMES_4 = 2,
+  TIMES_8 = 3
+};
+
+class Instr {
+ public:
+  static const uint32_t kBreakPointInstruction = 0x0000000D;
+
+  bool IsBreakPoint() {
+    return ((*reinterpret_cast<const uint32_t*>(this)) & 0xFC00003F) == kBreakPointInstruction;
+  }
+
+  // Instructions are read out of a code stream. The only way to get a
+  // reference to an instruction is to convert a pointer. There is no way
+  // to allocate or create instances of class Instr.
+  // Use the At(pc) function to create references to Instr.
+  static Instr* At(uintptr_t pc) { return reinterpret_cast<Instr*>(pc); }
+
+ private:
+  DISALLOW_IMPLICIT_CONSTRUCTORS(Instr);
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_COMPILER_UTILS_RISCV64_CONSTANTS_RISCV64_H_
diff --git a/compiler/utils/riscv64/jni_macro_assembler_riscv64.cc b/compiler/utils/riscv64/jni_macro_assembler_riscv64.cc
new file mode 100644
index 0000000000..7ec10e5cc4
--- /dev/null
+++ b/compiler/utils/riscv64/jni_macro_assembler_riscv64.cc
@@ -0,0 +1,296 @@
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "jni_macro_assembler_riscv64.h"
+
+#include "entrypoints/quick/quick_entrypoints.h"
+#include "managed_register_riscv64.h"
+#include "offsets.h"
+#include "thread.h"
+
+namespace art {
+namespace riscv64 {
+
+#define __ asm_.
+
+Riscv64JNIMacroAssembler::~Riscv64JNIMacroAssembler() {
+}
+
+void Riscv64JNIMacroAssembler::FinalizeCode() {
+  __ FinalizeCode();
+}
+
+void Riscv64JNIMacroAssembler::GetCurrentThread(ManagedRegister tr) {
+  __ GetCurrentThread(tr);
+}
+
+void Riscv64JNIMacroAssembler::GetCurrentThread(FrameOffset offset, ManagedRegister scratch) {
+  __ GetCurrentThread(offset, scratch);
+}
+
+// See Riscv64 PCS Section 5.2.2.1.
+void Riscv64JNIMacroAssembler::IncreaseFrameSize(size_t adjust) {
+  __ IncreaseFrameSize(adjust);
+}
+
+// See Riscv64 PCS Section 5.2.2.1.
+void Riscv64JNIMacroAssembler::DecreaseFrameSize(size_t adjust) {
+  __ DecreaseFrameSize(adjust);
+}
+
+void Riscv64JNIMacroAssembler::Store(FrameOffset offs, ManagedRegister m_src, size_t size) {
+  __ Store(offs, m_src, size);
+}
+
+void Riscv64JNIMacroAssembler::StoreRef(FrameOffset offs, ManagedRegister m_src) {
+  __ StoreRef(offs, m_src);
+}
+
+void Riscv64JNIMacroAssembler::StoreRawPtr(FrameOffset offs, ManagedRegister m_src) {
+  __ StoreRawPtr(offs, m_src);
+}
+
+void Riscv64JNIMacroAssembler::StoreImmediateToFrame(FrameOffset offs,
+                                                   uint32_t imm,
+                                                   ManagedRegister m_scratch) {
+  __ StoreImmediateToFrame(offs, imm, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::StoreStackOffsetToThread(ThreadOffset64 tr_offs,
+                                                      FrameOffset fr_offs,
+                                                      ManagedRegister m_scratch) {
+  __ StoreStackOffsetToThread(tr_offs, fr_offs, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::StoreStackPointerToThread(ThreadOffset64 tr_offs) {
+  __ StoreStackPointerToThread(tr_offs);
+}
+
+void Riscv64JNIMacroAssembler::StoreSpanning(FrameOffset dest_off,
+                                           ManagedRegister m_source,
+                                           FrameOffset in_off,
+                                           ManagedRegister m_scratch) {
+  __ StoreSpanning(dest_off, m_source, in_off, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::Load(ManagedRegister m_dst, FrameOffset src, size_t size) {
+  __ Load(m_dst, src, size);
+}
+
+void Riscv64JNIMacroAssembler::LoadFromThread(ManagedRegister m_dst,
+                                            ThreadOffset64 src,
+                                            size_t size) {
+  __ LoadFromThread(m_dst, src, size);
+}
+
+void Riscv64JNIMacroAssembler::LoadRef(ManagedRegister m_dst, FrameOffset offs) {
+  __ LoadRef(m_dst, offs);
+}
+
+void Riscv64JNIMacroAssembler::LoadRef(ManagedRegister m_dst,
+                                     ManagedRegister m_base,
+                                     MemberOffset offs,
+                                     bool unpoison_reference) {
+  __ LoadRef(m_dst, m_base, offs, unpoison_reference);
+}
+
+void Riscv64JNIMacroAssembler::LoadRawPtr(ManagedRegister m_dst,
+                                        ManagedRegister m_base,
+                                        Offset offs) {
+  __ LoadRawPtr(m_dst, m_base, offs);
+}
+
+void Riscv64JNIMacroAssembler::LoadRawPtrFromThread(ManagedRegister m_dst, ThreadOffset64 offs) {
+  __ LoadRawPtrFromThread(m_dst, offs);
+}
+
+// Copying routines.
+void Riscv64JNIMacroAssembler::Move(ManagedRegister m_dst, ManagedRegister m_src, size_t size) {
+  __ Move(m_dst, m_src, size);
+}
+
+void Riscv64JNIMacroAssembler::CopyRawPtrFromThread(FrameOffset fr_offs,
+                                                  ThreadOffset64 tr_offs,
+                                                  ManagedRegister m_scratch) {
+  __ CopyRawPtrFromThread(fr_offs, tr_offs, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::CopyRawPtrToThread(ThreadOffset64 tr_offs,
+                                                FrameOffset fr_offs,
+                                                ManagedRegister m_scratch) {
+  __ CopyRawPtrToThread(tr_offs, fr_offs, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::CopyRef(FrameOffset dest, FrameOffset src, ManagedRegister m_scratch) {
+  __ CopyRef(dest, src, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::Copy(FrameOffset dest,
+                                  FrameOffset src,
+                                  ManagedRegister m_scratch,
+                                  size_t size) {
+  __ Copy(dest, src, m_scratch, size);
+}
+
+void Riscv64JNIMacroAssembler::Copy(FrameOffset dest,
+                                  ManagedRegister src_base,
+                                  Offset src_offset,
+                                  ManagedRegister m_scratch,
+                                  size_t size) {
+  __ Copy(dest, src_base, src_offset, m_scratch, size);
+}
+
+void Riscv64JNIMacroAssembler::Copy(ManagedRegister m_dest_base,
+                                  Offset dest_offs,
+                                  FrameOffset src,
+                                  ManagedRegister m_scratch,
+                                  size_t size) {
+  __ Copy(m_dest_base, dest_offs, src, m_scratch, size);
+}
+
+void Riscv64JNIMacroAssembler::Copy(FrameOffset dst,
+                                  FrameOffset src_base,
+                                  Offset src_offset,
+                                  ManagedRegister mscratch,
+                                  size_t size) {
+  __ Copy(dst, src_base, src_offset, mscratch, size);
+}
+
+void Riscv64JNIMacroAssembler::Copy(ManagedRegister m_dest,
+                                  Offset dest_offset,
+                                  ManagedRegister m_src,
+                                  Offset src_offset,
+                                  ManagedRegister m_scratch,
+                                  size_t size) {
+  __ Copy(m_dest, dest_offset, m_src, src_offset, m_scratch, size);
+}
+
+void Riscv64JNIMacroAssembler::Copy(FrameOffset dst,
+                                  Offset dest_offset,
+                                  FrameOffset src,
+                                  Offset src_offset,
+                                  ManagedRegister scratch,
+                                  size_t size) {
+  __ Copy(dst, dest_offset, src, src_offset, scratch, size);
+}
+
+void Riscv64JNIMacroAssembler::MemoryBarrier(ManagedRegister m_scratch) {
+  __ MemoryBarrier(m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::SignExtend(ManagedRegister mreg, size_t size) {
+  __ SignExtend(mreg, size);
+}
+
+void Riscv64JNIMacroAssembler::ZeroExtend(ManagedRegister mreg, size_t size) {
+  __ ZeroExtend(mreg, size);
+}
+
+void Riscv64JNIMacroAssembler::VerifyObject(ManagedRegister m_src, bool could_be_null) {
+  // TODO: not validating references.
+  __ VerifyObject(m_src, could_be_null);
+}
+
+void Riscv64JNIMacroAssembler::VerifyObject(FrameOffset src, bool could_be_null) {
+  // TODO: not validating references.
+  __ VerifyObject(src, could_be_null);
+}
+
+void Riscv64JNIMacroAssembler::Call(ManagedRegister m_base, Offset offs, ManagedRegister m_scratch) {
+  __ Call(m_base, offs, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::Call(FrameOffset base, Offset offs, ManagedRegister m_scratch) {
+  __ Call(base, offs, m_scratch);
+}
+
+void Riscv64JNIMacroAssembler::CallFromThread(ThreadOffset64 offset,
+                                            ManagedRegister scratch) {
+  __ CallFromThread(offset, scratch);
+}
+
+void Riscv64JNIMacroAssembler::CreateHandleScopeEntry(ManagedRegister m_out_reg,
+                                                    FrameOffset handle_scope_offs,
+                                                    ManagedRegister m_in_reg,
+                                                    bool null_allowed) {
+  __ CreateHandleScopeEntry(m_out_reg, handle_scope_offs, m_in_reg, null_allowed);
+}
+
+void Riscv64JNIMacroAssembler::CreateHandleScopeEntry(FrameOffset out_off,
+                                                    FrameOffset handle_scope_offset,
+                                                    ManagedRegister m_scratch,
+                                                    bool null_allowed) {
+  __ CreateHandleScopeEntry(out_off, handle_scope_offset, m_scratch, null_allowed);
+}
+
+void Riscv64JNIMacroAssembler::LoadReferenceFromHandleScope(ManagedRegister m_out_reg,
+                                                          ManagedRegister m_in_reg) {
+  __ LoadReferenceFromHandleScope(m_out_reg, m_in_reg);
+}
+
+void Riscv64JNIMacroAssembler::ExceptionPoll(ManagedRegister m_scratch, size_t stack_adjust) {
+  __ ExceptionPoll(m_scratch, stack_adjust);
+}
+
+std::unique_ptr<JNIMacroLabel> Riscv64JNIMacroAssembler::CreateLabel() {
+  return std::unique_ptr<JNIMacroLabel>(new Riscv64JNIMacroLabel());
+}
+
+void Riscv64JNIMacroAssembler::Jump(JNIMacroLabel* label) {
+  CHECK(label != nullptr);
+  __ Bc(down_cast<Riscv64Label*>(Riscv64JNIMacroLabel::Cast(label)->AsRiscv64()));
+}
+
+void Riscv64JNIMacroAssembler::Jump(JNIMacroLabel* label,
+                                  JNIMacroUnaryCondition condition,
+                                  ManagedRegister test) {
+  CHECK(label != nullptr);
+
+  switch (condition) {
+    case JNIMacroUnaryCondition::kZero:
+      __ Beqzc(test.AsRiscv64().AsGpuRegister(), down_cast<Riscv64Label*>(Riscv64JNIMacroLabel::Cast(label)->AsRiscv64()));
+      break;
+    case JNIMacroUnaryCondition::kNotZero:
+      __ Bnezc(test.AsRiscv64().AsGpuRegister(), down_cast<Riscv64Label*>(Riscv64JNIMacroLabel::Cast(label)->AsRiscv64()));
+      break;
+    default:
+      LOG(FATAL) << "Not implemented unary condition: " << static_cast<int>(condition);
+      UNREACHABLE();
+  }
+}
+
+void Riscv64JNIMacroAssembler::Bind(JNIMacroLabel* label) {
+  CHECK(label != nullptr);
+  __ Bind(Riscv64JNIMacroLabel::Cast(label)->AsRiscv64());
+}
+
+void Riscv64JNIMacroAssembler::BuildFrame(size_t frame_size,
+                                        ManagedRegister method_reg,
+                                        ArrayRef<const ManagedRegister> callee_save_regs,
+                                        const ManagedRegisterEntrySpills& entry_spills) {
+  __ BuildFrame(frame_size, method_reg, callee_save_regs, entry_spills);
+}
+
+void Riscv64JNIMacroAssembler::RemoveFrame(size_t frame_size,
+                                         ArrayRef<const ManagedRegister> callee_save_regs,
+                                         bool may_suspend) {
+  __ RemoveFrame(frame_size, callee_save_regs, may_suspend);
+}
+
+#undef ___
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/utils/riscv64/jni_macro_assembler_riscv64.h b/compiler/utils/riscv64/jni_macro_assembler_riscv64.h
new file mode 100644
index 0000000000..f161b05dc0
--- /dev/null
+++ b/compiler/utils/riscv64/jni_macro_assembler_riscv64.h
@@ -0,0 +1,221 @@
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_UTILS_RISCV64_JNI_MACRO_ASSEMBLER_RISCV64_H_
+#define ART_COMPILER_UTILS_RISCV64_JNI_MACRO_ASSEMBLER_RISCV64_H_
+
+#include <stdint.h>
+#include <memory>
+#include <vector>
+
+#include <android-base/logging.h>
+
+#include "assembler_riscv64.h"
+#include "base/arena_containers.h"
+#include "base/enums.h"
+#include "base/macros.h"
+#include "offsets.h"
+#include "utils/assembler.h"
+#include "utils/jni_macro_assembler.h"
+
+namespace art {
+namespace riscv64 {
+
+class Riscv64JNIMacroAssembler final : public JNIMacroAssemblerFwd<Riscv64Assembler, PointerSize::k64> {
+ public:
+  explicit Riscv64JNIMacroAssembler(ArenaAllocator* allocator)
+      : JNIMacroAssemblerFwd<Riscv64Assembler, PointerSize::k64>(allocator) {}
+  ~Riscv64JNIMacroAssembler();
+
+  // Finalize the code.
+  void FinalizeCode() override;
+
+  // Emit code that will create an activation on the stack.
+  void BuildFrame(size_t frame_size,
+                  ManagedRegister method_reg,
+                  ArrayRef<const ManagedRegister> callee_save_regs,
+                  const ManagedRegisterEntrySpills& entry_spills) override;
+
+  // Emit code that will remove an activation from the stack.
+  void RemoveFrame(size_t frame_size,
+                   ArrayRef<const ManagedRegister> callee_save_regs,
+                   bool may_suspend) override;
+
+  void IncreaseFrameSize(size_t adjust) override;
+  void DecreaseFrameSize(size_t adjust) override;
+
+  // Store routines.
+  void Store(FrameOffset offs, ManagedRegister src, size_t size) override;
+  void StoreRef(FrameOffset dest, ManagedRegister src) override;
+  void StoreRawPtr(FrameOffset dest, ManagedRegister src) override;
+  void StoreImmediateToFrame(FrameOffset dest, uint32_t imm, ManagedRegister scratch) override;
+  void StoreStackOffsetToThread(ThreadOffset64 thr_offs,
+                                FrameOffset fr_offs,
+                                ManagedRegister scratch) override;
+  void StoreStackPointerToThread(ThreadOffset64 thr_offs) override;
+  void StoreSpanning(FrameOffset dest,
+                     ManagedRegister src,
+                     FrameOffset in_off,
+                     ManagedRegister scratch) override;
+
+  // Load routines.
+  void Load(ManagedRegister dest, FrameOffset src, size_t size) override;
+  void LoadFromThread(ManagedRegister dest, ThreadOffset64 src, size_t size) override;
+  void LoadRef(ManagedRegister dest, FrameOffset src) override;
+  void LoadRef(ManagedRegister dest,
+               ManagedRegister base,
+               MemberOffset offs,
+               bool unpoison_reference) override;
+  void LoadRawPtr(ManagedRegister dest, ManagedRegister base, Offset offs) override;
+  void LoadRawPtrFromThread(ManagedRegister dest, ThreadOffset64 offs) override;
+
+  // Copying routines.
+  void Move(ManagedRegister dest, ManagedRegister src, size_t size) override;
+  void CopyRawPtrFromThread(FrameOffset fr_offs,
+                            ThreadOffset64 thr_offs,
+                            ManagedRegister scratch) override;
+  void CopyRawPtrToThread(ThreadOffset64 thr_offs, FrameOffset fr_offs, ManagedRegister scratch)
+      override;
+  void CopyRef(FrameOffset dest, FrameOffset src, ManagedRegister scratch) override;
+  void Copy(FrameOffset dest, FrameOffset src, ManagedRegister scratch, size_t size) override;
+  void Copy(FrameOffset dest,
+            ManagedRegister src_base,
+            Offset src_offset,
+            ManagedRegister scratch,
+            size_t size) override;
+  void Copy(ManagedRegister dest_base,
+            Offset dest_offset,
+            FrameOffset src,
+            ManagedRegister scratch,
+            size_t size) override;
+  void Copy(FrameOffset dest,
+            FrameOffset src_base,
+            Offset src_offset,
+            ManagedRegister scratch,
+            size_t size) override;
+  void Copy(ManagedRegister dest,
+            Offset dest_offset,
+            ManagedRegister src,
+            Offset src_offset,
+            ManagedRegister scratch,
+            size_t size) override;
+  void Copy(FrameOffset dest,
+            Offset dest_offset,
+            FrameOffset src,
+            Offset src_offset,
+            ManagedRegister scratch,
+            size_t size) override;
+  void MemoryBarrier(ManagedRegister scratch) override;
+
+  // Sign extension.
+  void SignExtend(ManagedRegister mreg, size_t size) override;
+
+  // Zero extension.
+  void ZeroExtend(ManagedRegister mreg, size_t size) override;
+
+  // Exploit fast access in managed code to Thread::Current().
+  void GetCurrentThread(ManagedRegister tr) override;
+  void GetCurrentThread(FrameOffset dest_offset, ManagedRegister scratch) override;
+
+  // Set up out_reg to hold a Object** into the handle scope, or to be null if the
+  // value is null and null_allowed. in_reg holds a possibly stale reference
+  // that can be used to avoid loading the handle scope entry to see if the value is
+  // null.
+  void CreateHandleScopeEntry(ManagedRegister out_reg,
+                              FrameOffset handlescope_offset,
+                              ManagedRegister in_reg,
+                              bool null_allowed) override;
+
+  // Set up out_off to hold a Object** into the handle scope, or to be null if the
+  // value is null and null_allowed.
+  void CreateHandleScopeEntry(FrameOffset out_off,
+                              FrameOffset handlescope_offset,
+                              ManagedRegister scratch,
+                              bool null_allowed) override;
+
+  // src holds a handle scope entry (Object**) load this into dst.
+  void LoadReferenceFromHandleScope(ManagedRegister dst, ManagedRegister src) override;
+
+  // Heap::VerifyObject on src. In some cases (such as a reference to this) we
+  // know that src may not be null.
+  void VerifyObject(ManagedRegister m_src, bool could_be_null) override;
+  void VerifyObject(FrameOffset src, bool could_be_null) override;
+
+  // Call to address held at [base+offset].
+  void Call(ManagedRegister base, Offset offset, ManagedRegister scratch) override;
+  void Call(FrameOffset base, Offset offset, ManagedRegister scratch) override;
+  void CallFromThread(ThreadOffset64 offset, ManagedRegister scratch) override;
+
+  // Generate code to check if Thread::Current()->exception_ is non-null
+  // and branch to a ExceptionSlowPath if it is.
+  void ExceptionPoll(ManagedRegister scratch, size_t stack_adjust) override;
+
+  // Create a new label that can be used with Jump/Bind calls.
+  std::unique_ptr<JNIMacroLabel> CreateLabel() override;
+  // Emit an unconditional jump to the label.
+  void Jump(JNIMacroLabel* label) override;
+  // Emit a conditional jump to the label by applying a unary condition test to the register.
+  void Jump(JNIMacroLabel* label, JNIMacroUnaryCondition cond, ManagedRegister test) override;
+  // Code at this offset will serve as the target for the Jump call.
+  void Bind(JNIMacroLabel* label) override;
+
+ private:
+ /*
+  void StoreWToOffset(StoreOperandType type,
+                      GpuRegister source,
+                      GpuRegister base,
+                      int32_t offset);
+  void StoreToOffset(GpuRegister source, GpuRegister base, int32_t offset);
+  void StoreSToOffset(FpuRegister source, GpuRegister base, int32_t offset);
+  void StoreDToOffset(FpuRegister source, GpuRegister base, int32_t offset);
+
+  void LoadImmediate(XRegister dest,
+                     int32_t value,
+                     vixl::aarch64::Condition cond = vixl::aarch64::al);
+  void Load(Riscv64ManagedRegister dst, XRegister src, int32_t src_offset, size_t size);
+  void LoadWFromOffset(LoadOperandType type,
+                       WRegister dest,
+                       XRegister base,
+                       int32_t offset);
+  void LoadFromOffset(XRegister dest, XRegister base, int32_t offset);
+  void LoadSFromOffset(SRegister dest, XRegister base, int32_t offset);
+  void LoadDFromOffset(DRegister dest, XRegister base, int32_t offset);
+  void AddConstant(XRegister rd,
+                   int32_t value,
+                   vixl::aarch64::Condition cond = vixl::aarch64::al);
+  void AddConstant(XRegister rd,
+                   XRegister rn,
+                   int32_t value,
+                   vixl::aarch64::Condition cond = vixl::aarch64::al);
+*/
+  // List of exception blocks to generate at the end of the code cache.
+  // ArenaVector<std::unique_ptr<Riscv64Exception>> exception_blocks_;
+};
+
+class Riscv64JNIMacroLabel final
+    : public JNIMacroLabelCommon<Riscv64JNIMacroLabel,
+                                 art::Label,
+                                 InstructionSet::kRiscv64> {
+ public:
+  art::Label* AsRiscv64() {
+    return AsPlatformLabel();
+  }
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_COMPILER_UTILS_RISCV64_JNI_MACRO_ASSEMBLER_RISCV64_H_
diff --git a/compiler/utils/riscv64/managed_register_riscv64.cc b/compiler/utils/riscv64/managed_register_riscv64.cc
new file mode 100644
index 0000000000..b6b8b83452
--- /dev/null
+++ b/compiler/utils/riscv64/managed_register_riscv64.cc
@@ -0,0 +1,57 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "managed_register_riscv64.h"
+
+#include "base/globals.h"
+
+namespace art {
+namespace riscv64 {
+
+bool Riscv64ManagedRegister::Overlaps(const Riscv64ManagedRegister& other) const {
+  if (IsNoRegister() || other.IsNoRegister()) return false;
+  CHECK(IsValidManagedRegister());
+  CHECK(other.IsValidManagedRegister());
+  if (Equals(other)) return true;
+  if (IsFpuRegister() && other.IsVectorRegister()) {
+    return (AsFpuRegister() == other.AsOverlappingFpuRegister());
+  } else if (IsVectorRegister() && other.IsFpuRegister()) {
+    return (AsVectorRegister() == other.AsOverlappingVectorRegister());
+  }
+  return false;
+}
+
+void Riscv64ManagedRegister::Print(std::ostream& os) const {
+  if (!IsValidManagedRegister()) {
+    os << "No Register";
+  } else if (IsGpuRegister()) {
+    os << "GPU: " << static_cast<int>(AsGpuRegister());
+  } else if (IsFpuRegister()) {
+     os << "FpuRegister: " << static_cast<int>(AsFpuRegister());
+  } else if (IsVectorRegister()) {
+     os << "VectorRegister: " << static_cast<int>(AsVectorRegister());
+  } else {
+    os << "??: " << RegId();
+  }
+}
+
+std::ostream& operator<<(std::ostream& os, const Riscv64ManagedRegister& reg) {
+  reg.Print(os);
+  return os;
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/compiler/utils/riscv64/managed_register_riscv64.h b/compiler/utils/riscv64/managed_register_riscv64.h
new file mode 100644
index 0000000000..0ab6065df6
--- /dev/null
+++ b/compiler/utils/riscv64/managed_register_riscv64.h
@@ -0,0 +1,163 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_UTILS_RISCV64_MANAGED_REGISTER_RISCV64_H_
+#define ART_COMPILER_UTILS_RISCV64_MANAGED_REGISTER_RISCV64_H_
+
+#include "constants_riscv64.h"
+#include "utils/managed_register.h"
+
+namespace art {
+namespace riscv64 {
+
+const int kNumberOfGpuRegIds = kNumberOfGpuRegisters;
+const int kNumberOfGpuAllocIds = kNumberOfGpuRegisters;
+
+const int kNumberOfFpuRegIds = kNumberOfFpuRegisters;
+const int kNumberOfFpuAllocIds = kNumberOfFpuRegisters;
+
+const int kNumberOfVecRegIds = kNumberOfVectorRegisters;
+const int kNumberOfVecAllocIds = kNumberOfVectorRegisters;
+
+const int kNumberOfRegIds = kNumberOfGpuRegIds + kNumberOfFpuRegIds + kNumberOfVecRegIds;
+const int kNumberOfAllocIds = kNumberOfGpuAllocIds + kNumberOfFpuAllocIds + kNumberOfVecAllocIds;
+
+// Register ids map:
+//   [0..R[  core registers (enum GpuRegister)
+//   [R..F[  floating-point registers (enum FpuRegister)
+//   [F..W[  MSA vector registers (enum VectorRegister)
+// where
+//   R = kNumberOfGpuRegIds
+//   F = R + kNumberOfFpuRegIds
+//   W = F + kNumberOfVecRegIds
+
+// An instance of class 'ManagedRegister' represents a single Riscv64 register.
+// A register can be one of the following:
+//  * core register (enum GpuRegister)
+//  * floating-point register (enum FpuRegister)
+//  * MSA vector register (enum VectorRegister)
+//
+// 'ManagedRegister::NoRegister()' provides an invalid register.
+// There is a one-to-one mapping between ManagedRegister and register id.
+class Riscv64ManagedRegister : public ManagedRegister {
+ public:
+  constexpr GpuRegister AsGpuRegister() const {
+    CHECK(IsGpuRegister());
+    return static_cast<GpuRegister>(id_);
+  }
+
+  constexpr FpuRegister AsFpuRegister() const {
+    CHECK(IsFpuRegister());
+    return static_cast<FpuRegister>(id_ - kNumberOfGpuRegIds);
+  }
+
+  constexpr VectorRegister AsVectorRegister() const {
+    CHECK(IsVectorRegister());
+    return static_cast<VectorRegister>(id_ - (kNumberOfGpuRegIds + kNumberOfFpuRegisters));
+  }
+
+  constexpr FpuRegister AsOverlappingFpuRegister() const {
+    CHECK(IsValidManagedRegister());
+    return static_cast<FpuRegister>(AsVectorRegister());
+  }
+
+  constexpr VectorRegister AsOverlappingVectorRegister() const {
+    CHECK(IsValidManagedRegister());
+    return static_cast<VectorRegister>(AsFpuRegister());
+  }
+
+  constexpr bool IsGpuRegister() const {
+    CHECK(IsValidManagedRegister());
+    return (0 <= id_) && (id_ < kNumberOfGpuRegIds);
+  }
+
+  constexpr bool IsFpuRegister() const {
+    CHECK(IsValidManagedRegister());
+    const int test = id_ - kNumberOfGpuRegIds;
+    return (0 <= test) && (test < kNumberOfFpuRegIds);
+  }
+
+  constexpr bool IsVectorRegister() const {
+    CHECK(IsValidManagedRegister());
+    const int test = id_ - (kNumberOfGpuRegIds + kNumberOfFpuRegIds);
+    return (0 <= test) && (test < kNumberOfVecRegIds);
+  }
+
+  void Print(std::ostream& os) const;
+
+  // Returns true if the two managed-registers ('this' and 'other') overlap.
+  // Either managed-register may be the NoRegister. If both are the NoRegister
+  // then false is returned.
+  bool Overlaps(const Riscv64ManagedRegister& other) const;
+
+  static constexpr Riscv64ManagedRegister FromGpuRegister(GpuRegister r) {
+    CHECK_NE(r, kNoGpuRegister);
+    return FromRegId(r);
+  }
+
+  static constexpr Riscv64ManagedRegister FromFpuRegister(FpuRegister r) {
+    CHECK_NE(r, kNoFpuRegister);
+    return FromRegId(r + kNumberOfGpuRegIds);
+  }
+
+  static constexpr Riscv64ManagedRegister FromVectorRegister(VectorRegister r) {
+    CHECK_NE(r, kNoVectorRegister);
+    return FromRegId(r + kNumberOfGpuRegIds + kNumberOfFpuRegIds);
+  }
+
+ private:
+  constexpr bool IsValidManagedRegister() const {
+    return (0 <= id_) && (id_ < kNumberOfRegIds);
+  }
+
+  constexpr int RegId() const {
+    CHECK(!IsNoRegister());
+    return id_;
+  }
+
+  int AllocId() const {
+    CHECK(IsValidManagedRegister());
+    CHECK_LT(id_, kNumberOfAllocIds);
+    return id_;
+  }
+
+  int AllocIdLow() const;
+  int AllocIdHigh() const;
+
+  friend class ManagedRegister;
+
+  explicit constexpr Riscv64ManagedRegister(int reg_id) : ManagedRegister(reg_id) {}
+
+  static constexpr Riscv64ManagedRegister FromRegId(int reg_id) {
+    Riscv64ManagedRegister reg(reg_id);
+    CHECK(reg.IsValidManagedRegister());
+    return reg;
+  }
+};
+
+std::ostream& operator<<(std::ostream& os, const Riscv64ManagedRegister& reg);
+
+}  // namespace riscv64
+
+constexpr inline riscv64::Riscv64ManagedRegister ManagedRegister::AsRiscv64() const {
+  riscv64::Riscv64ManagedRegister reg(id_);
+  CHECK(reg.IsNoRegister() || reg.IsValidManagedRegister());
+  return reg;
+}
+
+}  // namespace art
+
+#endif  // ART_COMPILER_UTILS_RISCV64_MANAGED_REGISTER_RISCV64_H_
diff --git a/compiler/utils/riscv64/managed_register_riscv64_test.cc b/compiler/utils/riscv64/managed_register_riscv64_test.cc
new file mode 100644
index 0000000000..687c70b6b7
--- /dev/null
+++ b/compiler/utils/riscv64/managed_register_riscv64_test.cc
@@ -0,0 +1,481 @@
+/*
+ * Copyright (C) 2017 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "managed_register_riscv64.h"
+
+#include "base/globals.h"
+#include "gtest/gtest.h"
+
+namespace art {
+namespace riscv64 {
+
+TEST(Riscv64ManagedRegister, NoRegister) {
+  Riscv64ManagedRegister reg = ManagedRegister::NoRegister().AsRiscv64();
+  EXPECT_TRUE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.Overlaps(reg));
+}
+
+TEST(Riscv64ManagedRegister, GpuRegister) {
+  Riscv64ManagedRegister reg = Riscv64ManagedRegister::FromGpuRegister(ZERO);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(ZERO, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(AT);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(AT, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(V0);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(V0, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(A0);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(A0, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(A7);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(A7, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(T0);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(T0, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(T3);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(T3, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(S0);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(S0, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(GP);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(GP, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(SP);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(SP, reg.AsGpuRegister());
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(RA);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_TRUE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_EQ(RA, reg.AsGpuRegister());
+}
+
+TEST(Riscv64ManagedRegister, FpuRegister) {
+  Riscv64ManagedRegister reg = Riscv64ManagedRegister::FromFpuRegister(FT0);
+  Riscv64ManagedRegister vreg = Riscv64ManagedRegister::FromVectorRegister(W0);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_TRUE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(vreg));
+  EXPECT_EQ(F0, reg.AsFpuRegister());
+  EXPECT_EQ(W0, reg.AsOverlappingVectorRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+
+  reg = Riscv64ManagedRegister::FromFpuRegister(FT1);
+  vreg = Riscv64ManagedRegister::FromVectorRegister(W1);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_TRUE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(vreg));
+  EXPECT_EQ(FT1, reg.AsFpuRegister());
+  EXPECT_EQ(W1, reg.AsOverlappingVectorRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromFpuRegister(FT1)));
+
+  reg = Riscv64ManagedRegister::FromFpuRegister(FS4);
+  vreg = Riscv64ManagedRegister::FromVectorRegister(W20);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_TRUE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(vreg));
+  EXPECT_EQ(FS4, reg.AsFpuRegister());
+  EXPECT_EQ(W20, reg.AsOverlappingVectorRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromFpuRegister(FS4)));
+
+  reg = Riscv64ManagedRegister::FromFpuRegister(FT11);
+  vreg = Riscv64ManagedRegister::FromVectorRegister(W31);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_TRUE(reg.IsFpuRegister());
+  EXPECT_FALSE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(vreg));
+  EXPECT_EQ(FT11, reg.AsFpuRegister());
+  EXPECT_EQ(W31, reg.AsOverlappingVectorRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+}
+
+TEST(Riscv64ManagedRegister, VectorRegister) {
+  Riscv64ManagedRegister reg = Riscv64ManagedRegister::FromVectorRegister(W0);
+  Riscv64ManagedRegister freg = Riscv64ManagedRegister::FromFpuRegister(FT0);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_TRUE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(freg));
+  EXPECT_EQ(W0, reg.AsVectorRegister());
+  EXPECT_EQ(F0, reg.AsOverlappingFpuRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W2);
+  freg = Riscv64ManagedRegister::FromFpuRegister(FT2);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_TRUE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(freg));
+  EXPECT_EQ(W2, reg.AsVectorRegister());
+  EXPECT_EQ(FT2, reg.AsOverlappingFpuRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromVectorRegister(W2)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W13);
+  freg = Riscv64ManagedRegister::FromFpuRegister(FA3);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_TRUE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(freg));
+  EXPECT_EQ(W13, reg.AsVectorRegister());
+  EXPECT_EQ(FA3, reg.AsOverlappingFpuRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromVectorRegister(W13)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W29);
+  freg = Riscv64ManagedRegister::FromFpuRegister(FT9);
+  EXPECT_FALSE(reg.IsNoRegister());
+  EXPECT_FALSE(reg.IsGpuRegister());
+  EXPECT_FALSE(reg.IsFpuRegister());
+  EXPECT_TRUE(reg.IsVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(freg));
+  EXPECT_EQ(W29, reg.AsVectorRegister());
+  EXPECT_EQ(FT9, reg.AsOverlappingFpuRegister());
+  EXPECT_TRUE(reg.Equals(Riscv64ManagedRegister::FromVectorRegister(W29)));
+}
+
+TEST(Riscv64ManagedRegister, Equals) {
+  ManagedRegister no_reg = ManagedRegister::NoRegister();
+  EXPECT_TRUE(no_reg.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(no_reg.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(no_reg.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(no_reg.Equals(Riscv64ManagedRegister::FromGpuRegister(S2)));
+  EXPECT_FALSE(no_reg.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(no_reg.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  Riscv64ManagedRegister reg_ZERO = Riscv64ManagedRegister::FromGpuRegister(ZERO);
+  EXPECT_FALSE(reg_ZERO.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_TRUE(reg_ZERO.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_ZERO.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_ZERO.Equals(Riscv64ManagedRegister::FromGpuRegister(S2)));
+  EXPECT_FALSE(reg_ZERO.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg_ZERO.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  Riscv64ManagedRegister reg_A1 = Riscv64ManagedRegister::FromGpuRegister(A1);
+  EXPECT_FALSE(reg_A1.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(reg_A1.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_A1.Equals(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_TRUE(reg_A1.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_A1.Equals(Riscv64ManagedRegister::FromGpuRegister(S2)));
+  EXPECT_FALSE(reg_A1.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg_A1.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  Riscv64ManagedRegister reg_S2 = Riscv64ManagedRegister::FromGpuRegister(S2);
+  EXPECT_FALSE(reg_S2.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(reg_S2.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_S2.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_S2.Equals(Riscv64ManagedRegister::FromGpuRegister(S1)));
+  EXPECT_TRUE(reg_S2.Equals(Riscv64ManagedRegister::FromGpuRegister(S2)));
+  EXPECT_FALSE(reg_S2.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg_S2.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  Riscv64ManagedRegister reg_F0 = Riscv64ManagedRegister::FromFpuRegister(FT0);
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::FromGpuRegister(S2)));
+  EXPECT_TRUE(reg_F0.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::FromFpuRegister(FT1)));
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg_F0.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  Riscv64ManagedRegister reg_F31 = Riscv64ManagedRegister::FromFpuRegister(FT11);
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::FromGpuRegister(S2)));
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::FromFpuRegister(FT1)));
+  EXPECT_TRUE(reg_F31.Equals(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg_F31.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+
+  Riscv64ManagedRegister reg_W0 = Riscv64ManagedRegister::FromVectorRegister(W0);
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::FromGpuRegister(S1)));
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_TRUE(reg_W0.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::FromVectorRegister(W1)));
+  EXPECT_FALSE(reg_W0.Equals(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  Riscv64ManagedRegister reg_W31 = Riscv64ManagedRegister::FromVectorRegister(W31);
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::NoRegister()));
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::FromGpuRegister(A1)));
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::FromGpuRegister(S1)));
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg_W31.Equals(Riscv64ManagedRegister::FromVectorRegister(W1)));
+  EXPECT_TRUE(reg_W31.Equals(Riscv64ManagedRegister::FromVectorRegister(W31)));
+}
+
+TEST(Riscv64ManagedRegister, Overlaps) {
+  Riscv64ManagedRegister reg = Riscv64ManagedRegister::FromFpuRegister(FT0);
+  Riscv64ManagedRegister reg_o = Riscv64ManagedRegister::FromVectorRegister(W0);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(F0, reg_o.AsOverlappingFpuRegister());
+  EXPECT_EQ(W0, reg.AsOverlappingVectorRegister());
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromFpuRegister(FT4);
+  reg_o = Riscv64ManagedRegister::FromVectorRegister(W4);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(FT4, reg_o.AsOverlappingFpuRegister());
+  EXPECT_EQ(W4, reg.AsOverlappingVectorRegister());
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromFpuRegister(FA6);
+  reg_o = Riscv64ManagedRegister::FromVectorRegister(W16);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(FA6, reg_o.AsOverlappingFpuRegister());
+  EXPECT_EQ(W16, reg.AsOverlappingVectorRegister());
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromFpuRegister(FT11);
+  reg_o = Riscv64ManagedRegister::FromVectorRegister(W31);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(FT11, reg_o.AsOverlappingFpuRegister());
+  EXPECT_EQ(W31, reg.AsOverlappingVectorRegister());
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W0);
+  reg_o = Riscv64ManagedRegister::FromFpuRegister(FT0);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(W0, reg_o.AsOverlappingVectorRegister());
+  EXPECT_EQ(F0, reg.AsOverlappingFpuRegister());
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W4);
+  reg_o = Riscv64ManagedRegister::FromFpuRegister(FT4);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(W4, reg_o.AsOverlappingVectorRegister());
+  EXPECT_EQ(FT4, reg.AsOverlappingFpuRegister());
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W16);
+  reg_o = Riscv64ManagedRegister::FromFpuRegister(FA6);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(W16, reg_o.AsOverlappingVectorRegister());
+  EXPECT_EQ(FA6, reg.AsOverlappingFpuRegister());
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromVectorRegister(W31);
+  reg_o = Riscv64ManagedRegister::FromFpuRegister(FT11);
+  EXPECT_TRUE(reg.Overlaps(reg_o));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_EQ(W31, reg_o.AsOverlappingVectorRegister());
+  EXPECT_EQ(FT11, reg.AsOverlappingFpuRegister());
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(ZERO);
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(A0);
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(S0);
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+
+  reg = Riscv64ManagedRegister::FromGpuRegister(RA);
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(ZERO)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(A0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(S0)));
+  EXPECT_TRUE(reg.Overlaps(Riscv64ManagedRegister::FromGpuRegister(RA)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FA6)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromFpuRegister(FT11)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W0)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W4)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W16)));
+  EXPECT_FALSE(reg.Overlaps(Riscv64ManagedRegister::FromVectorRegister(W31)));
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/dex2oat/Android.bp b/dex2oat/Android.bp
index 12701b2a21..ca7cdc0d3a 100644
--- a/dex2oat/Android.bp
+++ b/dex2oat/Android.bp
@@ -52,6 +52,11 @@ art_cc_defaults {
                 "linker/mips64/relative_patcher_mips64.cc",
             ],
         },
+        riscv64: {
+            srcs: [
+                "linker/riscv64/relative_patcher_riscv64.cc",
+            ],
+        },
         x86: {
             srcs: [
                 "linker/x86/relative_patcher_x86.cc",
@@ -211,6 +216,11 @@ cc_defaults {
                 profile_file: "art/dex2oat_arm_arm64.profdata",
             },
         },
+        android_riscv64: {
+            pgo: {
+                profile_file: "art/dex2oat_arm_riscv64.profdata",
+            },
+        },
         android_x86_64: {
             pgo: {
                 profile_file: "art/dex2oat_x86_x86_64.profdata",
@@ -266,7 +276,7 @@ art_cc_binary {
     target: {
         android: {
             lto: {
-                 thin: true,
+                 thin: false,
             },
             static_libs: [
                 "libz",
@@ -446,6 +456,13 @@ art_cc_test {
                 "linker/mips64/relative_patcher_mips64_test.cc",
             ],
         },
+		/*
+        riscv64: {
+            srcs: [
+                "linker/riscv64/relative_patcher_riscv64_test.cc",
+            ],
+        },
+		*/
         x86: {
             srcs: [
                 "linker/x86/relative_patcher_x86_test.cc",
diff --git a/dex2oat/dex2oat.cc b/dex2oat/dex2oat.cc
index 278523ec9a..2cb4193b64 100644
--- a/dex2oat/dex2oat.cc
+++ b/dex2oat/dex2oat.cc
@@ -289,7 +289,7 @@ NO_RETURN static void Usage(const char* fmt, ...) {
   UsageError("      Example: --android-root=out/host/linux-x86");
   UsageError("      Default: $ANDROID_ROOT");
   UsageError("");
-  UsageError("  --instruction-set=(arm|arm64|mips|mips64|x86|x86_64): compile for a particular");
+  UsageError("  --instruction-set=(arm|arm64|mips|mips64|x86|x86_64|riscv64): compile for a particular");
   UsageError("      instruction set.");
   UsageError("      Example: --instruction-set=x86");
   UsageError("      Default: arm");
@@ -957,7 +957,7 @@ class Dex2Oat final {
       int64_t timeout = parser_options->watch_dog_timeout_in_ms > 0
                             ? parser_options->watch_dog_timeout_in_ms
                             : WatchDog::kDefaultWatchdogTimeoutInMS;
-      watchdog_.reset(new WatchDog(timeout));
+      watchdog_.reset(new WatchDog(timeout*5));
     }
 
     // Fill some values into the key-value store for the oat header.
diff --git a/dex2oat/driver/compiler_driver.cc b/dex2oat/driver/compiler_driver.cc
index 8893d67f3c..f3955675b5 100644
--- a/dex2oat/driver/compiler_driver.cc
+++ b/dex2oat/driver/compiler_driver.cc
@@ -406,6 +406,7 @@ static bool InstructionSetHasGenericJniStub(InstructionSet isa) {
     case InstructionSet::kThumb2:
     case InstructionSet::kMips:
     case InstructionSet::kMips64:
+    case InstructionSet::kRiscv64:
     case InstructionSet::kX86:
     case InstructionSet::kX86_64: return true;
     default: return false;
diff --git a/dex2oat/linker/relative_patcher.cc b/dex2oat/linker/relative_patcher.cc
index 4db0e8a94e..71e5605251 100644
--- a/dex2oat/linker/relative_patcher.cc
+++ b/dex2oat/linker/relative_patcher.cc
@@ -35,6 +35,9 @@
 #ifdef ART_ENABLE_CODEGEN_x86_64
 #include "linker/x86_64/relative_patcher_x86_64.h"
 #endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+#include "linker/riscv64/relative_patcher_riscv64.h"
+#endif
 #include "stream/output_stream.h"
 
 namespace art {
@@ -126,6 +129,10 @@ std::unique_ptr<RelativePatcher> RelativePatcher::Create(
 #ifdef ART_ENABLE_CODEGEN_mips64
     case InstructionSet::kMips64:
       return std::unique_ptr<RelativePatcher>(new Mips64RelativePatcher());
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64:
+      return std::unique_ptr<RelativePatcher>(new Riscv64RelativePatcher());
 #endif
     default:
       return std::unique_ptr<RelativePatcher>(new RelativePatcherNone);
diff --git a/dex2oat/linker/riscv64/relative_patcher_riscv64.cc b/dex2oat/linker/riscv64/relative_patcher_riscv64.cc
new file mode 100644
index 0000000000..b761fddade
--- /dev/null
+++ b/dex2oat/linker/riscv64/relative_patcher_riscv64.cc
@@ -0,0 +1,102 @@
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "linker/riscv64/relative_patcher_riscv64.h"
+
+#include "compiled_method.h"
+#include "debug/method_debug_info.h"
+#include "linker/linker_patch.h"
+
+namespace art {
+namespace linker {
+
+uint32_t Riscv64RelativePatcher::ReserveSpace(
+    uint32_t offset,
+    const CompiledMethod* compiled_method ATTRIBUTE_UNUSED,
+    MethodReference method_ref ATTRIBUTE_UNUSED) {
+  return offset;  // No space reserved; no limit on relative call distance.
+}
+
+uint32_t Riscv64RelativePatcher::ReserveSpaceEnd(uint32_t offset) {
+  return offset;  // No space reserved; no limit on relative call distance.
+}
+
+uint32_t Riscv64RelativePatcher::WriteThunks(OutputStream* out ATTRIBUTE_UNUSED, uint32_t offset) {
+  return offset;  // No thunks added; no limit on relative call distance.
+}
+
+void Riscv64RelativePatcher::PatchCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                                      uint32_t literal_offset ATTRIBUTE_UNUSED,
+                                      uint32_t patch_offset ATTRIBUTE_UNUSED,
+                                      uint32_t target_offset ATTRIBUTE_UNUSED) {
+  UNIMPLEMENTED(FATAL) << "PatchCall unimplemented on RISCV64";
+}
+
+void Riscv64RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
+                                                     const LinkerPatch& patch,
+                                                     uint32_t patch_offset,
+                                                     uint32_t target_offset) {
+  uint32_t anchor_literal_offset = patch.PcInsnOffset();
+  uint32_t literal_offset = patch.LiteralOffset();
+  bool high_patch = ((*code)[literal_offset + 3] == 0x12) && ((*code)[literal_offset + 2] == 0x34)
+                    && ((((*code)[literal_offset + 1])&0xF0) == 0x50);
+
+  // Perform basic sanity checks.
+  if (high_patch) {
+    // auipc reg, offset_high
+    DCHECK_EQ(((*code)[literal_offset + 0] & 0x7F), 0x17);
+  } else {
+    // instr reg(s), offset_low
+    CHECK_EQ(((*code)[literal_offset + 2])&0xF0, 0x80);
+    CHECK_EQ((*code)[literal_offset + 3], 0x67);
+  }
+
+  // Apply patch.
+  uint32_t anchor_offset = patch_offset - literal_offset + anchor_literal_offset;
+  uint32_t diff = target_offset - anchor_offset;
+  // Note that a combination of auipc with an instruction that adds a sign-extended
+  // 12-bit immediate operand (e.g. ld) provides a PC-relative range of
+  // PC-0x80000000 to PC+0x7FFFF7FF on RISCV64, that is, short of 2GB on one end
+  // by 2KB.
+  diff += (diff & 0x800) << 1;  // Account for sign extension in "instr reg(s), offset_low".
+
+  if (high_patch) {
+    // auipc reg, offset_high
+    (*code)[literal_offset + 3] = static_cast<uint8_t>(diff >> 24);
+    (*code)[literal_offset + 2] = static_cast<uint8_t>(diff >> 16);
+    (*code)[literal_offset + 1] = ((static_cast<uint8_t>(diff >> 8)) & 0xF0) |
+                                   (((*code)[literal_offset + 1]) & 0x0F);
+  } else {
+    // instr reg(s), offset_low
+    (*code)[literal_offset + 3] = static_cast<uint8_t>(diff >> 4);
+    (*code)[literal_offset + 2] = (((static_cast<uint8_t>(diff >> 0)) & 0x0F) << 4) |
+                                   (((*code)[literal_offset + 2]) & 0x0F);
+  }
+}
+
+void Riscv64RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                                                        const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                                                        uint32_t patch_offset ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "UNIMPLEMENTED";
+}
+
+std::vector<debug::MethodDebugInfo> Riscv64RelativePatcher::GenerateThunkDebugInfo(
+    uint32_t executable_offset ATTRIBUTE_UNUSED) {
+  return std::vector<debug::MethodDebugInfo>();  // No thunks added.
+}
+
+}  // namespace linker
+}  // namespace art
diff --git a/dex2oat/linker/riscv64/relative_patcher_riscv64.h b/dex2oat/linker/riscv64/relative_patcher_riscv64.h
new file mode 100644
index 0000000000..6c3f89b3d8
--- /dev/null
+++ b/dex2oat/linker/riscv64/relative_patcher_riscv64.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_DEX2OAT_LINKER_RISCV64_RELATIVE_PATCHER_RISCV64_H_
+#define ART_DEX2OAT_LINKER_RISCV64_RELATIVE_PATCHER_RISCV64_H_
+
+#include "linker/relative_patcher.h"
+
+namespace art {
+namespace linker {
+
+class Riscv64RelativePatcher final : public RelativePatcher {
+ public:
+  Riscv64RelativePatcher() {}
+
+  uint32_t ReserveSpace(uint32_t offset,
+                        const CompiledMethod* compiled_method,
+                        MethodReference method_ref) override;
+  uint32_t ReserveSpaceEnd(uint32_t offset) override;
+  uint32_t WriteThunks(OutputStream* out, uint32_t offset) override;
+  void PatchCall(std::vector<uint8_t>* code,
+                 uint32_t literal_offset,
+                 uint32_t patch_offset,
+                 uint32_t target_offset) override;
+  void PatchPcRelativeReference(std::vector<uint8_t>* code,
+                                const LinkerPatch& patch,
+                                uint32_t patch_offset,
+                                uint32_t target_offset) override;
+  void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
+                                   const LinkerPatch& patch,
+                                   uint32_t patch_offset) override;
+  std::vector<debug::MethodDebugInfo> GenerateThunkDebugInfo(uint32_t executable_offset) override;
+
+ private:
+  DISALLOW_COPY_AND_ASSIGN(Riscv64RelativePatcher);
+};
+
+}  // namespace linker
+}  // namespace art
+
+#endif  // ART_DEX2OAT_LINKER_RISCV64_RELATIVE_PATCHER_RISCV64_H_
diff --git a/dex2oat/linker/riscv64/relative_patcher_riscv64_base.cc b/dex2oat/linker/riscv64/relative_patcher_riscv64_base.cc
new file mode 100644
index 0000000000..79e696b4dc
--- /dev/null
+++ b/dex2oat/linker/riscv64/relative_patcher_riscv64_base.cc
@@ -0,0 +1,538 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "linker/arm/relative_patcher_arm_base.h"
+
+#include "base/stl_util.h"
+#include "compiled_method-inl.h"
+#include "debug/method_debug_info.h"
+#include "dex/dex_file_types.h"
+#include "linker/linker_patch.h"
+#include "oat.h"
+#include "oat_quick_method_header.h"
+#include "stream/output_stream.h"
+
+namespace art {
+namespace linker {
+
+class Riscv64BaseRelativePatcher::ThunkData {
+ public:
+  ThunkData(ArrayRef<const uint8_t> code, const std::string& debug_name, uint32_t max_next_offset)
+      : code_(code),
+        debug_name_(debug_name),
+        offsets_(),
+        max_next_offset_(max_next_offset),
+        pending_offset_(0u) {
+    DCHECK(NeedsNextThunk());  // The data is constructed only when we expect to need the thunk.
+  }
+
+  ThunkData(ThunkData&& src) = default;
+
+  size_t CodeSize() const {
+    return code_.size();
+  }
+
+  ArrayRef<const uint8_t> GetCode() const {
+    return code_;
+  }
+
+  const std::string& GetDebugName() const {
+    return debug_name_;
+  }
+
+  bool NeedsNextThunk() const {
+    return max_next_offset_ != 0u;
+  }
+
+  uint32_t MaxNextOffset() const {
+    DCHECK(NeedsNextThunk());
+    return max_next_offset_;
+  }
+
+  void ClearMaxNextOffset() {
+    DCHECK(NeedsNextThunk());
+    max_next_offset_ = 0u;
+  }
+
+  void SetMaxNextOffset(uint32_t max_next_offset) {
+    DCHECK(!NeedsNextThunk());
+    max_next_offset_ = max_next_offset;
+  }
+
+  // Adjust the MaxNextOffset() down if needed to fit the code before the next thunk.
+  // Returns true if it was adjusted, false if the old value was kept.
+  bool MakeSpaceBefore(const ThunkData& next_thunk, size_t alignment) {
+    DCHECK(NeedsNextThunk());
+    DCHECK(next_thunk.NeedsNextThunk());
+    DCHECK_ALIGNED_PARAM(MaxNextOffset(), alignment);
+    DCHECK_ALIGNED_PARAM(next_thunk.MaxNextOffset(), alignment);
+    if (next_thunk.MaxNextOffset() - CodeSize() < MaxNextOffset()) {
+      max_next_offset_ = RoundDown(next_thunk.MaxNextOffset() - CodeSize(), alignment);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  uint32_t ReserveOffset(size_t offset) {
+    DCHECK(NeedsNextThunk());
+    DCHECK_LE(offset, max_next_offset_);
+    max_next_offset_ = 0u;  // The reserved offset should satisfy all pending references.
+    offsets_.push_back(offset);
+    return offset + CodeSize();
+  }
+
+  bool HasReservedOffset() const {
+    return !offsets_.empty();
+  }
+
+  uint32_t LastReservedOffset() const {
+    DCHECK(HasReservedOffset());
+    return offsets_.back();
+  }
+
+  bool HasPendingOffset() const {
+    return pending_offset_ != offsets_.size();
+  }
+
+  uint32_t GetPendingOffset() const {
+    DCHECK(HasPendingOffset());
+    return offsets_[pending_offset_];
+  }
+
+  void MarkPendingOffsetAsWritten() {
+    DCHECK(HasPendingOffset());
+    ++pending_offset_;
+  }
+
+  bool HasWrittenOffset() const {
+    return pending_offset_ != 0u;
+  }
+
+  uint32_t LastWrittenOffset() const {
+    DCHECK(HasWrittenOffset());
+    return offsets_[pending_offset_ - 1u];
+  }
+
+  size_t IndexOfFirstThunkAtOrAfter(uint32_t offset) const {
+    size_t number_of_thunks = NumberOfThunks();
+    for (size_t i = 0; i != number_of_thunks; ++i) {
+      if (GetThunkOffset(i) >= offset) {
+        return i;
+      }
+    }
+    return number_of_thunks;
+  }
+
+  size_t NumberOfThunks() const {
+    return offsets_.size();
+  }
+
+  uint32_t GetThunkOffset(size_t index) const {
+    DCHECK_LT(index, NumberOfThunks());
+    return offsets_[index];
+  }
+
+ private:
+  const ArrayRef<const uint8_t> code_;  // The code of the thunk.
+  const std::string debug_name_;        // The debug name of the thunk.
+  std::vector<uint32_t> offsets_;       // Offsets at which the thunk needs to be written.
+  uint32_t max_next_offset_;            // The maximum offset at which the next thunk can be placed.
+  uint32_t pending_offset_;             // The index of the next offset to write.
+};
+
+class Riscv64BaseRelativePatcher::PendingThunkComparator {
+ public:
+  bool operator()(const ThunkData* lhs, const ThunkData* rhs) const {
+    DCHECK(lhs->HasPendingOffset());
+    DCHECK(rhs->HasPendingOffset());
+    // The top of the heap is defined to contain the highest element and we want to pick
+    // the thunk with the smallest pending offset, so use the reverse ordering, i.e. ">".
+    return lhs->GetPendingOffset() > rhs->GetPendingOffset();
+  }
+};
+
+uint32_t Riscv64BaseRelativePatcher::ReserveSpace(uint32_t offset,
+                                              const CompiledMethod* compiled_method,
+                                              MethodReference method_ref) {
+  return ReserveSpaceInternal(offset, compiled_method, method_ref, 0u);
+}
+
+uint32_t Riscv64BaseRelativePatcher::ReserveSpaceEnd(uint32_t offset) {
+  // For multi-oat compilations (boot image), ReserveSpaceEnd() is called for each oat file.
+  // Since we do not know here whether this is the last file or whether the next opportunity
+  // to place thunk will be soon enough, we need to reserve all needed thunks now. Code for
+  // subsequent oat files can still call back to them.
+  if (!unprocessed_method_call_patches_.empty()) {
+    ResolveMethodCalls(offset, MethodReference(nullptr, dex::kDexNoIndex));
+  }
+  for (ThunkData* data : unreserved_thunks_) {
+    uint32_t thunk_offset = CompiledCode::AlignCode(offset, instruction_set_);
+    offset = data->ReserveOffset(thunk_offset);
+  }
+  unreserved_thunks_.clear();
+  // We also need to delay initiating the pending_thunks_ until the call to WriteThunks().
+  // Check that the `pending_thunks_.capacity()` indicates that no WriteThunks() has taken place.
+  DCHECK_EQ(pending_thunks_.capacity(), 0u);
+  return offset;
+}
+
+uint32_t Riscv64BaseRelativePatcher::WriteThunks(OutputStream* out, uint32_t offset) {
+  if (pending_thunks_.capacity() == 0u) {
+    if (thunks_.empty()) {
+      return offset;
+    }
+    // First call to WriteThunks(), prepare the thunks for writing.
+    pending_thunks_.reserve(thunks_.size());
+    for (auto& entry : thunks_) {
+      ThunkData* data = &entry.second;
+      if (data->HasPendingOffset()) {
+        pending_thunks_.push_back(data);
+      }
+    }
+    std::make_heap(pending_thunks_.begin(), pending_thunks_.end(), PendingThunkComparator());
+  }
+  uint32_t aligned_offset = CompiledMethod::AlignCode(offset, instruction_set_);
+  while (!pending_thunks_.empty() &&
+         pending_thunks_.front()->GetPendingOffset() == aligned_offset) {
+    // Write alignment bytes and code.
+    uint32_t aligned_code_delta = aligned_offset - offset;
+    if (aligned_code_delta != 0u && UNLIKELY(!WriteCodeAlignment(out, aligned_code_delta))) {
+      return 0u;
+    }
+    if (UNLIKELY(!WriteThunk(out, pending_thunks_.front()->GetCode()))) {
+      return 0u;
+    }
+    offset = aligned_offset + pending_thunks_.front()->CodeSize();
+    // Mark the thunk as written at the pending offset and update the `pending_thunks_` heap.
+    std::pop_heap(pending_thunks_.begin(), pending_thunks_.end(), PendingThunkComparator());
+    pending_thunks_.back()->MarkPendingOffsetAsWritten();
+    if (pending_thunks_.back()->HasPendingOffset()) {
+      std::push_heap(pending_thunks_.begin(), pending_thunks_.end(), PendingThunkComparator());
+    } else {
+      pending_thunks_.pop_back();
+    }
+    aligned_offset = CompiledMethod::AlignCode(offset, instruction_set_);
+  }
+  DCHECK(pending_thunks_.empty() || pending_thunks_.front()->GetPendingOffset() > aligned_offset);
+  return offset;
+}
+
+std::vector<debug::MethodDebugInfo> Riscv64BaseRelativePatcher::GenerateThunkDebugInfo(
+    uint32_t executable_offset) {
+  // For multi-oat compilation (boot image), `thunks_` records thunks for all oat files.
+  // To return debug info for the current oat file, we must ignore thunks before the
+  // `executable_offset` as they are in the previous oat files and this function must be
+  // called before reserving thunk positions for subsequent oat files.
+  size_t number_of_thunks = 0u;
+  for (auto&& entry : thunks_) {
+    const ThunkData& data = entry.second;
+    number_of_thunks += data.NumberOfThunks() - data.IndexOfFirstThunkAtOrAfter(executable_offset);
+  }
+  std::vector<debug::MethodDebugInfo> result;
+  result.reserve(number_of_thunks);
+  for (auto&& entry : thunks_) {
+    const ThunkData& data = entry.second;
+    size_t start = data.IndexOfFirstThunkAtOrAfter(executable_offset);
+    if (start == data.NumberOfThunks()) {
+      continue;
+    }
+    // Get the base name to use for the first occurrence of the thunk.
+    const std::string& base_name = data.GetDebugName();
+    for (size_t i = start, num = data.NumberOfThunks(); i != num; ++i) {
+      debug::MethodDebugInfo info = {};
+      if (i == 0u) {
+        info.custom_name = base_name;
+      } else {
+        // Add a disambiguating tag for subsequent identical thunks. Since the `thunks_`
+        // keeps records also for thunks in previous oat files, names based on the thunk
+        // index shall be unique across the whole multi-oat output.
+        info.custom_name = base_name + "_" + std::to_string(i);
+      }
+      info.isa = instruction_set_;
+      info.is_code_address_text_relative = true;
+      info.code_address = data.GetThunkOffset(i) - executable_offset;
+      info.code_size = data.CodeSize();
+      result.push_back(std::move(info));
+    }
+  }
+  return result;
+}
+
+Riscv64BaseRelativePatcher::Riscv64BaseRelativePatcher(RelativePatcherThunkProvider* thunk_provider,
+                                               RelativePatcherTargetProvider* target_provider,
+                                               InstructionSet instruction_set)
+    : thunk_provider_(thunk_provider),
+      target_provider_(target_provider),
+      instruction_set_(instruction_set),
+      thunks_(),
+      unprocessed_method_call_patches_(),
+      method_call_thunk_(nullptr),
+      pending_thunks_() {
+}
+
+Riscv64BaseRelativePatcher::~Riscv64BaseRelativePatcher() {
+  // All work done by member destructors.
+}
+
+uint32_t Riscv64BaseRelativePatcher::ReserveSpaceInternal(uint32_t offset,
+                                                      const CompiledMethod* compiled_method,
+                                                      MethodReference method_ref,
+                                                      uint32_t max_extra_space) {
+  // Adjust code size for extra space required by the subclass.
+  uint32_t max_code_size = compiled_method->GetQuickCode().size() + max_extra_space;
+  uint32_t code_offset;
+  uint32_t next_aligned_offset;
+  while (true) {
+    code_offset = compiled_method->AlignCode(offset + sizeof(OatQuickMethodHeader));
+    next_aligned_offset = compiled_method->AlignCode(code_offset + max_code_size);
+    if (unreserved_thunks_.empty() ||
+        unreserved_thunks_.front()->MaxNextOffset() >= next_aligned_offset) {
+      break;
+    }
+    ThunkData* thunk = unreserved_thunks_.front();
+    if (thunk == method_call_thunk_) {
+      ResolveMethodCalls(code_offset, method_ref);
+      // This may have changed `method_call_thunk_` data, so re-check if we need to reserve.
+      if (unreserved_thunks_.empty() ||
+          unreserved_thunks_.front()->MaxNextOffset() >= next_aligned_offset) {
+        break;
+      }
+      // We need to process the new `front()` whether it's still the `method_call_thunk_` or not.
+      thunk = unreserved_thunks_.front();
+    }
+    unreserved_thunks_.pop_front();
+    uint32_t thunk_offset = CompiledCode::AlignCode(offset, instruction_set_);
+    offset = thunk->ReserveOffset(thunk_offset);
+    if (thunk == method_call_thunk_) {
+      // All remaining method call patches will be handled by this thunk.
+      DCHECK(!unprocessed_method_call_patches_.empty());
+      DCHECK_LE(thunk_offset - unprocessed_method_call_patches_.front().GetPatchOffset(),
+                MaxPositiveDisplacement(GetMethodCallKey()));
+      unprocessed_method_call_patches_.clear();
+    }
+  }
+
+  // Process patches and check that adding thunks for the current method did not push any
+  // thunks (previously existing or newly added) before `next_aligned_offset`. This is
+  // essentially a check that we never compile a method that's too big. The calls or branches
+  // from the method should be able to reach beyond the end of the method and over any pending
+  // thunks. (The number of different thunks should be relatively low and their code short.)
+  ProcessPatches(compiled_method, code_offset);
+  CHECK(unreserved_thunks_.empty() ||
+        unreserved_thunks_.front()->MaxNextOffset() >= next_aligned_offset);
+
+  return offset;
+}
+
+uint32_t Riscv64BaseRelativePatcher::CalculateMethodCallDisplacement(uint32_t patch_offset,
+                                                                 uint32_t target_offset) {
+  DCHECK(method_call_thunk_ != nullptr);
+  // Unsigned arithmetic with its well-defined overflow behavior is just fine here.
+  uint32_t displacement = target_offset - patch_offset;
+  uint32_t max_positive_displacement = MaxPositiveDisplacement(GetMethodCallKey());
+  uint32_t max_negative_displacement = MaxNegativeDisplacement(GetMethodCallKey());
+  // NOTE: With unsigned arithmetic we do mean to use && rather than || below.
+  if (displacement > max_positive_displacement && displacement < -max_negative_displacement) {
+    // Unwritten thunks have higher offsets, check if it's within range.
+    DCHECK(!method_call_thunk_->HasPendingOffset() ||
+           method_call_thunk_->GetPendingOffset() > patch_offset);
+    if (method_call_thunk_->HasPendingOffset() &&
+        method_call_thunk_->GetPendingOffset() - patch_offset <= max_positive_displacement) {
+      displacement = method_call_thunk_->GetPendingOffset() - patch_offset;
+    } else {
+      // We must have a previous thunk then.
+      DCHECK(method_call_thunk_->HasWrittenOffset());
+      DCHECK_LT(method_call_thunk_->LastWrittenOffset(), patch_offset);
+      displacement = method_call_thunk_->LastWrittenOffset() - patch_offset;
+      DCHECK_GE(displacement, -max_negative_displacement);
+    }
+  }
+  return displacement;
+}
+
+uint32_t Riscv64BaseRelativePatcher::GetThunkTargetOffset(const ThunkKey& key, uint32_t patch_offset) {
+  auto it = thunks_.find(key);
+  CHECK(it != thunks_.end());
+  const ThunkData& data = it->second;
+  if (data.HasWrittenOffset()) {
+    uint32_t offset = data.LastWrittenOffset();
+    DCHECK_LT(offset, patch_offset);
+    if (patch_offset - offset <= MaxNegativeDisplacement(key)) {
+      return offset;
+    }
+  }
+  DCHECK(data.HasPendingOffset());
+  uint32_t offset = data.GetPendingOffset();
+  DCHECK_GT(offset, patch_offset);
+  DCHECK_LE(offset - patch_offset, MaxPositiveDisplacement(key));
+  return offset;
+}
+
+Riscv64BaseRelativePatcher::ThunkKey Riscv64BaseRelativePatcher::GetMethodCallKey() {
+  return ThunkKey(ThunkType::kMethodCall);
+}
+
+Riscv64BaseRelativePatcher::ThunkKey Riscv64BaseRelativePatcher::GetBakerThunkKey(
+    const LinkerPatch& patch) {
+  DCHECK_EQ(patch.GetType(), LinkerPatch::Type::kBakerReadBarrierBranch);
+  return ThunkKey(ThunkType::kBakerReadBarrier,
+                  patch.GetBakerCustomValue1(),
+                  patch.GetBakerCustomValue2());
+}
+
+void Riscv64BaseRelativePatcher::ProcessPatches(const CompiledMethod* compiled_method,
+                                            uint32_t code_offset) {
+  for (const LinkerPatch& patch : compiled_method->GetPatches()) {
+    uint32_t patch_offset = code_offset + patch.LiteralOffset();
+    ThunkKey key(static_cast<ThunkType>(-1));
+    ThunkData* old_data = nullptr;
+    if (patch.GetType() == LinkerPatch::Type::kCallRelative) {
+      key = GetMethodCallKey();
+      unprocessed_method_call_patches_.emplace_back(patch_offset, patch.TargetMethod());
+      if (method_call_thunk_ == nullptr) {
+        uint32_t max_next_offset = CalculateMaxNextOffset(patch_offset, key);
+        auto it = thunks_.Put(key, ThunkDataForPatch(patch, max_next_offset));
+        method_call_thunk_ = &it->second;
+        AddUnreservedThunk(method_call_thunk_);
+      } else {
+        old_data = method_call_thunk_;
+      }
+    } else if (patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch) {
+      key = GetBakerThunkKey(patch);
+      auto lb = thunks_.lower_bound(key);
+      if (lb == thunks_.end() || thunks_.key_comp()(key, lb->first)) {
+        uint32_t max_next_offset = CalculateMaxNextOffset(patch_offset, key);
+        auto it = thunks_.PutBefore(lb, key, ThunkDataForPatch(patch, max_next_offset));
+        AddUnreservedThunk(&it->second);
+      } else {
+        old_data = &lb->second;
+      }
+    }
+    if (old_data != nullptr) {
+      // Shared path where an old thunk may need an update.
+      DCHECK(key.GetType() != static_cast<ThunkType>(-1));
+      DCHECK(!old_data->HasReservedOffset() || old_data->LastReservedOffset() < patch_offset);
+      if (old_data->NeedsNextThunk()) {
+        // Patches for a method are ordered by literal offset, so if we still need to place
+        // this thunk for a previous patch, that thunk shall be in range for this patch.
+        DCHECK_LE(old_data->MaxNextOffset(), CalculateMaxNextOffset(patch_offset, key));
+      } else {
+        if (!old_data->HasReservedOffset() ||
+            patch_offset - old_data->LastReservedOffset() > MaxNegativeDisplacement(key)) {
+          old_data->SetMaxNextOffset(CalculateMaxNextOffset(patch_offset, key));
+          AddUnreservedThunk(old_data);
+        }
+      }
+    }
+  }
+}
+
+void Riscv64BaseRelativePatcher::AddUnreservedThunk(ThunkData* data) {
+  DCHECK(data->NeedsNextThunk());
+  size_t index = unreserved_thunks_.size();
+  while (index != 0u && data->MaxNextOffset() < unreserved_thunks_[index - 1u]->MaxNextOffset()) {
+    --index;
+  }
+  unreserved_thunks_.insert(unreserved_thunks_.begin() + index, data);
+  // We may need to update the max next offset(s) if the thunk code would not fit.
+  size_t alignment = GetInstructionSetAlignment(instruction_set_);
+  if (index + 1u != unreserved_thunks_.size()) {
+    // Note: Ignore the return value as we need to process previous thunks regardless.
+    data->MakeSpaceBefore(*unreserved_thunks_[index + 1u], alignment);
+  }
+  // Make space for previous thunks. Once we find a pending thunk that does
+  // not need an adjustment, we can stop.
+  while (index != 0u && unreserved_thunks_[index - 1u]->MakeSpaceBefore(*data, alignment)) {
+    --index;
+    data = unreserved_thunks_[index];
+  }
+}
+
+void Riscv64BaseRelativePatcher::ResolveMethodCalls(uint32_t quick_code_offset,
+                                                MethodReference method_ref) {
+  DCHECK(!unreserved_thunks_.empty());
+  DCHECK(!unprocessed_method_call_patches_.empty());
+  DCHECK(method_call_thunk_ != nullptr);
+  uint32_t max_positive_displacement = MaxPositiveDisplacement(GetMethodCallKey());
+  uint32_t max_negative_displacement = MaxNegativeDisplacement(GetMethodCallKey());
+  // Process as many patches as possible, stop only on unresolved targets or calls too far back.
+  while (!unprocessed_method_call_patches_.empty()) {
+    MethodReference target_method = unprocessed_method_call_patches_.front().GetTargetMethod();
+    uint32_t patch_offset = unprocessed_method_call_patches_.front().GetPatchOffset();
+    DCHECK(!method_call_thunk_->HasReservedOffset() ||
+           method_call_thunk_->LastReservedOffset() <= patch_offset);
+    if (!method_call_thunk_->HasReservedOffset() ||
+        patch_offset - method_call_thunk_->LastReservedOffset() > max_negative_displacement) {
+      // No previous thunk in range, check if we can reach the target directly.
+      if (target_method == method_ref) {
+        DCHECK_GT(quick_code_offset, patch_offset);
+        if (quick_code_offset - patch_offset > max_positive_displacement) {
+          break;
+        }
+      } else {
+        auto result = target_provider_->FindMethodOffset(target_method);
+        if (!result.first) {
+          break;
+        }
+        uint32_t target_offset = result.second - CompiledCode::CodeDelta(instruction_set_);
+        if (target_offset >= patch_offset) {
+          DCHECK_LE(target_offset - patch_offset, max_positive_displacement);
+        } else if (patch_offset - target_offset > max_negative_displacement) {
+          break;
+        }
+      }
+    }
+    unprocessed_method_call_patches_.pop_front();
+  }
+  if (!unprocessed_method_call_patches_.empty()) {
+    // Try to adjust the max next offset in `method_call_thunk_`. Do this conservatively only if
+    // the thunk shall be at the end of the `unreserved_thunks_` to avoid dealing with overlaps.
+    uint32_t new_max_next_offset =
+        unprocessed_method_call_patches_.front().GetPatchOffset() + max_positive_displacement;
+    if (new_max_next_offset >
+        unreserved_thunks_.back()->MaxNextOffset() + unreserved_thunks_.back()->CodeSize()) {
+      method_call_thunk_->ClearMaxNextOffset();
+      method_call_thunk_->SetMaxNextOffset(new_max_next_offset);
+      if (method_call_thunk_ != unreserved_thunks_.back()) {
+        RemoveElement(unreserved_thunks_, method_call_thunk_);
+        unreserved_thunks_.push_back(method_call_thunk_);
+      }
+    }
+  } else {
+    // We have resolved all method calls, we do not need a new thunk anymore.
+    method_call_thunk_->ClearMaxNextOffset();
+    RemoveElement(unreserved_thunks_, method_call_thunk_);
+  }
+}
+
+inline uint32_t Riscv64BaseRelativePatcher::CalculateMaxNextOffset(uint32_t patch_offset,
+                                                               const ThunkKey& key) {
+  return RoundDown(patch_offset + MaxPositiveDisplacement(key),
+                   GetInstructionSetAlignment(instruction_set_));
+}
+
+inline Riscv64BaseRelativePatcher::ThunkData Riscv64BaseRelativePatcher::ThunkDataForPatch(
+    const LinkerPatch& patch, uint32_t max_next_offset) {
+  ArrayRef<const uint8_t> code;
+  std::string debug_name;
+  thunk_provider_->GetThunkCode(patch, &code, &debug_name);
+  DCHECK(!code.empty());
+  return ThunkData(code, debug_name, max_next_offset);
+}
+
+}  // namespace linker
+}  // namespace art
diff --git a/dex2oat/linker/riscv64/relative_patcher_riscv64_base.h b/dex2oat/linker/riscv64/relative_patcher_riscv64_base.h
new file mode 100644
index 0000000000..5170665fc2
--- /dev/null
+++ b/dex2oat/linker/riscv64/relative_patcher_riscv64_base.h
@@ -0,0 +1,158 @@
+/*
+ * Copyright (C) 2015 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_DEX2OAT_LINKER_RISCV64_RELATIVE_PATCHER_RISCV64_BASE_H_
+#define ART_DEX2OAT_LINKER_RISCV64_RELATIVE_PATCHER_RISCV64_BASE_H_
+
+#include <deque>
+#include <vector>
+
+#include "base/safe_map.h"
+#include "dex/method_reference.h"
+#include "linker/relative_patcher.h"
+
+namespace art {
+namespace linker {
+
+class Riscv64BaseRelativePatcher : public RelativePatcher {
+ public:
+  uint32_t ReserveSpace(uint32_t offset,
+                        const CompiledMethod* compiled_method,
+                        MethodReference method_ref) override;
+  uint32_t ReserveSpaceEnd(uint32_t offset) override;
+  uint32_t WriteThunks(OutputStream* out, uint32_t offset) override;
+  std::vector<debug::MethodDebugInfo> GenerateThunkDebugInfo(uint32_t executable_offset) override;
+
+ protected:
+  Riscv64BaseRelativePatcher(RelativePatcherThunkProvider* thunk_provider,
+                         RelativePatcherTargetProvider* target_provider,
+                         InstructionSet instruction_set);
+  ~Riscv64BaseRelativePatcher();
+
+  enum class ThunkType {
+    kMethodCall,              // Method call thunk.
+    kBakerReadBarrier,        // Baker read barrier.
+  };
+
+  class ThunkKey {
+   public:
+    explicit ThunkKey(ThunkType type, uint32_t custom_value1 = 0u, uint32_t custom_value2 = 0u)
+        : type_(type), custom_value1_(custom_value1), custom_value2_(custom_value2) { }
+
+    ThunkType GetType() const {
+      return type_;
+    }
+
+    uint32_t GetCustomValue1() const {
+      return custom_value1_;
+    }
+
+    uint32_t GetCustomValue2() const {
+      return custom_value2_;
+    }
+
+   private:
+    ThunkType type_;
+    uint32_t custom_value1_;
+    uint32_t custom_value2_;
+  };
+
+  class ThunkKeyCompare {
+   public:
+    bool operator()(const ThunkKey& lhs, const ThunkKey& rhs) const {
+      if (lhs.GetType() != rhs.GetType()) {
+        return lhs.GetType() < rhs.GetType();
+      }
+      if (lhs.GetCustomValue1() != rhs.GetCustomValue1()) {
+        return lhs.GetCustomValue1() < rhs.GetCustomValue1();
+      }
+      return lhs.GetCustomValue2() < rhs.GetCustomValue2();
+    }
+  };
+
+  static ThunkKey GetMethodCallKey();
+  static ThunkKey GetBakerThunkKey(const LinkerPatch& patch);
+
+  uint32_t ReserveSpaceInternal(uint32_t offset,
+                                const CompiledMethod* compiled_method,
+                                MethodReference method_ref,
+                                uint32_t max_extra_space);
+  uint32_t GetThunkTargetOffset(const ThunkKey& key, uint32_t patch_offset);
+
+  uint32_t CalculateMethodCallDisplacement(uint32_t patch_offset,
+                                           uint32_t target_offset);
+
+  virtual uint32_t MaxPositiveDisplacement(const ThunkKey& key) = 0;
+  virtual uint32_t MaxNegativeDisplacement(const ThunkKey& key) = 0;
+
+ private:
+  class ThunkData;
+
+  void ProcessPatches(const CompiledMethod* compiled_method, uint32_t code_offset);
+  void AddUnreservedThunk(ThunkData* data);
+
+  void ResolveMethodCalls(uint32_t quick_code_offset, MethodReference method_ref);
+
+  uint32_t CalculateMaxNextOffset(uint32_t patch_offset, const ThunkKey& key);
+  ThunkData ThunkDataForPatch(const LinkerPatch& patch, uint32_t max_next_offset);
+
+  RelativePatcherThunkProvider* const thunk_provider_;
+  RelativePatcherTargetProvider* const target_provider_;
+  const InstructionSet instruction_set_;
+
+  // The data for all thunks.
+  // SafeMap<> nodes don't move after being inserted, so we can use direct pointers to the data.
+  using ThunkMap = SafeMap<ThunkKey, ThunkData, ThunkKeyCompare>;
+  ThunkMap thunks_;
+
+  // ReserveSpace() tracks unprocessed method call patches. These may be resolved later.
+  class UnprocessedMethodCallPatch {
+   public:
+    UnprocessedMethodCallPatch(uint32_t patch_offset, MethodReference target_method)
+        : patch_offset_(patch_offset), target_method_(target_method) { }
+
+    uint32_t GetPatchOffset() const {
+      return patch_offset_;
+    }
+
+    MethodReference GetTargetMethod() const {
+      return target_method_;
+    }
+
+   private:
+    uint32_t patch_offset_;
+    MethodReference target_method_;
+  };
+  std::deque<UnprocessedMethodCallPatch> unprocessed_method_call_patches_;
+  // Once we have compiled a method call thunk, cache pointer to the data.
+  ThunkData* method_call_thunk_;
+
+  // Thunks
+  std::deque<ThunkData*> unreserved_thunks_;
+
+  class PendingThunkComparator;
+  std::vector<ThunkData*> pending_thunks_;  // Heap with the PendingThunkComparator.
+
+  friend class Arm64RelativePatcherTest;
+  friend class Thumb2RelativePatcherTest;
+
+  DISALLOW_COPY_AND_ASSIGN(Riscv64BaseRelativePatcher);
+};
+
+}  // namespace linker
+}  // namespace art
+
+#endif  // ART_DEX2OAT_LINKER_RISCV64_RELATIVE_PATCHER_RISCV64_BASE_H_
diff --git a/dex2oat/linker/riscv64/relative_patcher_riscv64_test.cc b/dex2oat/linker/riscv64/relative_patcher_riscv64_test.cc
new file mode 100644
index 0000000000..a5c6d43ae4
--- /dev/null
+++ b/dex2oat/linker/riscv64/relative_patcher_riscv64_test.cc
@@ -0,0 +1,97 @@
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "linker/riscv64/relative_patcher_riscv64.h"
+
+#include "linker/relative_patcher_test.h"
+
+namespace art {
+namespace linker {
+
+class Riscv64RelativePatcherTest : public RelativePatcherTest {
+ public:
+  Riscv64RelativePatcherTest() : RelativePatcherTest(InstructionSet::kRiscv64, "default") {}
+
+ protected:
+  static const uint8_t kUnpatchedPcRelativeRawCode[];
+  static const uint8_t kUnpatchedPcRelativeCallRawCode[];
+  static const uint32_t kLiteralOffsetHigh;
+  static const uint32_t kLiteralOffsetLow1;
+  static const uint32_t kLiteralOffsetLow2;
+  static const uint32_t kAnchorOffset;
+  static const ArrayRef<const uint8_t> kUnpatchedPcRelativeCode;
+
+  uint32_t GetMethodOffset(uint32_t method_idx) {
+    auto result = method_offset_map_.FindMethodOffset(MethodRef(method_idx));
+    CHECK(result.first);
+    return result.second;
+  }
+
+  void CheckPcRelativePatch(const ArrayRef<const LinkerPatch>& patches, uint32_t target_offset);
+  void TestStringBssEntry(uint32_t bss_begin, uint32_t string_entry_offset);
+  void TestStringReference(uint32_t string_offset);
+};
+
+const uint8_t Riscv64RelativePatcherTest::kUnpatchedPcRelativeRawCode[] = {
+    0x34, 0x12, 0x5E, 0xEE,  // auipc  s2, high(diff); placeholder = 0x1234
+    0x78, 0x56, 0x52, 0x66,  // daddiu s2, s2, low(diff); placeholder = 0x5678
+    0x78, 0x56, 0x52, 0x9E,  // lwu    s2, (low(diff))(s2) ; placeholder = 0x5678
+};
+const uint32_t Riscv64RelativePatcherTest::kLiteralOffsetHigh = 0;  // At auipc.
+const uint32_t Riscv64RelativePatcherTest::kLiteralOffsetLow1 = 4;  // At daddiu.
+const uint32_t Riscv64RelativePatcherTest::kLiteralOffsetLow2 = 8;  // At lwu.
+const uint32_t Riscv64RelativePatcherTest::kAnchorOffset = 0;  // At auipc (where PC+0 points).
+const ArrayRef<const uint8_t> Riscv64RelativePatcherTest::kUnpatchedPcRelativeCode(
+    kUnpatchedPcRelativeRawCode);
+
+void Riscv64RelativePatcherTest::CheckPcRelativePatch(const ArrayRef<const LinkerPatch>& patches,
+                                                     uint32_t target_offset) {
+  AddCompiledMethod(MethodRef(1u), kUnpatchedPcRelativeCode, ArrayRef<const LinkerPatch>(patches));
+  Link();
+
+  auto result = method_offset_map_.FindMethodOffset(MethodRef(1u));
+  ASSERT_TRUE(result.first);
+
+  uint32_t diff = target_offset - (result.second + kAnchorOffset);
+  diff += (diff & 0x8000) << 1;  // Account for sign extension in daddiu/lwu.
+
+  const uint8_t expected_code[] = {
+      static_cast<uint8_t>(diff >> 16), static_cast<uint8_t>(diff >> 24), 0x5E, 0xEE,
+      static_cast<uint8_t>(diff), static_cast<uint8_t>(diff >> 8), 0x52, 0x66,
+      static_cast<uint8_t>(diff), static_cast<uint8_t>(diff >> 8), 0x52, 0x9E,
+  };
+  EXPECT_TRUE(CheckLinkedMethod(MethodRef(1u), ArrayRef<const uint8_t>(expected_code)));
+}
+
+void Riscv64RelativePatcherTest::TestStringBssEntry(uint32_t bss_begin,
+                                                   uint32_t string_entry_offset) {
+  constexpr uint32_t kStringIndex = 1u;
+  string_index_to_offset_map_.Put(kStringIndex, string_entry_offset);
+  bss_begin_ = bss_begin;
+  LinkerPatch patches[] = {
+      LinkerPatch::StringBssEntryPatch(kLiteralOffsetHigh, nullptr, kAnchorOffset, kStringIndex),
+      LinkerPatch::StringBssEntryPatch(kLiteralOffsetLow1, nullptr, kAnchorOffset, kStringIndex),
+      LinkerPatch::StringBssEntryPatch(kLiteralOffsetLow2, nullptr, kAnchorOffset, kStringIndex)
+  };
+  CheckPcRelativePatch(ArrayRef<const LinkerPatch>(patches), bss_begin_ + string_entry_offset);
+}
+
+TEST_F(Riscv64RelativePatcherTest, StringBssEntry) {
+  TestStringBssEntry(/* bss_begin */ 0x12345678, /* string_entry_offset */ 0x1234);
+}
+
+}  // namespace linker
+}  // namespace art
diff --git a/dexlayout/Android.bp b/dexlayout/Android.bp
index 838510be37..a78528a61e 100644
--- a/dexlayout/Android.bp
+++ b/dexlayout/Android.bp
@@ -76,7 +76,7 @@ art_cc_library {
     target: {
         android: {
             lto: {
-                 thin: true,
+                 thin: false,
             },
         },
         windows: {
diff --git a/disassembler/Android.bp b/disassembler/Android.bp
index 5aa159e98b..324ee314db 100644
--- a/disassembler/Android.bp
+++ b/disassembler/Android.bp
@@ -30,6 +30,9 @@ art_cc_defaults {
         arm64: {
             srcs: ["disassembler_arm64.cc"]
         },
+        riscv64: {
+            srcs: ["disassembler_riscv64.cc"]
+        },
         // TODO: We should also conditionally include the MIPS32/MIPS64 and the
         // x86/x86-64 disassembler definitions (b/119090273). However, using the
         // following syntax here:
@@ -84,3 +87,23 @@ art_cc_library {
         "libvixld",
     ],
 }
+
+art_cc_test {
+    name: "disassembler_host_tests",
+    device_supported: false,
+    defaults: [
+        "art_gtest_defaults",
+    ],
+    codegen: {
+        riscv64: {
+            srcs: [
+                "disassembler_riscv64_test.cc",
+            ],
+        },
+    },
+    shared_libs: [
+        "libartd-compiler",
+        "libvixld",
+        "libartd-disassembler",
+    ],
+}
diff --git a/disassembler/disassembler.cc b/disassembler/disassembler.cc
index 0662334852..f7d24fd69a 100644
--- a/disassembler/disassembler.cc
+++ b/disassembler/disassembler.cc
@@ -37,6 +37,10 @@
 # include "disassembler_x86.h"
 #endif
 
+#ifdef ART_ENABLE_CODEGEN_riscv64
+# include "disassembler_riscv64.h"
+#endif
+
 using android::base::StringPrintf;
 
 namespace art {
@@ -72,6 +76,10 @@ Disassembler* Disassembler::Create(InstructionSet instruction_set, DisassemblerO
 #ifdef ART_ENABLE_CODEGEN_x86_64
     case InstructionSet::kX86_64:
       return new x86::DisassemblerX86(options, /* supports_rex= */ true);
+#endif
+#ifdef ART_ENABLE_CODEGEN_riscv64
+    case InstructionSet::kRiscv64:
+      return new riscv64::DisassemblerRiscv64(options);
 #endif
     default:
       UNIMPLEMENTED(FATAL) << static_cast<uint32_t>(instruction_set);
diff --git a/disassembler/disassembler_riscv64.cc b/disassembler/disassembler_riscv64.cc
new file mode 100644
index 0000000000..a982817db9
--- /dev/null
+++ b/disassembler/disassembler_riscv64.cc
@@ -0,0 +1,598 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "disassembler_riscv64.h"
+
+#include <ostream>
+#include <sstream>
+#include <memory>
+#include <iostream>
+#include <tuple>
+
+#include "android-base/logging.h"
+#include "android-base/stringprintf.h"
+
+#include "base/bit_utils.h"
+
+using android::base::StringPrintf;
+
+namespace art {
+namespace riscv64 {
+
+enum GpuReg{};
+enum FpuReg{};
+enum CSReg{};
+enum Ordering{};
+
+template<typename Tuple, typename Func, size_t N>
+struct __tuple_processor {
+  inline static void __tuple_process(Tuple &t, size_t size, Func& f) {
+    __tuple_processor<Tuple, Func, N - 1>::__tuple_process(t, size, f);
+    f(size, N - 1, std::get<N - 1>(t));
+  }
+};
+
+template<typename Tuple, typename Func>
+struct __tuple_processor<Tuple, Func, 1> {
+  inline static void __tuple_process(Tuple &t, size_t size, Func& f) {
+    f(size, 0, std::get<0>(t));
+  }
+};
+
+template<typename Tuple, typename Func>
+struct __tuple_processor<Tuple, Func, 0> {
+  inline static void __tuple_process(Tuple &, size_t, Func& ) {
+  }
+};
+
+template<typename... Args, typename Func>
+void tuple_for_each(std::tuple<Args...> &t, Func &&f) {
+  __tuple_processor<decltype(t), Func, sizeof...(Args)>::__tuple_process(t, sizeof...(Args), f);
+}
+
+template<typename Type>
+std::string ToString(Type v) {
+  std::stringstream ss;
+  ss << v;
+  return ss.str();
+}
+
+std::string ToString(GpuReg id) {
+  static const char* names[32] = {
+    "zero", "ra", "sp", "gp", "tp", "t0", "t1", "t2",
+    "fp", "s1", "a0", "a1", "a2", "a3", "a4", "a5",
+    "a6", "a7", "s2", "s3", "s4", "s5", "s6", "s7",
+    "s8", "s9", "s10", "s11", "t3", "t4", "t5", "t6"
+  };
+  return names[id];
+}
+
+std::string ToString(FpuReg id) {
+  static const char* names[32] = {
+    "ft0", "ft1", "ft2", "ft3", "ft4", "ft5", "ft6", "ft7",
+    "fs0", "fs1", "fa0", "fa1", "fa2", "fa3", "fa4", "fa5",
+    "fa6", "fa7", "fs2", "fs3", "fs4", "fs5", "fs6", "fs7",
+    "fs8", "fs9", "fs10", "fs11", "ft8", "ft9", "ft10", "ft11"
+  };
+  return names[id];
+}
+
+std::string ToString(CSReg id) {
+  switch (static_cast<uint32_t>(id)) {
+  case 0x000: return "ustatus";
+  case 0x001: return "fflags";
+  case 0x002: return "frm";
+  case 0x003: return "fcsr";
+  case 0x004: return "uie";
+  case 0x005: return "utvec";
+  case 0x040: return "uscratch";
+  case 0x041: return "uepc";
+  case 0x042: return "ucause";
+  case 0x043: return "utval";
+  case 0x044: return "uip";
+  case 0xC00: return "cycle";
+  case 0xC01: return "time";
+  case 0xC02: return "instret";
+  default:
+    return StringPrintf("0x%x", id);
+  }
+}
+
+std::string ToString(Ordering order) {
+  std::stringstream ss;
+  if ((0xF & order) == 0) {
+    ss << "unknown";
+  } else {
+    if ((0x8 & order) != 0) ss << "i";
+    if ((0x4 & order) != 0) ss << "o";
+    if ((0x2 & order) != 0) ss << "r";
+    if ((0x1 & order) != 0) ss << "w";
+  }
+  return ss.str();
+}
+
+template<typename Type>
+bool IsReg(Type) { return false; }
+
+template<>
+bool IsReg(GpuReg) { return true; }
+
+struct InstructionInfo {
+  uint32_t mask;
+  uint32_t value;
+  const char* name;
+  // const char* type;
+  std::string type;
+};
+
+struct Instruction {
+  explicit Instruction(const char* name) : name_(name) {}
+  virtual ~Instruction() {}
+
+  std::string Dump(uint32_t data) {
+    DecodeFields(data);
+
+    std::stringstream ss;
+    ss << this->name_;
+
+    std::string fields_str = DumpFields(data);
+    if (!fields_str.empty()) {
+      ss << " " << fields_str;
+    }
+
+    std::string comment_str = DumpComments(data);
+    if (!comment_str.empty()) {
+      ss << ";" << comment_str;
+    }
+
+    return ss.str();
+  }
+
+  virtual void DecodeFields(uint32_t data ATTRIBUTE_UNUSED) { }
+  virtual std::string DumpFields(uint32_t data ATTRIBUTE_UNUSED) { return ""; }
+  virtual std::string DumpComments(uint32_t data ATTRIBUTE_UNUSED) { return ""; }
+
+  const char* name_;
+};
+
+template<typename _Type, uint32_t bits, uint32_t shift, uint32_t position = 0>
+struct InstructionField {
+  using Type = _Type;
+
+  void Decode(uint32_t encoding) {
+    uint32_t mask = 0xFFFFFFFF >> (32-bits) << shift;
+    int32_t sign_extend = static_cast<int32_t>(encoding & mask) >> shift << position;
+    value = static_cast<Type>(sign_extend);
+  }
+
+  Type value;
+};
+
+template<typename _Type, typename ..._SubFields>
+struct InstructionFieldComp {
+  using Type = _Type;
+  using SubFields = std::tuple<_SubFields...>;
+
+  void Decode(uint32_t encoding) {
+    tuple_for_each(fields, [this, &encoding](size_t, size_t, auto& field) {
+      field.Decode(encoding);
+      value = value | field.value;
+    });
+  }
+
+  Type value = 0;
+  SubFields fields;
+};
+
+template<bool address_mode = false, typename ...Fields>
+struct InstructionBase : public Instruction {
+  using Instruction::Instruction;
+  using FieldTuple = std::tuple<Fields...>;
+
+  void DecodeFields(uint32_t data) override {
+    tuple_for_each(fields, [&](size_t, size_t, auto& field) { field.Decode(data); });
+  }
+
+  std::string DumpFields(uint32_t data ATTRIBUTE_UNUSED) override {
+    std::stringstream ss;
+    tuple_for_each(fields, [&](size_t size, size_t i, auto& field) {
+      ss << ToString(field.value);
+
+      if (i == size - 2 && address_mode) {
+        if (IsReg(field.value)) {
+          ss << ",(";
+        } else {
+          ss << "(";
+        }
+      } else if (i == size - 1 && address_mode) {
+        ss << ")";
+      } else if (i < size - 1) {
+        ss << ",";
+      }
+    });
+    return ss.str();
+  }
+
+  FieldTuple fields;
+};
+
+typedef InstructionField<GpuReg, 5, 7> RD;
+typedef InstructionField<GpuReg, 5, 15> RS1;
+typedef InstructionField<GpuReg, 5, 20> RS2;
+typedef InstructionField<FpuReg, 5, 7> FRD;
+typedef InstructionField<FpuReg, 5, 15> FRS1;
+typedef InstructionField<FpuReg, 5, 20> FRS2;
+typedef InstructionField<FpuReg, 5, 27> FRS3;
+typedef InstructionField<CSReg, 12, 20> CSR;
+typedef InstructionField<int32_t, 12, 20> II;  // I type instruction, signed imm12
+typedef InstructionField<uint32_t, 12, 20> IU;  // I type instruction, unsigned imm12
+typedef InstructionField<uint32_t, 5, 20> Shamt;
+typedef InstructionField<int32_t, 20, 12> UI;
+
+// Naming rule: Instruction[print-mode][instruction-type][operand-type]
+// print-mode         G:general, A:address likely
+// instruction-type   R/I/S/B/U/J
+// operand-type       r:general register, f:float point register, i: immediate
+typedef InstructionBase<false, RD, RS1, RS2> InstructionRrrr;
+typedef InstructionBase<true, RD, RS1> InstructionARrr;
+typedef InstructionBase<true, RD, RS2, RS1> InstructionARrrr;
+typedef InstructionBase<false, FRD, FRS1> InstructionRff;
+typedef InstructionBase<false, RD, FRS1> InstructionRrf;
+typedef InstructionBase<false, FRD, RS1> InstructionRfr;
+typedef InstructionBase<false, FRD, FRS1, FRS2> InstructionRfff;
+typedef InstructionBase<false, RD, FRS1, FRS2> InstructionRrff;
+typedef InstructionBase<false, FRD, FRS1, FRS2, FRS3> InstructionRffff;
+
+typedef InstructionBase<false, RD, RS1, II> InstructionIrri;
+typedef InstructionBase<false, RD, II, RS1> InstructionIrir;
+typedef InstructionBase<true, RD, II, RS1> InstructionAIrir;
+typedef InstructionBase<true, FRD, II, RS1> InstructionAIfir;
+typedef InstructionBase<false, RD, RS1, IU> InstructionIrru;
+typedef InstructionBase<false, RD, InstructionField<CSReg, 12, 20>,
+    InstructionField<uint16_t, 5, 15>> InstructionIrii;
+typedef InstructionBase<false, InstructionField<Ordering, 4, 24>,
+    InstructionField<Ordering, 4, 20>> InstructionIii;
+typedef InstructionBase<false> InstructionI0;
+
+typedef InstructionBase<true, RS2, InstructionFieldComp<int16_t,
+    InstructionField<uint32_t, 5, 7, 0>,
+    InstructionField<int32_t, 7, 25, 5>>, RS1> InstructionS;
+
+typedef InstructionBase<true, FRS2, InstructionFieldComp<int16_t,
+    InstructionField<uint32_t, 5, 7, 0>,
+    InstructionField<int32_t, 7, 25, 5>>, RS1> InstructionSfir;
+
+typedef InstructionBase<false, RS1, RS2, InstructionFieldComp<int16_t,
+    InstructionField<uint32_t, 1, 7, 11>,
+    InstructionField<uint32_t, 4, 8, 1>,
+    InstructionField<uint32_t, 6, 25, 5>,
+    InstructionField<int32_t, 1, 31, 12>>> InstructionB;
+typedef InstructionBase<false, RD, RS1, Shamt> InstructionRrri;
+typedef InstructionBase<false, RD, UI> InstructionU;
+typedef InstructionBase<false, RD, InstructionFieldComp<int32_t,
+    InstructionField<uint32_t, 8, 12, 12>,
+    InstructionField<uint32_t, 1, 20, 11>,
+    InstructionField<uint32_t, 10, 21, 1>,
+    InstructionField<int32_t, 1, 31, 20>>> InstructionJ;
+
+std::unique_ptr<Instruction> CreateInstruction(InstructionInfo* info) {
+  if (info->type == "Rrrr") {
+    return std::make_unique<InstructionRrrr>(info->name);
+  } else if (info->type == "ARrr") {
+    return std::make_unique<InstructionARrr>(info->name);
+  } else if (info->type == "ARrrr") {
+    return std::make_unique<InstructionARrrr>(info->name);
+  } else if (info->type == "Rff") {
+    return std::make_unique<InstructionRff>(info->name);
+  } else if (info->type == "Rrf") {
+    return std::make_unique<InstructionRrf>(info->name);
+  } else if (info->type == "Rfr") {
+    return std::make_unique<InstructionRfr>(info->name);
+  } else if (info->type == "Rfff") {
+    return std::make_unique<InstructionRfff>(info->name);
+  } else if (info->type == "Rrff") {
+    return std::make_unique<InstructionRrff>(info->name);
+  } else if (info->type == "Rffff") {
+    return std::make_unique<InstructionRffff>(info->name);
+  } else if (info->type == "Rrri") {
+    return std::make_unique<InstructionRrri>(info->name);
+  } else if (info->type == "AIrir") {
+    return std::make_unique<InstructionAIrir>(info->name);
+  } else if (info->type == "AIfir") {
+    return std::make_unique<InstructionAIfir>(info->name);
+  } else if (info->type == "Irri") {
+    return std::make_unique<InstructionIrri>(info->name);
+  } else if (info->type == "Irii") {
+    return std::make_unique<InstructionIrii>(info->name);
+  } else if (info->type == "Irir") {
+    return std::make_unique<InstructionIrir>(info->name);
+  } else if (info->type == "Iii") {
+    return std::make_unique<InstructionIii>(info->name);
+  } else if (info->type == "I0") {
+    return std::make_unique<InstructionI0>(info->name);
+  } else if (info->type == "S") {
+    return std::make_unique<InstructionS>(info->name);
+  } else if (info->type == "Sfir") {
+    return std::make_unique<InstructionSfir>(info->name);
+  } else if (info->type == "B") {
+    return std::make_unique<InstructionB>(info->name);
+  } else if (info->type == "U") {
+    return std::make_unique<InstructionU>(info->name);
+  } else if (info->type == "J") {
+    return std::make_unique<InstructionJ>(info->name);
+  }
+  return nullptr;
+}
+
+#define COMP_OPCODE_R(opcode, funct3, funct7)   \
+    (static_cast<uint32_t>(funct7) << 25 | static_cast<uint32_t>(funct3) << 12 | opcode)
+#define COMP_OPCODE_R5(opcode, funct3, funct5)   \
+    (static_cast<uint32_t>(funct5) << 27 | static_cast<uint32_t>(funct3) << 12 | opcode)
+#define COMP_OPCODE_R4(opcode, funct3, funct2)   \
+    (static_cast<uint32_t>(funct2) << 25 | static_cast<uint32_t>(funct3) << 12 | opcode)
+#define COMP_OPCODE_R2(opcode, funct3, funct5, funct7)   \
+    (static_cast<uint32_t>(funct7) << 25 | static_cast<uint32_t>(funct5) << 20 \
+    |static_cast<uint32_t>(funct3) << 12 | opcode)
+#define COMP_OPCODE_S(opcode, funct3)   \
+    (static_cast<uint32_t>(funct3) << 12 | opcode)
+#define COMP_OPCODE_B(opcode, funct3)   COMP_OPCODE_S(opcode, funct3)
+#define COMP_OPCODE_I(opcode, funct3)   COMP_OPCODE_S(opcode, funct3)
+#define COMP_OPCODE_IS(opcode, funct3, funct12)   \
+    (static_cast<uint32_t>(funct12) << 20 | static_cast<uint32_t>(funct3) << 12 | opcode)
+#define COMP_OPCODE_U(opcode)   (opcode)
+#define COMP_OPCODE_J(opcode)   COMP_OPCODE_U(opcode)
+
+// static const uint32_t kOpcodeShift = 26;
+// static const uint32_t kMsaSpecialMask = (0x3f << kOpcodeShift);
+
+static const uint32_t kRTypeMask = (uint32_t)0x7f << 25 | (uint32_t)0x7 << 12 | 0x7f;
+static const uint32_t kR5TypeMask = (uint32_t)0x1f << 27 | (uint32_t)0x7 << 12 | 0x7f;
+static const uint32_t kR4TypeMask = (uint32_t)0x03 << 25 | (uint32_t)0x7 << 12 | 0x7f;
+static const uint32_t kR2TypeMask = (uint32_t)0x7f << 25 | (uint32_t)0x1f << 20 | (uint32_t)0x7 << 12 | 0x7f;
+static const uint32_t kSTypeMask = (uint32_t)0x7 << 12 | 0x7f;
+static const uint32_t kBTypeMask = kSTypeMask;
+static const uint32_t kITypeMask = kSTypeMask;
+static const uint32_t kISTypeMask = (uint32_t)0xFFF << 20 | (uint32_t)0x7 << 12 | 0x7f;
+static const uint32_t kUTypeMask = 0x7f;
+static const uint32_t kJTypeMask = kUTypeMask;
+
+static const uint32_t kFRM = 0x7;
+
+InstructionInfo gInstructions[] = {
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x0,  0x0), "add", "Rrrr"},  // RV32I-R
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x0, 0x20), "sub", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x1,  0x0), "sll", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x2,  0x0), "slt", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x3,  0x0), "sltu", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x4,  0x0), "xor", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x5,  0x0), "srl", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x5, 0x20), "sra", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x6,  0x0), "or", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x7,  0x0), "and", "Rrrr"},
+  {kSTypeMask, COMP_OPCODE_S(0x23,  0x0), "sb", "S"},  // RV32I-S
+  {kSTypeMask, COMP_OPCODE_S(0x23,  0x1), "sh", "S"},
+  {kSTypeMask, COMP_OPCODE_S(0x23,  0x2), "sw", "S"},
+  {kBTypeMask, COMP_OPCODE_B(0x63,  0x0), "beq", "B"},  // RV32I-B
+  {kBTypeMask, COMP_OPCODE_B(0x63,  0x1), "bne", "B"},
+  {kBTypeMask, COMP_OPCODE_B(0x63,  0x4), "blt", "B"},
+  {kBTypeMask, COMP_OPCODE_B(0x63,  0x5), "bge", "B"},
+  {kBTypeMask, COMP_OPCODE_B(0x63,  0x6), "bltu", "B"},
+  {kBTypeMask, COMP_OPCODE_B(0x63,  0x7), "bgeu", "B"},
+  {kITypeMask, COMP_OPCODE_I(0x67,  0x0), "jalr", "AIrir"},  // RV32I-I
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x0), "lb", "AIrir"},
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x1), "lh", "AIrir"},
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x2), "lw", "AIrir"},
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x4), "lbu", "AIrir"},
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x5), "lhu", "AIrir"},
+  {kITypeMask, COMP_OPCODE_I(0x13,  0x0), "addi", "Irri"},
+  {kITypeMask, COMP_OPCODE_I(0x13,  0x2), "slti", "Irri"},
+  {kITypeMask, COMP_OPCODE_I(0x13,  0x3), "sltiu", "Irri"},
+  {kITypeMask, COMP_OPCODE_I(0x13,  0x4), "xori", "Irri"},
+  {kITypeMask, COMP_OPCODE_I(0x13,  0x6), "ori", "Irri"},
+  {kITypeMask, COMP_OPCODE_I(0x13,  0x7), "andi", "Irri"},
+  {kITypeMask, COMP_OPCODE_I(0x0f,  0x0), "fence", "Iii"},
+  {kITypeMask, COMP_OPCODE_I(0x0f,  0x1), "fence.i", "I0"},
+  {kISTypeMask, COMP_OPCODE_IS(0x73, 0x0, 0x0), "ecall", "I0"},
+  {kISTypeMask, COMP_OPCODE_IS(0x73, 0x0, 0x1), "ebreak", "I0"},
+  {kITypeMask, COMP_OPCODE_I(0x73,  0x1), "csrrw", "Irir"},  // @todo need confirm
+  {kITypeMask, COMP_OPCODE_I(0x73,  0x2), "csrrs", "Irir"},  // @todo need confirm
+  {kITypeMask, COMP_OPCODE_I(0x73,  0x3), "csrrc", "Irir"},  // @todo need confirm
+  {kITypeMask, COMP_OPCODE_I(0x73,  0x5), "csrrwi", "Irii"},
+  {kITypeMask, COMP_OPCODE_I(0x73,  0x6), "csrrsi", "Irii"},
+  {kITypeMask, COMP_OPCODE_I(0x73,  0x7), "csrrci", "Irii"},
+  {kRTypeMask, COMP_OPCODE_R(0x13,  0x1,  0x0), "slli", "Rrri"},
+  {kRTypeMask, COMP_OPCODE_R(0x13,  0x5,  0x0), "srli", "Rrri"},
+  {kRTypeMask, COMP_OPCODE_R(0x13,  0x5, 0x20), "srai", "Rrri"},
+  {kUTypeMask, COMP_OPCODE_U(0x37), "lui", "U"},  // RV32I-U
+  {kUTypeMask, COMP_OPCODE_U(0x17), "auipc", "U"},
+  {kJTypeMask, COMP_OPCODE_J(0x6f), "jal", "J"},
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x6), "lwu", "AIrir"},  // RV64I-I
+  {kITypeMask, COMP_OPCODE_I(0x03,  0x3), "ld", "AIrir"},
+  {kITypeMask, COMP_OPCODE_I(0x1b,  0x0), "addiw", "Irri"},
+  {kSTypeMask, COMP_OPCODE_S(0x23,  0x3), "sd", "S"},  // RV64I-S
+  {kRTypeMask, COMP_OPCODE_R(0x1b,  0x1,  0x0), "slliw", "Rrri"},  // RV64I-R
+  {kRTypeMask, COMP_OPCODE_R(0x1b,  0x5,  0x0), "srliw", "Rrri"},
+  {kRTypeMask, COMP_OPCODE_R(0x1b,  0x5, 0x20), "sraiw", "Rrri"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x0,  0x0), "addw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x0, 0x20), "subw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x1,  0x0), "sllw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x5,  0x0), "srlw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x5, 0x20), "sraw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x0,  0x1), "mul", "Rrrr"},  // RV32M-R
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x1,  0x1), "mulh", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x2,  0x1), "mulhsu", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x3,  0x1), "mulhu", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x4,  0x1), "div", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x5,  0x1), "divu", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x6,  0x1), "rem", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x33,  0x7,  0x1), "remu", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x0,  0x1), "mulw", "Rrrr"},  // RV64M-R
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x4,  0x1), "divw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x5,  0x1), "divuw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x6,  0x1), "remw", "Rrrr"},
+  {kRTypeMask, COMP_OPCODE_R(0x3b,  0x7,  0x1), "remuw", "Rrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0x2), "lr.w", "ARrr"},  // RV32A-R
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0x3), "sc.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0x1), "amoswap.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0x0), "amoadd.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0x4), "amoxor.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0xc), "amoand.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2,  0x8), "amoor.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2, 0x10), "amomin.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2, 0x14), "amomax.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2, 0x18), "amominu.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x2, 0x1c), "amomaxu.w", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0x2), "lr.d", "ARrr"},  // RV64A-R
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0x3), "sc.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0x1), "amoswap.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0x0), "amoadd.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0x4), "amoxor.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0xc), "amoand.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3,  0x8), "amoor.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3, 0x10), "amomin.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3, 0x14), "amomax.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3, 0x18), "amominu.d", "ARrrr"},
+  {kR5TypeMask, COMP_OPCODE_R5(0x2f,  0x3, 0x1c), "amomaxu.d", "ARrrr"},
+  {kITypeMask, COMP_OPCODE_I(0x07,  0x2), "flw", "AIfir"},  // RV32F-I
+  {kSTypeMask, COMP_OPCODE_S(0x27,  0x2), "fsw", "Sfir"},  // RV32F-S
+  {kR4TypeMask, COMP_OPCODE_R4(0x43, kFRM,  0x0), "fmadd.s", "Rffff"},  // RV32F-R
+  {kR4TypeMask, COMP_OPCODE_R4(0x47, kFRM,  0x0), "fmsub.s", "Rffff"},
+  {kR4TypeMask, COMP_OPCODE_R4(0x4b, kFRM,  0x0), "fnmsub.s", "Rffff"},
+  {kR4TypeMask, COMP_OPCODE_R4(0x4f, kFRM,  0x0), "fnmadd.s", "Rffff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0x0), "fadd.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0x4), "fsub.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0x8), "fmul.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0xc), "fdiv.s", "Rfff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x0, 0x2c), "fsqrt.s", "Rff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x0, 0x10), "fsgnj.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x1, 0x10), "fsgnjn.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x2, 0x10), "fsgnjx.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x0, 0x14), "fmin.s", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x1, 0x14), "fmax.s", "Rfff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x0, 0x60), "fcvt.w.s", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x1, 0x60), "fcvt.wu.s", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00,  0x0, 0x70), "fmv.x.w", "Rrf"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x2, 0x50), "feq.s", "Rrff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x1, 0x50), "flt.s", "Rrff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x0, 0x50), "fle.s", "Rrff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x01, 0x0, 0x70), "fclass.s", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM, 0x0, 0x68), "fcvt.s.w", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM, 0x1, 0x68), "fcvt.s.wu", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00, 0x0, 0x78), "fmv.w.x", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x2, 0x60), "fcvt.l.s", "Rrf"},  // RV64F-R
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x3, 0x60), "fcvt.lu.s", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM, 0x2, 0x68), "fcvt.s.l", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM, 0x3, 0x68), "fcvt.s.lu", "Rfr"},
+  {kITypeMask, COMP_OPCODE_I(0x07,  0x3), "fld", "AIfir"},  // RV32D-I
+  {kSTypeMask, COMP_OPCODE_S(0x27,  0x3), "fsd", "Sfir"},  // RV32D-S
+  {kR4TypeMask, COMP_OPCODE_R4(0x43, kFRM,  0x1), "fmadd.d", "Rffff"},  // RV32D-R
+  {kR4TypeMask, COMP_OPCODE_R4(0x47, kFRM,  0x1), "fmsub.d", "Rffff"},
+  {kR4TypeMask, COMP_OPCODE_R4(0x4b, kFRM,  0x1), "fnmsub.d", "Rffff"},
+  {kR4TypeMask, COMP_OPCODE_R4(0x4f, kFRM,  0x1), "fnmadd.d", "Rffff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0x1), "fadd.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0x5), "fsub.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0x9), "fmul.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53, kFRM,  0xd), "fdiv.d", "Rfff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x0, 0x2d), "fsqrt.d", "Rff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x0, 0x11), "fsgnj.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x1, 0x11), "fsgnjn.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x2, 0x11), "fsgnjx.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x0, 0x15), "fmin.d", "Rfff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x1, 0x15), "fmax.d", "Rfff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x1, 0x20), "fcvt.s.d", "Rff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00,  0x0, 0x21), "fcvt.d.s", "Rff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x2, 0x51), "feq.d", "Rrff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x1, 0x51), "flt.d", "Rrff"},
+  {kRTypeMask, COMP_OPCODE_R(0x53,  0x0, 0x51), "fle.d", "Rrff"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x01, 0x0, 0x71), "fclass.d", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x0, 0x61), "fcvt.w.d", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x1, 0x61), "fcvt.wu.d", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00, 0x0, 0x69), "fcvt.d.w", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00, 0x1, 0x69), "fcvt.d.wu", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x2, 0x61), "fcvt.l.d", "Rrf"},  // RV64D-R
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM,  0x3, 0x61), "fcvt.lu.d", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00, 0x0, 0x71), "fmv.x.d", "Rrf"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM, 0x2, 0x69), "fcvt.d.l", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, kFRM, 0x3, 0x69), "fcvt.d.lu", "Rfr"},
+  {kR2TypeMask, COMP_OPCODE_R2(0x53, 0x00, 0x0, 0x79), "fmv.d.x", "Rfr"},
+};
+
+static uint32_t ReadU32(const uint8_t* ptr) {
+  // We only support little-endian RISCV64.
+  return ptr[0] | (ptr[1] << 8) | (ptr[2] << 16) | (ptr[3] << 24);
+}
+
+std::string DisassemblerRiscv64::DumpInstruction(uint32_t encoding) {
+  uint32_t op = encoding & 0x7f;
+  std::string ass_instructon = StringPrintf("op=%x", op);
+
+  for (size_t iinst = 0; iinst < arraysize(gInstructions); iinst++) {
+    InstructionInfo& inst_info = gInstructions[iinst];
+    uint32_t opcode = inst_info.mask & encoding;
+    if (opcode == inst_info.value) {
+      std::unique_ptr<Instruction> inst = CreateInstruction(&inst_info);
+      if (!inst) {
+        // std::cout << "not handle Instruction type :"<< inst_info.name << std::endl;
+        break;
+      }
+
+      ass_instructon = inst->Dump(encoding);
+      int32_t base = 0;
+      int32_t offset = 0;
+
+      if (inst_info.type == "AIrir") {
+        InstructionAIrir* ptr = reinterpret_cast<InstructionAIrir*>(inst.get());
+        offset = std::get<1>(ptr->fields).value;
+        base = std::get<2>(ptr->fields).value;
+      } else if (inst_info.type == "S") {
+        InstructionS* ptr = reinterpret_cast<InstructionS*>(inst.get());
+        offset = std::get<1>(ptr->fields).value;
+        base = std::get<2>(ptr->fields).value;
+      }
+
+      if ((inst_info.type == "AIrir" || inst_info.type == "S") && base == 9) {
+        std::stringstream ss;
+        GetDisassemblerOptions()->thread_offset_name_function_(ss, offset);
+        ass_instructon += "  ; ";
+        ass_instructon += ss.str();
+      }
+      break;
+    }
+  }
+  return ass_instructon;
+}
+
+size_t DisassemblerRiscv64::Dump(std::ostream& os, const uint8_t* instr_ptr) {
+  uint32_t instruction = ReadU32(instr_ptr);
+  std::string ass_instruction = DumpInstruction(instruction);
+
+  os << FormatInstructionPointer(instr_ptr)
+      << StringPrintf(": %08x\t%-7s ", instruction, ass_instruction.c_str())
+      << '\n';
+
+  last_ptr_ = instr_ptr;
+  last_instr_ = instruction;
+  return 4;
+}
+
+void DisassemblerRiscv64::Dump(std::ostream& os, const uint8_t* begin, const uint8_t* end) {
+  for (const uint8_t* cur = begin; cur < end; cur += 4) {
+    Dump(os, cur);
+  }
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/disassembler/disassembler_riscv64.h b/disassembler/disassembler_riscv64.h
new file mode 100644
index 0000000000..46bb56cd34
--- /dev/null
+++ b/disassembler/disassembler_riscv64.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_DISASSEMBLER_DISASSEMBLER_RISCV64_H_
+#define ART_DISASSEMBLER_DISASSEMBLER_RISCV64_H_
+
+#include <vector>
+
+#include "disassembler.h"
+
+namespace art {
+namespace riscv64 {
+
+class DisassemblerRiscv64 final : public Disassembler {
+ public:
+  explicit DisassemblerRiscv64(DisassemblerOptions* options)
+      : Disassembler(options),
+        last_ptr_(nullptr),
+        last_instr_(0) {}
+
+  size_t Dump(std::ostream& os, const uint8_t* begin) override;
+  void Dump(std::ostream& os, const uint8_t* begin, const uint8_t* end) override;
+
+  std::string DumpInstruction(uint32_t instruction);
+
+ private:
+  // Address and encoding of the last disassembled instruction.
+  // Needed to produce more readable disassembly of certain 2-instruction sequences.
+  const uint8_t* last_ptr_;
+  uint32_t last_instr_;
+
+  DISALLOW_COPY_AND_ASSIGN(DisassemblerRiscv64);
+
+  // friend class art::DisassemblerRiscv64Test;
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_DISASSEMBLER_DISASSEMBLER_RISCV64_H_
diff --git a/disassembler/disassembler_riscv64_test.cc b/disassembler/disassembler_riscv64_test.cc
new file mode 100644
index 0000000000..fe009582f0
--- /dev/null
+++ b/disassembler/disassembler_riscv64_test.cc
@@ -0,0 +1,713 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "gtest/gtest.h"
+
+#include "base/malloc_arena_pool.h"
+#include "utils/riscv64/assembler_riscv64.h"
+#include "disassembler_riscv64.h"
+#include "utils/riscv64/constants_riscv64.h"
+
+#include "android-base/stringprintf.h"
+
+using android::base::StringPrintf;  // NOLINT(build/namespaces)
+
+#define COMPARE(ASM, EXP)                                                \
+  do {                                                                   \
+    GetAssembler()->ASM;                                                 \
+    GetAssembler()->FinalizeCode();                                      \
+    std::vector<uint8_t> data(GetAssembler()->CodeSize());               \
+    MemoryRegion code(data.data(), data.size());                         \
+    GetAssembler()->FinalizeInstructions(code);                          \
+    uint32_t encoding = *reinterpret_cast<uint32_t*>(&data[0]);          \
+    std::string instruction = GetDisassembler()->DumpInstruction(encoding); \
+    EXPECT_EQ(instruction, EXP); \
+  } while (0)
+
+#define __ GetAssembler()->
+
+void ThreadOffsetNameFunctionDumy(std::ostream&, uint32_t) {
+}
+
+namespace art {
+namespace riscv64 {
+class DisassemblerRiscv64Test : public ::testing::Test {
+ public:
+  using Ass = riscv64::Riscv64Assembler;
+  using Dss = riscv64::DisassemblerRiscv64;
+
+  Ass* GetAssembler() {
+    return assembler_.get();
+  }
+
+  Dss* GetDisassembler() {
+    return disassembler_.get();
+  }
+
+ protected:
+  void SetUp() override {
+    allocator_.reset(new ArenaAllocator(&pool_));
+    assembler_.reset(CreateAssembler(allocator_.get()));
+    disassembler_.reset(CreateDisassembler());
+  }
+
+ private:
+  virtual Ass* CreateAssembler(ArenaAllocator* allocator) {
+    return new (allocator) Ass(allocator);
+  }
+
+  virtual Dss* CreateDisassembler() {
+    return new Dss(new DisassemblerOptions(false, nullptr, nullptr, false,
+        &ThreadOffsetNameFunctionDumy));
+  }
+
+  MallocArenaPool pool_;
+  std::unique_ptr<ArenaAllocator> allocator_;
+  std::unique_ptr<Ass> assembler_;
+  std::unique_ptr<Dss> disassembler_;
+};
+
+#if 1
+TEST_F(DisassemblerRiscv64Test, Add) {
+  COMPARE(Add(A0, A1, A2), "add a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sub) {
+  COMPARE(Sub(A0, A1, A2), "sub a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sll) {
+  COMPARE(Sll(A0, A1, A2), "sll a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Slt) {
+  COMPARE(Slt(A0, A1, A2), "slt a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sltu) {
+  COMPARE(Sltu(A0, A1, A2), "sltu a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Xor) {
+  COMPARE(Xor(A0, A1, A2), "xor a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Srl) {
+  COMPARE(Srl(A0, A1, A2), "srl a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sra) {
+  COMPARE(Sra(A0, A1, A2), "sra a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Or) {
+  COMPARE(Or(A0, A1, A2), "or a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, And) {
+  COMPARE(And(A0, A1, A2), "and a0,a1,a2");
+}
+
+#endif
+// RV32I-S
+TEST_F(DisassemblerRiscv64Test, Sb) {
+  COMPARE(Sb(A0, A1, -255), "sb a0,-255(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sh) {
+  COMPARE(Sh(A0, A1, -255), "sh a0,-255(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sw) {
+  COMPARE(Sw(A0, A1, -255), "sw a0,-255(a1)");
+}
+
+// RV32I-B
+TEST_F(DisassemblerRiscv64Test, Beq) {
+  COMPARE(Beq(A0, A1, -256), "beq a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Bne) {
+  COMPARE(Bne(A0, A1, -256), "bne a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Blt) {
+  COMPARE(Blt(A0, A1, -256), "blt a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Bge) {
+  COMPARE(Bge(A0, A1, -256), "bge a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Bltu) {
+  COMPARE(Bltu(A0, A1, -256), "bltu a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Bgeu) {
+  COMPARE(Bgeu(A0, A1, -256), "bgeu a0,a1,-256");
+}
+
+// RV32I-I
+TEST_F(DisassemblerRiscv64Test, Jalr) {
+  COMPARE(Jalr(A0, A1, -2047), "jalr a0,-2047(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Lb) {
+  COMPARE(Lb(A0, A1, -256), "lb a0,-256(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Lh) {
+  COMPARE(Lh(A0, A1, -256), "lh a0,-256(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Lw) {
+  COMPARE(Lw(A0, A1, -256), "lw a0,-256(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Lbu) {
+  COMPARE(Lbu(A0, A1, -256), "lbu a0,-256(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Lhu) {
+  COMPARE(Lhu(A0, A1, -256), "lhu a0,-256(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Addi) {
+  COMPARE(Addi(A0, A1, -256), "addi a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Fence) {
+  COMPARE(Fence(15, 15), "fence iorw,iorw");
+}
+
+TEST_F(DisassemblerRiscv64Test, FenceI) {
+  COMPARE(FenceI(), "fence.i ");
+}
+
+TEST_F(DisassemblerRiscv64Test, Ecall) {
+  COMPARE(Ecall(), "ecall ");
+}
+
+TEST_F(DisassemblerRiscv64Test, Ebreak) {
+  COMPARE(Ebreak(), "ebreak ");
+}
+
+TEST_F(DisassemblerRiscv64Test, Csrrw) {
+  COMPARE(Csrrw(A0, A1, 0x1F), "csrrw a0,31,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, Csrrs) {
+  COMPARE(Csrrs(A0, A1, 0x1F), "csrrs a0,31,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, Csrrc) {
+  COMPARE(Csrrc(A0, A1, 0x1F), "csrrc a0,31,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, Csrrwi) {
+  COMPARE(Csrrwi(A0, 0x001, 1), "csrrwi a0,fflags,1");
+}
+
+TEST_F(DisassemblerRiscv64Test, Csrrsi) {
+  COMPARE(Csrrsi(A0, 0x001, 1), "csrrsi a0,fflags,1");
+}
+
+TEST_F(DisassemblerRiscv64Test, Csrrci) {
+  COMPARE(Csrrci(A0, 0x001, 1), "csrrci a0,fflags,1");
+}
+
+// RV32I-
+TEST_F(DisassemblerRiscv64Test, Slti) {
+  COMPARE(Slti(A0, A1, -256), "slti a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sltiu) {
+  COMPARE(Sltiu(A0, A1, -256), "sltiu a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Xori) {
+  COMPARE(Xori(A0, A1, -256), "xori a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Ori) {
+  COMPARE(Ori(A0, A1, -256), "ori a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Andi) {
+  COMPARE(Andi(A0, A1, -256), "andi a0,a1,-256");
+}
+
+TEST_F(DisassemblerRiscv64Test, Slli) {
+  COMPARE(Slli(A0, A1, 16), "slli a0,a1,16");
+}
+
+TEST_F(DisassemblerRiscv64Test, Srli) {
+  COMPARE(Srli(A0, A1, 16), "srli a0,a1,16");
+}
+
+TEST_F(DisassemblerRiscv64Test, Srai) {
+  COMPARE(Srai(A0, A1, 16), "srai a0,a1,16");
+}
+
+TEST_F(DisassemblerRiscv64Test, Lui) {
+  COMPARE(Lui(A0, -524287), "lui a0,-524287");
+}
+
+TEST_F(DisassemblerRiscv64Test, Auipc) {
+  COMPARE(Auipc(A0, -524287), "auipc a0,-524287");
+}
+
+// RV32I-J
+TEST_F(DisassemblerRiscv64Test, Jal) {
+  COMPARE(Jal(A0, -1048574), "jal a0,-1048574");
+}
+
+// RV64I-I
+TEST_F(DisassemblerRiscv64Test, Lwu) {
+  COMPARE(Lwu(A0, A1, -2047), "lwu a0,-2047(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Ld) {
+  COMPARE(Ld(A0, A1, -2047), "ld a0,-2047(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, Addiw) {
+  COMPARE(Addiw(A0, A1, -2047), "addiw a0,a1,-2047");
+}
+
+// RV64I-S
+TEST_F(DisassemblerRiscv64Test, Sd) {
+  COMPARE(Sd(A0, A1, -2047), "sd a0,-2047(a1)");
+}
+
+// RV64I-R
+TEST_F(DisassemblerRiscv64Test, Slliw) {
+  COMPARE(Slliw(A0, A1, 16), "slliw a0,a1,16");
+}
+
+TEST_F(DisassemblerRiscv64Test, Srliw) {
+  COMPARE(Srliw(A0, A1, 16), "srliw a0,a1,16");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sraiw) {
+  COMPARE(Sraiw(A0, A1, 16), "sraiw a0,a1,16");
+}
+
+TEST_F(DisassemblerRiscv64Test, Addw) {
+  COMPARE(Addw(A0, A1, A2), "addw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Subw) {
+  COMPARE(Subw(A0, A1, A2), "subw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sllw) {
+  COMPARE(Sllw(A0, A1, A2), "sllw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Srlw) {
+  COMPARE(Srlw(A0, A1, A2), "srlw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Sraw) {
+  COMPARE(Sraw(A0, A1, A2), "sraw a0,a1,a2");
+}
+
+// RV32M-R
+TEST_F(DisassemblerRiscv64Test, Mul) {
+  COMPARE(Mul(A0, A1, A2), "mul a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Mulh) {
+  COMPARE(Mulh(A0, A1, A2), "mulh a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Mulhsu) {
+  COMPARE(Mulhsu(A0, A1, A2), "mulhsu a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Mulhu) {
+  COMPARE(Mulhu(A0, A1, A2), "mulhu a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Div) {
+  COMPARE(Div(A0, A1, A2), "div a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Divu) {
+  COMPARE(Divu(A0, A1, A2), "divu a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Rem) {
+  COMPARE(Rem(A0, A1, A2), "rem a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Remu) {
+  COMPARE(Remu(A0, A1, A2), "remu a0,a1,a2");
+}
+
+// RV64M-R
+TEST_F(DisassemblerRiscv64Test, Mulw) {
+  COMPARE(Mulw(A0, A1, A2), "mulw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Divw) {
+  COMPARE(Divw(A0, A1, A2), "divw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Divuw) {
+  COMPARE(Divuw(A0, A1, A2), "divuw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Remw) {
+  COMPARE(Remw(A0, A1, A2), "remw a0,a1,a2");
+}
+
+TEST_F(DisassemblerRiscv64Test, Remuw) {
+  COMPARE(Remuw(A0, A1, A2), "remuw a0,a1,a2");
+}
+
+// RV32A-R
+TEST_F(DisassemblerRiscv64Test, LrW) {
+  COMPARE(LrW(A0, A1), "lr.w a0,(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, ScW) {
+  COMPARE(ScW(A0, A1, A2), "sc.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoSwapW) {
+  COMPARE(AmoSwapW(A0, A1, A2), "amoswap.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoAddW) {
+  COMPARE(AmoAddW(A0, A1, A2), "amoadd.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoXorW) {
+  COMPARE(AmoXorW(A0, A1, A2), "amoxor.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoAndW) {
+  COMPARE(AmoAndW(A0, A1, A2), "amoand.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoOrW) {
+  COMPARE(AmoOrW(A0, A1, A2), "amoor.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMinW) {
+  COMPARE(AmoMinW(A0, A1, A2), "amomin.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMaxW) {
+  COMPARE(AmoMaxW(A0, A1, A2), "amomax.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMinuW) {
+  COMPARE(AmoMinuW(A0, A1, A2), "amominu.w a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMaxuW) {
+  COMPARE(AmoMaxuW(A0, A1, A2), "amomaxu.w a0,a1,(a2)");
+}
+
+// RV64A-R
+TEST_F(DisassemblerRiscv64Test, LrD) {
+  COMPARE(LrD(A0, A1), "lr.d a0,(a1)");
+}
+
+TEST_F(DisassemblerRiscv64Test, ScD) {
+  COMPARE(ScD(A0, A1, A2), "sc.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoSwapD) {
+  COMPARE(AmoSwapD(A0, A1, A2), "amoswap.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoAddD) {
+  COMPARE(AmoAddD(A0, A1, A2), "amoadd.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoXorD) {
+  COMPARE(AmoXorD(A0, A1, A2), "amoxor.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoAndD) {
+  COMPARE(AmoAndD(A0, A1, A2), "amoand.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoOrD) {
+  COMPARE(AmoOrD(A0, A1, A2), "amoor.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMinD) {
+  COMPARE(AmoMinD(A0, A1, A2), "amomin.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMaxD) {
+  COMPARE(AmoMaxD(A0, A1, A2), "amomax.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMinuD) {
+  COMPARE(AmoMinuD(A0, A1, A2), "amominu.d a0,a1,(a2)");
+}
+
+TEST_F(DisassemblerRiscv64Test, AmoMaxuD) {
+  COMPARE(AmoMaxuD(A0, A1, A2), "amomaxu.d a0,a1,(a2)");
+}
+
+// RV32F-I
+TEST_F(DisassemblerRiscv64Test, FLw) {
+  COMPARE(FLw(FT0, A1, -2047), "flw ft0,-2047(a1)");
+}
+
+// RV32F-S
+TEST_F(DisassemblerRiscv64Test, FSw) {
+  COMPARE(FSw(FT0, A1, -2047), "fsw ft0,-2047(a1)");
+}
+
+// RV32F-R
+TEST_F(DisassemblerRiscv64Test, FMAddS) {
+  COMPARE(FMAddS(FT0, FT1, FT2, FT3), "fmadd.s ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMSubS) {
+  COMPARE(FMSubS(FT0, FT1, FT2, FT3), "fmsub.s ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FNMSubS) {
+  COMPARE(FNMSubS(FT0, FT1, FT2, FT3), "fnmsub.s ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FNMAddS) {
+  COMPARE(FNMAddS(FT0, FT1, FT2, FT3), "fnmadd.s ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FAddS) {
+  COMPARE(FAddS(FT0, FT1, FT2), "fadd.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSubS) {
+  COMPARE(FSubS(FT0, FT1, FT2), "fsub.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMulS) {
+  COMPARE(FMulS(FT0, FT1, FT2), "fmul.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FDivS) {
+  COMPARE(FDivS(FT0, FT1, FT2), "fdiv.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSqrtS) {
+  COMPARE(FSqrtS(FT0, FT1), "fsqrt.s ft0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSgnjS) {
+  COMPARE(FSgnjS(FT0, FT1, FT2), "fsgnj.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSgnjnS) {
+  COMPARE(FSgnjnS(FT0, FT1, FT2), "fsgnjn.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSgnjxS) {
+  COMPARE(FSgnjxS(FT0, FT1, FT2), "fsgnjx.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMinS) {
+  COMPARE(FMinS(FT0, FT1, FT2), "fmin.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMaxS) {
+  COMPARE(FMaxS(FT0, FT1, FT2), "fmax.s ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtWS) {
+  COMPARE(FCvtWS(A0, FT1), "fcvt.w.s a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtWuS) {
+  COMPARE(FCvtWuS(A0, FT1), "fcvt.wu.s a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMvXW) {
+  COMPARE(FMvXW(A0, FT1), "fmv.x.w a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FEqS) {
+  COMPARE(FEqS(A0, FT1, FT2), "feq.s a0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FClassS) {
+  COMPARE(FClassS(A0, FT1), "fclass.s a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtSW) {
+  COMPARE(FCvtSW(FT0, A1), "fcvt.s.w ft0,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtSWu) {
+  COMPARE(FCvtSWu(FT0, A1), "fcvt.s.wu ft0,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMvWX) {
+  COMPARE(FMvWX(FT0, A1), "fmv.w.x ft0,a1");
+}
+
+// RV64F-R
+TEST_F(DisassemblerRiscv64Test, FCvtLS) {
+  COMPARE(FCvtLS(A0, FT1), "fcvt.l.s a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtLuS) {
+  COMPARE(FCvtLuS(A0, FT1), "fcvt.lu.s a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtSL) {
+  COMPARE(FCvtSL(FT0, A1), "fcvt.s.l ft0,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtSLu) {
+  COMPARE(FCvtSLu(FT0, A1), "fcvt.s.lu ft0,a1");
+}
+
+// RV32D-I
+TEST_F(DisassemblerRiscv64Test, FLd) {
+  COMPARE(FLd(FT0, A1, -2047), "fld ft0,-2047(a1)");
+}
+
+// RV32D-S
+TEST_F(DisassemblerRiscv64Test, FSd) {
+  COMPARE(FSd(FT0, A1, -2047), "fsd ft0,-2047(a1)");
+}
+
+// RV32D-R
+TEST_F(DisassemblerRiscv64Test, FMAddD) {
+  COMPARE(FMAddD(FT0, FT1, FT2, FT3), "fmadd.d ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMSubD) {
+  COMPARE(FMSubD(FT0, FT1, FT2, FT3), "fmsub.d ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FNMSubD) {
+  COMPARE(FNMSubD(FT0, FT1, FT2, FT3), "fnmsub.d ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FNMAddD) {
+  COMPARE(FNMAddD(FT0, FT1, FT2, FT3), "fnmadd.d ft0,ft1,ft2,ft3");
+}
+
+TEST_F(DisassemblerRiscv64Test, FAddD) {
+  COMPARE(FAddD(FT0, FT1, FT2), "fadd.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSubD) {
+  COMPARE(FSubD(FT0, FT1, FT2), "fsub.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMulD) {
+  COMPARE(FMulD(FT0, FT1, FT2), "fmul.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FDivD) {
+  COMPARE(FDivD(FT0, FT1, FT2), "fdiv.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSqrtD) {
+  COMPARE(FSqrtD(FT0, FT1), "fsqrt.d ft0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSgnjD) {
+  COMPARE(FSgnjD(FT0, FT1, FT2), "fsgnj.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSgnjnD) {
+  COMPARE(FSgnjnD(FT0, FT1, FT2), "fsgnjn.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FSgnjxD) {
+  COMPARE(FSgnjxD(FT0, FT1, FT2), "fsgnjx.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMinD) {
+  COMPARE(FMinD(FT0, FT1, FT2), "fmin.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMaxD) {
+  COMPARE(FMaxD(FT0, FT1, FT2), "fmax.d ft0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtSD) {
+  COMPARE(FCvtSD(FT0, FT1), "fcvt.s.d ft0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtDS) {
+  COMPARE(FCvtDS(FT0, FT1), "fcvt.d.s ft0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FEqD) {
+  COMPARE(FEqD(A0, FT1, FT2), "feq.d a0,ft1,ft2");
+}
+
+TEST_F(DisassemblerRiscv64Test, FClassD) {
+  COMPARE(FClassD(A0, FT1), "fclass.d a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtWD) {
+  COMPARE(FCvtWD(A0, FT1), "fcvt.w.d a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtWuD) {
+  COMPARE(FCvtWuD(A0, FT1), "fcvt.wu.d a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtDW) {
+  COMPARE(FCvtDW(FT0, A1), "fcvt.d.w ft0,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtDWu) {
+  COMPARE(FCvtDWu(FT0, A1), "fcvt.d.wu ft0,a1");
+}
+
+// RV64D-R
+TEST_F(DisassemblerRiscv64Test, FCvtLD) {
+  COMPARE(FCvtLD(A0, FT1), "fcvt.l.d a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtLuD) {
+  COMPARE(FCvtLuD(A0, FT1), "fcvt.lu.d a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMvXD) {
+  COMPARE(FMvXD(A0, FT1), "fmv.x.d a0,ft1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtDL) {
+  COMPARE(FCvtDL(FT0, A1), "fcvt.d.l ft0,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FCvtDLu) {
+  COMPARE(FCvtDLu(FT0, A1), "fcvt.d.lu ft0,a1");
+}
+
+TEST_F(DisassemblerRiscv64Test, FMvDX) {
+  COMPARE(FMvDX(FT0, A1), "fmv.d.x ft0,a1");
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/libartbase/arch/instruction_set.cc b/libartbase/arch/instruction_set.cc
index 8d4fbf4422..5695181333 100644
--- a/libartbase/arch/instruction_set.cc
+++ b/libartbase/arch/instruction_set.cc
@@ -31,6 +31,7 @@ void InstructionSetAbort(InstructionSet isa) {
     case InstructionSet::kX86_64:
     case InstructionSet::kMips:
     case InstructionSet::kMips64:
+    case InstructionSet::kRiscv64:
     case InstructionSet::kNone:
       LOG(FATAL) << "Unsupported instruction set " << isa;
       UNREACHABLE();
@@ -54,6 +55,8 @@ const char* GetInstructionSetString(InstructionSet isa) {
       return "mips";
     case InstructionSet::kMips64:
       return "mips64";
+    case InstructionSet::kRiscv64:
+      return "riscv64";
     case InstructionSet::kNone:
       return "none";
   }
@@ -76,6 +79,8 @@ InstructionSet GetInstructionSetFromString(const char* isa_str) {
     return InstructionSet::kMips;
   } else if (strcmp("mips64", isa_str) == 0) {
     return InstructionSet::kMips64;
+  } else if (strcmp("riscv64", isa_str) == 0) {
+    return InstructionSet::kRiscv64;
   }
 
   return InstructionSet::kNone;
@@ -97,6 +102,8 @@ size_t GetInstructionSetAlignment(InstructionSet isa) {
       // Fall-through.
     case InstructionSet::kMips64:
       return kMipsAlignment;
+    case InstructionSet::kRiscv64:
+      return kRiscv64Alignment;
     case InstructionSet::kNone:
       LOG(FATAL) << "ISA kNone does not have alignment.";
       UNREACHABLE();
diff --git a/libartbase/arch/instruction_set.h b/libartbase/arch/instruction_set.h
index 7e071bd9e2..103c5e46e6 100644
--- a/libartbase/arch/instruction_set.h
+++ b/libartbase/arch/instruction_set.h
@@ -34,7 +34,8 @@ enum class InstructionSet {
   kX86_64,
   kMips,
   kMips64,
-  kLast = kMips64
+  kRiscv64,
+  kLast = kRiscv64
 };
 std::ostream& operator<<(std::ostream& os, const InstructionSet& rhs);
 
@@ -50,6 +51,8 @@ static constexpr InstructionSet kRuntimeISA = InstructionSet::kMips64;
 static constexpr InstructionSet kRuntimeISA = InstructionSet::kX86;
 #elif defined(__x86_64__)
 static constexpr InstructionSet kRuntimeISA = InstructionSet::kX86_64;
+#elif defined(__riscv) && (__riscv_xlen == 64)
+static constexpr InstructionSet kRuntimeISA = InstructionSet::kRiscv64;
 #else
 static constexpr InstructionSet kRuntimeISA = InstructionSet::kNone;
 #endif
@@ -61,6 +64,7 @@ static constexpr PointerSize kMipsPointerSize = PointerSize::k32;
 static constexpr PointerSize kMips64PointerSize = PointerSize::k64;
 static constexpr PointerSize kX86PointerSize = PointerSize::k32;
 static constexpr PointerSize kX86_64PointerSize = PointerSize::k64;
+static constexpr PointerSize kRiscv64PointerSize = PointerSize::k64;
 
 // ARM instruction alignment. ARM processors require code to be 4-byte aligned,
 // but ARM ELF requires 8..
@@ -76,6 +80,9 @@ static constexpr size_t kMipsAlignment = 8;
 // X86 instruction alignment. This is the recommended alignment for maximum performance.
 static constexpr size_t kX86Alignment = 16;
 
+// RISCV64 instruction alignment. This is the recommended alignment for maximum performance.
+static constexpr size_t kRiscv64Alignment = 16;
+
 // Different than code alignment since code alignment is only first instruction of method.
 static constexpr size_t kThumb2InstructionAlignment = 2;
 static constexpr size_t kArm64InstructionAlignment = 4;
@@ -83,6 +90,7 @@ static constexpr size_t kX86InstructionAlignment = 1;
 static constexpr size_t kX86_64InstructionAlignment = 1;
 static constexpr size_t kMipsInstructionAlignment = 4;
 static constexpr size_t kMips64InstructionAlignment = 4;
+static constexpr size_t kRiscv64InstructionAlignment = 4;
 
 const char* GetInstructionSetString(InstructionSet isa);
 
@@ -108,6 +116,8 @@ constexpr PointerSize GetInstructionSetPointerSize(InstructionSet isa) {
       return kMipsPointerSize;
     case InstructionSet::kMips64:
       return kMips64PointerSize;
+    case InstructionSet::kRiscv64:
+      return kRiscv64PointerSize;
 
     case InstructionSet::kNone:
       break;
@@ -131,6 +141,8 @@ constexpr size_t GetInstructionSetInstructionAlignment(InstructionSet isa) {
       return kMipsInstructionAlignment;
     case InstructionSet::kMips64:
       return kMips64InstructionAlignment;
+    case InstructionSet::kRiscv64:
+      return kRiscv64InstructionAlignment;
 
     case InstructionSet::kNone:
       break;
@@ -147,6 +159,7 @@ constexpr bool IsValidInstructionSet(InstructionSet isa) {
     case InstructionSet::kX86_64:
     case InstructionSet::kMips:
     case InstructionSet::kMips64:
+    case InstructionSet::kRiscv64:
       return true;
 
     case InstructionSet::kNone:
@@ -168,6 +181,7 @@ constexpr bool Is64BitInstructionSet(InstructionSet isa) {
     case InstructionSet::kArm64:
     case InstructionSet::kX86_64:
     case InstructionSet::kMips64:
+    case InstructionSet::kRiscv64:
       return true;
 
     case InstructionSet::kNone:
@@ -196,6 +210,8 @@ constexpr size_t GetBytesPerGprSpillLocation(InstructionSet isa) {
       return 4;
     case InstructionSet::kMips64:
       return 8;
+    case InstructionSet::kRiscv64:
+      return 8;
 
     case InstructionSet::kNone:
       break;
@@ -219,6 +235,8 @@ constexpr size_t GetBytesPerFprSpillLocation(InstructionSet isa) {
       return 4;
     case InstructionSet::kMips64:
       return 8;
+    case InstructionSet::kRiscv64:
+      return 8;
 
     case InstructionSet::kNone:
       break;
@@ -230,7 +248,8 @@ namespace instruction_set_details {
 
 #if !defined(ART_STACK_OVERFLOW_GAP_arm) || !defined(ART_STACK_OVERFLOW_GAP_arm64) || \
     !defined(ART_STACK_OVERFLOW_GAP_mips) || !defined(ART_STACK_OVERFLOW_GAP_mips64) || \
-    !defined(ART_STACK_OVERFLOW_GAP_x86) || !defined(ART_STACK_OVERFLOW_GAP_x86_64)
+    !defined(ART_STACK_OVERFLOW_GAP_x86) || !defined(ART_STACK_OVERFLOW_GAP_x86_64) || \
+    !defined(ART_STACK_OVERFLOW_GAP_riscv64)
 #error "Missing defines for stack overflow gap"
 #endif
 
@@ -240,6 +259,7 @@ static constexpr size_t kMipsStackOverflowReservedBytes   = ART_STACK_OVERFLOW_G
 static constexpr size_t kMips64StackOverflowReservedBytes = ART_STACK_OVERFLOW_GAP_mips64;
 static constexpr size_t kX86StackOverflowReservedBytes    = ART_STACK_OVERFLOW_GAP_x86;
 static constexpr size_t kX86_64StackOverflowReservedBytes = ART_STACK_OVERFLOW_GAP_x86_64;
+static constexpr size_t kRiscv64StackOverflowReservedBytes = ART_STACK_OVERFLOW_GAP_riscv64;
 
 NO_RETURN void GetStackOverflowReservedBytesFailure(const char* error_msg);
 
@@ -261,6 +281,9 @@ constexpr size_t GetStackOverflowReservedBytes(InstructionSet isa) {
     case InstructionSet::kMips64:
       return instruction_set_details::kMips64StackOverflowReservedBytes;
 
+    case InstructionSet::kRiscv64:
+      return instruction_set_details::kRiscv64StackOverflowReservedBytes;
+
     case InstructionSet::kX86:
       return instruction_set_details::kX86StackOverflowReservedBytes;
 
@@ -313,7 +336,8 @@ static inline constexpr TwoWordReturn GetTwoWordSuccessValue(uintptr_t hi, uintp
   return ((hi64 << 32) | lo32);
 }
 
-#elif defined(__x86_64__) || defined(__aarch64__) || (defined(__mips__) && defined(__LP64__))
+#elif defined(__x86_64__) || defined(__aarch64__) || (defined(__mips__) && defined(__LP64__)) \
+     || (defined(__riscv) && (__riscv_xlen == 64))
 
 // Note: TwoWordReturn can't be constexpr for 64-bit targets. We'd need a constexpr constructor,
 //       which would violate C-linkage in the entrypoint functions.
diff --git a/libartbase/arch/instruction_set_test.cc b/libartbase/arch/instruction_set_test.cc
index 12a117d7a1..382cb6155d 100644
--- a/libartbase/arch/instruction_set_test.cc
+++ b/libartbase/arch/instruction_set_test.cc
@@ -29,6 +29,7 @@ TEST(InstructionSetTest, GetInstructionSetFromString) {
   EXPECT_EQ(InstructionSet::kX86_64, GetInstructionSetFromString("x86_64"));
   EXPECT_EQ(InstructionSet::kMips, GetInstructionSetFromString("mips"));
   EXPECT_EQ(InstructionSet::kMips64, GetInstructionSetFromString("mips64"));
+  EXPECT_EQ(InstructionSet::kRiscv64, GetInstructionSetFromString("riscv64"));
   EXPECT_EQ(InstructionSet::kNone, GetInstructionSetFromString("none"));
   EXPECT_EQ(InstructionSet::kNone, GetInstructionSetFromString("random-string"));
 }
@@ -41,6 +42,7 @@ TEST(InstructionSetTest, GetInstructionSetString) {
   EXPECT_STREQ("x86_64", GetInstructionSetString(InstructionSet::kX86_64));
   EXPECT_STREQ("mips", GetInstructionSetString(InstructionSet::kMips));
   EXPECT_STREQ("mips64", GetInstructionSetString(InstructionSet::kMips64));
+  EXPECT_STREQ("riscv64", GetInstructionSetString(InstructionSet::kRiscv64));
   EXPECT_STREQ("none", GetInstructionSetString(InstructionSet::kNone));
 }
 
@@ -57,6 +59,8 @@ TEST(InstructionSetTest, GetInstructionSetInstructionAlignment) {
             kMipsInstructionAlignment);
   EXPECT_EQ(GetInstructionSetInstructionAlignment(InstructionSet::kMips64),
             kMips64InstructionAlignment);
+  EXPECT_EQ(GetInstructionSetInstructionAlignment(InstructionSet::kRiscv64),
+            kRiscv64InstructionAlignment);
 }
 
 TEST(InstructionSetTest, TestRoundTrip) {
diff --git a/libartbase/base/mem_map.h b/libartbase/base/mem_map.h
index 525e622690..f2208bd6fe 100644
--- a/libartbase/base/mem_map.h
+++ b/libartbase/base/mem_map.h
@@ -30,7 +30,8 @@
 namespace art {
 
 #if defined(__LP64__) && !defined(__Fuchsia__) && \
-    (defined(__aarch64__) || defined(__mips__) || defined(__APPLE__))
+    (defined(__aarch64__) || defined(__mips__) || defined(__APPLE__) || \
+    defined(__riscv))
 #define USE_ART_LOW_4G_ALLOCATOR 1
 #else
 #if defined(__LP64__) && !defined(__Fuchsia__) && !defined(__x86_64__)
diff --git a/libelffile/dwarf/register.h b/libelffile/dwarf/register.h
index eadb441f15..3836aca5b4 100644
--- a/libelffile/dwarf/register.h
+++ b/libelffile/dwarf/register.h
@@ -51,6 +51,8 @@ class Reg {
     return Reg(num < 8 ? map[num] : num);
   }
   static Reg X86_64Fp(int num) { return Reg(17 + num); }
+  static Reg Riscv64Core(int num) { return Reg(num); }  // FIXME: T-HEAD.
+  static Reg Riscv64Fp(int num) { return Reg(32 + num); }
 
  private:
   int num_;
diff --git a/libelffile/elf/elf_builder.h b/libelffile/elf/elf_builder.h
index b528f6a221..2c9f26b4f5 100644
--- a/libelffile/elf/elf_builder.h
+++ b/libelffile/elf/elf_builder.h
@@ -824,6 +824,11 @@ class ElfBuilder final {
         elf_header.e_flags = 0;
         break;
       }
+      case InstructionSet::kRiscv64: {
+        elf_header.e_machine = EM_RISCV;
+        elf_header.e_flags = 0;
+        break;
+      }
       case InstructionSet::kNone: {
         LOG(FATAL) << "No instruction set";
         break;
diff --git a/libelffile/elf/elf_utils.h b/libelffile/elf/elf_utils.h
index a20312a1a8..60cba32231 100644
--- a/libelffile/elf/elf_utils.h
+++ b/libelffile/elf/elf_utils.h
@@ -74,6 +74,8 @@ struct ElfTypes64 {
 
 #define EM_AARCH64 183
 
+#define EM_RISCV        243     /* RISC-V */
+
 #define DT_BIND_NOW 24
 #define DT_INIT_ARRAY 25
 #define DT_FINI_ARRAY 26
diff --git a/runtime/Android.bp b/runtime/Android.bp
index 7bf662c652..68bbf505a3 100644
--- a/runtime/Android.bp
+++ b/runtime/Android.bp
@@ -227,6 +227,8 @@ libart_cc_defaults {
         "arch/x86/instruction_set_features_x86.cc",
         "arch/x86/registers_x86.cc",
         "arch/x86_64/registers_x86_64.cc",
+        "arch/riscv64/instruction_set_features_riscv64.cc",
+        "arch/riscv64/registers_riscv64.cc",
         "entrypoints/entrypoint_utils.cc",
         "entrypoints/jni/jni_entrypoints.cc",
         "entrypoints/math_entrypoints.cc",
@@ -331,6 +333,25 @@ libart_cc_defaults {
                 "arch/mips64/fault_handler_mips64.cc",
             ],
         },
+        riscv64: {
+            srcs: [
+                "interpreter/mterp/mterp.cc",
+                ":libart_mterp.riscv64",
+                "arch/riscv64/context_riscv64.cc",
+                "arch/riscv64/entrypoints_init_riscv64.cc",
+                "arch/riscv64/jni_entrypoints_riscv64.S",
+                //"arch/riscv64/memcmp16_riscv64.S",
+                "arch/riscv64/quick_entrypoints_riscv64.S",
+                "arch/riscv64/thread_riscv64.cc",
+                "monitor_pool.cc",
+                "arch/riscv64/fault_handler_riscv64.cc",
+            ],
+            asflags: [
+                "-fno-integrated-as",
+              "-Xassembler",
+              "-mno-relax",
+            ],
+        },
     },
     target: {
         android: {
@@ -526,7 +547,7 @@ art_cc_library {
     target: {
         android: {
             lto: {
-                 thin: true,
+                 thin: false,
             },
         },
     },
@@ -731,3 +752,11 @@ genrule {
   tool_files: ["interpreter/mterp/gen_mterp.py", "interpreter/mterp/common/gen_setup.py"],
   cmd: "$(location interpreter/mterp/gen_mterp.py) $(out) $(in)",
 }
+
+genrule {
+  name: "libart_mterp.riscv64",
+  out: ["mterp_riscv64.S"],
+  srcs: ["interpreter/mterp/riscv64/*.S"],
+  tool_files: ["interpreter/mterp/gen_mterp.py", "interpreter/mterp/common/gen_setup.py"],
+  cmd: "$(location interpreter/mterp/gen_mterp.py) $(out) $(in)",
+}
diff --git a/runtime/arch/arch_test.cc b/runtime/arch/arch_test.cc
index 12ad84b2dd..3dc71f9e32 100644
--- a/runtime/arch/arch_test.cc
+++ b/runtime/arch/arch_test.cc
@@ -165,6 +165,24 @@ static constexpr size_t kFrameSizeSaveEverything = FRAME_SIZE_SAVE_EVERYTHING;
 #undef FRAME_SIZE_SAVE_EVERYTHING
 }  // namespace x86_64
 
+
+namespace riscv64 {
+#include "arch/riscv64/asm_support_riscv64.h"
+static constexpr size_t kFrameSizeSaveAllCalleeSaves = FRAME_SIZE_SAVE_ALL_CALLEE_SAVES;
+#undef FRAME_SIZE_SAVE_ALL_CALLEE_SAVES
+static constexpr size_t kFrameSizeSaveRefsOnly = FRAME_SIZE_SAVE_REFS_ONLY;
+#undef FRAME_SIZE_SAVE_REFS_ONLY
+static constexpr size_t kFrameSizeSaveRefsAndArgs = FRAME_SIZE_SAVE_REFS_AND_ARGS;
+#undef FRAME_SIZE_SAVE_REFS_AND_ARGS
+static constexpr size_t kFrameSizeSaveEverythingForClinit = FRAME_SIZE_SAVE_EVERYTHING_FOR_CLINIT;
+#undef FRAME_SIZE_SAVE_EVERYTHING_FOR_CLINIT
+static constexpr size_t kFrameSizeSaveEverythingForSuspendCheck =
+    FRAME_SIZE_SAVE_EVERYTHING_FOR_SUSPEND_CHECK;
+#undef FRAME_SIZE_SAVE_EVERYTHING_FOR_SUSPEND_CHECK
+static constexpr size_t kFrameSizeSaveEverything = FRAME_SIZE_SAVE_EVERYTHING;
+#undef FRAME_SIZE_SAVE_EVERYTHING
+}  // namespace riscv64
+
 // Check architecture specific constants are sound.
 // We expect the return PC to be stored at the highest address slot in the frame.
 #define TEST_ARCH_TYPE(Arch, arch, type)                                              \
@@ -187,5 +205,5 @@ TEST_ARCH(Mips, mips)
 TEST_ARCH(Mips64, mips64)
 TEST_ARCH(X86, x86)
 TEST_ARCH(X86_64, x86_64)
-
+TEST_ARCH(Riscv64, riscv64)
 }  // namespace art
diff --git a/runtime/arch/context-inl.h b/runtime/arch/context-inl.h
index ddcbbb18e5..f6fe76946a 100644
--- a/runtime/arch/context-inl.h
+++ b/runtime/arch/context-inl.h
@@ -40,6 +40,9 @@
 #elif defined(__x86_64__)
 #include "x86_64/context_x86_64.h"
 #define RUNTIME_CONTEXT_TYPE x86_64::X86_64Context
+#elif defined(__riscv)
+#include "riscv64/context_riscv64.h"
+#define RUNTIME_CONTEXT_TYPE riscv64::Riscv64Context
 #else
 #error unimplemented
 #endif
diff --git a/runtime/arch/instruction_set_features.cc b/runtime/arch/instruction_set_features.cc
index c5c2d31158..9fb61bb918 100644
--- a/runtime/arch/instruction_set_features.cc
+++ b/runtime/arch/instruction_set_features.cc
@@ -32,6 +32,7 @@
 #include "mips64/instruction_set_features_mips64.h"
 #include "x86/instruction_set_features_x86.h"
 #include "x86_64/instruction_set_features_x86_64.h"
+#include "riscv64/instruction_set_features_riscv64.h"
 
 namespace art {
 
@@ -51,6 +52,8 @@ std::unique_ptr<const InstructionSetFeatures> InstructionSetFeatures::FromVarian
       return X86InstructionSetFeatures::FromVariant(variant, error_msg);
     case InstructionSet::kX86_64:
       return X86_64InstructionSetFeatures::FromVariant(variant, error_msg);
+    case InstructionSet::kRiscv64:
+      return Riscv64InstructionSetFeatures::FromVariant(variant, error_msg);
 
     case InstructionSet::kNone:
       break;
@@ -82,6 +85,9 @@ std::unique_ptr<const InstructionSetFeatures> InstructionSetFeatures::FromBitmap
     case InstructionSet::kX86_64:
       result = X86_64InstructionSetFeatures::FromBitmap(bitmap);
       break;
+    case InstructionSet::kRiscv64:
+      result = Riscv64InstructionSetFeatures::FromBitmap(bitmap);
+      break;
 
     case InstructionSet::kNone:
     default:
@@ -107,6 +113,8 @@ std::unique_ptr<const InstructionSetFeatures> InstructionSetFeatures::FromCppDef
       return X86InstructionSetFeatures::FromCppDefines();
     case InstructionSet::kX86_64:
       return X86_64InstructionSetFeatures::FromCppDefines();
+    case InstructionSet::kRiscv64:
+      return Riscv64InstructionSetFeatures::FromCppDefines();
 
     case InstructionSet::kNone:
       break;
@@ -141,6 +149,8 @@ std::unique_ptr<const InstructionSetFeatures> InstructionSetFeatures::FromCpuInf
       return X86InstructionSetFeatures::FromCpuInfo();
     case InstructionSet::kX86_64:
       return X86_64InstructionSetFeatures::FromCpuInfo();
+    case InstructionSet::kRiscv64:
+      return Riscv64InstructionSetFeatures::FromCpuInfo();
 
     case InstructionSet::kNone:
       break;
@@ -164,6 +174,8 @@ std::unique_ptr<const InstructionSetFeatures> InstructionSetFeatures::FromHwcap(
       return X86InstructionSetFeatures::FromHwcap();
     case InstructionSet::kX86_64:
       return X86_64InstructionSetFeatures::FromHwcap();
+    case InstructionSet::kRiscv64:
+      return Riscv64InstructionSetFeatures::FromHwcap();
 
     case InstructionSet::kNone:
       break;
@@ -187,6 +199,8 @@ std::unique_ptr<const InstructionSetFeatures> InstructionSetFeatures::FromAssemb
       return X86InstructionSetFeatures::FromAssembly();
     case InstructionSet::kX86_64:
       return X86_64InstructionSetFeatures::FromAssembly();
+    case InstructionSet::kRiscv64:
+      return Riscv64InstructionSetFeatures::FromAssembly();
 
     case InstructionSet::kNone:
       break;
@@ -280,6 +294,11 @@ const X86_64InstructionSetFeatures* InstructionSetFeatures::AsX86_64InstructionS
   return down_cast<const X86_64InstructionSetFeatures*>(this);
 }
 
+const Riscv64InstructionSetFeatures* InstructionSetFeatures::AsRiscv64InstructionSetFeatures() const {
+  DCHECK_EQ(InstructionSet::kRiscv64, GetInstructionSet());
+  return down_cast<const Riscv64InstructionSetFeatures*>(this);
+}
+
 bool InstructionSetFeatures::FindVariantInArray(const char* const variants[], size_t num_variants,
                                                 const std::string& variant) {
   const char* const * begin = variants;
diff --git a/runtime/arch/instruction_set_features.h b/runtime/arch/instruction_set_features.h
index 9222a7bad7..555ca3835c 100644
--- a/runtime/arch/instruction_set_features.h
+++ b/runtime/arch/instruction_set_features.h
@@ -32,6 +32,7 @@ class MipsInstructionSetFeatures;
 class Mips64InstructionSetFeatures;
 class X86InstructionSetFeatures;
 class X86_64InstructionSetFeatures;
+class Riscv64InstructionSetFeatures;
 
 // Abstraction used to describe features of a different instruction sets.
 class InstructionSetFeatures {
@@ -126,6 +127,9 @@ class InstructionSetFeatures {
   // Down cast this X86_64InstructionFeatures.
   const X86_64InstructionSetFeatures* AsX86_64InstructionSetFeatures() const;
 
+  // Down cast this Riscv64InstructionFeatures.
+  const Riscv64InstructionSetFeatures* AsRiscv64InstructionSetFeatures() const;
+
   virtual ~InstructionSetFeatures() {}
 
  protected:
diff --git a/runtime/arch/mips64/quick_entrypoints_mips64.S b/runtime/arch/mips64/quick_entrypoints_mips64.S
index ebf1d5b0b4..458d50d455 100644
--- a/runtime/arch/mips64/quick_entrypoints_mips64.S
+++ b/runtime/arch/mips64/quick_entrypoints_mips64.S
@@ -449,9 +449,7 @@
     s.d    $f1, 24($sp)
     s.d    $f0, 16($sp)
 
-    # load appropriate callee-save-method
-    ld      $t1, %got(_ZN3art7Runtime9instance_E)($gp)
-    ld      $t1, 0($t1)
+    # load appropriate callee-save-method208
     ld      $t1, \runtime_method_offset($t1)
     sd      $t1, 0($sp)                                # Place ArtMethod* at bottom of stack.
     # Place sp in Thread::Current()->top_quick_frame.
diff --git a/runtime/arch/riscv64/asm_support_riscv64.S b/runtime/arch/riscv64/asm_support_riscv64.S
new file mode 100644
index 0000000000..1f8e4eab77
--- /dev/null
+++ b/runtime/arch/riscv64/asm_support_riscv64.S
@@ -0,0 +1,94 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_ASM_SUPPORT_RISCV64_S_
+#define ART_RUNTIME_ARCH_RISCV64_ASM_SUPPORT_RISCV64_S_
+
+#include "asm_support_riscv64.h"
+
+// Define special registers.
+
+// Register holding suspend check count down.
+#define rSUSPEND s11
+// Register holding Thread::Current().
+#define rSELF s1
+
+
+    // Declare a function called name, doesn't set up $gp.
+.macro ENTRY_NO_GP_CUSTOM_CFA name, cfa_offset
+    .type \name, %function
+    .global \name
+    // Cache alignment for function entry.
+    .balign 16
+\name:
+    .cfi_startproc
+     // Ensure we get a sane starting CFA.
+    .cfi_def_cfa sp, \cfa_offset
+.endm
+
+    // Declare a function called name, doesn't set up $gp.
+.macro ENTRY_NO_GP name
+    ENTRY_NO_GP_CUSTOM_CFA \name, 0
+.endm
+
+    // Declare a function called name, sets up $gp.
+    // This macro modifies t8.
+.macro ENTRY name
+    ENTRY_NO_GP \name
+    // Set up $gp and store the previous $gp value to $t8. It will be pushed to the
+    // stack after the frame has been constructed.
+    // FIXME: T-HEAD, Need check here in future.
+    // .cpsetup $t9, $t8, \name
+    // Declare a local convenience label to be branched to when $gp is already set up.
+.L\name\()_gp_set:
+.endm
+
+.macro END name
+    .cfi_endproc
+    .size \name, .-\name
+.endm
+
+.macro UNIMPLEMENTED name
+    ENTRY \name
+    break
+    break
+    END \name
+.endm
+
+// Macros to poison (negate) the reference for heap poisoning.
+.macro POISON_HEAP_REF rRef
+#ifdef USE_HEAP_POISONING
+    dsubu \rRef, zero, \rRef
+    dext  \rRef, \rRef, 0, 32
+#endif  // USE_HEAP_POISONING
+.endm
+
+// Macros to unpoison (negate) the reference for heap poisoning.
+.macro UNPOISON_HEAP_REF rRef
+#ifdef USE_HEAP_POISONING
+    dsubu \rRef, zero, \rRef
+    dext  \rRef, \rRef, 0, 32
+#endif  // USE_HEAP_POISONING
+.endm
+
+// Byte size of the instructions (un)poisoning heap references.
+#ifdef USE_HEAP_POISONING
+#define HEAP_POISON_INSTR_SIZE 8
+#else
+#define HEAP_POISON_INSTR_SIZE 0
+#endif  // USE_HEAP_POISONING
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_ASM_SUPPORT_RISCV64_S_
diff --git a/runtime/arch/riscv64/asm_support_riscv64.h b/runtime/arch/riscv64/asm_support_riscv64.h
new file mode 100644
index 0000000000..400f371162
--- /dev/null
+++ b/runtime/arch/riscv64/asm_support_riscv64.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_ASM_SUPPORT_RISCV64_H_
+#define ART_RUNTIME_ARCH_RISCV64_ASM_SUPPORT_RISCV64_H_
+
+#include "asm_support.h"
+
+// 80 ($f18-$f27) + 16 (f8-f9) + 96($s0-$s11) + 8 ($ra) + 1x8 bytes padding
+#define FRAME_SIZE_SAVE_ALL_CALLEE_SAVES 208
+// 72 ($s2-$s10) + 8 ($s0) + 8 ($ra) + 1x8 bytes padding
+#define FRAME_SIZE_SAVE_REFS_ONLY 96
+// $f10-$f17, $a1-$a7, $s2-$s10 + $s0 + $ra, 26 total + 1x8 bytes padding + method*
+#define FRAME_SIZE_SAVE_REFS_AND_ARGS 224
+// $f0-$f31, $a0-$a7, $t0-$t6, $s1-$s11, $s0, $ra + padding + method*
+#define FRAME_SIZE_SAVE_EVERYTHING 496
+#define FRAME_SIZE_SAVE_EVERYTHING_FOR_CLINIT FRAME_SIZE_SAVE_EVERYTHING
+#define FRAME_SIZE_SAVE_EVERYTHING_FOR_SUSPEND_CHECK FRAME_SIZE_SAVE_EVERYTHING
+
+// &art_quick_read_barrier_mark_introspection is the first of many entry points:
+//   20 entry points for long field offsets, large array indices and variable array indices
+//     (see macro BRB_FIELD_LONG_OFFSET_ENTRY)
+//   20 entry points for short field offsets and small array indices
+//     (see macro BRB_FIELD_SHORT_OFFSET_ENTRY)
+//   20 entry points for GC roots
+//     (see macro BRB_GC_ROOT_ENTRY)
+
+// There are as many entry points of each kind as there are registers that
+// can hold a reference: V0-V1, A0-A7, T0-T2, S2-S8.
+#define BAKER_MARK_INTROSPECTION_REGISTER_COUNT 21
+
+#define BAKER_MARK_INTROSPECTION_FIELD_ARRAY_ENTRY_SIZE (10 * 4)  // 8 instructions in
+                                                                 // BRB_FIELD_*_OFFSET_ENTRY.
+
+#define BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRIES_OFFSET \
+    (2 * BAKER_MARK_INTROSPECTION_REGISTER_COUNT * BAKER_MARK_INTROSPECTION_FIELD_ARRAY_ENTRY_SIZE)
+
+#define BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRY_SIZE (6 * 4)  // 4 instructions in BRB_GC_ROOT_ENTRY.
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_ASM_SUPPORT_RISCV64_H_
diff --git a/runtime/arch/riscv64/callee_save_frame_riscv64.h b/runtime/arch/riscv64/callee_save_frame_riscv64.h
new file mode 100644
index 0000000000..eb42c19199
--- /dev/null
+++ b/runtime/arch/riscv64/callee_save_frame_riscv64.h
@@ -0,0 +1,133 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_CALLEE_SAVE_FRAME_RISCV64_H_
+#define ART_RUNTIME_ARCH_RISCV64_CALLEE_SAVE_FRAME_RISCV64_H_
+
+#include "arch/instruction_set.h"
+#include "base/bit_utils.h"
+#include "base/callee_save_type.h"
+#include "base/enums.h"
+#include "quick/quick_method_frame_info.h"
+#include "registers_riscv64.h"
+#include "runtime_globals.h"
+
+namespace art {
+namespace riscv64 {
+
+static constexpr uint32_t kRiscv64CalleeSaveAlwaysSpills =
+    (1 << art::riscv64::RA);
+static constexpr uint32_t kRiscv64CalleeSaveRefSpills =
+    (1 << art::riscv64::S0) |
+    (1 << art::riscv64::S2) | (1 << art::riscv64::S3) | (1 << art::riscv64::S4) |
+    (1 << art::riscv64::S5) | (1 << art::riscv64::S6) | (1 << art::riscv64::S7) |
+    (1 << art::riscv64::S8) | (1 << art::riscv64::S9) | (1 << art::riscv64::S10);
+// A0 is the method pointer. Not saved.
+static constexpr uint32_t kRiscv64CalleeSaveArgSpills =
+    (1 << art::riscv64::A1) | (1 << art::riscv64::A2) | (1 << art::riscv64::A3) |
+    (1 << art::riscv64::A4) | (1 << art::riscv64::A5) | (1 << art::riscv64::A6) |
+    (1 << art::riscv64::A7);
+
+static constexpr uint32_t kRiscv64CalleeSaveAllSpills =
+    (1 << art::riscv64::S1) | (1 << art::riscv64::S11);
+static constexpr uint32_t kRiscv64CalleeSaveEverythingSpills =
+    (1 << art::riscv64::A0) | (1 << art::riscv64::A1) | (1 << art::riscv64::A2) |
+    (1 << art::riscv64::A3) | (1 << art::riscv64::A4) | (1 << art::riscv64::A5) |
+    (1 << art::riscv64::A6) | (1 << art::riscv64::A7) | (1 << art::riscv64::T0) |
+    (1 << art::riscv64::T1) | (1 << art::riscv64::T2) | (1 << art::riscv64::T3) |
+    (1 << art::riscv64::T4) | (1 << art::riscv64::T5) | (1 << art::riscv64::T6) |
+    (1 << art::riscv64::S1) | (1 << art::riscv64::S11);
+
+
+static constexpr uint32_t kRiscv64CalleeSaveFpRefSpills = 0;
+/* Registers for arguments*/
+static constexpr uint32_t kRiscv64CalleeSaveFpArgSpills =
+    (1 << art::riscv64::FA0) | (1 << art::riscv64::FA1) | (1 << art::riscv64::FA2) |
+    (1 << art::riscv64::FA3) | (1 << art::riscv64::FA4) | (1 << art::riscv64::FA5) |
+    (1 << art::riscv64::FA6) | (1 << art::riscv64::FA7);
+/* Registers that callee has to save*/
+static constexpr uint32_t kRiscv64CalleeSaveFpAllSpills =
+    (1 << art::riscv64::FS0) | (1 << art::riscv64::FS1) | (1 << art::riscv64::FS2) |
+    (1 << art::riscv64::FS3) | (1 << art::riscv64::FS4) | (1 << art::riscv64::FS5) |
+    (1 << art::riscv64::FS6) | (1 << art::riscv64::FS7) | (1 << art::riscv64::FS8) |
+    (1 << art::riscv64::FS9) | (1 << art::riscv64::FS10) | (1 << art::riscv64::FS11);
+/* Save all float registers*/
+static constexpr uint32_t kRiscv64CalleeSaveFpEverythingSpills =
+    (1 << art::riscv64::FA0) | (1 << art::riscv64::FA1) | (1 << art::riscv64::FA2) |
+    (1 << art::riscv64::FA3) | (1 << art::riscv64::FA4) | (1 << art::riscv64::FA5) |
+    (1 << art::riscv64::FA6) | (1 << art::riscv64::FA7) | (1 << art::riscv64::FT0) |
+    (1 << art::riscv64::FT1) | (1 << art::riscv64::FT2) | (1 << art::riscv64::FT3) |
+    (1 << art::riscv64::FT4) | (1 << art::riscv64::FT5) | (1 << art::riscv64::FT6) |
+    (1 << art::riscv64::FT7) | (1 << art::riscv64::FT8) | (1 << art::riscv64::FT9) |
+    (1 << art::riscv64::FT10) | (1 << art::riscv64::FT11) | (1 << art::riscv64::FS0) |
+    (1 << art::riscv64::FS1) | (1 << art::riscv64::FS2) | (1 << art::riscv64::FS3) |
+    (1 << art::riscv64::FS4) | (1 << art::riscv64::FS5) | (1 << art::riscv64::FS6) |
+    (1 << art::riscv64::FS7) | (1 << art::riscv64::FS8) | (1 << art::riscv64::FS9) |
+    (1 << art::riscv64::FS10) | (1 << art::riscv64::FS11);
+
+class Riscv64CalleeSaveFrame {
+ public:
+  static constexpr uint32_t GetCoreSpills(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return kRiscv64CalleeSaveAlwaysSpills | kRiscv64CalleeSaveRefSpills |
+        (type == CalleeSaveType::kSaveRefsAndArgs ? kRiscv64CalleeSaveArgSpills : 0) |
+        (type == CalleeSaveType::kSaveAllCalleeSaves ? kRiscv64CalleeSaveAllSpills : 0) |
+        (type == CalleeSaveType::kSaveEverything ? kRiscv64CalleeSaveEverythingSpills : 0);
+  }
+
+  static constexpr uint32_t GetFpSpills(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return kRiscv64CalleeSaveFpRefSpills |
+        (type == CalleeSaveType::kSaveRefsAndArgs ? kRiscv64CalleeSaveFpArgSpills : 0) |
+        (type == CalleeSaveType::kSaveAllCalleeSaves ? kRiscv64CalleeSaveFpAllSpills : 0) |
+        (type == CalleeSaveType::kSaveEverything ? kRiscv64CalleeSaveFpEverythingSpills : 0);
+  }
+
+  static constexpr uint32_t GetFrameSize(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return RoundUp((POPCOUNT(GetCoreSpills(type)) /* gprs */ +
+                    POPCOUNT(GetFpSpills(type))   /* fprs */ +
+                    + 1 /* Method* */) * static_cast<size_t>(kRiscv64PointerSize), kStackAlignment);
+  }
+
+  static constexpr QuickMethodFrameInfo GetMethodFrameInfo(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return QuickMethodFrameInfo(GetFrameSize(type), GetCoreSpills(type), GetFpSpills(type));
+  }
+
+  static constexpr size_t GetFpr1Offset(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return GetFrameSize(type) -
+           (POPCOUNT(GetCoreSpills(type)) +
+            POPCOUNT(GetFpSpills(type))) * static_cast<size_t>(kRiscv64PointerSize);
+  }
+
+  static constexpr size_t GetGpr1Offset(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return GetFrameSize(type) -
+           POPCOUNT(GetCoreSpills(type)) * static_cast<size_t>(kRiscv64PointerSize);
+  }
+
+  static constexpr size_t GetReturnPcOffset(CalleeSaveType type) {
+    type = GetCanonicalCalleeSaveType(type);
+    return GetFrameSize(type) - static_cast<size_t>(kRiscv64PointerSize);
+  }
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_CALLEE_SAVE_FRAME_RISCV64_H_
diff --git a/runtime/arch/riscv64/context_riscv64.cc b/runtime/arch/riscv64/context_riscv64.cc
new file mode 100644
index 0000000000..bb4ff9757c
--- /dev/null
+++ b/runtime/arch/riscv64/context_riscv64.cc
@@ -0,0 +1,143 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "context_riscv64.h"
+
+#include "base/bit_utils.h"
+#include "base/bit_utils_iterator.h"
+#include "quick/quick_method_frame_info.h"
+
+namespace art {
+namespace riscv64 {
+
+static constexpr uintptr_t gZero = 0;
+
+void Riscv64Context::Reset() {
+  std::fill_n(gprs_, arraysize(gprs_), nullptr);
+  std::fill_n(fprs_, arraysize(fprs_), nullptr);
+  gprs_[SP] = &sp_;
+  gprs_[T6] = &t6_;
+  gprs_[A0] = &arg0_;
+  // Initialize registers with easy to spot debug values.
+  sp_ = Riscv64Context::kBadGprBase + SP;
+  t6_ = Riscv64Context::kBadGprBase + T6;
+  arg0_ = 0;
+}
+
+void Riscv64Context::FillCalleeSaves(uint8_t* frame, const QuickMethodFrameInfo& frame_info) {
+  int spill_pos = 0;
+  int gpr_spill_pos = spill_pos + 2;
+
+  // Core registers come first, from the highest down to the lowest.
+  for (uint32_t core_reg : HighToLowBits(frame_info.CoreSpillMask())) {
+    if (core_reg == RA) {
+      // RA is at top of the frame
+      gprs_[RA] = CalleeSaveAddress(frame, 0, frame_info.FrameSizeInBytes());
+    } else if (core_reg == S0) {
+      // FP is at the next to the top of the frame
+      gprs_[S0] = CalleeSaveAddress(frame, 1, frame_info.FrameSizeInBytes());
+    } else {
+      // the other saved register is from (top - 2) offset
+      gprs_[core_reg] = CalleeSaveAddress(frame, gpr_spill_pos, frame_info.FrameSizeInBytes());
+      ++gpr_spill_pos;
+    }
+    ++spill_pos;
+  }
+  DCHECK_EQ(spill_pos, POPCOUNT(frame_info.CoreSpillMask()));
+
+  // FP registers come second, from the highest down to the lowest.
+  for (uint32_t fp_reg : HighToLowBits(frame_info.FpSpillMask())) {
+    fprs_[fp_reg] = CalleeSaveAddress(frame, spill_pos, frame_info.FrameSizeInBytes());
+    ++spill_pos;
+  }
+  DCHECK_EQ(spill_pos, POPCOUNT(frame_info.CoreSpillMask()) + POPCOUNT(frame_info.FpSpillMask()));
+}
+
+void Riscv64Context::SetGPR(uint32_t reg, uintptr_t value) {
+  CHECK_LT(reg, static_cast<uint32_t>(kNumberOfGpuRegisters));
+  DCHECK(IsAccessibleGPR(reg));
+  CHECK_NE(gprs_[reg], &gZero);  // Can't overwrite this static value since they are never reset.
+  *gprs_[reg] = value;
+}
+
+void Riscv64Context::SetFPR(uint32_t reg, uintptr_t value) {
+  CHECK_LT(reg, static_cast<uint32_t>(kNumberOfFpuRegisters));
+  DCHECK(IsAccessibleFPR(reg));
+  CHECK_NE(fprs_[reg], &gZero);  // Can't overwrite this static value since they are never reset.
+  *fprs_[reg] = value;
+}
+
+void Riscv64Context::SmashCallerSaves() {
+  // This needs to be 0 because we want a null/zero return value.
+  // gprs_[A0] = const_cast<uintptr_t*>(&gZero);
+  // gprs_[A1] = const_cast<uintptr_t*>(&gZero);
+  gprs_[A0] = const_cast<uintptr_t*>(&gZero);
+  gprs_[A1] = const_cast<uintptr_t*>(&gZero);
+  gprs_[A2] = nullptr;
+  gprs_[A3] = nullptr;
+  gprs_[A4] = nullptr;
+  gprs_[A5] = nullptr;
+  gprs_[A6] = nullptr;
+  gprs_[A7] = nullptr;
+
+  // gprs_[T0] = nullptr;
+  // gprs_[T1] = nullptr;
+  // gprs_[T2] = nullptr;
+  // gprs_[T3] = nullptr;
+  // gprs_[T4] = nullptr;
+  // gprs_[T5] = nullptr;
+  // gprs_[T6] = nullptr;
+
+  // f0-f7 / f10-f17 / f28-f31 are caller-saved;
+  fprs_[FA0] = nullptr;
+  fprs_[FA1] = nullptr;
+  fprs_[FA2] = nullptr;
+  fprs_[FA3] = nullptr;
+  fprs_[FA4] = nullptr;
+  fprs_[FA5] = nullptr;
+  fprs_[FA6] = nullptr;
+  fprs_[FA7] = nullptr;
+
+  fprs_[FT0] = nullptr;
+  fprs_[FT1] = nullptr;
+  fprs_[FT2] = nullptr;
+  fprs_[FT3] = nullptr;
+  fprs_[FT4] = nullptr;
+  fprs_[FT5] = nullptr;
+  fprs_[FT6] = nullptr;
+  fprs_[FT7] = nullptr;
+  fprs_[FT8] = nullptr;
+  fprs_[FT9] = nullptr;
+  fprs_[FT10] = nullptr;
+  fprs_[FT11] = nullptr;
+}
+
+extern "C" NO_RETURN void art_quick_do_long_jump(uint64_t*, uint64_t*);
+
+void Riscv64Context::DoLongJump() {
+  uintptr_t gprs[kNumberOfGpuRegisters];
+  uintptr_t fprs[kNumberOfFpuRegisters];
+  for (size_t i = 0; i < kNumberOfGpuRegisters; ++i) {
+    gprs[i] = gprs_[i] != nullptr ? *gprs_[i] : Riscv64Context::kBadGprBase + i;
+  }
+  for (size_t i = 0; i < kNumberOfFpuRegisters; ++i) {
+    fprs[i] = fprs_[i] != nullptr ? *fprs_[i] : Riscv64Context::kBadFprBase + i;
+  }
+  art_quick_do_long_jump(gprs, fprs);
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/runtime/arch/riscv64/context_riscv64.h b/runtime/arch/riscv64/context_riscv64.h
new file mode 100644
index 0000000000..4543054bae
--- /dev/null
+++ b/runtime/arch/riscv64/context_riscv64.h
@@ -0,0 +1,99 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_CONTEXT_RISCV64_H_
+#define ART_RUNTIME_ARCH_RISCV64_CONTEXT_RISCV64_H_
+
+#include <android-base/logging.h>
+
+#include "arch/context.h"
+#include "base/macros.h"
+#include "registers_riscv64.h"
+
+namespace art {
+namespace riscv64 {
+
+class Riscv64Context : public Context {
+ public:
+  Riscv64Context() {
+    Reset();
+  }
+  virtual ~Riscv64Context() {}
+
+  void Reset() override;
+
+  void FillCalleeSaves(uint8_t* frame, const QuickMethodFrameInfo& fr) override;
+
+  void SetSP(uintptr_t new_sp) override {
+    SetGPR(SP, new_sp);
+  }
+
+  void SetPC(uintptr_t new_pc) override {
+    SetGPR(T6, new_pc);
+  }
+
+  bool IsAccessibleGPR(uint32_t reg) override {
+    DCHECK_LT(reg, static_cast<uint32_t>(kNumberOfGpuRegisters));
+    return gprs_[reg] != nullptr;
+  }
+
+  uintptr_t* GetGPRAddress(uint32_t reg) override {
+    DCHECK_LT(reg, static_cast<uint32_t>(kNumberOfGpuRegisters));
+    return gprs_[reg];
+  }
+
+  uintptr_t GetGPR(uint32_t reg) override {
+    CHECK_LT(reg, static_cast<uint32_t>(kNumberOfGpuRegisters));
+    DCHECK(IsAccessibleGPR(reg));
+    return *gprs_[reg];
+  }
+
+  void SetGPR(uint32_t reg, uintptr_t value) override;
+
+  bool IsAccessibleFPR(uint32_t reg) override {
+    CHECK_LT(reg, static_cast<uint32_t>(kNumberOfFpuRegisters));
+    return fprs_[reg] != nullptr;
+  }
+
+  uintptr_t GetFPR(uint32_t reg) override {
+    CHECK_LT(reg, static_cast<uint32_t>(kNumberOfFpuRegisters));
+    DCHECK(IsAccessibleFPR(reg));
+    return *fprs_[reg];
+  }
+
+  void SetFPR(uint32_t reg, uintptr_t value) override;
+
+  void SmashCallerSaves() override;
+  NO_RETURN void DoLongJump() override;
+
+  void SetArg0(uintptr_t new_arg0_value) override {
+    SetGPR(A0, new_arg0_value);
+  }
+
+ private:
+  // Pointers to registers in the stack, initialized to null except for the special cases below.
+  uintptr_t* gprs_[kNumberOfGpuRegisters];
+  uint64_t* fprs_[kNumberOfFpuRegisters];
+  // Hold values for sp and pc if they are not located within a stack frame. We use t9 for the
+  // PC (as ra is required to be valid for single-frame deopt and must not be clobbered). We
+  // also need the first argument for single-frame deopt.
+  uintptr_t sp_, t6_, arg0_;
+};
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_CONTEXT_RISCV64_H_
diff --git a/runtime/arch/riscv64/entrypoints_init_riscv64.cc b/runtime/arch/riscv64/entrypoints_init_riscv64.cc
new file mode 100644
index 0000000000..b3fd0e58de
--- /dev/null
+++ b/runtime/arch/riscv64/entrypoints_init_riscv64.cc
@@ -0,0 +1,212 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <math.h>
+#include <string.h>
+#include <atomic>
+
+#include "arch/riscv64/asm_support_riscv64.h"
+#include "base/atomic.h"
+#include "base/quasi_atomic.h"
+#include "entrypoints/entrypoint_utils.h"
+#include "entrypoints/jni/jni_entrypoints.h"
+#include "entrypoints/math_entrypoints.h"
+#include "entrypoints/quick/quick_alloc_entrypoints.h"
+#include "entrypoints/quick/quick_default_externs.h"
+#include "entrypoints/quick/quick_default_init_entrypoints.h"
+#include "entrypoints/quick/quick_entrypoints.h"
+#include "entrypoints/runtime_asm_entrypoints.h"
+#include "interpreter/interpreter.h"
+
+namespace art {
+
+// Cast entrypoints.
+extern "C" size_t artInstanceOfFromCode(mirror::Object* obj, mirror::Class* ref_class);
+
+// Read barrier entrypoints.
+// art_quick_read_barrier_mark_regXX uses a non-standard calling
+// convention: it expects its input in register XX+1 and returns its
+// result in that same register, and saves and restores all
+// caller-save registers.
+
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg04(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg05(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg06(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg07(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg09(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg10(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg11(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg12(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg13(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg14(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg15(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg16(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg17(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg18(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg19(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg20(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg21(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg22(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg23(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg24(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_reg25(mirror::Object*);
+
+  extern "C" mirror::Object* art_quick_read_barrier_mark_introspection(mirror::Object*);
+  extern "C" mirror::Object* art_quick_read_barrier_mark_introspection_gc_roots(mirror::Object*);
+  extern "C" void art_quick_read_barrier_mark_introspection_end_of_entries(void);
+
+// Math entrypoints.
+extern int32_t CmpgDouble(double a, double b);
+extern int32_t CmplDouble(double a, double b);
+extern int32_t CmpgFloat(float a, float b);
+extern int32_t CmplFloat(float a, float b);
+extern "C" int64_t artLmul(int64_t a, int64_t b);
+extern "C" int64_t artLdiv(int64_t a, int64_t b);
+extern "C" int64_t artLmod(int64_t a, int64_t b);
+
+// Math conversions.
+extern "C" int32_t __fixsfsi(float op1);      // FLOAT_TO_INT
+extern "C" int32_t __fixdfsi(double op1);     // DOUBLE_TO_INT
+extern "C" float __floatdisf(int64_t op1);    // LONG_TO_FLOAT
+extern "C" double __floatdidf(int64_t op1);   // LONG_TO_DOUBLE
+extern "C" int64_t __fixsfdi(float op1);      // FLOAT_TO_LONG
+extern "C" int64_t __fixdfdi(double op1);     // DOUBLE_TO_LONG
+
+// Single-precision FP arithmetics.
+extern "C" float fmodf(float a, float b);      // REM_FLOAT[_2ADDR]
+
+// Double-precision FP arithmetics.
+extern "C" double fmod(double a, double b);     // REM_DOUBLE[_2ADDR]
+
+// Long long arithmetics - REM_LONG[_2ADDR] and DIV_LONG[_2ADDR]
+extern "C" int64_t __divdi3(int64_t, int64_t);
+extern "C" int64_t __moddi3(int64_t, int64_t);
+
+// No read barrier entrypoints for marking registers.
+void UpdateReadBarrierEntrypoints(QuickEntryPoints* qpoints, bool is_active) {
+  intptr_t introspection_field_array_entries_size =
+      reinterpret_cast<intptr_t>(&art_quick_read_barrier_mark_introspection_gc_roots) -
+      reinterpret_cast<intptr_t>(&art_quick_read_barrier_mark_introspection);
+  static_assert(
+      BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRIES_OFFSET == 2 *
+          BAKER_MARK_INTROSPECTION_REGISTER_COUNT * BAKER_MARK_INTROSPECTION_FIELD_ARRAY_ENTRY_SIZE,
+      "Expecting equal");
+  DCHECK_EQ(introspection_field_array_entries_size,
+            BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRIES_OFFSET);
+  intptr_t introspection_gc_root_entries_size =
+      reinterpret_cast<intptr_t>(&art_quick_read_barrier_mark_introspection_end_of_entries) -
+      reinterpret_cast<intptr_t>(&art_quick_read_barrier_mark_introspection_gc_roots);
+  DCHECK_EQ(introspection_gc_root_entries_size,
+            BAKER_MARK_INTROSPECTION_REGISTER_COUNT * BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRY_SIZE);
+  qpoints->pReadBarrierMarkReg00 = is_active ? art_quick_read_barrier_mark_introspection : nullptr;
+  qpoints->pReadBarrierMarkReg04 = is_active ? art_quick_read_barrier_mark_reg04 : nullptr;
+  qpoints->pReadBarrierMarkReg05 = is_active ? art_quick_read_barrier_mark_reg05 : nullptr;
+  qpoints->pReadBarrierMarkReg06 = is_active ? art_quick_read_barrier_mark_reg06 : nullptr;
+  qpoints->pReadBarrierMarkReg07 = is_active ? art_quick_read_barrier_mark_reg07 : nullptr;
+  qpoints->pReadBarrierMarkReg09 = is_active ? art_quick_read_barrier_mark_reg09 : nullptr;
+  qpoints->pReadBarrierMarkReg10 = is_active ? art_quick_read_barrier_mark_reg10 : nullptr;
+  qpoints->pReadBarrierMarkReg11 = is_active ? art_quick_read_barrier_mark_reg11 : nullptr;
+  qpoints->pReadBarrierMarkReg12 = is_active ? art_quick_read_barrier_mark_reg12 : nullptr;
+  qpoints->pReadBarrierMarkReg13 = is_active ? art_quick_read_barrier_mark_reg13 : nullptr;
+  qpoints->pReadBarrierMarkReg14 = is_active ? art_quick_read_barrier_mark_reg14 : nullptr;
+  qpoints->pReadBarrierMarkReg15 = is_active ? art_quick_read_barrier_mark_reg15 : nullptr;
+  qpoints->pReadBarrierMarkReg16 = is_active ? art_quick_read_barrier_mark_reg16 : nullptr;
+  qpoints->pReadBarrierMarkReg17 = is_active ? art_quick_read_barrier_mark_reg17 : nullptr;
+  qpoints->pReadBarrierMarkReg18 = is_active ? art_quick_read_barrier_mark_reg18 : nullptr;
+  qpoints->pReadBarrierMarkReg19 = is_active ? art_quick_read_barrier_mark_reg19 : nullptr;
+  qpoints->pReadBarrierMarkReg20 = is_active ? art_quick_read_barrier_mark_reg20 : nullptr;
+  qpoints->pReadBarrierMarkReg21 = is_active ? art_quick_read_barrier_mark_reg21 : nullptr;
+  qpoints->pReadBarrierMarkReg22 = is_active ? art_quick_read_barrier_mark_reg22 : nullptr;
+  qpoints->pReadBarrierMarkReg23 = is_active ? art_quick_read_barrier_mark_reg23 : nullptr;
+  qpoints->pReadBarrierMarkReg24 = is_active ? art_quick_read_barrier_mark_reg24 : nullptr;
+  qpoints->pReadBarrierMarkReg25 = is_active ? art_quick_read_barrier_mark_reg25 : nullptr;
+}
+
+void InitEntryPoints(JniEntryPoints* jpoints, QuickEntryPoints* qpoints) {
+  DefaultInitEntryPoints(jpoints, qpoints);
+
+  // Cast
+  qpoints->pInstanceofNonTrivial = artInstanceOfFromCode;
+  qpoints->pCheckInstanceOf = art_quick_check_instance_of;
+
+  // Math
+  qpoints->pCmpgDouble = CmpgDouble;
+  qpoints->pCmpgFloat = CmpgFloat;
+  qpoints->pCmplDouble = CmplDouble;
+  qpoints->pCmplFloat = CmplFloat;
+  qpoints->pFmod = fmod;
+  qpoints->pL2d = art_l2d;
+  qpoints->pFmodf = fmodf;
+  qpoints->pL2f = art_l2f;
+  qpoints->pD2iz = art_d2i;
+  qpoints->pF2iz = art_f2i;
+  qpoints->pIdivmod = nullptr;
+  qpoints->pD2l = art_d2l;
+  qpoints->pF2l = art_f2l;
+  qpoints->pLdiv = artLdiv;
+  qpoints->pLmod = artLmod;
+  qpoints->pLmul = artLmul;
+  qpoints->pShlLong = nullptr;
+  qpoints->pShrLong = nullptr;
+  qpoints->pUshrLong = nullptr;
+
+  // More math.
+  qpoints->pCos = cos;
+  qpoints->pSin = sin;
+  qpoints->pAcos = acos;
+  qpoints->pAsin = asin;
+  qpoints->pAtan = atan;
+  qpoints->pAtan2 = atan2;
+  qpoints->pPow = pow;
+  qpoints->pCbrt = cbrt;
+  qpoints->pCosh = cosh;
+  qpoints->pExp = exp;
+  qpoints->pExpm1 = expm1;
+  qpoints->pHypot = hypot;
+  qpoints->pLog = log;
+  qpoints->pLog10 = log10;
+  qpoints->pNextAfter = nextafter;
+  qpoints->pSinh = sinh;
+  qpoints->pTan = tan;
+  qpoints->pTanh = tanh;
+
+  // Intrinsics
+  qpoints->pIndexOf = art_quick_indexof;
+  qpoints->pStringCompareTo = art_quick_string_compareto;
+  qpoints->pMemcpy = memcpy;
+
+  // TODO - use lld/scd instructions for Riscv64
+  // Atomic 64-bit load/store
+  qpoints->pA64Load = QuasiAtomic::Read64;
+  qpoints->pA64Store = QuasiAtomic::Write64;
+
+  // Read barrier.
+  qpoints->pReadBarrierJni = ReadBarrierJni;
+  UpdateReadBarrierEntrypoints(qpoints, /*is_active=*/ false);
+  // Cannot use the following registers to pass arguments:
+  // 0(ZERO), 1(AT), 15(T3), 16(S0), 17(S1), 24(T8), 25(T9), 26(K0), 27(K1), 28(GP), 29(SP), 31(RA).
+  // Note that there are 30 entry points only: 00 for register 1(AT), ..., 29 for register 30(S8).
+  qpoints->pReadBarrierMarkReg05 = nullptr;
+  qpoints->pReadBarrierSlow = artReadBarrierSlow;
+  qpoints->pReadBarrierForRootSlow = artReadBarrierForRootSlow;
+}
+
+// Helper function for art_quick_imt_conflict_trampoline to call __atomic_load_16 method.
+extern "C" __int128_t artInvokeAtomicMethod(__int128_t *mem, int model) {
+  return __atomic_load_n(mem, model);
+}
+
+}  // namespace art
diff --git a/runtime/arch/riscv64/fault_handler_riscv64.cc b/runtime/arch/riscv64/fault_handler_riscv64.cc
new file mode 100644
index 0000000000..b2456ecf86
--- /dev/null
+++ b/runtime/arch/riscv64/fault_handler_riscv64.cc
@@ -0,0 +1,151 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fault_handler.h"
+
+#include <asm/ucontext.h>
+
+#include "arch/instruction_set.h"
+#include "arch/riscv64/callee_save_frame_riscv64.h"
+#include "art_method.h"
+#include "base/callee_save_type.h"
+#include "base/hex_dump.h"
+#include "base/logging.h"  // For VLOG.
+#include "base/macros.h"
+#include "registers_riscv64.h"
+#include "runtime_globals.h"
+#include "thread-current-inl.h"
+
+extern "C" void art_quick_throw_stack_overflow();
+extern "C" void art_quick_throw_null_pointer_exception_from_signal();
+
+//
+// Riscv64 specific fault handler functions.
+//
+
+namespace art {
+
+void FaultManager::GetMethodAndReturnPcAndSp(siginfo_t* siginfo, void* context,
+                                             ArtMethod** out_method,
+                                             uintptr_t* out_return_pc, uintptr_t* out_sp) {
+  struct ucontext* uc = reinterpret_cast<struct ucontext*>(context);
+  struct sigcontext *sc = reinterpret_cast<struct sigcontext*>(&uc->uc_mcontext);
+  *out_sp = static_cast<uintptr_t>(sc->sc_regs.sp);
+  VLOG(signals) << "sp: " << *out_sp;
+  if (*out_sp == 0) {
+    return;
+  }
+
+  // In the case of a stack overflow, the stack is not valid and we can't
+  // get the method from the top of the stack.  However it's in r0.
+  uintptr_t* fault_addr = reinterpret_cast<uintptr_t*>(siginfo->si_addr);  // BVA addr
+  uintptr_t* overflow_addr = reinterpret_cast<uintptr_t*>(
+      reinterpret_cast<uint8_t*>(*out_sp) - GetStackOverflowReservedBytes(InstructionSet::kRiscv64));
+  if (overflow_addr == fault_addr) {
+    *out_method = reinterpret_cast<ArtMethod*>(sc->sc_regs.a0);
+  } else {
+    // The method is at the top of the stack.
+    *out_method = *reinterpret_cast<ArtMethod**>(*out_sp);
+  }
+
+  // Work out the return PC.  This will be the address of the instruction
+  // following the faulting ldr/str instruction.
+
+  VLOG(signals) << "pc: " << std::hex
+      << static_cast<void*>(reinterpret_cast<uint8_t*>(sc->sc_regs.pc));
+
+  *out_return_pc = sc->sc_regs.pc + 4;
+}
+
+bool NullPointerHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info, void* context) {
+  // FIXME: T-HEAD, use arm64 implementation here.
+  if (!IsValidImplicitCheck(info)) {
+    return false;
+  }
+  // The code that looks for the catch location needs to know the value of the
+  // PC at the point of call.  For Null checks we insert a GC map that is immediately after
+  // the load/store instruction that might cause the fault.
+
+  struct ucontext *uc = reinterpret_cast<struct ucontext*>(context);
+  struct sigcontext *sc = reinterpret_cast<struct sigcontext*>(&uc->uc_mcontext);
+
+  // Decrement $sp by the frame size of the kSaveEverything method and store
+  // the fault address in the padding right after the ArtMethod*.
+  assert(riscv64::Riscv64CalleeSaveFrame::GetFrameSize(CalleeSaveType::kSaveEverything) == 496);
+  sc->sc_regs.sp -= riscv64::Riscv64CalleeSaveFrame::GetFrameSize(CalleeSaveType::kSaveEverything);
+  uintptr_t* padding = reinterpret_cast<uintptr_t*>(sc->sc_regs.sp) + /* ArtMethod* */ 1;
+  *padding = reinterpret_cast<uintptr_t>(info->si_addr);
+
+  sc->sc_regs.ra = sc->sc_regs.pc + 4;      // RA needs to point to gc map location
+  sc->sc_regs.pc = reinterpret_cast<uintptr_t>(art_quick_throw_null_pointer_exception_from_signal);
+  VLOG(signals) << "Generating null pointer exception";
+  return true;
+}
+
+bool SuspensionHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info ATTRIBUTE_UNUSED,
+                               void* context ATTRIBUTE_UNUSED) {
+  return false;
+}
+
+// Stack overflow fault handler.
+//
+// This checks that the fault address is equal to the current stack pointer
+// minus the overflow region size (16K typically). The instruction that
+// generates this signal is:
+//
+// lw zero, -16384(sp)
+//
+// It will fault if sp is inside the protected region on the stack.
+//
+// If we determine this is a stack overflow we need to move the stack pointer
+// to the overflow region below the protected region.
+
+bool StackOverflowHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info, void* context) {
+  struct ucontext* uc = reinterpret_cast<struct ucontext*>(context);
+  struct sigcontext *sc = reinterpret_cast<struct sigcontext*>(&uc->uc_mcontext);
+  VLOG(signals) << "stack overflow handler with sp at " << std::hex << &uc;
+  VLOG(signals) << "sigcontext: " << std::hex << sc;
+
+  uintptr_t sp = sc->sc_regs.sp;
+  VLOG(signals) << "sp: " << std::hex << sp;
+
+  uintptr_t fault_addr = reinterpret_cast<uintptr_t>(info->si_addr);  // BVA addr
+  VLOG(signals) << "fault_addr: " << std::hex << fault_addr;
+  VLOG(signals) << "checking for stack overflow, sp: " << std::hex << sp <<
+    ", fault_addr: " << fault_addr;
+
+  uintptr_t overflow_addr = sp - GetStackOverflowReservedBytes(InstructionSet::kRiscv64);
+
+  // Check that the fault address is the value expected for a stack overflow.
+  if (fault_addr != overflow_addr) {
+    VLOG(signals) << "Not a stack overflow";
+    return false;
+  }
+
+  VLOG(signals) << "Stack overflow found";
+
+  // Now arrange for the signal handler to return to art_quick_throw_stack_overflow_from.
+  // The value of RA must be the same as it was when we entered the code that
+  // caused this fault.  This will be inserted into a callee save frame by
+  // the function to which this handler returns (art_quick_throw_stack_overflow).
+  sc->sc_regs.pc = reinterpret_cast<uintptr_t>(art_quick_throw_stack_overflow);
+
+  sc->sc_regs.t6 = sc->sc_regs.pc;          // make sure T6 points to the function
+
+  // The kernel will now return to the address in sc->arm_pc.
+  return true;
+}
+}       // namespace art
diff --git a/runtime/arch/riscv64/instruction_set_features_riscv64.cc b/runtime/arch/riscv64/instruction_set_features_riscv64.cc
new file mode 100644
index 0000000000..83788c1270
--- /dev/null
+++ b/runtime/arch/riscv64/instruction_set_features_riscv64.cc
@@ -0,0 +1,105 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "instruction_set_features_riscv64.h"
+
+#include <fstream>
+#include <sstream>
+
+#include "android-base/stringprintf.h"
+#include "android-base/strings.h"
+
+#include "base/logging.h"
+
+namespace art {
+
+using android::base::StringPrintf;
+
+Riscv64FeaturesUniquePtr Riscv64InstructionSetFeatures::FromVariant(
+    const std::string& variant, std::string* error_msg ATTRIBUTE_UNUSED) {
+  if (variant != "generic") {
+    LOG(WARNING) << "Unexpected CPU variant for Riscv64 using defaults: " << variant;
+  }
+  uint32_t bits = kIBitfield | kMBitfield | kFBitfield | kDBitfield;
+  return Riscv64FeaturesUniquePtr(new Riscv64InstructionSetFeatures(bits));
+}
+
+Riscv64FeaturesUniquePtr Riscv64InstructionSetFeatures::FromBitmap(uint32_t bitmap) {
+  return Riscv64FeaturesUniquePtr(new Riscv64InstructionSetFeatures(bitmap));
+}
+
+Riscv64FeaturesUniquePtr Riscv64InstructionSetFeatures::FromCppDefines() {
+  uint32_t bits = kIBitfield | kMBitfield | kFBitfield | kDBitfield;
+  return Riscv64FeaturesUniquePtr(new Riscv64InstructionSetFeatures(bits));
+}
+
+Riscv64FeaturesUniquePtr Riscv64InstructionSetFeatures::FromCpuInfo() {
+  // Look in /proc/cpuinfo for features we need.  Only use this when we can guarantee that
+  // the kernel puts the appropriate feature flags in here.  Sometimes it doesn't.
+  uint32_t bits = 0;
+  std::ifstream in("/proc/cpuinfo");
+  if (!in.fail()) {
+    // TODO(wendong) analysis string like 'rv64imafdcu'
+    in.close();
+    bits = kIBitfield | kMBitfield | kFBitfield | kDBitfield;
+  } else {
+    LOG(ERROR) << "Failed to open /proc/cpuinfo";
+  }
+  return Riscv64FeaturesUniquePtr(new Riscv64InstructionSetFeatures(bits));
+}
+
+Riscv64FeaturesUniquePtr Riscv64InstructionSetFeatures::FromHwcap() {
+  UNIMPLEMENTED(WARNING);
+  return FromCppDefines();
+}
+
+Riscv64FeaturesUniquePtr Riscv64InstructionSetFeatures::FromAssembly() {
+  UNIMPLEMENTED(WARNING);
+  return FromCppDefines();
+}
+
+bool Riscv64InstructionSetFeatures::Equals(const InstructionSetFeatures* other) const {
+  if (InstructionSet::kRiscv64 != other->GetInstructionSet()) {
+    return false;
+  }
+  const Riscv64InstructionSetFeatures* other_as_riscv64 = other->AsRiscv64InstructionSetFeatures();
+  return bits_ == other_as_riscv64->bits_;
+}
+
+uint32_t Riscv64InstructionSetFeatures::AsBitmap() const {
+  uint32_t bits = kIBitfield | kMBitfield | kFBitfield | kDBitfield;
+  return bits;
+}
+
+std::string Riscv64InstructionSetFeatures::GetFeatureString() const {
+  std::string result = "rv64imaf";
+  if (bits_ & kMBitfield) {
+    result += "d";
+  }
+  if (bits_ & kCBitfield) {
+    result += "c";
+  }
+  return result;
+}
+
+std::unique_ptr<const InstructionSetFeatures>
+Riscv64InstructionSetFeatures::AddFeaturesFromSplitString(
+    const std::vector<std::string>& features ATTRIBUTE_UNUSED, std::string* error_msg ATTRIBUTE_UNUSED) const {
+  uint32_t bits = kIBitfield | kMBitfield | kFBitfield | kDBitfield;
+  return std::unique_ptr<const InstructionSetFeatures>(new Riscv64InstructionSetFeatures(bits));
+}
+
+}  // namespace art
diff --git a/runtime/arch/riscv64/instruction_set_features_riscv64.h b/runtime/arch/riscv64/instruction_set_features_riscv64.h
new file mode 100644
index 0000000000..b70b487611
--- /dev/null
+++ b/runtime/arch/riscv64/instruction_set_features_riscv64.h
@@ -0,0 +1,90 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_INSTRUCTION_SET_FEATURES_RISCV64_H_
+#define ART_RUNTIME_ARCH_RISCV64_INSTRUCTION_SET_FEATURES_RISCV64_H_
+
+#include "arch/instruction_set_features.h"
+
+namespace art {
+
+class Riscv64InstructionSetFeatures;
+using Riscv64FeaturesUniquePtr = std::unique_ptr<const Riscv64InstructionSetFeatures>;
+
+// Instruction set features relevant to the RISCV64 architecture.
+class Riscv64InstructionSetFeatures final : public InstructionSetFeatures {
+ public:
+  static Riscv64FeaturesUniquePtr FromVariant(const std::string& variant,
+                                             std::string* error_msg);
+
+  // Parse a bitmap and create an InstructionSetFeatures.
+  static Riscv64FeaturesUniquePtr FromBitmap(uint32_t bitmap);
+
+  // Turn C pre-processor #defines into the equivalent instruction set features.
+  static Riscv64FeaturesUniquePtr FromCppDefines();
+
+  // Process /proc/cpuinfo and use kRuntimeISA to produce InstructionSetFeatures.
+  static Riscv64FeaturesUniquePtr FromCpuInfo();
+
+  // Process the auxiliary vector AT_HWCAP entry and use kRuntimeISA to produce
+  // InstructionSetFeatures.
+  static Riscv64FeaturesUniquePtr FromHwcap();
+
+  // Use assembly tests of the current runtime (ie kRuntimeISA) to determine the
+  // InstructionSetFeatures. This works around kernel bugs in AT_HWCAP and /proc/cpuinfo.
+  static Riscv64FeaturesUniquePtr FromAssembly();
+
+  bool Equals(const InstructionSetFeatures* other) const override;
+
+  InstructionSet GetInstructionSet() const override {
+    return InstructionSet::kRiscv64;
+  }
+
+  uint32_t AsBitmap() const override;
+
+  std::string GetFeatureString() const override;
+
+  virtual ~Riscv64InstructionSetFeatures() {}
+
+ protected:
+  std::unique_ptr<const InstructionSetFeatures>
+      AddFeaturesFromSplitString(const std::vector<std::string>& features,
+                                 std::string* error_msg) const override;
+
+ private:
+  explicit Riscv64InstructionSetFeatures(uint32_t bits) : InstructionSetFeatures(), bits_(bits) {
+  }
+
+  // Bitmap positions for encoding features as a bitmap.
+  enum {
+    kIBitfield = (1 << 0),
+    kMBitfield = (1 << 1),
+    kFBitfield = (1 << 2),
+    kDBitfield = (1 << 3),
+    kCBitfield = (1 << 4),
+    kABitfield = (1 << 5),
+    kVBitfield = (1 << 6),
+  };
+
+  // Bits for AFDCM etc, will process late
+  const uint32_t bits_;
+
+  DISALLOW_COPY_AND_ASSIGN(Riscv64InstructionSetFeatures);
+};
+
+}  // namespace art
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_INSTRUCTION_SET_FEATURES_RISCV64_H_
diff --git a/runtime/arch/riscv64/instruction_set_features_riscv64_test.cc b/runtime/arch/riscv64/instruction_set_features_riscv64_test.cc
new file mode 100644
index 0000000000..6eb40db34c
--- /dev/null
+++ b/runtime/arch/riscv64/instruction_set_features_riscv64_test.cc
@@ -0,0 +1,50 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "instruction_set_features_riscv64.h"
+
+#include <gtest/gtest.h>
+
+namespace art {
+
+TEST(Riscv64InstructionSetFeaturesTest, Riscv64FeaturesFromDefaultVariant) {
+  std::string error_msg;
+  std::unique_ptr<const InstructionSetFeatures> riscv64_features(
+      InstructionSetFeatures::FromVariant(InstructionSet::kRiscv64, "default", &error_msg));
+  ASSERT_TRUE(riscv64_features.get() != nullptr) << error_msg;
+  EXPECT_EQ(riscv64_features->GetInstructionSet(), InstructionSet::kRiscv64);
+  EXPECT_TRUE(riscv64_features->Equals(riscv64_features.get()));
+  EXPECT_STREQ("msa", riscv64_features->GetFeatureString().c_str());
+  EXPECT_EQ(riscv64_features->AsBitmap(), 1U);
+}
+
+TEST(Riscv64InstructionSetFeaturesTest, Riscv64FeaturesFromR6Variant) {
+  std::string error_msg;
+  std::unique_ptr<const InstructionSetFeatures> riscv64r6_features(
+      InstructionSetFeatures::FromVariant(InstructionSet::kRiscv64, "riscv64r6", &error_msg));
+  ASSERT_TRUE(riscv64r6_features.get() != nullptr) << error_msg;
+  EXPECT_EQ(riscv64r6_features->GetInstructionSet(), InstructionSet::kRiscv64);
+  EXPECT_TRUE(riscv64r6_features->Equals(riscv64r6_features.get()));
+  EXPECT_STREQ("msa", riscv64r6_features->GetFeatureString().c_str());
+  EXPECT_EQ(riscv64r6_features->AsBitmap(), 1U);
+
+  std::unique_ptr<const InstructionSetFeatures> riscv64_default_features(
+      InstructionSetFeatures::FromVariant(InstructionSet::kRiscv64, "default", &error_msg));
+  ASSERT_TRUE(riscv64_default_features.get() != nullptr) << error_msg;
+  EXPECT_TRUE(riscv64r6_features->Equals(riscv64_default_features.get()));
+}
+
+}  // namespace art
diff --git a/runtime/arch/riscv64/jni_entrypoints_riscv64.S b/runtime/arch/riscv64/jni_entrypoints_riscv64.S
new file mode 100644
index 0000000000..77dc6e9837
--- /dev/null
+++ b/runtime/arch/riscv64/jni_entrypoints_riscv64.S
@@ -0,0 +1,79 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "asm_support_riscv64.S"
+
+    .balign 16
+
+    /*
+     * Jni dlsym lookup stub.
+     */
+    .extern artFindNativeMethod
+ENTRY art_jni_dlsym_lookup_stub
+    addi sp, sp, -80        # save a0-a7 and $ra
+    .cfi_adjust_cfa_offset 80
+    sd     ra, 64(sp)
+    .cfi_rel_offset 31, 64
+    sd     a7, 56(sp)
+    .cfi_rel_offset 11, 56
+    sd     a6, 48(sp)
+    .cfi_rel_offset 10, 48
+    sd     a5, 40(sp)
+    .cfi_rel_offset 9, 40
+    sd     a4, 32(sp)
+    .cfi_rel_offset 8, 32
+    sd     a3, 24(sp)
+    .cfi_rel_offset 7, 24
+    sd     a2, 16(sp)
+    .cfi_rel_offset 6, 16
+    sd     a1, 8(sp)
+    .cfi_rel_offset 5, 8
+    sd     a0, 0(sp)
+    .cfi_rel_offset 4, 0
+    move   a0, s1             # pass Thread::Current()
+
+    la t6, artFindNativeMethod  # (Thread*)
+    jalr ra, t6
+    move t6, a0;               # put method code result in $t9 (t6 in riscv64)
+    ld     a0, 0(sp)          # restore registers from stack
+    .cfi_restore 4
+    ld     a1, 8(sp)
+    .cfi_restore 5
+    ld     a2, 16(sp)
+    .cfi_restore 6
+    ld     a3, 24(sp)
+    .cfi_restore 7
+    ld     a4, 32(sp)
+    .cfi_restore 8
+    ld     a5, 40(sp)
+    .cfi_restore 9
+    ld     a6, 48(sp)
+    .cfi_restore 10
+    ld     a7, 56(sp)
+    .cfi_restore 11
+    ld     ra, 64(sp)
+    .cfi_restore 31
+    addi sp, sp, 80         # restore the stack
+    .cfi_adjust_cfa_offset -80
+
+    beq    t6, zero, .Lno_native_code_found
+    nop
+    jalr   zero, t6           # leaf call to method's code
+    nop
+.Lno_native_code_found:
+    jalr   zero, ra
+    nop
+END art_jni_dlsym_lookup_stub
diff --git a/runtime/arch/riscv64/memcmp16_riscv64.S b/runtime/arch/riscv64/memcmp16_riscv64.S
new file mode 100644
index 0000000000..9099908313
--- /dev/null
+++ b/runtime/arch/riscv64/memcmp16_riscv64.S
@@ -0,0 +1,50 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_MEMCMP16_RISCV64_S_
+#define ART_RUNTIME_ARCH_RISCV64_MEMCMP16_RISCV64_S_
+
+#include "asm_support_riscv64.S"
+
+.set noreorder
+
+// u4 __memcmp16(const u2*, const u2*, size_t);
+ENTRY_NO_GP __memcmp16
+  move  $t0, $zero
+  move  $t1, $zero
+  beqz  $a2, done       /* 0 length string */
+  nop
+  beq   $a0, $a1, done  /* addresses are identical */
+  nop
+
+1:
+  lhu   $t0, 0($a0)
+  lhu   $t1, 0($a1)
+  bne   $t0, $t1, done
+  nop
+  daddu $a0, 2
+  daddu $a1, 2
+  dsubu $a2, 1
+  bnez  $a2, 1b
+  nop
+
+done:
+  dsubu $v0, $t0, $t1
+  j     $ra
+  nop
+END __memcmp16
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_MEMCMP16_RISCV64_S_
diff --git a/runtime/arch/riscv64/quick_entrypoints_riscv64.S b/runtime/arch/riscv64/quick_entrypoints_riscv64.S
new file mode 100644
index 0000000000..a75df338db
--- /dev/null
+++ b/runtime/arch/riscv64/quick_entrypoints_riscv64.S
@@ -0,0 +1,3351 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "asm_support_riscv64.S"
+#include "interpreter/cfi_asm_support.h"
+
+#include "arch/quick_alloc_entrypoints.S"
+
+    //.set noreorder
+    .balign 16
+
+    /* Deliver the given exception */
+    .extern artDeliverExceptionFromCode
+    /* Deliver an exception pending on a thread */
+    .extern artDeliverPendingExceptionFromCode
+
+    /*
+     * Macro that sets up $gp and stores the previous $gp value to $t5.
+     * This macro modifies v1 and t5.
+     */
+// FIXME: T-HEAD, Riscv64 might not use GP now.
+.macro SETUP_GP
+#    c.mv a1, ra
+#    nop # @todo bal 1f
+#    nop
+#1:
+#    nop # @todo .cpsetup $ra, $t5, 1b
+#    c.mv ra, a1
+.endm
+
+    /*
+     * Macro that sets up the callee save frame to conform with
+     * Runtime::CreateCalleeSaveMethod(kSaveAllCalleeSaves)
+     * callee-save: padding + $f8-$f9 + $f18-$f27 + $s1-$s11 + $ra + $s0 = 25 total + 1x8 bytes padding
+     */
+.macro SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
+    addi   sp, sp, -208
+    .cfi_adjust_cfa_offset 208
+
+     // Ugly compile-time check, but we only have the preprocessor.
+#if (FRAME_SIZE_SAVE_ALL_CALLEE_SAVES != 208)
+#error "FRAME_SIZE_SAVE_ALL_CALLEE_SAVES(RISCV64) size not as expected."
+#endif
+
+    sd     ra, 200(sp)
+    .cfi_rel_offset 31, 200
+    sd     s0, 192(sp)
+    .cfi_rel_offset 30, 192
+    sd     s11, 184(sp)
+    .cfi_rel_offset 28, 184
+    sd     s10, 176(sp)
+    .cfi_rel_offset 23, 176
+    sd     s9, 168(sp)
+    .cfi_rel_offset 22, 168
+    sd     s8, 160(sp)
+    .cfi_rel_offset 21, 160
+    sd     s7, 152(sp)
+    .cfi_rel_offset 20, 152
+    sd     s6,  144(sp)
+    .cfi_rel_offset 19, 144
+    sd     s5,  136(sp)
+    .cfi_rel_offset 18, 136
+    sd     s4,  128(sp)
+    .cfi_rel_offset 17, 128
+    sd     s3,  120(sp)
+    .cfi_rel_offset 16, 120
+    sd     s2,  112(sp)
+    .cfi_rel_offset 17, 112
+    sd     s1,  104(sp)
+    .cfi_rel_offset 16, 104
+
+    // FP callee-saves
+    fsd    f27, 96(sp)
+    fsd    f26, 88(sp)
+    fsd    f25, 80(sp)
+    fsd    f24, 72(sp)
+    fsd    f23, 64(sp)
+    fsd    f22, 56(sp)
+    fsd    f21, 48(sp)
+    fsd    f20, 40(sp)
+    fsd    f19, 32(sp)
+    fsd    f18, 24(sp)
+    fsd    f9, 16(sp)
+    fsd    f8,  8(sp)
+
+    # load appropriate callee-save-method
+    la      t1, _ZN3art7Runtime9instance_E      # @todo ld      t1, %got(_ZN3art7Runtime9instance_E)(gp)
+    ld      t1, 0(t1)
+    ld      t1, RUNTIME_SAVE_ALL_CALLEE_SAVES_METHOD_OFFSET(t1)
+    sd      t1, 0(sp)                                # Place ArtMethod* at bottom of stack.
+    sd      sp, THREAD_TOP_QUICK_FRAME_OFFSET(rSELF)  # Place sp in Thread::Current()->top_quick_frame.
+.endm
+
+    /*
+     * Macro that sets up the callee save frame to conform with
+     * Runtime::CreateCalleeSaveMethod(kSaveRefsOnly). Restoration assumes
+     * non-moving GC.
+     * Does not include rSUSPEND or rSELF
+     * callee-save: padding + $s2-$s10 + $ra + $s0 = 11 total + 1x8 bytes padding
+     */
+.macro SETUP_SAVE_REFS_ONLY_FRAME
+    addi   sp, sp, -96
+    .cfi_adjust_cfa_offset 96
+
+    // Ugly compile-time check, but we only have the preprocessor.
+#if (FRAME_SIZE_SAVE_REFS_ONLY != 96)
+#error "FRAME_SIZE_SAVE_REFS_ONLY(RISCV64) size not as expected."
+#endif
+
+    sd     ra, 88(sp)
+    .cfi_rel_offset 34, 88
+    sd     s0, 80(sp)
+    .cfi_rel_offset 33, 80
+    sd     s10, 72(sp)
+    .cfi_rel_offset 26, 72
+    sd     s9, 64(sp)
+    .cfi_rel_offset 25, 64
+    sd     s8, 56(sp)
+    .cfi_rel_offset 24, 56
+    sd     s7, 48(sp)
+    .cfi_rel_offset 23, 48
+    sd     s6, 40(sp)
+    .cfi_rel_offset 22, 40
+    sd     s5, 32(sp)
+    .cfi_rel_offset 21, 32
+    sd     s4, 24(sp)
+    .cfi_rel_offset 20, 24
+    sd     s3, 16(sp)
+    .cfi_rel_offset 19, 16
+    sd     s2, 8(sp)
+    .cfi_rel_offset 18, 8
+
+    # load appropriate callee-save-method
+    la      t1, _ZN3art7Runtime9instance_E     # ld      t1, %got(_ZN3art7Runtime9instance_E)(gp)
+    ld      t1, 0(t1)
+    ld      t1, RUNTIME_SAVE_REFS_ONLY_METHOD_OFFSET(t1)
+    sd      t1, 0(sp)                                # Place Method* at bottom of stack.
+    sd      sp, THREAD_TOP_QUICK_FRAME_OFFSET(rSELF)  # Place sp in Thread::Current()->top_quick_frame.
+.endm
+
+
+.macro RESTORE_SAVE_REFS_ONLY_FRAME
+    ld     ra, 88(sp)
+    .cfi_restore 34
+    ld     s0, 80(sp)
+    .cfi_restore 33
+    ld     s10, 72(sp)
+    .cfi_restore 26
+    ld     s9, 64(sp)
+    .cfi_restore 25
+    ld     s8, 56(sp)
+    .cfi_restore 24
+    ld     s7, 48(sp)
+    .cfi_restore 23
+    ld     s6, 40(sp)
+    .cfi_restore 22
+    ld     s5, 32(sp)
+    .cfi_restore 21
+    ld     s4, 24(sp)
+    .cfi_restore 20
+    ld     s3, 16(sp)
+    .cfi_restore 19
+    ld     s2, 8(sp)
+    .cfi_restore 18
+
+    addi   sp, sp, 96
+    .cfi_adjust_cfa_offset -96
+    nop # @todo .cpreturn
+.endm
+
+
+.macro RESTORE_SAVE_REFS_ONLY_FRAME_AND_RETURN
+    ld     ra, 88(sp)
+    .cfi_restore 34
+    ld     s0, 80(sp)
+    .cfi_restore 33
+    ld     s10, 72(sp)
+    .cfi_restore 26
+    ld     s9, 64(sp)
+    .cfi_restore 25
+    ld     s8, 56(sp)
+    .cfi_restore 24
+    ld     s7, 48(sp)
+    .cfi_restore 23
+    ld     s6, 40(sp)
+    .cfi_restore 22
+    ld     s5, 32(sp)
+    .cfi_restore 21
+    ld     s4, 24(sp)
+    .cfi_restore 20
+    ld     s3, 16(sp)
+    .cfi_restore 19
+    ld     s2, 8(sp)
+    .cfi_restore 18
+
+    addi sp, sp, 96
+    .cfi_adjust_cfa_offset -96
+    jalr   zero, 0(ra)
+.endm
+
+// This assumes the top part of these stack frame types are identical.
+#define REFS_AND_ARGS_MINUS_REFS_SIZE (FRAME_SIZE_SAVE_REFS_AND_ARGS - FRAME_SIZE_SAVE_REFS_ONLY)
+
+    /*
+     * Individually usable part of macro SETUP_SAVE_REFS_AND_ARGS_FRAME_INTERNAL.
+     */
+.macro SETUP_SAVE_REFS_AND_ARGS_FRAME_S4_THRU_S8
+    sd      s0, 208(sp)                   # s0(Riscv64) == s8(mips64) == fp
+    .cfi_rel_offset 8, 208
+    sd      s10, 200(sp)
+    .cfi_rel_offset 26, 200
+    sd      s9, 192(sp)
+    .cfi_rel_offset 25, 192
+    sd      s8, 184(sp)
+    .cfi_rel_offset 24, 184
+    sd      s7, 176(sp)
+    .cfi_rel_offset 23, 176
+    sd      s6, 168(sp)
+    .cfi_rel_offset 22, 168
+    sd      s5, 160(sp)
+    .cfi_rel_offset 21, 160
+    sd      s4, 152(sp)
+    .cfi_rel_offset 20, 152
+.endm
+
+.macro SETUP_SAVE_REFS_AND_ARGS_FRAME_INTERNAL save_s4_thru_s8=1
+    addi   sp, sp, -224
+    .cfi_adjust_cfa_offset 224
+
+    // Ugly compile-time check, but we only have the preprocessor.
+#if (FRAME_SIZE_SAVE_REFS_AND_ARGS != 224)
+#error "FRAME_SIZE_SAVE_REFS_AND_ARGS(RISCV64) size not as expected."
+#endif
+    sd      ra, 216(sp)           # = kQuickCalleeSaveFrame_RefAndArgs_LrOffset
+    .cfi_rel_offset 1, 216
+                                  # FIXME: T-HEAD, Don't touch gp rightnow. (t8 holds caller's gp, now save it to the stack.)
+    .if \save_s4_thru_s8
+      SETUP_SAVE_REFS_AND_ARGS_FRAME_S4_THRU_S8
+    .endif
+    sd      s3, 144(sp)
+    .cfi_rel_offset 19, 144
+    sd      s2, 136(sp)
+    .cfi_rel_offset 18, 136
+    sd      a7, 128(sp)
+    .cfi_rel_offset 17, 128
+    sd      a6, 120(sp)
+    .cfi_rel_offset 16, 120
+    sd      a5, 112(sp)
+    .cfi_rel_offset 15, 112
+    sd      a4, 104(sp)
+    .cfi_rel_offset 14, 104
+    sd      a3,  96(sp)
+    .cfi_rel_offset 13, 96
+    sd      a2,  88(sp)
+    .cfi_rel_offset 12, 88
+    sd      a1,  80(sp)           # = kQuickCalleeSaveFrame_RefAndArgs_Gpr1Offset
+    .cfi_rel_offset 11, 80
+
+    fsd     f17, 72(sp)
+    fsd     f16, 64(sp)
+    fsd     f15, 56(sp)
+    fsd     f14, 48(sp)
+    fsd     f13, 40(sp)
+    fsd     f12, 32(sp)
+    fsd     f11, 24(sp)
+    fsd     f10, 16(sp)           # = kQuickCalleeSaveFrame_RefAndArgs_Fpr1Offset
+    # 1x8 bytes padding + Method*
+.endm
+
+    /*
+     * Macro that sets up the callee save frame to conform with
+     * Runtime::CreateCalleeSaveMethod(kSaveRefsAndArgs). Restoration assumes
+     * non-moving GC.
+     * callee-save: padding + $f10-$f17 + $a1-$a7 + $s2-$s10 + $ra + $s0 = 26 total + 1 words padding + Method*
+     */
+.macro SETUP_SAVE_REFS_AND_ARGS_FRAME save_s4_thru_s8_only=0
+    .if \save_s4_thru_s8_only
+      // It is expected that `SETUP_SAVE_REFS_AND_ARGS_FRAME_INTERNAL /* save_s4_thru_s8 */ 0`
+      // has been done prior to `SETUP_SAVE_REFS_AND_ARGS_FRAME /* save_s4_thru_s8_only */ 1`.
+      SETUP_SAVE_REFS_AND_ARGS_FRAME_S4_THRU_S8
+    .else
+      SETUP_SAVE_REFS_AND_ARGS_FRAME_INTERNAL
+    .endif
+    # load appropriate callee-save-method
+    la	    t1, _ZN3art7Runtime9instance_E       #ld $t1, %got(_ZN3art7Runtime9instance_E)($gp)
+    ld      t1, 0(t1)
+    ld      t1, RUNTIME_SAVE_REFS_AND_ARGS_METHOD_OFFSET(t1)
+    sd      t1, 0(sp)                                # Place Method* at bottom of stack.
+    sd      sp, THREAD_TOP_QUICK_FRAME_OFFSET(rSELF)  # Place sp in Thread::Current()->top_quick_frame.
+.endm
+
+.macro SETUP_SAVE_REFS_AND_ARGS_FRAME_WITH_METHOD_IN_A0
+    SETUP_SAVE_REFS_AND_ARGS_FRAME_INTERNAL
+    sd      a0, 0(sp)                                # Place Method* at bottom of stack.
+    sd      sp, THREAD_TOP_QUICK_FRAME_OFFSET(rSELF)  # Place sp in Thread::Current()->top_quick_frame.
+.endm
+
+    /*
+     * Individually usable part of macro RESTORE_SAVE_REFS_AND_ARGS_FRAME.
+     */
+.macro RESTORE_SAVE_REFS_AND_ARGS_FRAME_A1
+    ld      a1,  80(sp)
+    .cfi_restore 11
+.endm
+
+.macro RESTORE_SAVE_REFS_AND_ARGS_FRAME restore_s4_thru_s8=1
+    ld      ra, 216(sp)
+    .cfi_restore 1
+    .if \restore_s4_thru_s8
+      ld    s0, 208(sp)           # s0(Riscv64) == s8(mips64) == fp
+      .cfi_restore 8
+    .endif
+
+    .if \restore_s4_thru_s8
+      ld    s10, 200(sp)
+      .cfi_restore 26
+      ld    s9, 192(sp)
+      .cfi_restore 25
+      ld    s8, 184(sp)
+      .cfi_restore 24
+      ld    s7, 176(sp)
+      .cfi_restore 23
+      ld    s6, 168(sp)
+      .cfi_restore 22
+      ld    s5, 160(sp)
+      .cfi_restore 21
+      ld    s4, 152(sp)
+      .cfi_restore 20
+    .endif
+    ld      s3, 144(sp)
+    .cfi_restore 19
+    ld      s2, 136(sp)
+    .cfi_restore 18
+    ld      a7, 128(sp)
+    .cfi_restore 17
+    ld      a6, 120(sp)
+    .cfi_restore 16
+    ld      a5, 112(sp)
+    .cfi_restore 15
+    ld      a4, 104(sp)
+    .cfi_restore 14
+    ld      a3,  96(sp)
+    .cfi_restore 13
+    ld      a2,  88(sp)
+    .cfi_restore 12
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME_A1
+
+    fld     f17, 72(sp)
+    fld     f16, 64(sp)
+    fld     f15, 56(sp)
+    fld     f14, 48(sp)
+    fld     f13, 40(sp)
+    fld     f12, 32(sp)
+    fld     f11, 24(sp)
+    fld     f10, 16(sp)
+
+    addi  sp, sp, 224
+    .cfi_adjust_cfa_offset -224
+.endm
+
+    /*
+     * Macro that sets up the callee save frame to conform with
+     * Runtime::CreateCalleeSaveMethod(kSaveEverything).
+     * when the $sp has already been decremented by FRAME_SIZE_SAVE_EVERYTHING.
+     * callee-save: $a0-$a7 + $t0-$t6 + $s1-$s11 + $fp(s0) + $ra,
+     *              $f0-$f31; 28(GPR)+ 32(FPR) + 1x8 bytes padding + method*
+     * This macro sets up $gp; entrypoints using it should start with ENTRY_NO_GP.
+     */
+.macro SETUP_SAVE_EVERYTHING_FRAME_DECREMENTED_SP runtime_method_offset = RUNTIME_SAVE_EVERYTHING_METHOD_OFFSET
+     // Ugly compile-time check, but we only have the preprocessor.
+#if (FRAME_SIZE_SAVE_EVERYTHING != 496)
+#error "FRAME_SIZE_SAVE_EVERYTHING(RISCV64) size not as expected."
+#endif
+
+    // Save core registers.
+    # save ra, fp
+    sd     ra, 488(sp)
+    .cfi_rel_offset 31, 488
+    sd     s0, 480(sp)
+    .cfi_rel_offset 30, 480
+
+    # save t3 - t6
+    sd     t6, 472(sp)
+    .cfi_rel_offset 25, 472
+    sd     t5, 464(sp)
+    .cfi_rel_offset 24, 464
+    sd     t4, 456(sp)
+    .cfi_rel_offset 25, 456
+    sd     t3, 448(sp)
+    .cfi_rel_offset 24, 448
+
+    # save s2 - s11
+    sd     s11, 440(sp)
+    .cfi_rel_offset 25, 440
+    sd     s10, 432(sp)
+    .cfi_rel_offset 24, 432
+    sd     s9, 424(sp)
+    .cfi_rel_offset 24, 424
+    sd     s8, 416(sp)
+    .cfi_rel_offset 23, 416
+    sd     s7, 408(sp)
+    .cfi_rel_offset 22, 408
+    sd     s6, 400(sp)
+    .cfi_rel_offset 21, 400
+    sd     s5, 392(sp)
+    .cfi_rel_offset 20, 392
+    sd     s4, 384(sp)
+    .cfi_rel_offset 23, 384
+    sd     s3, 376(sp)
+    .cfi_rel_offset 22, 376
+    sd     s2, 368(sp)
+    .cfi_rel_offset 21, 368
+
+    # save a0 - a7
+    sd     a7, 360(sp)
+    .cfi_rel_offset 20, 360
+    sd     a6,  352(sp)
+    .cfi_rel_offset 19, 352
+    sd     a5,  344(sp)
+    .cfi_rel_offset 18, 344
+    sd     a4,  336(sp)
+    .cfi_rel_offset 17, 336
+    sd     a3, 328(sp)
+    .cfi_rel_offset 11, 328
+    sd     a2, 320(sp)
+    .cfi_rel_offset 10, 320
+    sd     a1, 312(sp)
+    .cfi_rel_offset 9, 312
+    sd     a0, 304(sp)
+    .cfi_rel_offset 8, 304
+
+    # save s1
+    sd     s1,  296(sp)
+    .cfi_rel_offset 7, 296
+
+    # save t0 - t2
+    sd     t2,  288(sp)
+    .cfi_rel_offset 6, 288
+    sd     t1,  280(sp)
+    .cfi_rel_offset 5, 280
+    sd     t0,  272(sp)
+    .cfi_rel_offset 4, 272
+
+    // Save FP registers.
+    fsd    f31, 264(sp)
+    fsd    f30, 256(sp)
+    fsd    f29, 248(sp)
+    fsd    f28, 240(sp)
+    fsd    f27, 232(sp)
+    fsd    f26, 224(sp)
+    fsd    f25, 216(sp)
+    fsd    f24, 208(sp)
+    fsd    f23, 200(sp)
+    fsd    f22, 192(sp)
+    fsd    f21, 184(sp)
+    fsd    f20, 176(sp)
+    fsd    f19, 168(sp)
+    fsd    f18, 160(sp)
+    fsd    f17, 152(sp)
+    fsd    f16, 144(sp)
+    fsd    f15, 136(sp)
+    fsd    f14, 128(sp)
+    fsd    f13, 120(sp)
+    fsd    f12, 112(sp)
+    fsd    f11, 104(sp)
+    fsd    f10, 96(sp)
+    fsd    f9, 88(sp)
+    fsd    f8, 80(sp)
+    fsd    f7, 72(sp)
+    fsd    f6, 64(sp)
+    fsd    f5, 56(sp)
+    fsd    f4, 48(sp)
+    fsd    f3, 40(sp)
+    fsd    f2, 32(sp)
+    fsd    f1, 24(sp)
+    fsd    f0, 16(sp)
+
+    # load appropriate callee-save-method
+    la      t1, _ZN3art7Runtime9instance_E
+    ld      t1, 0(t1)
+    ld      t1, \runtime_method_offset(t1)
+    sd      t1, 0(sp)                                # Place ArtMethod* at bottom of stack.
+    sd      sp, THREAD_TOP_QUICK_FRAME_OFFSET(rSELF)  # Place sp in Thread::Current()->top_quick_frame.
+.endm
+
+    /*
+     * Macro that sets up the callee save frame to conform with
+     * Runtime::CreateCalleeSaveMethod(kSaveEverything).
+     * callee-save: $at + $v0-$v1 + $a0-$a7 + $t0-$t3 + $s0-$s7 + $t5-$t6 + $gp + $s8 + $ra + $s8,
+     *              $f0-$f31; 28(GPR)+ 32(FPR) + 1x8 bytes padding + method*
+     * This macro sets up $gp; entrypoints using it should start with ENTRY_NO_GP.
+     */
+.macro SETUP_SAVE_EVERYTHING_FRAME runtime_method_offset = RUNTIME_SAVE_EVERYTHING_METHOD_OFFSET
+    addi sp, sp, -(FRAME_SIZE_SAVE_EVERYTHING)
+    .cfi_adjust_cfa_offset (FRAME_SIZE_SAVE_EVERYTHING)
+    SETUP_SAVE_EVERYTHING_FRAME_DECREMENTED_SP \runtime_method_offset
+.endm
+
+.macro RESTORE_SAVE_EVERYTHING_FRAME restore_a0=1
+    // Restore FP registers.
+    fld    f31, 264(sp)
+    fld    f30, 256(sp)
+    fld    f29, 248(sp)
+    fld    f28, 240(sp)
+    fld    f27, 232(sp)
+    fld    f26, 224(sp)
+    fld    f25, 216(sp)
+    fld    f24, 208(sp)
+    fld    f23, 200(sp)
+    fld    f22, 192(sp)
+    fld    f21, 184(sp)
+    fld    f20, 176(sp)
+    fld    f19, 168(sp)
+    fld    f18, 160(sp)
+    fld    f17, 152(sp)
+    fld    f16, 144(sp)
+    fld    f15, 136(sp)
+    fld    f14, 128(sp)
+    fld    f13, 120(sp)
+    fld    f12, 112(sp)
+    fld    f11, 104(sp)
+    fld    f10, 96(sp)
+    fld    f9, 88(sp)
+    fld    f8, 80(sp)
+    fld    f7, 72(sp)
+    fld    f6, 64(sp)
+    fld    f5, 56(sp)
+    fld    f4, 48(sp)
+    fld    f3, 40(sp)
+    fld    f2, 32(sp)
+    fld    f1, 24(sp)
+    fld    f0, 16(sp)
+
+    // Restore core registers.
+    # restore ra, fp
+    ld     ra, 488(sp)
+    .cfi_restore 31
+    ld     s0, 480(sp)
+    .cfi_restore 30
+
+    # restore t0 - t6
+    ld     t6, 472(sp)
+    .cfi_restore 25
+    ld     t5, 464(sp)
+    .cfi_restore 25
+    ld     t4, 456(sp)
+    .cfi_restore 25
+    ld     t3, 448(sp)
+
+    .cfi_restore 25
+    ld     s11, 440(sp)
+    .cfi_restore 25
+    ld     s10, 432(sp)
+    .cfi_restore 25
+    ld     s9, 424(sp)
+    .cfi_restore 25
+
+    # restore s1 - s11
+    ld     s8, 416(sp)
+    .cfi_restore 25
+    ld     s7, 408(sp)
+    .cfi_restore 25
+    ld     s6, 400(sp)
+    .cfi_restore 25
+    ld     s5, 392(sp)
+    .cfi_restore 25
+    ld     s4, 384(sp)
+    .cfi_restore 25
+    ld     s3, 376(sp)
+    .cfi_restore 25
+    ld     s2, 368(sp)
+    .cfi_restore 25
+    ld     a7, 360(sp)
+    .cfi_restore 25
+    ld     a6,  352(sp)
+    .cfi_restore 25
+    ld     a5,  344(sp)
+    .cfi_restore 25
+    ld     a4,  336(sp)
+    .cfi_restore 25
+
+    # restore a0 - a7
+    ld     a3, 328(sp)
+    .cfi_restore 25
+    ld     a2, 320(sp)
+    .cfi_restore 25
+    ld     a1, 312(sp)
+    .cfi_restore 25
+    .if \restore_a0
+    ld     a0, 304(sp)
+    .cfi_restore 25
+    .endif
+    ld     s1,  296(sp)
+    .cfi_restore 25
+    ld     t2,  288(sp)
+    .cfi_restore 25
+    ld     t1,  280(sp)
+    .cfi_restore 25
+    ld     t0,  272(sp)
+    .cfi_restore 4
+    addi sp, sp, 496
+    .cfi_adjust_cfa_offset -496
+.endm
+
+    /*
+     * Macro that calls through to artDeliverPendingExceptionFromCode, where the pending
+     * exception is Thread::Current()->exception_ when the runtime method frame is ready.
+     * Requires $gp properly set up.
+     */
+.macro DELIVER_PENDING_EXCEPTION_FRAME_READY
+    la      t6, artDeliverPendingExceptionFromCode   # load artDeliverPendingExceptionFromCode to t6
+    c.mv    a0, rSELF                   # pass Thread::Current
+    jalr    zero, 0(t6)                 # artDeliverPendingExceptionFromCode(Thread*)
+.endm
+
+    /*
+     * Macro that calls through to artDeliverPendingExceptionFromCode, where the pending
+     * exception is Thread::Current()->exception_.
+     */
+.macro DELIVER_PENDING_EXCEPTION
+    SETUP_GP
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME    # save callee saves for throw
+    DELIVER_PENDING_EXCEPTION_FRAME_READY
+.endm
+
+.macro RETURN_IF_NO_EXCEPTION
+    ld     t0, THREAD_EXCEPTION_OFFSET(rSELF) # load Thread::Current()->exception_
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    bne    t0, zero, 1f                      # success if no exception is pending
+    nop
+    jalr   zero, 0(ra)
+    nop
+1:
+    DELIVER_PENDING_EXCEPTION
+.endm
+
+.macro RETURN_IF_ZERO
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    bne    a0, zero, 1f                   # success?
+    nop
+    jalr   zero, 0(ra)                    # return on success
+    nop
+1:
+    DELIVER_PENDING_EXCEPTION
+.endm
+
+.macro RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    beq    a0, zero, 1f                   # success?
+    nop
+    jalr   zero, 0(ra)                    # return on success
+    nop
+1:
+    DELIVER_PENDING_EXCEPTION
+.endm
+
+    /*
+     * On stack replacement stub.
+     * On entry:
+     *   a0 = stack to copy
+     *   a1 = size of stack
+     *   a2 = pc to call
+     *   a3 = JValue* result
+     *   a4 = shorty
+     *   a5 = thread
+     */
+ENTRY art_quick_osr_stub
+    c.mv   t0, sp               # save stack pointer
+    addi   t1, sp, -224         # reserve stack space
+    srli   t1, t1, 4            # enforce 16 byte stack alignment
+    slli   sp, t1, 4            # update stack pointer
+
+    // Save callee floating point registers. fs0 -- fs11
+    fsd     f27, 216(sp)
+    .cfi_rel_offset 31, 216
+    fsd     f26, 208(sp)
+    .cfi_rel_offset 31, 208
+    fsd     f25, 200(sp)
+    .cfi_rel_offset 31, 200
+    fsd     f24, 192(sp)
+    .cfi_rel_offset 31, 192
+    fsd     f23, 184(sp)
+    .cfi_rel_offset 31, 184
+    fsd     f22, 176(sp)
+    .cfi_rel_offset 31, 176
+    fsd     f21, 168(sp)
+    .cfi_rel_offset 31, 168
+    fsd     f20, 160(sp)
+    .cfi_rel_offset 31, 160
+    fsd     f19, 152(sp)
+    .cfi_rel_offset 31, 152
+    fsd     f18, 144(sp)
+    .cfi_rel_offset 31, 144
+    fsd     f9, 136(sp)
+    .cfi_rel_offset 31, 136
+    fsd     f8, 128(sp)
+    .cfi_rel_offset 31, 128
+
+    // Save callee general purpose registers, SP, RA, A3, and A4 (8x14 bytes)
+    sd     ra, 120(sp)
+    .cfi_rel_offset 31, 120
+    sd     s0, 112(sp)
+    .cfi_rel_offset 30, 112
+    sd     t0, 104(sp)           # save original stack pointer stored in t0
+    .cfi_rel_offset 29, 88
+    sd     s11, 96(sp)
+    .cfi_rel_offset 28, 80
+    sd     s10, 88(sp)
+    .cfi_rel_offset 28, 80
+    sd     s9, 80(sp)
+    .cfi_rel_offset 28, 80
+    sd     s8, 72(sp)
+    .cfi_rel_offset 28, 80
+    sd     s7, 64(sp)
+    .cfi_rel_offset 23, 72
+    sd     s6, 56(sp)
+    .cfi_rel_offset 22, 64
+    sd     s5, 48(sp)
+    .cfi_rel_offset 21, 56
+    sd     s4, 40(sp)
+    .cfi_rel_offset 20, 48
+    sd     s3, 32(sp)
+    .cfi_rel_offset 19, 40
+    sd     s2, 24(sp)
+    .cfi_rel_offset 18, 32
+    sd     s1, 16(sp)
+    .cfi_rel_offset 17, 24
+    sd     a4, 8(sp)
+    .cfi_rel_offset 8, 8
+    sd     a3, 0(sp)
+    .cfi_rel_offset 7, 0
+    c.mv   rSELF, a5                      # Save managed thread pointer into rSELF
+
+    addi sp, sp, -16
+    sd     zero, 0(sp)                   # Store null for ArtMethod* at bottom of frame
+    jal    .Losr_entry
+
+    addi sp, sp, 16
+
+    // Restore callee floating point registers. fs0 -- fs11
+    fld     f27, 216(sp)
+    .cfi_rel_offset 31, 216
+    fld     f26, 208(sp)
+    .cfi_rel_offset 31, 208
+    fld     f25, 200(sp)
+    .cfi_rel_offset 31, 200
+    fld     f24, 192(sp)
+    .cfi_rel_offset 31, 192
+    fld     f23, 184(sp)
+    .cfi_rel_offset 31, 184
+    fld     f22, 176(sp)
+    .cfi_rel_offset 31, 176
+    fld     f21, 168(sp)
+    .cfi_rel_offset 31, 168
+    fld     f20, 160(sp)
+    .cfi_rel_offset 31, 160
+    fld     f19, 152(sp)
+    .cfi_rel_offset 31, 152
+    fld     f18, 144(sp)
+    .cfi_rel_offset 31, 144
+    fld     f9, 136(sp)
+    .cfi_rel_offset 31, 136
+    fld     f8, 128(sp)
+    .cfi_rel_offset 31, 128
+
+    // Restore callee registers
+    ld     ra, 120(sp)
+    .cfi_rel_offset 31, 104
+    ld     s0, 112(sp)
+    .cfi_rel_offset 30, 96
+    ld     t0, 104(sp)           # save original stack pointer stored in t0
+    .cfi_rel_offset 29, 88
+    ld     s11, 96(sp)
+    .cfi_rel_offset 28, 80
+    ld     s10, 88(sp)
+    .cfi_rel_offset 28, 80
+    ld     s9, 80(sp)
+    .cfi_rel_offset 28, 80
+    ld     s8, 72(sp)
+    .cfi_rel_offset 28, 80
+    ld     s7, 64(sp)
+    .cfi_rel_offset 23, 72
+    ld     s6, 56(sp)
+    .cfi_rel_offset 22, 64
+    ld     s5, 48(sp)
+    .cfi_rel_offset 21, 56
+    ld     s4, 40(sp)
+    .cfi_rel_offset 20, 48
+    ld     s3, 32(sp)
+    .cfi_rel_offset 19, 40
+    ld     s2, 24(sp)
+    .cfi_rel_offset 18, 32
+    ld     s1, 16(sp)
+    .cfi_rel_offset 17, 24
+    // Restore return value address and shorty address
+    ld     a4, 8(sp)                     # shorty address
+    .cfi_restore 8
+    ld     a3, 0(sp)                     # result value address
+    .cfi_restore 7
+
+    c.mv   sp, t0                       # restore original SP
+
+    lbu    t1, 0(a4)                     # load return type
+    li     t2, 'D'                        # put char 'D' into t2
+    beq    t1, t2, .Losr_d_result       # branch if result type char == 'D'
+    li     t2, 'F'                        # put char 'F' into t2
+    beq    t1, t2, .Losr_f_result       # branch if result type char == 'F'
+    sd     a0, 0(a3)                    #  Non-FP result
+    jalr   zero, 0(ra)
+.Losr_f_result:
+    fmv.x.w  a0, f10                    # put Float result in a0
+    sw     a0, 0(a3)                    #  Non-FP result
+    jalr   zero, 0(ra)
+.Losr_d_result:
+    fmv.x.d  a0, f10                    # put Double result in a0
+    sd     a0, 0(a3)                    #  Non-FP result
+    jalr   zero, 0(ra)
+
+.Losr_entry:
+    sub    sp, sp, a1                   # Reserve space for callee stack
+    addi   a1, a1, -8
+    add    t0, a1, sp
+    sd     ra, 0(t0)                     # Store RA
+
+    // Copy arguments into callee stack
+    // Use simple copy routine for now.
+    // 4 bytes per slot.
+    // a0 = source address
+    // a1 = args length in bytes (does not include 8 bytes for RA)
+    // sp = destination address
+    beqz   a1, .Losr_loop_exit
+    addi   a1, a1, -4
+    add    t1, a0, a1
+    add    t2, sp, a1
+.Losr_loop_entry:
+    lw     t0, 0(t1)
+    addi   t1, t1, -4
+    sw     t0, 0(t2)
+    addi   t2, t2, -4
+    bge    t2, sp, .Losr_loop_entry
+
+.Losr_loop_exit:
+    c.mv   t6, a2
+    jalr   zero, 0(t6)                        # Jump to the OSR entry point.
+    nop
+END art_quick_osr_stub
+
+    /*
+     * On entry $a0 is uint32_t* gprs_ and $a1 is uint32_t* fprs_
+     * FIXME: just guessing about the shape of the jmpbuf.  Where will pc be?
+     */
+ENTRY_NO_GP art_quick_do_long_jump
+    fld     f0, 0(a1)
+    fld     f1, 8(a1)
+    fld     f2, 16(a1)
+    fld     f3, 24(a1)
+    fld     f4, 32(a1)
+    fld     f5, 40(a1)
+    fld     f6, 48(a1)
+    fld     f7, 56(a1)
+    fld     f8, 64(a1)
+    fld     f9, 72(a1)
+    fld     f10, 80(a1)
+    fld     f11, 88(a1)
+    fld     f12, 96(a1)
+    fld     f13, 104(a1)
+    fld     f14, 112(a1)
+    fld     f15, 120(a1)
+    fld     f16, 128(a1)
+    fld     f17, 136(a1)
+    fld     f18, 144(a1)
+    fld     f19, 152(a1)
+    fld     f20, 160(a1)
+    fld     f21, 168(a1)
+    fld     f22, 176(a1)
+    fld     f23, 184(a1)
+    fld     f24, 192(a1)
+    fld     f25, 200(a1)
+    fld     f26, 208(a1)
+    fld     f27, 216(a1)
+    fld     f28, 224(a1)
+    fld     f29, 232(a1)
+    fld     f30, 240(a1)
+    fld     f31, 248(a1)
+
+    # no need to load zero
+    ld      ra, 8(a0)
+    ld      sp, 16(a0)
+    # skip gp and tp
+
+    # load t0 - t2
+    ld      t0, 40(a0)
+    ld      t1, 48(a0)
+    ld      t2, 56(a0)
+
+    # load s0/s1
+    ld      s0, 64(a0)
+    ld      s1, 72(a0)
+
+    # a0 has to be loaded last
+    # load a1 - a7
+    ld      a1, 88(a0)
+    ld      a2, 96(a0)
+    ld      a3, 104(a0)
+    ld      a4, 112(a0)
+    ld      a5, 120(a0)
+    ld      a6, 128(a0)
+    ld      a7, 136(a0)
+
+    # load s2 - s11
+    ld      s2, 144(a0)
+    ld      s3, 152(a0)
+    ld      s4, 160(a0)
+    ld      s5, 168(a0)
+    ld      s6, 176(a0)
+    ld      s7, 184(a0)
+    ld      s8, 192(a0)
+    ld      s9, 200(a0)
+    ld      s10, 208(a0)
+    ld      s11, 216(a0)
+
+    # load t3 - t6
+    ld      t3, 224(a0)
+    ld      t4, 232(a0)
+    ld      t5, 240(a0)
+    ld      t6, 248(a0)
+
+    # load a0 now.
+    ld      a0, 80(a0)
+
+    jalr    zero, 0(t6)       # do long jump (do not use ra, it must not be clobbered)
+END art_quick_do_long_jump
+
+    /*
+     * Called by managed code, saves most registers (forms basis of long jump
+     * context) and passes the bottom of the stack.
+     * artDeliverExceptionFromCode will place the callee save Method* at
+     * the bottom of the thread. On entry a0 holds Throwable*
+     */
+ENTRY art_quick_deliver_exception
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
+    la  t6, artDeliverExceptionFromCode  #@dla  $t6, artDeliverExceptionFromCode
+    c.mv a1, rSELF                 # pass Thread::Current
+    jalr zero, 0(t6)               # artDeliverExceptionFromCode(Throwable*, Thread*)
+END art_quick_deliver_exception
+
+    .hidden art_quick_throw_null_pointer_exception
+    .extern artThrowNullPointerExceptionFromCode
+    /*
+     * Called by managed code to create and deliver a NullPointerException
+     */
+ENTRY_NO_GP art_quick_throw_null_pointer_exception
+    // Note that setting up $gp does not rely on $t6 here, so branching here directly is OK,
+    // even after clobbering any registers we don't need to preserve, such as $gp or $t0.
+    SETUP_SAVE_EVERYTHING_FRAME
+    la  t6, artThrowNullPointerExceptionFromCode
+    c.mv a0, rSELF                   # pass Thread::Current
+    jalr zero, 0(t6)                 # artThrowNullPointerExceptionFromCode(Thread*)
+END art_quick_throw_null_pointer_exception
+
+    /*
+     * Call installed by a signal handler to create and deliver a NullPointerException
+     */
+    .extern artThrowNullPointerExceptionFromSignal
+ENTRY_NO_GP_CUSTOM_CFA art_quick_throw_null_pointer_exception_from_signal, FRAME_SIZE_SAVE_EVERYTHING
+    SETUP_SAVE_EVERYTHING_FRAME_DECREMENTED_SP
+    # Retrieve the fault address from the padding where the signal handler stores it.
+    ld   a0, (__SIZEOF_POINTER__)(sp)
+    la  t6, artThrowNullPointerExceptionFromSignal
+    c.mv a1, rSELF                   # pass Thread::Current
+    jalr zero, 0(t6)                 # artThrowNullPointerExceptionFromSignal(uinptr_t, Thread*)
+END art_quick_throw_null_pointer_exception_from_signal
+
+    /*
+     * Called by managed code to create and deliver an ArithmeticException
+     */
+    .extern artThrowDivZeroFromCode
+ENTRY_NO_GP art_quick_throw_div_zero
+    SETUP_SAVE_EVERYTHING_FRAME
+    la  t6, artThrowDivZeroFromCode
+    c.mv a0, rSELF                   # pass Thread::Current
+    jalr zero, 0(t6)                 # artThrowDivZeroFromCode(Thread*)
+END art_quick_throw_div_zero
+
+    /*
+     * Called by managed code to create and deliver an
+     * ArrayIndexOutOfBoundsException
+     */
+    .extern artThrowArrayBoundsFromCode
+ENTRY_NO_GP art_quick_throw_array_bounds
+    // Note that setting up $gp does not rely on $t6 here, so branching here directly is OK,
+    // even after clobbering any registers we don't need to preserve, such as $gp or $t0.
+    SETUP_SAVE_EVERYTHING_FRAME
+    la  t6, artThrowArrayBoundsFromCode
+    c.mv a2, rSELF                   # pass Thread::Current
+    jalr zero, 0(t6)                 # artThrowArrayBoundsFromCode(index, limit, Thread*)
+END art_quick_throw_array_bounds
+
+    /*
+     * Called by managed code to create and deliver a StringIndexOutOfBoundsException
+     * as if thrown from a call to String.charAt().
+     */
+    .extern artThrowStringBoundsFromCode
+ENTRY_NO_GP art_quick_throw_string_bounds
+    SETUP_SAVE_EVERYTHING_FRAME
+    la  t6, artThrowStringBoundsFromCode
+    c.mv a2, rSELF                   # pass Thread::Current
+    jalr zero, 0(t6)                 # artThrowStringBoundsFromCode(index, limit, Thread*)
+END art_quick_throw_string_bounds
+
+    /*
+     * Called by managed code to create and deliver a StackOverflowError.
+     */
+    .extern artThrowStackOverflowFromCode
+ENTRY art_quick_throw_stack_overflow
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
+    la  t6, artThrowStackOverflowFromCode
+    c.mv a0, rSELF                 # pass Thread::Current
+    jalr zero, 0(t6)                 # artThrowStackOverflowFromCode(Thread*)
+END art_quick_throw_stack_overflow
+
+    /*
+     * All generated callsites for interface invokes and invocation slow paths will load arguments
+     * as usual - except instead of loading arg0/$a0 with the target Method*, arg0/$a0 will contain
+     * the method_idx.  This wrapper will save arg1-arg3, load the caller's Method*, align the
+     * stack and call the appropriate C helper.
+     * NOTE: "this" is first visable argument of the target, and so can be found in arg1/$a1.
+     *
+     * The helper will attempt to locate the target and return a 128-bit result in $v0/$v1 consisting
+     * of the target Method* in $v0 and method->code_ in $v1.
+     *
+     * If unsuccessful, the helper will return null/null. There will be a pending exception in the
+     * thread and we branch to another stub to deliver it.
+     *
+     * On success this wrapper will restore arguments and *jump* to the target, leaving the ra
+     * pointing back to the original caller.
+     */
+.macro INVOKE_TRAMPOLINE_BODY cxx_name, save_s4_thru_s8_only=0
+    .extern \cxx_name
+    SETUP_SAVE_REFS_AND_ARGS_FRAME \save_s4_thru_s8_only  # save callee saves in case
+                                                          # allocation triggers GC
+    c.mv  a2, rSELF                        # pass Thread::Current
+    c.mv  a3, sp                           # pass $sp
+    jal   \cxx_name                        # (method_idx, this, Thread*, $sp)
+    c.mv  a0, a0                           # save target Method*
+    c.mv  t6, a1                           # save $v0->code_
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+    beq   a0, zero, 1f
+    nop
+    jalr  zero, 0(t6)
+    nop
+1:
+    DELIVER_PENDING_EXCEPTION
+.endm
+.macro INVOKE_TRAMPOLINE c_name, cxx_name
+ENTRY \c_name
+    INVOKE_TRAMPOLINE_BODY \cxx_name
+END \c_name
+.endm
+
+INVOKE_TRAMPOLINE art_quick_invoke_interface_trampoline_with_access_check, artInvokeInterfaceTrampolineWithAccessCheck
+
+INVOKE_TRAMPOLINE art_quick_invoke_static_trampoline_with_access_check, artInvokeStaticTrampolineWithAccessCheck
+INVOKE_TRAMPOLINE art_quick_invoke_direct_trampoline_with_access_check, artInvokeDirectTrampolineWithAccessCheck
+INVOKE_TRAMPOLINE art_quick_invoke_super_trampoline_with_access_check, artInvokeSuperTrampolineWithAccessCheck
+INVOKE_TRAMPOLINE art_quick_invoke_virtual_trampoline_with_access_check, artInvokeVirtualTrampolineWithAccessCheck
+
+    # On entry:
+    #   t0 = shorty[1] (skip 1 for return type)
+    #   t1 = ptr to arg_array
+    #   t5 = float/double arg count
+    # This macro modifies t3, t6, t5 and v0
+.macro LOOP_OVER_SHORTY_LOADING_INTEGER_REG gpu label_in lable_out
+\label_in:
+    lbu    t3, 0(t0)           # get argument type from shorty
+    c.addi   t0, 1
+    beqz   t3, \lable_out          # jump out
+
+    li     t6, 68               # put char 'D' into t6
+    beq    t6, t3, 1f          # branch if result type char == 'D'
+    li     t6, 70               # put char 'F' into t6
+    beq    t6, t3, 2f          # branch if result type char == 'F'
+    li     t6, 74               # put char 'J' into t6
+    beq    t6, t3, 3f          # branch if result type char == 'J'
+    nop
+    # found int (4 bytes)
+    lw     \gpu, 0(t1)
+    c.addi   t1, 4
+    c.j      4f
+
+1:  # found double and skip it if the count < 8
+    c.addi t5, 1
+    li t6, 8
+    ble t5, t6, 11f
+    ld \gpu, 0(t1)
+    c.addi   t1, 8
+    c.j      4f
+11:
+    c.addi t1, 8
+    c.j      \label_in
+
+2:  # found float and and skip it if the count < 8
+    c.addi t5, 1
+    li t6, 8
+    ble t5, t6, 22f
+    lw \gpu, 0(t1)
+    c.addi   t1, 4
+    c.j      4f
+22:
+    c.addi   t1, 4
+    c.j      \label_in
+
+3:  # found long (8 bytes)
+    lwu    t3, 0(t1)
+    lwu    t6, 4(t1)
+    slli   t6, t6, 32
+    or     \gpu, t6, t3
+    c.addi t1, 8
+4:
+.endm
+
+    # On entry:
+    #   t0 = shorty[1] (skip 1 for return type)
+    #   t1 = ptr to arg_array
+    # This macro modifies t3, t6 and v0
+.macro LOOP_OVER_SHORTY_LOADING_FLOAT_REG fpu label_in lable_out
+\label_in:
+    lbu    t3, 0(t0)           # get argument type from shorty
+    c.addi   t0, 1
+    beqz   t3, \lable_out          # jump out
+
+    li     t6, 68               # put char 'D' into t6
+    beq    t6, t3, 1f          # branch if result type char == 'D'
+    li     t6, 70               # put char 'F' into t6
+    beq    t6, t3, 2f          # branch if result type char == 'F'
+    li     t6, 74               # put char 'J' into t6
+    beq    t6, t3, 3f          # branch if result type char == 'J'
+    nop
+    # found int (4 bytes) skip it
+    c.addi   t1, 4
+    c.j      \label_in
+
+1:  # found double (8 bytes)
+    fld    \fpu, 0(t1)              # load double arg into \fpu
+    c.addi t1, 8
+    c.j      4f
+
+2:  # found float (4 bytes)
+    flw    \fpu, 0(t1)              # load float arg into \fpu
+    c.addi   t1, 4
+    c.j      4f
+
+3:  # found long (8 bytes) skip it
+    c.addi t1, 8
+    c.j      \label_in
+4:
+.endm
+
+    /*
+     * Invocation stub for quick code.
+     * On entry:
+     *   a0 = method pointer
+     *   a1 = argument array that must at least contain the this ptr.
+     *   a2 = size of argument array in bytes
+     *   a3 = (managed) thread pointer
+     *   a4 = JValue* result
+     *   a5 = shorty
+     */
+ENTRY_NO_GP art_quick_invoke_stub
+    # push a4, a5, s11(rSUSPEND), s1(rSELF), s0(fp), ra onto the stack
+    addi sp, sp, -48
+    .cfi_adjust_cfa_offset 48
+    sd     ra, 40(sp)
+    .cfi_rel_offset 31, 40
+    sd     s0, 32(sp)
+    .cfi_rel_offset 30, 32
+    sd     rSELF, 24(sp)
+    .cfi_rel_offset 17, 24
+    sd     rSUSPEND, 16(sp)
+    .cfi_rel_offset 16, 16
+    sd     a5, 8(sp)
+    .cfi_rel_offset 9, 8
+    sd     a4, 0(sp)
+    .cfi_rel_offset 8, 0
+
+    c.mv   rSELF, a3           # move managed thread pointer into s1 (rSELF)
+    c.mv   s0, sp              # save sp in s0 (fp)
+
+    addi   t3, a2, 24          # add 8 for ArtMethod* and 16 for stack alignment
+    srli   t3, t3, 4           # shift the frame size right 4
+    slli   t3, t3, 4           # shift the frame size left 4 to align to 16 bytes
+    sub    sp, sp, t3          # reserve stack space for argument array
+
+    addi   t0, a5, 1           # t0 = shorty[1] (skip 1 for return type)
+    addi   t1, a1, 4           # t1 = ptr to arg_array[4] (skip this ptr)
+    addi   t2, a2, -4          # t2 = number of argument bytes remain (skip this ptr)
+    addi   t4, sp, 12          # v0 (t4 in riscv64) points to where to copy arg_array
+
+    # Copy all args into stack
+1:
+    beqz   t2, 2f
+    addi   t2, t2, -4
+    lw     t3, 0(t1)           # load from argument array
+    addi   t1, t1, 4
+    sw     t3, 0(t4)           # save to stack
+    addi   t4, t4, 4
+    j 1b
+2:
+    sd     zero, 0(sp)
+
+    # Load float/double args into Floating Argument Registers
+    addi   t0, a5, 1           # t0 = shorty[1] (skip 1 for return type)
+    addi   t1, a1, 4           # t1 = ptr to arg_array[4] (skip this ptr)
+
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f10 f_a0 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f11 f_a1 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f12 f_a2 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f13 f_a3 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f14 f_a4 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f15 f_a5 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f16 f_a6 load_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f17 f_a7 load_integer
+
+    # Load int/long args into Integer Argument Registers
+load_integer:
+    addi   t0, a5, 1           # t0 = shorty[1] (skip 1 for return type)
+    addi   t1, a1, 4           # t1 = ptr to arg_array[4] (skip this ptr)
+    li   t5, 0               # t5 = float/double arg count
+
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a2 i_a2 call_fn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a3 i_a3 call_fn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a4 i_a4 call_fn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a5 i_a5 call_fn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a6 i_a6 call_fn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a7 i_a7 call_fn
+
+call_fn:
+    # call method (a0 and a1 have been untouched)
+    lwu    a1, 0(a1)           # make a1 = this ptr
+    sw     a1, 8(sp)           # copy this ptr (skip 8 bytes for ArtMethod*)
+    sd     zero, 0(sp)         # store null for ArtMethod* at bottom of frame
+    ld     t6, ART_METHOD_QUICK_CODE_OFFSET_64(a0)  # get pointer to the code
+    jalr   t6                  # call the method
+    nop
+    c.mv   sp, s0              # restore sp from fp(s0)
+
+    # pop a4, a5, s9(rSUSPEND), s1(rSELF), s0(fp), ra off of the stack
+    ld     a4, 0(sp)
+    .cfi_restore 8
+    ld     a5, 8(sp)
+    .cfi_restore 9
+    ld     rSUSPEND, 16(sp)
+    .cfi_restore 16
+    ld     rSELF, 24(sp)
+    .cfi_restore 17
+    ld     s0, 32(sp)
+    .cfi_restore 30
+    ld     ra, 40(sp)
+    .cfi_restore 31
+    addi   sp, sp, 48
+    .cfi_adjust_cfa_offset -48
+
+    # a4 = JValue* result
+    # a5 = shorty string
+
+    lbu   t1, 0(a5)
+    li    t2, 'V'
+    beq   t1, t2, 2f
+    li    t2, 'D'
+    beq   t1, t2, 1f
+    li    t2, 'F'
+    beq   t1, t2, 1f
+    sd    a0, 0(a4)
+    ret
+1:
+    fsd   f10, 0(a4)
+    ret
+2:
+    ret
+END art_quick_invoke_stub
+
+    /*
+     * Invocation static stub for quick code.
+     * On entry:
+     *   a0 = method pointer
+     *   a1 = argument array that must at least contain the this ptr.
+     *   a2 = size of argument array in bytes
+     *   a3 = (managed) thread pointer
+     *   a4 = JValue* result
+     *   a5 = shorty
+     */
+ENTRY_NO_GP art_quick_invoke_static_stub
+    # push a4, a5, s11(rSUSPEND), s1(rSELF), s0(fp), ra, onto the stack
+    addi sp, sp, -48
+    .cfi_adjust_cfa_offset 48
+    sd     ra, 40(sp)
+    .cfi_rel_offset 31, 40
+    sd     s0, 32(sp)
+    .cfi_rel_offset 30, 32
+    sd     rSELF, 24(sp)
+    .cfi_rel_offset 17, 24
+    sd     rSUSPEND, 16(sp)
+    .cfi_rel_offset 16, 16
+    sd     a5, 8(sp)
+    .cfi_rel_offset 9, 8
+    sd     a4, 0(sp)
+    .cfi_rel_offset 8, 0
+
+    c.mv   rSELF, a3           # move managed thread pointer into s1 (rSELF)
+    c.mv   s0, sp              # save sp in s0 (fp)
+
+    addi   t3, a2, 24          # add 8 for ArtMethod* and 16 for stack alignment
+    srli   t3, t3, 4           # shift the frame size right 4
+    slli   t3, t3, 4           # shift the frame size left 4 to align to 16 bytes
+    sub    sp, sp, t3         # reserve stack space for argument array
+
+    addi   t0, a5, 1           # t0 = shorty[1] (skip 1 for return type)
+    c.mv   t1, a1              # t1 = arg_array
+    c.mv   t2, a2              # t2 = number of argument bytes remain
+    addi   t4, sp, 8           # v0 (t4 in riscv64) points to where to copy arg_array
+
+    # Copy all args into stack
+1:
+    beqz   t2, 2f
+    addi   t2, t2, -4
+    lw     t3, 0(t1)           # load from argument array
+    addi   t1, t1, 4
+    sw     t3, 0(t4)           # save to stack
+    addi   t4, t4, 4
+    j 1b
+2:
+    sd     zero, 0(sp)
+
+    # Load float/double args into Floating Argument Registers
+    addi   t0, a5, 1           # t0 = shorty[1] (skip 1 for return type)
+    c.mv   t1, a1              # t1 = arg_array
+
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f10 sf_a0 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f11 sf_a1 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f12 sf_a2 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f13 sf_a3 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f14 sf_a4 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f15 sf_a5 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f16 sf_a6 sload_integer
+    LOOP_OVER_SHORTY_LOADING_FLOAT_REG f17 sf_a7 sload_integer
+
+    # Load int/long args into Integer Argument Registers
+sload_integer:
+    addi   t0, a5, 1           # t0 = shorty[1] (skip 1 for return type)
+    c.mv   t1, a1              # t1 = arg_array
+    li   t5, 0               # t5 = float/double arg count
+
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a1 si_a1 call_sfn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a2 si_a2 call_sfn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a3 si_a3 call_sfn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a4 si_a4 call_sfn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a5 si_a5 call_sfn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a6 si_a6 call_sfn
+    LOOP_OVER_SHORTY_LOADING_INTEGER_REG a7 si_a7 call_sfn
+
+call_sfn:
+    # call method (a0 has been untouched)
+    sd     zero, 0(sp)         # store null for ArtMethod* at bottom of frame
+    ld     t6, ART_METHOD_QUICK_CODE_OFFSET_64(a0)  # get pointer to the code
+    jalr   t6                   # call the method
+    nop
+    c.mv   sp, s0              # restore sp
+
+    # pop a4, a5, s11(rSUSPEND), s1(rSELF), s0(fp), ra off of the stack
+    ld     a4, 0(sp)
+    .cfi_restore 8
+    ld     a5, 8(sp)
+    .cfi_restore 9
+    ld     rSUSPEND, 16(sp)
+    .cfi_restore 16
+    ld     rSELF, 24(sp)
+    .cfi_restore 17
+    ld     s0, 32(sp)
+    .cfi_restore 30
+    ld     ra, 40(sp)
+    .cfi_restore 31
+    addi   sp, sp, 48
+    .cfi_adjust_cfa_offset -48
+
+    # a4 = JValue* result
+    # a5 = shorty string
+    lbu   t1, 0(a5)
+    li    t2, 'V'
+    beq   t1, t2, 2f
+    li    t2, 'D'
+    beq   t1, t2, 1f
+    li    t2, 'F'
+    beq   t1, t2, 1f
+    sd    a0, 0(a4)
+    ret
+1:
+    fsd   f10, 0(a4)
+    ret
+2:
+    ret
+END art_quick_invoke_static_stub
+
+    /*
+     * Entry from managed code that calls artHandleFillArrayDataFromCode and
+     * delivers exception on failure.
+     */
+    .extern artHandleFillArrayDataFromCode
+ENTRY art_quick_handle_fill_data
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case exception allocation triggers GC
+    ld      a2, FRAME_SIZE_SAVE_REFS_ONLY(sp)           # pass referrer's Method*
+    mv      a3, rSELF                                   # pass Thread::Current
+    jal     artHandleFillArrayDataFromCode              # (payload offset, Array*, method, Thread*)
+    RETURN_IF_ZERO
+END art_quick_handle_fill_data
+
+    /*
+     * Entry from managed code that calls artLockObjectFromCode, may block for GC.
+     */
+    .extern artLockObjectFromCode
+ENTRY art_quick_lock_object
+    bnez    a0, 1f
+    # Jmp to art_quick_throw_null_pointer_exception if a0 == zero
+    la      t6, art_quick_throw_null_pointer_exception
+    jr      t6
+1:
+    li      t6, LOCK_WORD_THIN_LOCK_COUNT_ONE
+    li      t3, LOCK_WORD_GC_STATE_MASK_SHIFTED_TOGGLED
+.Lretry_lock:
+    lw      t0, THREAD_ID_OFFSET(rSELF)    # TODO: Can the thread ID really change during the loop?
+    addi    t5, a0, MIRROR_OBJECT_LOCK_WORD_OFFSET
+    lr.w    t1, (t5)
+    and     t2, t1, t3                    # zero the gc bits
+    bnez    t2, .Lnot_unlocked            # already thin locked
+    # Unlocked case - $t1: original lock word that's zero except for the read barrier bits.
+    or      t2, t1, t0                 # $t2 holds thread id with count of 0 with preserved read barrier bits
+    addi    t5, a0, MIRROR_OBJECT_LOCK_WORD_OFFSET
+    sc.w    t2, t2, (t5)             #
+    bnez    t2, .Lretry_lock               # store failed, retry
+    fence                                  # full (LoadLoad|LoadStore) memory barrier
+    jr  ra
+.Lnot_unlocked:
+    # $t1: original lock word, $t0: thread_id with count of 0 and zero read barrier bits
+    srl     t2, t1, LOCK_WORD_STATE_SHIFT         # t2 = t1>>LOCK_WORD_STATE_SHIFT
+    bnez    t2, .Lslow_lock              # if either of the top two bits are set, go slow path
+    xor     t2, t1, t0                  # lock_word.ThreadId() ^ self->ThreadId()
+    li      t4, 0xFFFF
+    and     t2, t2, t4                   # zero top 16 bits
+    bnez    t2, .Lslow_lock              # lock word and self thread id's match -> recursive lock
+                                          # otherwise contention, go to slow path
+    and     t2, t1, t3                 # zero the gc bits
+    add     t2, t2, t6                 # increment count in lock word
+    srl     t2, t2, LOCK_WORD_STATE_SHIFT  # if the first gc state bit is set, we overflowed.
+    bnez    t2, .Lslow_lock              # if we overflow the count go slow path
+    add     t2, t1, t6                   # increment count for real
+    addi    t5, a0, MIRROR_OBJECT_LOCK_WORD_OFFSET
+    sc.w    t2, t2, (t5)
+    bnez    t2, .Lretry_lock             # store failed, retry
+    jr  ra
+.Lslow_lock:
+    # .cpsetup $t9, $t8, art_quick_lock_object
+    SETUP_SAVE_REFS_ONLY_FRAME            # save callee saves in case we block
+    move    a1, rSELF                    # pass Thread::Current
+    jal     artLockObjectFromCode         # (Object* obj, Thread*)
+    RETURN_IF_ZERO
+END art_quick_lock_object
+
+
+ENTRY_NO_GP art_quick_lock_object_no_inline
+    #beq     a0, zero, art_quick_throw_null_pointer_exception
+    bnez    a0, 1f
+    # Jmp to art_quick_throw_null_pointer_exception if a0 == zero
+    la      t6, art_quick_throw_null_pointer_exception
+    jr      t6
+1:
+    nop
+    nop # @todo .cpsetup $t6, $t5, art_quick_lock_object_no_inline
+    SETUP_SAVE_REFS_ONLY_FRAME            # save callee saves in case we block
+    mv      a1, rSELF                    # pass Thread::Current
+    jal     artLockObjectFromCode         # (Object* obj, Thread*)
+    RETURN_IF_ZERO
+END art_quick_lock_object_no_inline
+
+    /*
+     * Entry from managed code that calls artUnlockObjectFromCode and delivers exception on failure.
+     */
+      .extern artUnlockObjectFromCode
+ENTRY_NO_GP art_quick_unlock_object
+    bnez    a0, 1f
+    # Jmp to art_quick_throw_null_pointer_exception if a0 == zero
+    la      t6, art_quick_throw_null_pointer_exception
+    jr      t6
+1:
+    li      t6, LOCK_WORD_THIN_LOCK_COUNT_ONE
+    li      t3, LOCK_WORD_GC_STATE_MASK_SHIFTED_TOGGLED
+.Lretry_unlock:
+#ifndef USE_READ_BARRIER
+    lw      t1, MIRROR_OBJECT_LOCK_WORD_OFFSET(a0)
+#else
+    addi    t5, a0, MIRROR_OBJECT_LOCK_WORD_OFFSET    # Need to use atomic read-modify-write for read barrier
+    lr.w    t1, (t5)
+#endif
+    srlw    t2, t1, LOCK_WORD_STATE_SHIFT
+    bnez    t2, .Lslow_unlock         # if either of the top two bits are set, go slow path
+    lw      t0, THREAD_ID_OFFSET(rSELF)
+    and     t2, t1, t3              # zero the gc bits
+    xor     t2, t2, t0              # lock_word.ThreadId() ^ self->ThreadId()
+    li      t4, 0xFFFF
+    and     t2, t2, t4                   # zero top 16 bits
+    bnez    t2, .Lslow_unlock         # do lock word and self thread id's match?
+    and     t2, t1, t3              # zero the gc bits
+    bgeu    t2, t6, .Lrecursive_thin_unlock
+    # transition to unlocked
+    or      t2, zero, t3            # t2 = LOCK_WORD_GC_STATE_MASK_SHIFTED
+    not     t2, t2
+    and     t2, t1, t2              # t2: zero except for the preserved gc bits
+    fence                           # full (LoadStore|StoreStore) memory barrier
+#ifndef USE_READ_BARRIER
+    sw      t2, MIRROR_OBJECT_LOCK_WORD_OFFSET(a0)
+#else
+    addi    t5, a0, MIRROR_OBJECT_LOCK_WORD_OFFSET  # @todo sc      t2, MIRROR_OBJECT_LOCK_WORD_OFFSET(a0)
+    sc.w    t2, t2, (t5)
+    bnez    t2, .Lretry_unlock        # store failed, retry
+#endif
+    jr      ra    # @todo jic     ra, 0
+.Lrecursive_thin_unlock:
+    # t1: original lock word
+    sub     t2, t1, t6              # decrement count
+#ifndef USE_READ_BARRIER
+    sw      t2, MIRROR_OBJECT_LOCK_WORD_OFFSET(a0)
+#else
+    addi    t5, a0, MIRROR_OBJECT_LOCK_WORD_OFFSET
+    sc.w    t2, t2, (t5)
+    bnez    t2, .Lretry_unlock        # store failed, retry
+#endif
+    jr      ra
+.Lslow_unlock:
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case exception allocation triggers GC
+    mv      a1, rSELF                 # pass Thread::Current
+    jal     artUnlockObjectFromCode    # (Object* obj, Thread*)
+    RETURN_IF_ZERO
+END art_quick_unlock_object
+
+ENTRY_NO_GP art_quick_unlock_object_no_inline
+    # beq     a0, zero, art_quick_throw_null_pointer_exception
+    bnez    a0, 1f
+    # Jmp to art_quick_throw_null_pointer_exception if a0 == zero
+    la      t6, art_quick_throw_null_pointer_exception
+    jr      t6
+1:
+    nop
+    nop # @todo .cpsetup t6, t5, art_quick_unlock_object_no_inline
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case exception allocation triggers GC
+    mv      a1, rSELF                 # pass Thread::Current
+    jal     artUnlockObjectFromCode    # (Object* obj, Thread*)
+    RETURN_IF_ZERO
+END art_quick_unlock_object_no_inline
+
+    /*
+     * Entry from managed code that calls artInstanceOfFromCode and delivers exception on failure.
+     */
+    .extern artInstanceOfFromCode
+    .extern artThrowClassCastExceptionForObject
+ENTRY art_quick_check_instance_of
+    // Type check using the bit string passes null as the target class. In that case just throw.
+    beqz   a1, .Lthrow_class_cast_exception_for_bitstring_check
+
+    addi   sp, sp, -32
+    .cfi_adjust_cfa_offset 32
+    sd     ra, 24(sp)
+    .cfi_rel_offset 31, 24
+    sd     t6, 16(sp)
+    sd     a1, 8(sp)
+    sd     a0, 0(sp)
+    jal    artInstanceOfFromCode
+
+    ld     ra, 24(sp)
+    beq    a0, zero, .Lthrow_class_cast_exception
+
+    addi   sp, sp, 32
+    .cfi_adjust_cfa_offset -32
+    jalr   zero, ra
+
+.Lthrow_class_cast_exception:
+    ld     t6, 16(sp)
+    ld     a1, 8(sp)
+    ld     a0, 0(sp)
+    addi sp, sp, 32
+    .cfi_adjust_cfa_offset -32
+
+.Lthrow_class_cast_exception_for_bitstring_check:
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
+    la    t6, artThrowClassCastExceptionForObject
+    mv     a2, rSELF                 # pass Thread::Current
+    jalr   zero, t6                 # artThrowClassCastException (Object*, Class*, Thread*)
+END art_quick_check_instance_of
+
+
+    /*
+     * Restore rReg's value from offset($sp) if rReg is not the same as rExclude.
+     * nReg is the register number for rReg.
+     */
+.macro POP_REG_NE rReg, nReg, offset, rExclude
+    .ifnc \rReg, \rExclude
+        ld \rReg, \offset(sp)      # restore rReg
+        .cfi_restore \nReg
+    .endif
+.endm
+
+    /*
+     * Macro to insert read barrier, only used in art_quick_aput_obj.
+     * rObj and rDest are registers, offset is a defined literal such as MIRROR_OBJECT_CLASS_OFFSET.
+     * TODO: When read barrier has a fast path, add heap unpoisoning support for the fast path.
+     */
+.macro READ_BARRIER rDest, rObj, offset
+#ifdef USE_READ_BARRIER
+    # saved registers used in art_quick_aput_obj: a0-a2, t0-t1, t6, ra. 16B-aligned.
+    addi   sp, sp, -64
+    .cfi_adjust_cfa_offset 64
+    sd     ra, 56(sp)
+    .cfi_rel_offset 31, 56
+    sd     t6, 48(sp)
+    .cfi_rel_offset 25, 48
+    sd     t1, 40(sp)
+    .cfi_rel_offset 13, 40
+    sd     t0, 32(sp)
+    .cfi_rel_offset 12, 32
+    sd     a2, 16(sp)             # padding slot at offset 24 (padding can be any slot in the 64B)
+    .cfi_rel_offset 6, 16
+    sd     a1, 8(sp)
+    .cfi_rel_offset 5, 8
+    sd     a0, 0(sp)
+    .cfi_rel_offset 4, 0
+
+    # move a0, \rRef               # pass ref in a0 (no-op for now since parameter ref is unused)
+    .ifnc \rObj, a1
+        mv   a1, \rObj             # pass rObj
+    .endif
+    addi   a2, zero, \offset      # pass offset
+    jal artReadBarrierSlow          # artReadBarrierSlow(ref, rObj, offset)
+
+    # No need to unpoison return value in v0, artReadBarrierSlow() would do the unpoisoning.
+    mv    \rDest, a0 # @todo mv    \rDest, v0                # save return value in rDest
+                                    # (rDest cannot be v0 in art_quick_aput_obj)
+
+    ld     a0, 0(sp)              # restore registers except rDest
+                                    # (rDest can only be t0 or t1 in art_quick_aput_obj)
+    .cfi_restore 4
+    ld     a1, 8(sp)
+    .cfi_restore 5
+    ld     a2, 16(sp)
+    .cfi_restore 6
+    POP_REG_NE t0, 12, 32, \rDest
+    POP_REG_NE t1, 13, 40, \rDest
+    ld     t6, 48(sp)
+    .cfi_restore 25
+    ld     ra, 56(sp)             # restore ra
+    .cfi_restore 31
+    addi   sp, sp, 64
+    .cfi_adjust_cfa_offset -64
+    SETUP_GP                        # set up gp because we are not returning
+#else
+    lwu     \rDest, \offset(\rObj)
+    UNPOISON_HEAP_REF \rDest
+#endif  // USE_READ_BARRIER
+.endm
+
+ENTRY art_quick_aput_obj
+    beq  a2, zero, .Ldo_aput_null
+    nop
+    READ_BARRIER t0, a0, MIRROR_OBJECT_CLASS_OFFSET
+    READ_BARRIER t1, a2, MIRROR_OBJECT_CLASS_OFFSET
+    READ_BARRIER t0, t0, MIRROR_CLASS_COMPONENT_TYPE_OFFSET
+    bne t1, t0, .Lcheck_assignability  # value's type == array's component type - trivial assignability
+    nop
+.Ldo_aput:
+    slli  a1, a1, 2
+    add   t0, a0, a1
+    POISON_HEAP_REF a2
+    sw   a2, MIRROR_OBJECT_ARRAY_DATA_OFFSET(t0)
+    ld   t0, THREAD_CARD_TABLE_OFFSET(rSELF)
+    srli t1, a0, CARD_TABLE_CARD_SHIFT
+    add  t1, t1, t0
+    sb   t0, (t1)
+    jalr zero, ra
+.Ldo_aput_null:
+    slli  a1, a1, 2
+    add   t0, a0, a1
+    sw    a2, MIRROR_OBJECT_ARRAY_DATA_OFFSET(t0)
+    jalr  zero, ra
+.Lcheck_assignability:
+    addi  sp, sp, -64
+    .cfi_adjust_cfa_offset 64
+    sd     ra, 56(sp)
+    .cfi_rel_offset 31, 56
+    sd     t6, 24(sp)
+    sd     a2, 16(sp)
+    sd     a1, 8(sp)
+    sd     a0, 0(sp)
+    mv     a1, t1
+    mv     a0, t0
+    jal    artIsAssignableFromCode  # (Class*, Class*)
+
+    // Check for exception
+    beqz   a0, .Lthrow_array_store_exception
+
+    ld     ra, 56(sp)
+    ld     t6, 24(sp)
+    ld     a2, 16(sp)
+    ld     a1, 8(sp)
+    ld     a0, 0(sp)
+    addi   sp, sp, 64
+    .cfi_adjust_cfa_offset -64
+
+    j .Ldo_aput
+    nop
+.Lthrow_array_store_exception:
+    ld     ra, 56(sp)
+    ld     t6, 24(sp)
+    ld     a2, 16(sp)
+    ld     a1, 8(sp)
+    ld     a0, 0(sp)
+    addi   sp, sp, 64
+    .cfi_adjust_cfa_offset -64
+
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
+    mv     a1, a2
+    mv     a2, rSELF               # pass Thread::Current
+    la     t6, artThrowArrayStoreException
+    jalr zero, t6                 # artThrowArrayStoreException(Class*, Class*, Thread*)
+END art_quick_aput_obj
+
+
+// Macros taking opportunity of code similarities for downcalls.
+.macro ONE_ARG_REF_DOWNCALL name, entrypoint, return, extend=0
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME        # save callee saves in case of GC
+    la      t6, \entrypoint
+    mv      a1, rSELF                # pass Thread::Current
+    jalr    t6                       # (field_idx, Thread*)
+    .if     \extend
+    slliw    a0, a0, 0               # sign-extend 32-bit result
+    .endif
+    \return                           # RETURN_IF_NO_EXCEPTION or RETURN_IF_ZERO
+END \name
+.endm
+
+.macro TWO_ARG_REF_DOWNCALL name, entrypoint, return, extend=0
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME        # save callee saves in case of GC
+    la      t6, \entrypoint
+    mv      a2, rSELF                # pass Thread::Current
+    jalr    t6                       # (field_idx, Object*, Thread*) or
+                                      # (field_idx, new_val, Thread*)
+
+    .if     \extend
+    slliw    a0, a0, 0                # sign-extend 32-bit result
+    .endif
+    \return                           # RETURN_IF_NO_EXCEPTION or RETURN_IF_ZERO
+END \name
+.endm
+
+.macro THREE_ARG_REF_DOWNCALL name, entrypoint, return, extend=0
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME        # save callee saves in case of GC
+    la      t6, \entrypoint
+    mv      a3, rSELF                # pass Thread::Current
+    jalr    t6                       # (field_idx, Object*, new_val, Thread*)
+    .if     \extend
+    slliw     a0, a0, 0               # sign-extend 32-bit result
+    .endif
+    \return                           # RETURN_IF_NO_EXCEPTION or RETURN_IF_ZERO
+END \name
+.endm
+
+
+    /*
+     * Called by managed code to resolve a static/instance field and load/store a value.
+     *
+     * Note: Functions `art{Get,Set}<Kind>{Static,Instance}FromCompiledCode` are
+     * defined with a macro in runtime/entrypoints/quick/quick_field_entrypoints.cc.
+     */
+ONE_ARG_REF_DOWNCALL art_quick_get_byte_static, artGetByteStaticFromCompiledCode, RETURN_IF_NO_EXCEPTION
+ONE_ARG_REF_DOWNCALL art_quick_get_boolean_static, artGetBooleanStaticFromCompiledCode, RETURN_IF_NO_EXCEPTION
+ONE_ARG_REF_DOWNCALL art_quick_get_short_static, artGetShortStaticFromCompiledCode, RETURN_IF_NO_EXCEPTION
+ONE_ARG_REF_DOWNCALL art_quick_get_char_static, artGetCharStaticFromCompiledCode, RETURN_IF_NO_EXCEPTION
+ONE_ARG_REF_DOWNCALL art_quick_get32_static, artGet32StaticFromCompiledCode, RETURN_IF_NO_EXCEPTION, 1
+ONE_ARG_REF_DOWNCALL art_quick_get_obj_static, artGetObjStaticFromCompiledCode, RETURN_IF_NO_EXCEPTION
+ONE_ARG_REF_DOWNCALL art_quick_get64_static, artGet64StaticFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_get_byte_instance, artGetByteInstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_get_boolean_instance, artGetBooleanInstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_get_short_instance, artGetShortInstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_get_char_instance, artGetCharInstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_get32_instance, artGet32InstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION, 1
+TWO_ARG_REF_DOWNCALL art_quick_get_obj_instance, artGetObjInstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_get64_instance, artGet64InstanceFromCompiledCode, RETURN_IF_NO_EXCEPTION
+TWO_ARG_REF_DOWNCALL art_quick_set8_static, artSet8StaticFromCompiledCode, RETURN_IF_ZERO
+TWO_ARG_REF_DOWNCALL art_quick_set16_static, artSet16StaticFromCompiledCode, RETURN_IF_ZERO
+TWO_ARG_REF_DOWNCALL art_quick_set32_static, artSet32StaticFromCompiledCode, RETURN_IF_ZERO
+TWO_ARG_REF_DOWNCALL art_quick_set_obj_static, artSetObjStaticFromCompiledCode, RETURN_IF_ZERO
+TWO_ARG_REF_DOWNCALL art_quick_set64_static, artSet64StaticFromCompiledCode, RETURN_IF_ZERO
+THREE_ARG_REF_DOWNCALL art_quick_set8_instance, artSet8InstanceFromCompiledCode, RETURN_IF_ZERO
+THREE_ARG_REF_DOWNCALL art_quick_set16_instance, artSet16InstanceFromCompiledCode, RETURN_IF_ZERO
+THREE_ARG_REF_DOWNCALL art_quick_set32_instance, artSet32InstanceFromCompiledCode, RETURN_IF_ZERO
+THREE_ARG_REF_DOWNCALL art_quick_set_obj_instance, artSetObjInstanceFromCompiledCode, RETURN_IF_ZERO
+THREE_ARG_REF_DOWNCALL art_quick_set64_instance, artSet64InstanceFromCompiledCode, RETURN_IF_ZERO
+
+// Macro to facilitate adding new allocation entrypoints.
+.macro ONE_ARG_DOWNCALL name, entrypoint, return
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case of GC
+    mv      a1, rSELF                 # pass Thread::Current
+    jal     \entrypoint
+    \return
+END \name
+.endm
+
+// Macro to facilitate adding new allocation entrypoints.
+.macro TWO_ARG_DOWNCALL name, entrypoint, return
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case of GC
+    mv      a2, rSELF                 # pass Thread::Current
+    jal     \entrypoint
+    \return
+END \name
+.endm
+
+.macro THREE_ARG_DOWNCALL name, entrypoint, return
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case of GC
+    mv      a3, rSELF                 # pass Thread::Current
+    jal     \entrypoint
+    \return
+END \name
+.endm
+
+.macro FOUR_ARG_DOWNCALL name, entrypoint, return
+    .extern \entrypoint
+ENTRY \name
+    SETUP_SAVE_REFS_ONLY_FRAME         # save callee saves in case of GC
+    mv      a4, rSELF                 # pass Thread::Current
+    jal     \entrypoint
+    \return
+END \name
+.endm
+
+// Generate the allocation entrypoints for each allocator.
+GENERATE_ALLOC_ENTRYPOINTS_FOR_NON_TLAB_ALLOCATORS
+// Comment out allocators that have riscv64 specific asm.
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_RESOLVED(_region_tlab, RegionTLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_INITIALIZED(_region_tlab, RegionTLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_WITH_ACCESS_CHECK(_region_tlab, RegionTLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_OBJECT(_region_tlab, RegionTLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED(_region_tlab, RegionTLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED8(_region_tlab, RegionTLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED16(_region_tlab, RegionTLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED32(_region_tlab, RegionTLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED64(_region_tlab, RegionTLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_FROM_BYTES(_region_tlab, RegionTLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_FROM_CHARS(_region_tlab, RegionTLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_FROM_STRING(_region_tlab, RegionTLAB)
+
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_RESOLVED(_tlab, TLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_INITIALIZED(_tlab, TLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_WITH_ACCESS_CHECK(_tlab, TLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_OBJECT(_tlab, TLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED(_tlab, TLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED8(_tlab, TLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED16(_tlab, TLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED32(_tlab, TLAB)
+// GENERATE_ALLOC_ENTRYPOINTS_ALLOC_ARRAY_RESOLVED64(_tlab, TLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_FROM_BYTES(_tlab, TLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_FROM_CHARS(_tlab, TLAB)
+GENERATE_ALLOC_ENTRYPOINTS_ALLOC_STRING_FROM_STRING(_tlab, TLAB)
+
+// A hand-written override for:
+//   GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_RESOLVED(_rosalloc, RosAlloc)
+//   GENERATE_ALLOC_ENTRYPOINTS_ALLOC_OBJECT_INITIALIZED(_rosalloc, RosAlloc)
+.macro ART_QUICK_ALLOC_OBJECT_ROSALLOC c_name, cxx_name, isInitialized
+ENTRY_NO_GP \c_name
+    # Fast path rosalloc allocation
+    # a0: type
+    # s1: Thread::Current
+    # -----------------------------
+    # t1: object size
+    # t2: rosalloc run
+    # t3: thread stack top offset
+    # a4: thread stack bottom offset
+    # v0: free list head
+    #
+    # a5, a6 : temps
+    ld     t3, THREAD_LOCAL_ALLOC_STACK_TOP_OFFSET(s1)    # Check if thread local allocation stack
+    ld     a4, THREAD_LOCAL_ALLOC_STACK_END_OFFSET(s1)    # has any room left.
+    bgeu   t3, a4, .Lslow_path_\c_name
+
+    lwu    t1, MIRROR_CLASS_OBJECT_SIZE_ALLOC_FAST_PATH_OFFSET(a0)  # Load object size (t1).
+    li     a5, ROSALLOC_MAX_THREAD_LOCAL_BRACKET_SIZE      # Check if size is for a thread local
+                                                            # allocation. Also does the initialized
+                                                            # and finalizable checks.
+    # When isInitialized == 0, then the class is potentially not yet initialized.
+    # If the class is not yet initialized, the object size will be very large to force the branch
+    # below to be taken.
+    #
+    # See InitializeClassVisitors in class-inl.h for more details.
+    bltu   a5, t1, .Lslow_path_\c_name
+
+    # Compute the rosalloc bracket index from the size. Since the size is already aligned we can
+    # combine the two shifts together.
+    srli   t1, t1, (ROSALLOC_BRACKET_QUANTUM_SIZE_SHIFT - POINTER_SIZE_SHIFT)
+
+    add    t2, t1, s1
+    ld     t2, (THREAD_ROSALLOC_RUNS_OFFSET - __SIZEOF_POINTER__)(t2)  # Load rosalloc run (t2).
+
+    # Load the free list head (v0).
+    # NOTE: this will be the return val.
+    ld     t4, (ROSALLOC_RUN_FREE_LIST_OFFSET + ROSALLOC_RUN_FREE_LIST_HEAD_OFFSET)(t2)
+    beqz   t4, .Lslow_path_\c_name
+
+    # Load the next pointer of the head and update the list head with the next pointer.
+    ld     a5, ROSALLOC_SLOT_NEXT_OFFSET(t4)
+    sd     a5, (ROSALLOC_RUN_FREE_LIST_OFFSET + ROSALLOC_RUN_FREE_LIST_HEAD_OFFSET)(t2)
+
+    # Store the class pointer in the header. This also overwrites the first pointer. The offsets are
+    # asserted to match.
+
+#if ROSALLOC_SLOT_NEXT_OFFSET != MIRROR_OBJECT_CLASS_OFFSET
+#error "Class pointer needs to overwrite next pointer."
+#endif
+
+    POISON_HEAP_REF a0
+    sw     a0, MIRROR_OBJECT_CLASS_OFFSET(t4)
+
+    # Push the new object onto the thread local allocation stack and increment the thread local
+    # allocation stack top.
+    sw     t4, 0(t3)
+    addi   t3, t3, COMPRESSED_REFERENCE_SIZE
+    sd     t3, THREAD_LOCAL_ALLOC_STACK_TOP_OFFSET(s1)
+
+    # Decrement the size of the free list.
+    lw     a5, (ROSALLOC_RUN_FREE_LIST_OFFSET + ROSALLOC_RUN_FREE_LIST_SIZE_OFFSET)(t2)
+    addiw a5, a5, -1 # @todo addiu  a5, a5, -1
+    sw     a5, (ROSALLOC_RUN_FREE_LIST_OFFSET + ROSALLOC_RUN_FREE_LIST_SIZE_OFFSET)(t2)
+
+.if \isInitialized == 0
+    # This barrier is only necessary when the allocation also requires a class initialization check.
+    #
+    # If the class is already observably initialized, then new-instance allocations are protected
+    # from publishing by the compiler which inserts its own StoreStore barrier.
+    fence                        # Fence.
+.endif
+    mv      a0,   t4
+    jr      ra
+
+.Lslow_path_\c_name:
+    SETUP_SAVE_REFS_ONLY_FRAME
+    mv     a0,   t4
+    mv     a1 ,s1                              # Pass self as argument.
+    jal    \cxx_name
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END \c_name
+.endm
+
+ART_QUICK_ALLOC_OBJECT_ROSALLOC art_quick_alloc_object_resolved_rosalloc, artAllocObjectFromCodeResolvedRosAlloc, /* isInitialized */ 0
+ART_QUICK_ALLOC_OBJECT_ROSALLOC art_quick_alloc_object_initialized_rosalloc, artAllocObjectFromCodeInitializedRosAlloc, /* isInitialized */ 1
+
+// The common fast path code for art_quick_alloc_object_resolved/initialized_tlab
+// and art_quick_alloc_object_resolved/initialized_region_tlab.
+//
+// a0: type, s1(rSELF): Thread::Current
+// Need to preserve a0 to the slow path.
+//
+// If isInitialized=1 then the compiler assumes the object's class has already been initialized.
+// If isInitialized=0 the compiler can only assume it's been at least resolved.
+.macro ALLOC_OBJECT_RESOLVED_TLAB_FAST_PATH slowPathLabel isInitialized
+    ld     t3, THREAD_LOCAL_POS_OFFSET(rSELF)         # Load thread_local_pos.
+    ld     a2, THREAD_LOCAL_END_OFFSET(rSELF)         # Load thread_local_end.
+    lwu    t0, MIRROR_CLASS_OBJECT_SIZE_ALLOC_FAST_PATH_OFFSET(a0)  # Load the object size.
+    add    a3, t3, t0                                 # Add object size to tlab pos.
+
+    # When isInitialized == 0, then the class is potentially not yet initialized.
+    # If the class is not yet initialized, the object size will be very large to force the branch
+    # below to be taken.
+    #
+    # See InitializeClassVisitors in class-inl.h for more details.
+    bltu   a2, a3, \slowPathLabel                    # Check if it fits, overflow works since the
+                                                       # tlab pos and end are 32 bit values.
+    # "Point of no slow path". Won't go to the slow path from here on.
+    sd     a3, THREAD_LOCAL_POS_OFFSET(rSELF)         # Store new thread_local_pos.
+    ld     a2, THREAD_LOCAL_OBJECTS_OFFSET(rSELF)     # Increment thread_local_objects.
+    addi   a2, a2, 1
+    sd     a2, THREAD_LOCAL_OBJECTS_OFFSET(rSELF)
+    POISON_HEAP_REF a0
+    sw     a0, MIRROR_OBJECT_CLASS_OFFSET(t3)         # Store the class pointer.
+
+.if \isInitialized == 0
+    # This barrier is only necessary when the allocation also requires a class initialization check.
+    #
+    # If the class is already observably initialized, then new-instance allocations are protected
+    # from publishing by the compiler which inserts its own StoreStore barrier.
+    fence                                            # Fence.
+.endif
+    mv     a0,  t3
+    jr     ra
+.endm
+
+// The common code for art_quick_alloc_object_resolved/initialized_tlab
+// and art_quick_alloc_object_resolved/initialized_region_tlab.
+.macro GENERATE_ALLOC_OBJECT_TLAB name, entrypoint, isInitialized
+ENTRY_NO_GP \name
+    # Fast path tlab allocation.
+    # a0: type, s1(rSELF): Thread::Current.
+    ALLOC_OBJECT_RESOLVED_TLAB_FAST_PATH .Lslow_path_\name, \isInitialized
+.Lslow_path_\name:
+    SETUP_SAVE_REFS_ONLY_FRAME                         # Save callee saves in case of GC.
+    mv     a1, rSELF                                  # Pass Thread::Current.
+    jal    \entrypoint                                 # (mirror::Class*, Thread*)
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END \name
+.endm
+
+GENERATE_ALLOC_OBJECT_TLAB art_quick_alloc_object_resolved_region_tlab, artAllocObjectFromCodeResolvedRegionTLAB, /* isInitialized */ 0
+GENERATE_ALLOC_OBJECT_TLAB art_quick_alloc_object_initialized_region_tlab, artAllocObjectFromCodeInitializedRegionTLAB, /* isInitialized */ 1
+GENERATE_ALLOC_OBJECT_TLAB art_quick_alloc_object_resolved_tlab, artAllocObjectFromCodeResolvedTLAB, /* isInitialized */ 0
+GENERATE_ALLOC_OBJECT_TLAB art_quick_alloc_object_initialized_tlab, artAllocObjectFromCodeInitializedTLAB, /* isInitialized */ 1
+
+// The common fast path code for art_quick_alloc_array_resolved/initialized_tlab
+// and art_quick_alloc_array_resolved/initialized_region_tlab.
+//
+// a0: type, a1: component_count, a2: total_size, s1(rSELF): Thread::Current.
+// Need to preserve a0 and a1 to the slow path.
+.macro ALLOC_ARRAY_TLAB_FAST_PATH_RESOLVED_WITH_SIZE slowPathLabel
+    li    a3, OBJECT_ALIGNMENT_MASK_TOGGLED64       # Apply alignemnt mask (addr + 7) & ~7.
+    and    a2, a2, a3                               # The mask must be 64 bits to keep high
+                                                       # bits in case of overflow.
+    # Negative sized arrays are handled here since a1 holds a zero extended 32 bit value.
+    # Negative ints become large 64 bit unsigned ints which will always be larger than max signed
+    # 32 bit int. Since the max shift for arrays is 3, it can not become a negative 64 bit int.
+    li    a3, MIN_LARGE_OBJECT_THRESHOLD
+    bgeu  a2, a3, \slowPathLabel                    # Possibly a large object, go slow path.
+
+    ld     t3, THREAD_LOCAL_POS_OFFSET(rSELF)       # Load thread_local_pos.
+    ld     t1, THREAD_LOCAL_END_OFFSET(rSELF)       # Load thread_local_end.
+    sub    t2, t1, t3                               # Compute the remaining buffer size.
+    bltu   t2, a2, \slowPathLabel                   # Check tlab for space, note that we use
+                                                    # (end - begin) to handle negative size
+                                                    # arrays. It is assumed that a negative size
+                                                    # will always be greater unsigned than region
+                                                    # size.
+
+    # "Point of no slow path". Won't go to the slow path from here on.
+    add    a2, t3, a2                               # Add object size to tlab pos.
+    sd     a2, THREAD_LOCAL_POS_OFFSET(rSELF)         # Store new thread_local_pos.
+    ld     a2, THREAD_LOCAL_OBJECTS_OFFSET(rSELF)     # Increment thread_local_objects.
+    addi   a2, a2, 1
+    sd     a2, THREAD_LOCAL_OBJECTS_OFFSET(rSELF)
+    POISON_HEAP_REF a0
+    sw     a0, MIRROR_OBJECT_CLASS_OFFSET(t3)        # Store the class pointer.
+    sw     a1, MIRROR_ARRAY_LENGTH_OFFSET(t3)        # Store the array length.
+
+    mv     a0, t3
+    jr     ra
+.endm
+
+.macro GENERATE_ALLOC_ARRAY_TLAB name, entrypoint, size_setup
+ENTRY_NO_GP \name
+    # Fast path array allocation for region tlab allocation.
+    # a0: mirror::Class* type
+    # a1: int32_t component_count
+    # s1(rSELF): Thread::Current
+    slli   a4, a1, 32                              # Create zero-extended component_count. Value
+    srli   a4, a4, 32                              # in a1 is preserved in a case of slow path.
+
+    \size_setup .Lslow_path_\name
+    ALLOC_ARRAY_TLAB_FAST_PATH_RESOLVED_WITH_SIZE .Lslow_path_\name
+.Lslow_path_\name:
+    # a0: mirror::Class* type
+    # a1: int32_t component_count
+    # a2: Thread* self
+
+    SETUP_SAVE_REFS_ONLY_FRAME                        # Save callee saves in case of GC.
+    mv     a2, rSELF                                  # Pass Thread::Current.
+    jal    \entrypoint
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END \name
+.endm
+
+.macro COMPUTE_ARRAY_SIZE_UNKNOWN slow_path
+    # Array classes are never finalizable or uninitialized, no need to check.
+    lwu    a3, MIRROR_CLASS_COMPONENT_TYPE_OFFSET(a0) # Load component type.
+    UNPOISON_HEAP_REF a3
+    lw     a3, MIRROR_CLASS_OBJECT_PRIMITIVE_TYPE_OFFSET(a3)
+    srli   a3, a3, PRIMITIVE_TYPE_SIZE_SHIFT_SHIFT   # Component size shift is in high 16 bits.
+    sll    a2, a4, a3                               # Calculate data size.
+                                                       # Add array data offset and alignment.
+    addi   a2, a2, (MIRROR_INT_ARRAY_DATA_OFFSET + OBJECT_ALIGNMENT_MASK)
+#if MIRROR_WIDE_ARRAY_DATA_OFFSET != MIRROR_INT_ARRAY_DATA_OFFSET + 4
+#error Long array data offset must be 4 greater than int array data offset.
+#endif
+
+    addi   a3, a3, 1                                 # Add 4 to the length only if the component
+    andi   a3, a3, 4                                 # size shift is 3 (for 64 bit alignment).
+    add    a2, a2, a3
+.endm
+
+.macro COMPUTE_ARRAY_SIZE_8 slow_path
+    # Add array data offset and alignment.
+    addi   a2, a4, (MIRROR_INT_ARRAY_DATA_OFFSET + OBJECT_ALIGNMENT_MASK)
+.endm
+
+.macro COMPUTE_ARRAY_SIZE_16 slow_path
+    slli   a2, a4, 1
+    # Add array data offset and alignment.
+    addi   a2, a2, (MIRROR_INT_ARRAY_DATA_OFFSET + OBJECT_ALIGNMENT_MASK)
+.endm
+
+.macro COMPUTE_ARRAY_SIZE_32 slow_path
+    slli   a2, a4, 2
+    # Add array data offset and alignment.
+    addi   a2, a2, (MIRROR_INT_ARRAY_DATA_OFFSET + OBJECT_ALIGNMENT_MASK)
+.endm
+
+.macro COMPUTE_ARRAY_SIZE_64 slow_path
+    slli   a2, a4, 3
+    # Add array data offset and alignment.
+    addi   a2, a2, (MIRROR_WIDE_ARRAY_DATA_OFFSET + OBJECT_ALIGNMENT_MASK)
+.endm
+
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved_region_tlab, artAllocArrayFromCodeResolvedRegionTLAB, COMPUTE_ARRAY_SIZE_UNKNOWN
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved8_region_tlab, artAllocArrayFromCodeResolvedRegionTLAB, COMPUTE_ARRAY_SIZE_8
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved16_region_tlab, artAllocArrayFromCodeResolvedRegionTLAB, COMPUTE_ARRAY_SIZE_16
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved32_region_tlab, artAllocArrayFromCodeResolvedRegionTLAB, COMPUTE_ARRAY_SIZE_32
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved64_region_tlab, artAllocArrayFromCodeResolvedRegionTLAB, COMPUTE_ARRAY_SIZE_64
+
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved_tlab, artAllocArrayFromCodeResolvedTLAB, COMPUTE_ARRAY_SIZE_UNKNOWN
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved8_tlab, artAllocArrayFromCodeResolvedTLAB, COMPUTE_ARRAY_SIZE_8
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved16_tlab, artAllocArrayFromCodeResolvedTLAB, COMPUTE_ARRAY_SIZE_16
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved32_tlab, artAllocArrayFromCodeResolvedTLAB, COMPUTE_ARRAY_SIZE_32
+GENERATE_ALLOC_ARRAY_TLAB art_quick_alloc_array_resolved64_tlab, artAllocArrayFromCodeResolvedTLAB, COMPUTE_ARRAY_SIZE_64
+
+    /*
+     * Macro for resolution and initialization of indexed DEX file
+     * constants such as classes and strings. $a0 is both input and
+     * output.
+     */
+.macro ONE_ARG_SAVE_EVERYTHING_DOWNCALL name, entrypoint, runtime_method_offset = RUNTIME_SAVE_EVERYTHING_METHOD_OFFSET
+    .extern \entrypoint
+ENTRY_NO_GP \name
+    SETUP_SAVE_EVERYTHING_FRAME \runtime_method_offset  # Save everything in case of GC.
+    la      t6, \entrypoint
+    move    a1, rSELF                # Pass Thread::Current (in delay slot).
+    jalr    t6                       # (uint32_t index, Thread*)
+    beqz    a0, 1f
+    RESTORE_SAVE_EVERYTHING_FRAME 0  # Restore everything except $a0.
+    jalr    zero, ra                 # Return on success.
+1:
+    DELIVER_PENDING_EXCEPTION_FRAME_READY
+END \name
+.endm
+
+.macro ONE_ARG_SAVE_EVERYTHING_DOWNCALL_FOR_CLINIT name, entrypoint
+    ONE_ARG_SAVE_EVERYTHING_DOWNCALL \name, \entrypoint, RUNTIME_SAVE_EVERYTHING_FOR_CLINIT_METHOD_OFFSET
+.endm
+
+    /*
+     * Entry from managed code to resolve a method handle. On entry, A0 holds the method handle
+     * index. On success the MethodHandle is returned, otherwise an exception is raised.
+     */
+ONE_ARG_SAVE_EVERYTHING_DOWNCALL art_quick_resolve_method_handle, artResolveMethodHandleFromCode
+
+    /*
+     * Entry from managed code to resolve a method type. On entry, A0 holds the method type index.
+     * On success the MethodType is returned, otherwise an exception is raised.
+     */
+ONE_ARG_SAVE_EVERYTHING_DOWNCALL art_quick_resolve_method_type, artResolveMethodTypeFromCode
+
+    /*
+     * Entry from managed code to resolve a string, this stub will allocate a String and deliver an
+     * exception on error. On success the String is returned. A0 holds the string index. The fast
+     * path check for hit in strings cache has already been performed.
+     */
+ONE_ARG_SAVE_EVERYTHING_DOWNCALL art_quick_resolve_string, artResolveStringFromCode
+
+    /*
+     * Entry from managed code when uninitialized static storage, this stub will run the class
+     * initializer and deliver the exception on error. On success the static storage base is
+     * returned.
+     */
+ONE_ARG_SAVE_EVERYTHING_DOWNCALL_FOR_CLINIT art_quick_initialize_static_storage, artInitializeStaticStorageFromCode
+
+    /*
+     * Entry from managed code when dex cache misses for a type_idx.
+     */
+ONE_ARG_SAVE_EVERYTHING_DOWNCALL_FOR_CLINIT art_quick_resolve_type, artResolveTypeFromCode
+
+    /*
+     * Entry from managed code when type_idx needs to be checked for access and dex cache may also
+     * miss.
+     */
+ONE_ARG_SAVE_EVERYTHING_DOWNCALL art_quick_resolve_type_and_verify_access, artResolveTypeAndVerifyAccessFromCode
+
+    /*
+     * Called by managed code when the value in rSUSPEND has been decremented to 0.
+     */
+       .extern artTestSuspendFromCode
+ENTRY_NO_GP art_quick_test_suspend
+    SETUP_SAVE_EVERYTHING_FRAME RUNTIME_SAVE_EVERYTHING_FOR_SUSPEND_CHECK_METHOD_OFFSET
+                                              # save everything for stack crawl
+    mv     a0, rSELF
+    jal    artTestSuspendFromCode             # (Thread*)
+
+    RESTORE_SAVE_EVERYTHING_FRAME
+    jalr   zero, ra
+    nop
+END art_quick_test_suspend
+
+    /*
+     * Called by managed code that is attempting to call a method on a proxy class. On entry
+     * r0 holds the proxy method; r1, r2 and r3 may contain arguments.
+     */
+    .extern artQuickProxyInvokeHandler
+ENTRY art_quick_proxy_invoke_handler
+    SETUP_SAVE_REFS_AND_ARGS_FRAME_WITH_METHOD_IN_A0
+    mv      a2, rSELF             # pass Thread::Current
+    mv      a3, sp               # pass $sp
+    jal     artQuickProxyInvokeHandler  # (Method* proxy method, receiver, Thread*, SP)
+    ld      t0, THREAD_EXCEPTION_OFFSET(rSELF) # load Thread::Current()->exception_
+    addi    sp, sp, REFS_AND_ARGS_MINUS_REFS_SIZE  # skip a0-a7 and f10-f17
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    fmv.d.x f10, a0                # place return value to FP return value
+    bne     t0, zero, 1f
+    fmv.d.x f11, a1                # place return value to FP return value
+    jalr    zero, ra
+1:
+    DELIVER_PENDING_EXCEPTION
+END art_quick_proxy_invoke_handler
+
+    /*
+     * Called to resolve an imt conflict.
+     * a0 is the conflict ArtMethod.
+     * t0 is a hidden argument that holds the target interface method's dex method index.
+     *
+     * Mote that this stub writes to v0-v1, a0, t0-t3, t5-t6, f0-f11, f20-f23.
+     */
+     .extern artLookupResolvedMethod
+     # .extern __atomic_load_16
+     .extern artInvokeAtomicMethod       # For __int128_t std::atomic::load(std::memory_order).
+ENTRY art_quick_imt_conflict_trampoline
+    SETUP_SAVE_REFS_AND_ARGS_FRAME_INTERNAL /* save_s4_thru_s8 */ 0
+
+    ld      t1, FRAME_SIZE_SAVE_REFS_AND_ARGS(sp)  # $t1 = referrer.
+    // If the method is obsolete, just go through the dex cache miss slow path.
+    // The obsolete flag is set with suspended threads, so we do not need an acquire operation here.
+    lw      t6, ART_METHOD_ACCESS_FLAGS_OFFSET(t1)  # $t6 = access flags.
+    slliw     t6, t6, 31 - ACC_OBSOLETE_METHOD_SHIFT  # Move obsolete method bit to sign bit.
+    bltz   t6, .Limt_conflict_trampoline_dex_cache_miss
+    lwu     t1, ART_METHOD_DECLARING_CLASS_OFFSET(t1)  # $t1 = declaring class (no read barrier).
+    lwu     t1, MIRROR_CLASS_DEX_CACHE_OFFSET(t1)  # $t1 = dex cache (without read barrier).
+    UNPOISON_HEAP_REF t1
+    # la      t6, __atomic_load_16
+    la      t6, artInvokeAtomicMethod
+    ld      t1, MIRROR_DEX_CACHE_RESOLVED_METHODS_OFFSET(t1)  # $t1 = dex cache methods array.
+
+    slli    s2, t0, 32                         # $s2 = zero-extended method index
+    srli    s2, s2, 32                         # (callee-saved).
+    ld      s3, ART_METHOD_JNI_OFFSET_64(a0)      # $s3 = ImtConflictTable (callee-saved).
+
+    slli    t0, t0, 64-METHOD_DEX_CACHE_HASH_BITS  # $t0 = slot index.
+    srli    t0, t0, 64-METHOD_DEX_CACHE_HASH_BITS
+
+    li      a1, STD_MEMORY_ORDER_RELAXED           # $a1 = std::memory_order_relaxed.
+    slli    t5, t0, POINTER_SIZE_SHIFT + 1         # $a0 = DexCache method slot address.
+    add     a0, t1, t5
+    jalr    t6                                     # [$a0, $a1] = __atomic_load_16($a0, $a1).
+
+    bne    a1, s2, .Limt_conflict_trampoline_dex_cache_miss  # Branch if method index miss.
+
+.Limt_table_iterate:
+    ld      t1, 0(s3)                             # Load next entry in ImtConflictTable.
+    # Branch if found.
+    beq     t1, a0, .Limt_table_found
+    nop
+    # If the entry is null, the interface method is not in the ImtConflictTable.
+    beq   t1, zero, .Lconflict_trampoline
+    # Iterate over the entries of the ImtConflictTable.
+    addi  s3, s3, 2 * __SIZEOF_POINTER__        # Iterate to the next entry.
+    j      .Limt_table_iterate
+
+.Limt_table_found:
+    # We successfully hit an entry in the table. Load the target method and jump to it.
+    .cfi_remember_state
+    ld      a0, __SIZEOF_POINTER__(s3)
+    ld      t6, ART_METHOD_QUICK_CODE_OFFSET_64(a0)
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME /* restore_s4_thru_s8 */ 0
+    jr      t6
+    .cfi_restore_state
+
+.Lconflict_trampoline:
+    # Call the runtime stub to populate the ImtConflictTable and jump to the resolved method.
+    .cfi_remember_state
+    # interface method in a0
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME_A1             # Restore this.
+
+    INVOKE_TRAMPOLINE_BODY artInvokeInterfaceTrampoline, /* save_s4_thru_s8_only */ 1
+    .cfi_restore_state
+
+.Limt_conflict_trampoline_dex_cache_miss:
+    # We're not creating a proper runtime method frame here,
+    # artLookupResolvedMethod() is not allowed to walk the stack.
+    la      t6, artLookupResolvedMethod
+    ld      a1, FRAME_SIZE_SAVE_REFS_AND_ARGS(sp)  # $a1 = referrer.
+    slliw   a0, s2, 0                              # $a0 = sign-extended method index.
+    jalr    t6                                     # (uint32_t method_index, ArtMethod* referrer).
+
+    # If the method wasn't resolved, skip the lookup and go to artInvokeInterfaceTrampoline().
+    beq     a0, zero, .Lconflict_trampoline
+    nop
+    j      .Limt_table_iterate
+END art_quick_imt_conflict_trampoline
+
+   .extern artQuickResolutionTrampoline
+ENTRY art_quick_resolution_trampoline
+    SETUP_SAVE_REFS_AND_ARGS_FRAME
+    mv      a2, rSELF             # pass Thread::Current
+    mv      a3, sp               # pass $sp
+    jal     artQuickResolutionTrampoline  # (Method* called, receiver, Thread*, SP)
+    beq     a0, zero, 1f
+    mv      t6, a0               # code pointer must be in $t6 to generate the global pointer
+    ld      a0, 0(sp)            # load resolved method in $a0
+                                   # artQuickResolutionTrampoline puts resolved method in *SP
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+    jalr    zero, t6             # tail call to method
+    nop
+1:
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+    DELIVER_PENDING_EXCEPTION
+END art_quick_resolution_trampoline
+
+    .extern artQuickGenericJniTrampoline
+    .extern artQuickGenericJniEndTrampoline
+ENTRY art_quick_generic_jni_trampoline
+    SETUP_SAVE_REFS_AND_ARGS_FRAME_WITH_METHOD_IN_A0
+    mv      s0, sp               # save $sp to fp(s0)
+
+    # prepare for call to artQuickGenericJniTrampoline(Thread*, SP)
+    mv      a0, rSELF             # pass Thread::Current
+    mv      a1, sp               # pass $sp
+    addi    sp, sp, -2047        # reserve space on the stack. -5120 bytes
+    addi    sp, sp, -2047
+    addi    sp, sp, -1026
+    jal     artQuickGenericJniTrampoline   # (Thread*, SP)
+
+    # The C call will have registered the complete save-frame on success.
+    # The result of the call is:
+    # a0: ptr to native code, 0 on error.
+    # a1: ptr to the bottom of the used area of the alloca, can restore stack till here.
+    beq     a0, zero, 1f         # check entry error
+    mv      t6, a0               # save the code ptr
+    mv      sp, a1               # release part of the alloca
+
+    # Load parameters from stack into registers
+    ld      a0,   0(sp)
+    ld      a1,   8(sp)
+    ld      a2,  16(sp)
+    ld      a3,  24(sp)
+    ld      a4,  32(sp)
+    ld      a5,  40(sp)
+    ld      a6,  48(sp)
+    ld      a7,  56(sp)
+
+    fld     f10,  64(sp)
+    fld     f11,  72(sp)
+    fld     f12, 80(sp)
+    fld     f13, 88(sp)
+    fld     f14, 96(sp)
+    fld     f15, 104(sp)
+    fld     f16, 112(sp)
+    fld     f17, 120(sp)
+    addi    sp, sp, 128
+    jalr    t6                    # native call
+
+    # result sign extension is handled in C code
+    # prepare for call to artQuickGenericJniEndTrampoline(Thread*, result, result_f)
+    mv      a1, a0
+    mv      a0, rSELF             # pass Thread::Current
+    fmv.x.d   a2, f10
+    jal     artQuickGenericJniEndTrampoline
+
+    ld      t0, THREAD_EXCEPTION_OFFSET(rSELF) # load Thread::Current()->exception_
+    mv      sp, s0               # tear down the alloca (fp(s0) --> sp)
+    bne     t0, zero, 1f         # check for pending exceptions
+
+    # tear dpown the callee-save frame
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+
+    fmv.d.x f10, a0              # place return value to FP return value
+    ret
+1:
+    ld      t0, THREAD_TOP_QUICK_FRAME_OFFSET(rSELF)
+    addi    sp, t0, -1  // Remove the GenericJNI tag.
+    # This will create a new save-all frame, required by the runtime.
+    DELIVER_PENDING_EXCEPTION
+END art_quick_generic_jni_trampoline
+
+    .extern artQuickToInterpreterBridge
+ENTRY art_quick_to_interpreter_bridge
+    SETUP_SAVE_REFS_AND_ARGS_FRAME
+    mv      a1, rSELF             # pass Thread::Current
+    mv      a2, sp               # pass $sp
+    jal     artQuickToInterpreterBridge    # (Method* method, Thread*, SP)
+
+    ld      t0, THREAD_EXCEPTION_OFFSET(rSELF) # load Thread::Current()->exception_
+    addi    sp, sp, REFS_AND_ARGS_MINUS_REFS_SIZE  # skip a0-a7 and f10-f17
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    fmv.d.x f10, a0       # dmtc1   a0, f0  # place return value to FP return value
+    bne     t0, zero, 1f
+    fmv.d.x f11, a1       # dmtc1   a1, f1  # place return value to FP return value
+    jalr    zero, ra
+1:
+    DELIVER_PENDING_EXCEPTION
+END art_quick_to_interpreter_bridge
+
+
+    .extern artInvokeObsoleteMethod
+ENTRY art_invoke_obsolete_method_stub
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
+    mv      a1, rSELF                 # pass Thread::Current
+    jal     artInvokeObsoleteMethod    # (Method* method, Thread* self)
+END art_invoke_obsolete_method_stub
+
+ .extern artInstrumentationMethodEntryFromCode
+    .extern artInstrumentationMethodExitFromCode
+ENTRY art_quick_instrumentation_entry
+    SETUP_SAVE_REFS_AND_ARGS_FRAME
+    # Preserve $a0 knowing there is a spare slot in kSaveRefsAndArgs.
+    sd      a0, 8(sp)     # Save arg0.
+    mv      a3, sp        # Pass $sp.
+    mv      a2, rSELF      # pass Thread::Current
+    jal     artInstrumentationMethodEntryFromCode  # (Method*, Object*, Thread*, SP)
+    beq     a0, zero, .Ldeliver_instrumentation_entry_exception
+                            # Deliver exception if we got nullptr as function.
+    mv      t6, a0        # $t6 holds reference to code
+    ld      a0, 8(sp)     # Restore arg0.
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+    la      ra, art_quick_instrumentation_exit
+    jalr    zero, 0(t6)   # call method, returning to art_quick_instrumentation_exit.
+.Ldeliver_instrumentation_entry_exception:
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+    DELIVER_PENDING_EXCEPTION
+END art_quick_instrumentation_entry
+
+    .hidden art_quick_instrumentation_exit
+ENTRY_NO_GP art_quick_instrumentation_exit
+    mv    ra, zero      # RA points here, so clobber with 0 for later checks.
+    SETUP_SAVE_EVERYTHING_FRAME
+
+    addi    a3, sp, 96    #offset 96   Pass fpr_res pointer ($fa0 in SAVE_EVERYTHING_FRAME).
+    addi    a2, sp, 304   #offset 304  Pass gpr_res pointer ($a0 in SAVE_EVERYTHING_FRAME).
+    mv      a1, sp        # Pass $sp.
+    mv      a0, rSELF     # pass Thread::Current
+    jal     artInstrumentationMethodExitFromCode  # (Thread*, SP, gpr_res*, fpr_res*)
+
+    beq     a0, zero, .Ldo_deliver_instrumentation_exception
+                            # Deliver exception if we got nullptr as function.
+    nop
+    bne    a1, zero, .Ldeoptimize
+
+    # Normal return.
+    sd      a0, (FRAME_SIZE_SAVE_EVERYTHING-8)(sp)  # Set return pc.
+    RESTORE_SAVE_EVERYTHING_FRAME
+    jalr    zero, ra
+.Ldo_deliver_instrumentation_exception:
+    DELIVER_PENDING_EXCEPTION_FRAME_READY
+.Ldeoptimize:
+    la      t0, art_quick_deoptimize
+    sd      a1, (FRAME_SIZE_SAVE_EVERYTHING-8)(sp)
+                            # Fake a call from instrumentation return pc.
+    jalr    zero, t0
+
+END art_quick_instrumentation_exit
+
+    /*
+     * Instrumentation has requested that we deoptimize into the interpreter. The deoptimization
+     * will long jump to the upcall with a special exception of -1.
+     */
+
+    .hidden art_quick_deoptimize
+    .extern artDeoptimize
+ENTRY_NO_GP_CUSTOM_CFA art_quick_deoptimize, FRAME_SIZE_SAVE_EVERYTHING
+    # SETUP_SAVE_EVERYTHING_FRAME has been done by art_quick_instrumentation_exit.
+    .cfi_rel_offset 31, 488
+    .cfi_rel_offset 30, 480
+    .cfi_rel_offset 28, 472
+    .cfi_rel_offset 25, 464
+    .cfi_rel_offset 24, 456
+    .cfi_rel_offset 23, 448
+    .cfi_rel_offset 22, 440
+    .cfi_rel_offset 21, 432
+    .cfi_rel_offset 20, 424
+    .cfi_rel_offset 19, 416
+    .cfi_rel_offset 18, 408
+    .cfi_rel_offset 17, 400
+    .cfi_rel_offset 16, 392
+    .cfi_rel_offset 15, 384
+    .cfi_rel_offset 14, 376
+    .cfi_rel_offset 13, 368
+    .cfi_rel_offset 12, 360
+    .cfi_rel_offset 11, 352
+    .cfi_rel_offset 10, 344
+    .cfi_rel_offset 9, 336
+    .cfi_rel_offset 8, 328
+    .cfi_rel_offset 7, 320
+    .cfi_rel_offset 6, 312
+    .cfi_rel_offset 5, 304
+    .cfi_rel_offset 4, 296
+    .cfi_rel_offset 3, 288
+    .cfi_rel_offset 2, 280
+    .cfi_rel_offset 1, 272
+
+    mv      a0, rSELF      # pass Thread::current
+    jal     artDeoptimize   # artDeoptimize(Thread*)
+    ebreak
+END art_quick_deoptimize
+
+    /*
+     * Compiled code has requested that we deoptimize into the interpreter. The deoptimization
+     * will long jump to the upcall with a special exception of -1.
+     */
+    .extern artDeoptimizeFromCompiledCode
+ENTRY_NO_GP art_quick_deoptimize_from_compiled_code
+    SETUP_SAVE_EVERYTHING_FRAME
+    mv       a1, rSELF                       # pass Thread::current
+    jal      artDeoptimizeFromCompiledCode    # (DeoptimizationKind, Thread*)
+END art_quick_deoptimize_from_compiled_code
+
+
+/* java.lang.String.compareTo(String anotherString) */
+ENTRY_NO_GP art_quick_string_compareto
+/* $a0 holds address of "this" */
+/* $a1 holds address of "anotherString" */
+    mv     a2, zero
+    mv     a3, zero                               # return 0 (it returns a2 - a3)
+    beq    a0, a1, .Lstring_compareto_length_diff # this and anotherString are the same object
+
+#if (STRING_COMPRESSION_FEATURE)
+    lw     a4, MIRROR_STRING_COUNT_OFFSET(a0)     # 'count' field of this
+    lw     a5, MIRROR_STRING_COUNT_OFFSET(a1)     # 'count' field of anotherString
+    sra    a2, a4, 1                              # this.length()
+    sra    a3, a5, 1                              # anotherString.length()
+#else
+    lw     a2, MIRROR_STRING_COUNT_OFFSET(a0)     # this.length()
+    lw     a3, MIRROR_STRING_COUNT_OFFSET(a1)     # anotherString.length()
+#endif
+
+    sltu    t4, a2, a3                            #MINu   t2, a2, a3
+    bnez    t4, .Llessthan
+    mv      t2, a3
+    j       .Lmorethan
+.Llessthan:
+    mv      t2, a2
+
+.Lmorethan:
+    # $t2 now holds min(this.length(),anotherString.length())
+
+    # while min(this.length(),anotherString.length())-i != 0
+    beqz    t2, .Lstring_compareto_length_diff # if $t2==0
+                                               #     return (this.length() - anotherString.length())
+
+#if (STRING_COMPRESSION_FEATURE)
+    # Differ cases:
+    # dext   a6, a4, 0, 1                    # TODO: T-HEAD
+    beqz      a6, .Lstring_compareto_this_is_compressed
+    # dext   a6, a5, 0, 1                      # In branch delay slot.
+    beqz     a6, .Lstring_compareto_that_is_compressed
+    j      .Lstring_compareto_both_not_compressed
+
+.Lstring_compareto_this_is_compressed:
+    beqz   a6, .Lstring_compareto_both_compressed
+    /* If (this->IsCompressed() && that->IsCompressed() == false) */
+.Lstring_compareto_loop_comparison_this_compressed:
+    lb     t0, MIRROR_STRING_VALUE_OFFSET(a0)
+    lh     t1, MIRROR_STRING_VALUE_OFFSET(a1)
+    bne    t0, t1, .Lstring_compareto_char_diff
+    addi   a0, a0, 1      # point at this.charAt(i++) - compressed
+    addi   t2, t2, -1      # new value of min(this.length(),anotherString.length())-i
+    addi   a1, a1, 2      # point at anotherString.charAt(i++) - uncompressed
+    bnez   t2, .Lstring_compareto_loop_comparison_this_compressed
+    sub    a0, a2, a3    # return (this.length() - anotherString.length())
+    jalr   zero, ra
+
+.Lstring_compareto_that_is_compressed:
+    lh     t0, MIRROR_STRING_VALUE_OFFSET(a0)
+    lb     t1, MIRROR_STRING_VALUE_OFFSET(a1)
+    bne    t0, t1, .Lstring_compareto_char_diff
+    addi   a0, a0, 2      # point at this.charAt(i++) - uncompressed
+    addi   t2, t2, -1      # new value of min(this.length(),anotherString.length())-i
+    addi   a1, a1, 1      # point at anotherString.charAt(i++) - compressed
+    bnez   t2, .Lstring_compareto_that_is_compressed
+    sub    a0, a2, a3    # return (this.length() - anotherString.length())
+    jalr   zero, ra
+
+.Lstring_compareto_both_compressed:
+    lb     t0, MIRROR_STRING_VALUE_OFFSET(a0)
+    lb     t1, MIRROR_STRING_VALUE_OFFSET(a1)
+    bne    t0, t1, .Lstring_compareto_char_diff
+    addi   a0, a0, 1      # point at this.charAt(i++) - compressed
+    addi   t2, t2, -1      # new value of min(this.length(),anotherString.length())-i
+    addi   a1, a1, 1      # point at anotherString.charAt(i++) - compressed
+    bnez   t2, .Lstring_compareto_both_compressed
+
+    sub   a0, a2, a3    # return (this.length() - anotherString.length())
+    jalr   zero, ra
+#endif
+
+.Lstring_compareto_both_not_compressed:
+    lh     t0, MIRROR_STRING_VALUE_OFFSET(a0)    # while this.charAt(i) == anotherString.charAt(i)
+    lh     t1, MIRROR_STRING_VALUE_OFFSET(a1)
+    bne    t0, t1, .Lstring_compareto_char_diff  # if this.charAt(i) != anotherString.charAt(i)
+                            #     return (this.charAt(i) - anotherString.charAt(i))
+    addi   a0, a0, 2      # point at this.charAt(i++)
+    addi   t2, t2, -1      # new value of min(this.length(),anotherString.length())-i
+    addi   a1, a1, 2      # point at anotherString.charAt(i++)
+    bnez   t2, .Lstring_compareto_both_not_compressed
+
+.Lstring_compareto_length_diff:
+    sub    a0, a2, a3    # return (this.length() - anotherString.length())
+    jalr   zero, ra
+
+.Lstring_compareto_char_diff:
+    sub    a0, t0, t1    # return (this.charAt(i) - anotherString.charAt(i))
+    jalr   zero, ra
+
+END art_quick_string_compareto
+
+/* java.lang.String.indexOf(int ch, int fromIndex=0) */
+ENTRY_NO_GP art_quick_indexof
+/* $a0 holds address of "this" */
+/* $a1 holds "ch" */
+/* $a2 holds "fromIndex" */
+#if (STRING_COMPRESSION_FEATURE)
+    lw     a3, MIRROR_STRING_COUNT_OFFSET(a0)     # 'count' field of this
+#else
+    lw     t0, MIRROR_STRING_COUNT_OFFSET(a0)     # this.length()
+#endif
+    slt    t4, a2, zero      # if fromIndex < 0
+    bnez   t4, .Lneqz        # seleqz $a2, $a2, $at    # fromIndex = 0;
+    j      .Leqz
+.Lneqz:
+    li     a2, 0
+
+.Leqz:
+#if (STRING_COMPRESSION_FEATURE)
+    srl    t0, a3, 1        # $a3 holds count (with flag) and $t0 holds actual length
+#endif
+    sub    t0,  t0, a2      # this.length() - fromIndex
+    li     t5,  -1          #     return -1;
+    blez   t0,  6f          # if this.length()-fromIndex <= 0
+
+#if (STRING_COMPRESSION_FEATURE)
+    andi   a3, a3, 1       # dext   $a3, $a3, 0, 1   # Extract compression flag.
+    beqz   a3, .Lstring_indexof_compressed
+#endif
+
+    sll    t5, a2, 1      # $a0 += $a2 * 2
+    add    a0, a0, t5     #  "  ditto  "
+    mv     t5, a2         # Set i to fromIndex.
+
+1:
+    lhu    t3, MIRROR_STRING_VALUE_OFFSET(a0)     # if this.charAt(i) == ch
+    beq    t3, a1, 6f                             #     return i;
+    addi   a0, a0, 2        # i++
+    addi   t0, t0, -1       # this.length() - i
+    addi   t5, t5, 1        # i++
+    bnez   t0, 1b           # while this.length() - i > 0
+
+    li     t5, -1           # if this.length() - i <= 0
+                            #     return -1;
+
+6:
+    mv      a0,   t5
+    jr      ra
+
+
+#if (STRING_COMPRESSION_FEATURE)
+.Lstring_indexof_compressed:
+    mv   a4, a0         # Save a copy in $a4 to later compute result.
+    add  a0, a0, a2     # $a0 += $a2
+
+.Lstring_indexof_compressed_loop:
+    lbu    t3, MIRROR_STRING_VALUE_OFFSET(a0)
+    beq    t3, a1, .Lstring_indexof_compressed_matched
+    addi   t0, t0, -1
+    addi   a0, a0, 1
+    bgtz   t0, .Lstring_indexof_compressed_loop
+
+.Lstring_indexof_nomatch:
+    li     a0, -1          # return -1;
+    jalr   zero, ra
+
+.Lstring_indexof_compressed_matched:
+    sub    a0, a0, a4    # return (current - start);
+    jalr   zero, ra
+
+#endif
+END art_quick_indexof
+
+    /*
+     * Create a function `name` calling the ReadBarrier::Mark routine,
+     * getting its argument and returning its result through register
+     * `reg`, saving and restoring all caller-save registers.
+     */
+.macro READ_BARRIER_MARK_REG name, reg
+ENTRY \name
+    // Null check so that we can load the lock word.
+    bne   \reg, zero, .Lnot_null_\name
+    nop
+.Lret_rb_\name:
+    jalr    zero, ra   # return
+.Lnot_null_\name:
+    // Check lock word for mark bit, if marked return.
+    lw      t6, MIRROR_OBJECT_LOCK_WORD_OFFSET(\reg)
+
+    slliw     t5, t6, 31 - LOCK_WORD_MARK_BIT_SHIFT     # Move mark bit to sign bit.
+    bltz    t5, .Lret_rb_\name
+#if (LOCK_WORD_STATE_SHIFT != 30) || (LOCK_WORD_STATE_FORWARDING_ADDRESS != 3)
+    // The below code depends on the lock word state being in the highest bits
+    // and the "forwarding address" state having all bits set.
+#error "Unexpected lock word state shift or forwarding address state value."
+#endif
+    // Test that both the forwarding state bits are 1.
+    slliw     t5, t6, 1
+    and     t5, t5, t6                               # Sign bit = 1 IFF both bits are 1.
+    bltz    t5, .Lret_forwarding_address\name
+    # .set pop
+
+    addi  sp, sp, -280
+    .cfi_adjust_cfa_offset 280
+
+    sd      ra, 272(sp)
+    .cfi_rel_offset 31, 312
+    sd      t5, 264(sp)
+    .cfi_rel_offset 15, 296
+    sd      t4, 256(sp)
+    .cfi_rel_offset 14, 288
+    sd      t3, 248(sp)
+    .cfi_rel_offset 13, 280
+    sd      t2, 240(sp)
+    .cfi_rel_offset 12, 272
+    sd      t1, 232(sp)
+    .cfi_rel_offset 11, 264
+    sd      t0, 224(sp)
+    .cfi_rel_offset 10, 256
+    sd      a7, 216(sp)
+    .cfi_rel_offset 9, 248
+    sd      a6, 208(sp)
+    .cfi_rel_offset 8, 240
+    sd      a5, 200(sp)
+    .cfi_rel_offset 7, 232
+    sd      a4, 192(sp)
+    .cfi_rel_offset 6, 224
+    sd      a3, 184(sp)
+    .cfi_rel_offset 5, 216
+    sd      a2, 176(sp)
+    .cfi_rel_offset 4, 208
+    sd      a1, 168(sp)
+    .cfi_rel_offset 3, 200
+    sd      a0, 160(sp)
+    .cfi_rel_offset 2, 192
+
+    la     t6, artReadBarrierMark
+
+    fsd    f31, 152(sp)
+    fsd    f30, 144(sp)
+    fsd    f29, 136(sp)
+    fsd    f28, 128(sp)
+    fsd    f17, 120(sp)
+    fsd    f16, 112(sp)
+    fsd    f15, 104(sp)
+    fsd    f14,  96(sp)
+    fsd    f13,  88(sp)
+    fsd    f12,  80(sp)
+    fsd    f11,  72(sp)
+    fsd    f10,  64(sp)
+    fsd    f7,   56(sp)
+    fsd    f6,   48(sp)
+    fsd    f5,   40(sp)
+    fsd    f4,   32(sp)
+    fsd    f3,   24(sp)
+    fsd    f2,   16(sp)
+    fsd    f1,   8(sp)
+    fsd    f0,   0(sp)
+
+    .ifnc \reg, a0
+      mv  a0, \reg           # pass obj from `reg` in a0
+    .endif
+    jalr    t6                 # v0 <- artReadBarrierMark(obj)
+
+    ld      ra, 272(sp)
+    .cfi_restore 31
+    ld      t5, 264(sp)
+    .cfi_restore 15
+    ld      t4, 256(sp)
+    .cfi_restore 14
+    ld      t3, 248(sp)
+    .cfi_restore 13
+    ld      t2, 240(sp)
+    .cfi_restore 12
+    ld      t1, 232(sp)
+    .cfi_restore 11
+    ld      t0, 224(sp)
+    .cfi_restore 10
+    ld      a7, 216(sp)
+    .cfi_restore 9
+    ld      a6, 208(sp)
+    .cfi_restore 8
+    ld      a5, 200(sp)
+    .cfi_restore 7
+    ld      a4, 192(sp)
+    .cfi_restore 6
+    ld      a3, 184(sp)
+    .cfi_restore 5
+    ld      a2, 176(sp)
+    .cfi_restore 4
+    ld      a1, 168(sp)
+    .cfi_restore 3
+
+    .ifnc \reg, a0
+      mv  \reg, a0           # `reg` <- a0
+      ld    a0, 160(sp)
+      .cfi_restore 2
+    .endif
+
+    fld    f31, 152(sp)
+    fld    f30, 144(sp)
+    fld    f29, 136(sp)
+    fld    f28, 128(sp)
+    fld    f17, 120(sp)
+    fld    f16, 112(sp)
+    fld    f15, 104(sp)
+    fld    f14,  96(sp)
+    fld    f13,  88(sp)
+    fld    f12,  80(sp)
+    fld    f11,   72(sp)
+    fld    f10,   64(sp)
+    fld    f7,   56(sp)
+    fld    f6,   48(sp)
+    fld    f5,   40(sp)
+    fld    f4,   32(sp)
+    fld    f3,   24(sp)
+    fld    f2,   16(sp)
+    fld    f1,   8(sp)
+    fld    f0,   0(sp)
+
+    addi    sp, sp, 280
+    .cfi_adjust_cfa_offset -280
+    jalr    zero, ra
+
+.Lret_forwarding_address\name:
+    // Shift left by the forwarding address shift. This clears out the state bits since they are
+    // in the top 2 bits of the lock word.
+    slliw     \reg, t6, LOCK_WORD_STATE_FORWARDING_ADDRESS_SHIFT
+    slli     \reg, \reg, 32
+    srli     \reg, \reg, 32  # Get address from \reg[31:0] and Make sure the address is zero-extended.
+    jalr    zero, ra
+END \name
+.endm
+
+// Note that art_quick_read_barrier_mark_regXX corresponds to register XX+1.
+// ZERO, RA, SP, GP (registers 0 - 3) is reserved.
+// TP/TR (register 4) is  reserved as thread register
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg04, t0
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg05, t1
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg06, t2
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg07, s0
+// S1 (register 9) is reserved as thread register.
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg09, a0
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg10, a1
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg11, a2
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg12, a3
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg13, a4
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg14, a5
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg15, a6
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg16, a7
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg17, s2
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg18, s3
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg19, s4
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg20, s5
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg21, s6
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg22, s7
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg23, s8
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg24, s9
+READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg25, s10
+// S11 (register 27) is reserved as suspended register.
+// T3, T4, T5, t6(registers 28 - 31) are reserved as temporary/scratch registers.
+
+
+// Caller code:
+// Short constant offset/index:
+//  ld      $t6, pReadBarrierMarkReg00
+//  beqzc   $t6, skip_call
+//  nop
+//  jialc   $t6, thunk_disp
+// skip_call:
+//  lwu     `out`, ofs(`obj`)
+// [dsubu   `out`, $zero, `out`
+//  dext    `out`, `out`, 0, 32]  # Unpoison reference.
+.macro BRB_FIELD_SHORT_OFFSET_ENTRY obj
+    # Explicit null check. May be redundant (for array elements or when the field
+    # offset is larger than the page size, 4KB).
+    # $ra will be adjusted to point to lwu's stack map when throwing NPE.
+    beq   \obj, zero, .Lintrospection_throw_npe
+    la    t3, .Lintrospection_exits                  # $t3 = address of .Lintrospection_exits.
+
+    lw      t4, MIRROR_OBJECT_LOCK_WORD_OFFSET(\obj)
+    slliw   t4, t4, 31 - LOCK_WORD_READ_BARRIER_STATE_SHIFT   # Move barrier state bit
+                                                              # to sign bit.
+    c.mv      t5, \obj                                   # Move `obj` to $t5 for common code.
+    bltz    t4, .Lintrospection_field_array            # If gray, load reference, mark.
+
+    fence
+    jalr    zero, ra                                  # Otherwise, load-load barrier and return.
+    ebreak                                               # Padding to 10 instructions.
+    nop
+    nop
+.endm
+
+// Caller code:
+// Long constant offset/index:   | Variable index:
+//  ld      $t6, pReadBarrierMarkReg00
+//  beqz    $t6, skip_call       |  beqz    $t6, skip_call
+//  daui    $t5, `obj`, ofs_hi   |  dlsa    $t5, `index`, `obj`, 2
+//  jialc   $t6, thunk_disp      |  jialc   $t6, thunk_disp
+// skip_call:                    | skip_call:
+//  lwu     `out`, ofs_lo($t5)   |  lwu     `out`, ofs($t5)
+// [dsubu   `out`, $zero, `out`  | [dsubu   `out`, $zero, `out`
+//  dext    `out`, `out`, 0, 32] |  dext    `out`, `out`, 0, 32]  # Unpoison reference.
+.macro BRB_FIELD_LONG_OFFSET_ENTRY obj
+    # No explicit null check for variable indices or large constant indices/offsets
+    # as it must have been done earlier.
+    la    t3, .Lintrospection_exits                  # $t3 = address of .Lintrospection_exits.
+
+    lw      t4, MIRROR_OBJECT_LOCK_WORD_OFFSET(\obj)
+    slliw   t4, t4, 31 - LOCK_WORD_READ_BARRIER_STATE_SHIFT   # Move barrier state bit
+                                                                # to sign bit.
+    bltz   t4, .Lintrospection_field_array            # If gray, load reference, mark.
+
+    fence
+    jalr    zero, ra                                  # Otherwise, load-load barrier and return.
+
+    ebreak                                               # Padding to 10 instructions.
+    ebreak
+    nop
+    nop
+    nop
+    nop
+.endm
+
+.macro BRB_GC_ROOT_ENTRY root
+    la    t3, .Lintrospection_exit_\root             # $t3 = exit point address.
+    c.mv    t5, \root                                  # Move reference to $t5 for common code.
+    bne    \root, zero, .Lintrospection_common
+    nop
+    jalr    zero, ra                                  # Return if null.
+    nop                                               # padding to 6 bytes
+    nop
+.endm
+
+.macro BRB_FIELD_EXIT out
+.Lintrospection_exit_\out:
+    mv      \out, t5                                   # Return reference in expected register.
+    jalr    zero, ra
+.endm
+
+.macro BRB_FIELD_EXIT_BREAK
+    ebreak
+    ebreak
+.endm
+
+ENTRY_NO_GP art_quick_read_barrier_mark_introspection
+    # Entry points for offsets/indices not fitting into int16_t and for variable indices.
+    BRB_FIELD_LONG_OFFSET_ENTRY t0
+    BRB_FIELD_LONG_OFFSET_ENTRY t1
+    BRB_FIELD_LONG_OFFSET_ENTRY t2
+    BRB_FIELD_LONG_OFFSET_ENTRY s0
+    BRB_FIELD_LONG_OFFSET_ENTRY a0
+    BRB_FIELD_LONG_OFFSET_ENTRY a1
+    BRB_FIELD_LONG_OFFSET_ENTRY a2
+    BRB_FIELD_LONG_OFFSET_ENTRY a3
+    BRB_FIELD_LONG_OFFSET_ENTRY a4
+    BRB_FIELD_LONG_OFFSET_ENTRY a5
+    BRB_FIELD_LONG_OFFSET_ENTRY a6
+    BRB_FIELD_LONG_OFFSET_ENTRY a7
+    BRB_FIELD_LONG_OFFSET_ENTRY s2
+    BRB_FIELD_LONG_OFFSET_ENTRY s3
+    BRB_FIELD_LONG_OFFSET_ENTRY s4
+    BRB_FIELD_LONG_OFFSET_ENTRY s5
+    BRB_FIELD_LONG_OFFSET_ENTRY s6
+    BRB_FIELD_LONG_OFFSET_ENTRY s7
+    BRB_FIELD_LONG_OFFSET_ENTRY s8
+    BRB_FIELD_LONG_OFFSET_ENTRY s9
+    BRB_FIELD_LONG_OFFSET_ENTRY s10
+
+    # Entry points for offsets/indices fitting into int16_t.
+    BRB_FIELD_SHORT_OFFSET_ENTRY t0
+    BRB_FIELD_SHORT_OFFSET_ENTRY t1
+    BRB_FIELD_SHORT_OFFSET_ENTRY t2
+    BRB_FIELD_SHORT_OFFSET_ENTRY s0
+    BRB_FIELD_SHORT_OFFSET_ENTRY a0
+    BRB_FIELD_SHORT_OFFSET_ENTRY a1
+    BRB_FIELD_SHORT_OFFSET_ENTRY a2
+    BRB_FIELD_SHORT_OFFSET_ENTRY a3
+    BRB_FIELD_SHORT_OFFSET_ENTRY a4
+    BRB_FIELD_SHORT_OFFSET_ENTRY a5
+    BRB_FIELD_SHORT_OFFSET_ENTRY a6
+    BRB_FIELD_SHORT_OFFSET_ENTRY a7
+    BRB_FIELD_SHORT_OFFSET_ENTRY s2
+    BRB_FIELD_SHORT_OFFSET_ENTRY s3
+    BRB_FIELD_SHORT_OFFSET_ENTRY s4
+    BRB_FIELD_SHORT_OFFSET_ENTRY s5
+    BRB_FIELD_SHORT_OFFSET_ENTRY s6
+    BRB_FIELD_SHORT_OFFSET_ENTRY s7
+    BRB_FIELD_SHORT_OFFSET_ENTRY s8
+    BRB_FIELD_SHORT_OFFSET_ENTRY s9
+    BRB_FIELD_SHORT_OFFSET_ENTRY s10
+
+    .global art_quick_read_barrier_mark_introspection_gc_roots
+art_quick_read_barrier_mark_introspection_gc_roots:
+    # Entry points for GC roots.
+    BRB_GC_ROOT_ENTRY t0
+    BRB_GC_ROOT_ENTRY t1
+    BRB_GC_ROOT_ENTRY t2
+    BRB_GC_ROOT_ENTRY s0
+    BRB_GC_ROOT_ENTRY a0
+    BRB_GC_ROOT_ENTRY a1
+    BRB_GC_ROOT_ENTRY a2
+    BRB_GC_ROOT_ENTRY a3
+    BRB_GC_ROOT_ENTRY a4
+    BRB_GC_ROOT_ENTRY a5
+    BRB_GC_ROOT_ENTRY a6
+    BRB_GC_ROOT_ENTRY a7
+    BRB_GC_ROOT_ENTRY s2
+    BRB_GC_ROOT_ENTRY s3
+    BRB_GC_ROOT_ENTRY s4
+    BRB_GC_ROOT_ENTRY s5
+    BRB_GC_ROOT_ENTRY s6
+    BRB_GC_ROOT_ENTRY s7
+    BRB_GC_ROOT_ENTRY s8
+    BRB_GC_ROOT_ENTRY s9
+    BRB_GC_ROOT_ENTRY s10
+
+    .global art_quick_read_barrier_mark_introspection_end_of_entries
+art_quick_read_barrier_mark_introspection_end_of_entries:
+
+.Lintrospection_throw_npe:
+    addi    ra, ra, 4         # Skip lwu, make $ra point to lwu's stack map.
+
+    la      t6, art_quick_throw_null_pointer_exception
+    jr      t6
+
+    // Fields and array elements.
+
+.Lintrospection_field_array:
+    // Get the field/element address using $t5 and the offset from the lwu instruction.
+    lh      t4, 0(ra)         # $ra points to lwu: $at = low 16 bits of field/element offset.
+    addi    ra, ra, 4 + HEAP_POISON_INSTR_SIZE   # Skip lwu(+dsubu+dext).
+    add     t5, t5, t4       # $t5 = field/element address.
+
+    // Calculate the address of the exit point, store it in $t3 and load the reference into $t5.
+    lb      t4, (-HEAP_POISON_INSTR_SIZE - 2)(ra)   # $ra-HEAP_POISON_INSTR_SIZE-4 points to
+                                                      # "lwu `out`, ...".
+    andi    t4, t4, 31        # Extract `out` from lwu.
+
+    lw      t5, 0(t5)         # $t5 = reference.
+    UNPOISON_HEAP_REF t5
+
+    // Return if null reference.
+    slli   t4, t4, 3          # $t3 = address of the exit point
+    add    t3, t3, t4         # (BRB_FIELD_EXIT* macro is 8 bytes).
+
+    bne    t5, zero, .Lintrospection_common
+
+    // Early return through the exit point.
+.Lintrospection_return_early:
+    jalr    zero, t3          # Move $t5 to `out` and return.
+
+    // Code common for GC roots, fields and array elements.
+
+.Lintrospection_common:
+    // Check lock word for mark bit, if marked return.
+    lw      t6, MIRROR_OBJECT_LOCK_WORD_OFFSET(t5)
+    slliw   t4, t6, 31 - LOCK_WORD_MARK_BIT_SHIFT     # Move mark bit to sign bit.
+    bltz    t4, .Lintrospection_return_early
+#if (LOCK_WORD_STATE_SHIFT != 30) || (LOCK_WORD_STATE_FORWARDING_ADDRESS != 3)
+    // The below code depends on the lock word state being in the highest bits
+    // and the "forwarding address" state having all bits set.
+#error "Unexpected lock word state shift or forwarding address state value."
+#endif
+    // Test that both the forwarding state bits are 1.
+    slliw   t4, t6, 1
+    and     t4, t4, t6                               # Sign bit = 1 IFF both bits are 1.
+    bge     t4, zero, .Lintrospection_mark
+
+    # .set pop
+
+    // Shift left by the forwarding address shift. This clears out the state bits since they are
+    // in the top 2 bits of the lock word.
+    slliw     t5, t6, LOCK_WORD_STATE_FORWARDING_ADDRESS_SHIFT
+    slli      t5, t5,  32     # Make sure the address is zero-extended.
+    srli      t5, t5,  32
+    jalr    zero, t3          # Move $t5 to `out` and return.
+
+.Lintrospection_mark:
+    // Partially set up the stack frame preserving only $ra.
+    addi  sp, sp, -280
+    .cfi_adjust_cfa_offset 280
+    sd      ra, 272(sp)
+    .cfi_rel_offset 31, 272
+
+    // Finalize the stack frame and call.
+    sd      t5, 264(sp)
+    .cfi_rel_offset 15, 264
+    sd      t4, 256(sp)
+    .cfi_rel_offset 14, 256
+    sd      t3, 248(sp)             # Preserve the exit point address.
+    .cfi_rel_offset 13, 248
+    sd      t2, 240(sp)
+    .cfi_rel_offset 12, 240
+    sd      t1, 232(sp)
+    .cfi_rel_offset 11, 232
+    sd      t0, 224(sp)
+    .cfi_rel_offset 10, 224
+    sd      a7, 216(sp)
+    .cfi_rel_offset 9, 216
+    sd      a6, 208(sp)
+    .cfi_rel_offset 8, 208
+    sd      a5, 200(sp)
+    .cfi_rel_offset 7, 200
+    sd      a4, 192(sp)
+    .cfi_rel_offset 6, 192
+    sd      a3, 184(sp)
+    .cfi_rel_offset 5, 184
+    sd      a2, 176(sp)
+    .cfi_rel_offset 4, 176
+    sd      a1, 168(sp)
+    .cfi_rel_offset 3, 168
+    sd      a0, 160(sp)
+    .cfi_rel_offset 2, 160
+
+    la     t6, artReadBarrierMark
+
+    fsd    f31, 152(sp)
+    fsd    f30, 144(sp)
+    fsd    f29, 136(sp)
+    fsd    f28, 128(sp)
+    fsd    f17, 120(sp)
+    fsd    f16, 112(sp)
+    fsd    f15, 104(sp)
+    fsd    f14,  96(sp)
+    fsd    f13,  88(sp)
+    fsd    f12,  80(sp)
+    fsd    f11,  72(sp)
+    fsd    f10,  64(sp)
+    fsd    f7,   56(sp)
+    fsd    f6,   48(sp)
+    fsd    f5,   40(sp)
+    fsd    f4,   32(sp)
+    fsd    f3,   24(sp)
+    fsd    f2,   16(sp)
+    fsd    f1,   8(sp)
+    fsd    f0,   0(sp)
+
+    mv    a0, t5            # Pass reference in $a0.
+    jalr    t6                 # $v0 <- artReadBarrierMark(reference)
+
+    ld      ra, 272(sp)
+    .cfi_restore 31
+    ld      t5, 264(sp)
+    .cfi_restore 15
+    ld      t4, 256(sp)
+    .cfi_restore 14
+    ld      t3, 248(sp)
+    .cfi_restore 13
+    ld      t2, 240(sp)
+    .cfi_restore 12
+    ld      t1, 232(sp)
+    .cfi_restore 11
+    ld      t0, 224(sp)
+    .cfi_restore 10
+    ld      a7, 216(sp)
+    .cfi_restore 9
+    ld      a6, 208(sp)
+    .cfi_restore 8
+    ld      a5, 200(sp)
+    .cfi_restore 7
+    ld      a4, 192(sp)
+    .cfi_restore 6
+    ld      a3, 184(sp)
+    .cfi_restore 5
+    ld      a2, 176(sp)
+    .cfi_restore 4
+    ld      a1, 168(sp)
+    .cfi_restore 3
+
+    mv    t5, a0
+    ld    a0, 160(sp)
+    .cfi_restore 2
+
+
+    fld    f31, 152(sp)
+    fld    f30, 144(sp)
+    fld    f29, 136(sp)
+    fld    f28, 128(sp)
+    fld    f17, 120(sp)
+    fld    f16, 112(sp)
+    fld    f15, 104(sp)
+    fld    f14,  96(sp)
+    fld    f13,  88(sp)
+    fld    f12,  80(sp)
+    fld    f11,   72(sp)
+    fld    f10,   64(sp)
+    fld    f7,   56(sp)
+    fld    f6,   48(sp)
+    fld    f5,   40(sp)
+    fld    f4,   32(sp)
+    fld    f3,   24(sp)
+    fld    f2,   16(sp)
+    fld    f1,   8(sp)
+    fld    f0,   0(sp)
+
+    // Return through the exit point.
+    addi    sp, sp, 280
+    .cfi_adjust_cfa_offset -280
+    jalr    zero, t3          # Move $t5 to `out` and return.
+
+.Lintrospection_exits:
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT t0
+    BRB_FIELD_EXIT t1
+    BRB_FIELD_EXIT t2
+    BRB_FIELD_EXIT s0
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT a0
+    BRB_FIELD_EXIT a1
+    BRB_FIELD_EXIT a2
+    BRB_FIELD_EXIT a3
+    BRB_FIELD_EXIT a4
+    BRB_FIELD_EXIT a5
+    BRB_FIELD_EXIT a6
+    BRB_FIELD_EXIT a7
+    BRB_FIELD_EXIT s2
+    BRB_FIELD_EXIT s3
+    BRB_FIELD_EXIT s4
+    BRB_FIELD_EXIT s5
+    BRB_FIELD_EXIT s6
+    BRB_FIELD_EXIT s7
+    BRB_FIELD_EXIT s8
+    BRB_FIELD_EXIT s9
+    BRB_FIELD_EXIT s10
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+    BRB_FIELD_EXIT_BREAK
+END art_quick_read_barrier_mark_introspection
+
+   /*
+     * Polymorphic method invocation.
+     * On entry:
+     *   a0 = unused
+     *   a1 = receiver
+     */
+.extern artInvokePolymorphic
+ENTRY art_quick_invoke_polymorphic
+    SETUP_SAVE_REFS_AND_ARGS_FRAME
+    mv      a0, a1               # Make $a0 the receiver
+    mv      a1, rSELF             # Make $a1 an alias for the current Thread.
+    mv      a2, sp               # Make $a3 a pointer to the saved frame context.
+    jal     artInvokePolymorphic   # artInvokePolymorphic(receiver, Thread*, context)
+    ld      t0, THREAD_EXCEPTION_OFFSET(rSELF) # load Thread::Current()->exception_
+    addi    sp, sp, REFS_AND_ARGS_MINUS_REFS_SIZE  # skip a0-a7 and f12-f19
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    fmv.d.x f10, a0                 # place return value to FP return value
+    bne     t0, zero, 1f
+    fmv.d.x f11, a1                 # place return value to FP return value
+    jalr      zero, ra
+1:
+    DELIVER_PENDING_EXCEPTION
+END art_quick_invoke_polymorphic
+
+    /*
+     * InvokeCustom invocation.
+     * On entry:
+     *   a0 = call_site_idx
+     */
+.extern artInvokeCustom
+ENTRY art_quick_invoke_custom
+    SETUP_SAVE_REFS_AND_ARGS_FRAME
+    mv      a1, rSELF             # Make $a1 an alias for the current Thread.
+    mv      a2, sp
+    jal     artInvokeCustom        # Call artInvokeCustom(call_site_idx, Thread*, context).
+    ld      t0, THREAD_EXCEPTION_OFFSET(rSELF) # load Thread::Current()->exception_
+    addi    sp, sp, REFS_AND_ARGS_MINUS_REFS_SIZE  # skip a0-a7 and f12-f19
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    fmv.d.x f10, a0               # place return value to FP return value
+    bne     t0, zero, 1f
+    fmv.d.x f11, a1               # place return value to FP return value
+    jalr     zero, ra
+1:
+    DELIVER_PENDING_EXCEPTION
+END art_quick_invoke_custom
+  # .set pop                        # TODO: T-HEAD
+
+// Wrap ExecuteSwitchImpl in assembly method which specifies DEX PC for unwinding.
+//  Argument 0: a0: The context pointer for ExecuteSwitchImpl.
+//  Argument 1: a1: Pointer to the templated ExecuteSwitchImpl to call.
+//  Argument 2: a2: The value of DEX PC (memory address of the methods bytecode).
+ENTRY ExecuteSwitchImplAsm
+    addi   sp, sp, -16         # save ra and t5
+    sd     ra, 8(sp)
+    sd     t5, 0(sp)
+
+    mv t5, a2                  # t5 = DEX PC
+    CFI_DEFINE_DEX_PC_WITH_OFFSET(10 /* a0 */, 30 /* t5 */, 0)
+    jalr a1                     # Call the wrapped method.
+
+    ld     t5, 0(sp)     # restore ra and t5
+    ld     ra, 8(sp)
+    addi   sp, sp, 16
+    jalr   zero, ra
+END ExecuteSwitchImplAsm
diff --git a/runtime/arch/riscv64/registers_riscv64.cc b/runtime/arch/riscv64/registers_riscv64.cc
new file mode 100644
index 0000000000..9eef806a39
--- /dev/null
+++ b/runtime/arch/riscv64/registers_riscv64.cc
@@ -0,0 +1,65 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "registers_riscv64.h"
+
+#include <ostream>
+
+namespace art {
+namespace riscv64 {
+
+static const char* kRegisterNames[] = {
+  "zero", "ra", "sp", "gp", "tp", "t0", "t1", "t2", "s0", "s1",
+  "a0", "a1", "a2", "a3", "a4", "a5", "a6", "a7",
+  "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9", "s10", "s11",
+  "t3", "t4", "t5", "t6", "pc",
+};
+
+static const char* fRegisterNames[] = {
+  "ft0", "ft1", "ft2", "ft3", "ft4", "ft5", "ft6", "ft7",
+  "fs0", "fs1", "fa0", "fa1", "fa2", "fa3", "fa4", "fa5",
+  "fa6", "fa7", "fs2", "fs3", "fs4", "fs5", "fs6", "fs7",
+  "fs8", "fs9", "fs10", "fs11", "ft8", "ft9", "ft10", "ft11",
+};
+std::ostream& operator<<(std::ostream& os, const GpuRegister& rhs) {
+  if (rhs >= ZERO && rhs < kNumberOfGpuRegisters) {
+    os << kRegisterNames[rhs];
+  } else {
+    os << "GpuRegister[" << static_cast<int>(rhs) << "]";
+  }
+  return os;
+}
+
+std::ostream& operator<<(std::ostream& os, const FpuRegister& rhs) {
+  if (rhs >= FT0 && rhs < kNumberOfFpuRegisters) {
+    os << fRegisterNames[rhs];
+  } else {
+    os << "FpuRegister[" << static_cast<int>(rhs) << "]";
+  }
+  return os;
+}
+
+std::ostream& operator<<(std::ostream& os, const VectorRegister& rhs) {
+  if (rhs >= W0 && rhs < kNumberOfVectorRegisters) {
+    os << "w" << static_cast<int>(rhs);
+  } else {
+    os << "VectorRegister[" << static_cast<int>(rhs) << "]";
+  }
+  return os;
+}
+
+}  // namespace riscv64
+}  // namespace art
diff --git a/runtime/arch/riscv64/registers_riscv64.h b/runtime/arch/riscv64/registers_riscv64.h
new file mode 100644
index 0000000000..54a01f80cb
--- /dev/null
+++ b/runtime/arch/riscv64/registers_riscv64.h
@@ -0,0 +1,161 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_ARCH_RISCV64_REGISTERS_RISCV64_H_
+#define ART_RUNTIME_ARCH_RISCV64_REGISTERS_RISCV64_H_
+
+#include <iosfwd>
+
+#include "base/macros.h"
+
+namespace art {
+namespace riscv64 {
+
+enum GpuRegister {
+  ZERO =  0,
+  RA   =  1,  // Return Address
+  SP   =  2,  // Stack pointer.
+  GP   =  3,  // Global pointer
+  TP   =  4,  // Thread pointer.
+  T0   =  5,  // Temporary register.
+  T1   =  6,  // Temporary register.
+  T2   =  7,  // Temporary register.
+  S0   =  8,  // Saved register/Frame pointer.
+  S1   =  9,  // Saved register
+  A0   = 10,  // Function arguments/return values.
+  A1   = 11,  // Function arguments/return values.
+  A2   = 12,  // Function arguments.
+  A3   = 13,  // ...
+  A4   = 14,
+  A5   = 15,
+  A6   = 16,
+  A7   = 17,
+  S2   = 18,  // Saved register
+  S3   = 19,  // ...
+  S4   = 20,
+  S5   = 21,
+  S6   = 22,
+  S7   = 23,
+  S8   = 24,
+  S9   = 25,
+  S10  = 26,
+  S11  = 27,
+  T3   = 28,  // Temporary register.
+  T4   = 29,  // ...
+  T5   = 30,
+  T6   = 31,
+  PC   = 32,
+
+  FP   = S0,  // Frame pointer.
+  TMP  = T5,
+  TMP2 = T4,
+  AT   = T3,
+  V0   = A0,
+  V1   = A1,
+
+  TR   = S1,  // ART Thread Register (Same as the definition in asm_support_riscv64.S)
+  T9   = T6,
+  kNumberOfGpuRegisters = 32,
+  kNoGpuRegister = -1  // Signals an illegal register.
+};
+std::ostream& operator<<(std::ostream& os, const GpuRegister& rhs);
+
+// Values for floating point registers.
+enum FpuRegister {
+  FT0  =  0,  // Temporary register.
+  FT1  =  1,  // ...
+  FT2  =  2,
+  FT3  =  3,
+  FT4  =  4,
+  FT5  =  5,
+  FT6  =  6,
+  FT7  =  7,
+  FS0  =  8,  // Saved register
+  FS1  =  9,  // ...
+  FA0  = 10,  // Function arguments/return values.
+  FA1  = 11,  // Function arguments/return values.
+  FA2  = 12,  // Function arguments
+  FA3  = 13,
+  FA4  = 14,
+  FA5  = 15,
+  FA6  = 16,
+  FA7  = 17,
+  FS2  = 18,   // Saved register
+  FS3  = 19,
+  FS4  = 20,
+  FS5  = 21,
+  FS6  = 22,
+  FS7  = 23,
+  FS8  = 24,
+  FS9  = 25,
+  FS10 = 26,
+  FS11 = 27,
+  FT8  = 28,  // Temporary register.
+  FT9  = 29,
+  FT10 = 30,
+  FT11 = 31,
+
+  F0   = FA0,
+  FTMP = FT11,   // scratch register
+  FTMP2 = FT10,  // scratch register (in addition to FTMP, reserved for MSA instructions)
+  kNumberOfFpuRegisters = 32,
+  kNoFpuRegister = -1,
+};
+std::ostream& operator<<(std::ostream& os, const FpuRegister& rhs);
+
+// Values for vector registers.
+enum VectorRegister {
+  W0  =  0,
+  W1  =  1,
+  W2  =  2,
+  W3  =  3,
+  W4  =  4,
+  W5  =  5,
+  W6  =  6,
+  W7  =  7,
+  W8  =  8,
+  W9  =  9,
+  W10 = 10,
+  W11 = 11,
+  W12 = 12,
+  W13 = 13,
+  W14 = 14,
+  W15 = 15,
+  W16 = 16,
+  W17 = 17,
+  W18 = 18,
+  W19 = 19,
+  W20 = 20,
+  W21 = 21,
+  W22 = 22,
+  W23 = 23,
+  W24 = 24,
+  W25 = 25,
+  W26 = 26,
+  W27 = 27,
+  W28 = 28,
+  W29 = 29,
+  W30 = 30,
+  W31 = 31,
+  kNumberOfVectorRegisters = 32,
+  kNoVectorRegister = -1,
+};
+std::ostream& operator<<(std::ostream& os, const VectorRegister& rhs);
+
+}  // namespace riscv64
+}  // namespace art
+
+#endif  // ART_RUNTIME_ARCH_RISCV64_REGISTERS_RISCV64_H_
diff --git a/runtime/arch/riscv64/thread_riscv64.cc b/runtime/arch/riscv64/thread_riscv64.cc
new file mode 100644
index 0000000000..8d90aae815
--- /dev/null
+++ b/runtime/arch/riscv64/thread_riscv64.cc
@@ -0,0 +1,36 @@
+/*
+ * Copyright (C) 2014 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "thread.h"
+
+#include <android-base/logging.h>
+
+#include "asm_support_riscv64.h"
+#include "base/enums.h"
+
+namespace art {
+
+void Thread::InitCpu() {
+  CHECK_EQ(THREAD_FLAGS_OFFSET, ThreadFlagsOffset<PointerSize::k64>().Int32Value());
+  CHECK_EQ(THREAD_CARD_TABLE_OFFSET, CardTableOffset<PointerSize::k64>().Int32Value());
+  CHECK_EQ(THREAD_EXCEPTION_OFFSET, ExceptionOffset<PointerSize::k64>().Int32Value());
+}
+
+void Thread::CleanupCpu() {
+  // Do nothing.
+}
+
+}  // namespace art
diff --git a/runtime/arch/stub_test.cc b/runtime/arch/stub_test.cc
index c82b445f81..7557257889 100644
--- a/runtime/arch/stub_test.cc
+++ b/runtime/arch/stub_test.cc
@@ -520,7 +520,65 @@ class StubTest : public CommonRuntimeTest {
         : "memory");  // We spill and restore (almost) all registers, so only mention memory here.
 #undef PUSH
 #undef POP
+#elif defined(__riscv)
+    __asm__ __volatile__ (
+        // Spill a0-a7 which we say we don't clobber. May contain args.
+        "addi sp, sp, -64\n\t"
+        "sd a0, 0(sp)\n\t"
+        "sd a1, 8(sp)\n\t"
+        "sd a2, 16(sp)\n\t"
+        "sd a3, 24(sp)\n\t"
+        "sd a4, 32(sp)\n\t"
+        "sd a5, 40(sp)\n\t"
+        "sd a6, 48(sp)\n\t"
+        "sd a7, 56(sp)\n\t"
+
+        "addi sp, sp, -16\n\t"   // Reserve stack space, 16B aligned.
+        "sd %[referrer], 0(sp)\n\t"
+
+        // Push everything on the stack, so we don't rely on the order.
+        "addi sp, sp, -48\n\t"
+        "sd %[arg0], 0(sp)\n\t"
+        "sd %[arg1], 8(sp)\n\t"
+        "sd %[arg2], 16(sp)\n\t"
+        "sd %[code], 24(sp)\n\t"
+        "sd %[self], 32(sp)\n\t"
+        "sd %[hidden], 40(sp)\n\t"
+
+        // Load call params into the right registers.
+        "ld a0, 0(sp)\n\t"
+        "ld a1, 8(sp)\n\t"
+        "ld a2, 16(sp)\n\t"
+        "ld t4, 24(sp)\n\t"
+        "ld s1, 32(sp)\n\t"
+        "ld t0, 40(sp)\n\t"
+        "addi sp, sp, 48\n\t"
+
+        "jalr t4\n\t"              // Call the stub.
+        "move %[result], a0\n\t"  // Store the call result.
+
+        "addi sp, sp, 16\n\t"   // Drop the quick "frame".
+
+        // Restore stuff not named clobbered.
+        "ld a0, 0(sp)\n\t"
+        "ld a1, 8(sp)\n\t"
+        "ld a2, 16(sp)\n\t"
+        "ld a3, 24(sp)\n\t"
+        "ld a4, 32(sp)\n\t"
+        "ld a5, 40(sp)\n\t"
+        "ld a6, 48(sp)\n\t"
+        "ld a7, 56(sp)\n\t"
+        "addi sp, sp, 64\n\t"
+
+        : [result] "=r" (result)
+        : [arg0] "r"(arg0), [arg1] "r"(arg1), [arg2] "r"(arg2), [code] "r"(code), [self] "r"(self),
+          [referrer] "r"(referrer), [hidden] "r"(hidden)
+        // Instead aliases t0-t3, register names $12-$15 has been used in the clobber list because
+        // t0-t3 are ambiguous.
+        : "a0",
+          "memory");  // clobber.
 #else
+    std::cout << "not defined riscv" << std::endl;
     UNUSED(arg0, arg1, arg2, code, referrer, hidden);
     LOG(WARNING) << "Was asked to invoke for an architecture I do not understand.";
     result = 0;
@@ -546,7 +604,7 @@ class StubTest : public CommonRuntimeTest {
 
 
 TEST_F(StubTest, Memcpy) {
-#if defined(__i386__) || (defined(__x86_64__) && !defined(__APPLE__)) || defined(__mips__)
+#if defined(__i386__) || (defined(__x86_64__) && !defined(__APPLE__)) || defined(__mips__) || defined(__riscv)
   Thread* self = Thread::Current();
 
   uint32_t orig[20];
@@ -584,7 +642,7 @@ TEST_F(StubTest, Memcpy) {
 
 TEST_F(StubTest, LockObject) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   static constexpr size_t kThinLockLoops = 100;
 
   Thread* self = Thread::Current();
@@ -598,6 +656,7 @@ TEST_F(StubTest, LockObject) {
   StackHandleScope<2> hs(soa.Self());
   Handle<mirror::String> obj(
       hs.NewHandle(mirror::String::AllocFromModifiedUtf8(soa.Self(), "hello, world!")));
+
   LockWord lock = obj->GetLockWord(false);
   LockWord::LockState old_state = lock.GetState();
   EXPECT_EQ(LockWord::LockState::kUnlocked, old_state);
@@ -658,7 +717,7 @@ class RandGen {
 // NO_THREAD_SAFETY_ANALYSIS as we do not want to grab exclusive mutator lock for MonitorInfo.
 static void TestUnlockObject(StubTest* test) NO_THREAD_SAFETY_ANALYSIS {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   static constexpr size_t kThinLockLoops = 100;
 
   Thread* self = Thread::Current();
@@ -809,13 +868,13 @@ TEST_F(StubTest, UnlockObject) {
 }
 
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
 extern "C" void art_quick_check_instance_of(void);
 #endif
 
 TEST_F(StubTest, CheckCast) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   Thread* self = Thread::Current();
 
   const uintptr_t art_quick_check_instance_of =
@@ -904,7 +963,6 @@ TEST_F(StubTest, CheckCast) {
           self);
   EXPECT_TRUE(self->IsExceptionPending());
   self->ClearException();
-
 #else
   LOG(INFO) << "Skipping check_cast as I don't know how to do that on " << kRuntimeISA;
   // Force-print to std::cout so it's also outside the logcat.
@@ -914,7 +972,7 @@ TEST_F(StubTest, CheckCast) {
 
 TEST_F(StubTest, AllocObject) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   // This will lead to OOM  error messages in the log.
   ScopedLogSeverity sls(LogSeverity::FATAL);
 
@@ -1031,7 +1089,7 @@ TEST_F(StubTest, AllocObject) {
 
 TEST_F(StubTest, AllocObjectArray) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   // TODO: Check the "Unresolved" allocation stubs
 
   // This will lead to OOM  error messages in the log.
@@ -1096,7 +1154,7 @@ TEST_F(StubTest, StringCompareTo) {
   TEST_DISABLED_FOR_STRING_COMPRESSION();
   // There is no StringCompareTo runtime entrypoint for __arm__ or __aarch64__.
 #if defined(__i386__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   // TODO: Check the "Unresolved" allocation stubs
 
   Thread* self = Thread::Current();
@@ -1179,7 +1237,7 @@ static void GetSetBooleanStatic(ArtField* f, Thread* self,
                                 ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   constexpr size_t num_values = 5;
   uint8_t values[num_values] = { 0, 1, 2, 128, 0xFF };
 
@@ -1190,7 +1248,6 @@ static void GetSetBooleanStatic(ArtField* f, Thread* self,
                               StubTest::GetEntrypoint(self, kQuickSet8Static),
                               self,
                               referrer);
-
     size_t res = test->Invoke3WithReferrer(static_cast<size_t>(f->GetDexFieldIndex()),
                                            0U, 0U,
                                            StubTest::GetEntrypoint(self, kQuickGetBooleanStatic),
@@ -1241,7 +1298,7 @@ static void GetSetBooleanInstance(Handle<mirror::Object>* obj, ArtField* f, Thre
                                   ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   uint8_t values[] = { 0, true, 2, 128, 0xFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1276,7 +1333,7 @@ static void GetSetByteInstance(Handle<mirror::Object>* obj, ArtField* f,
                              Thread* self, ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   int8_t values[] = { -128, -64, 0, 64, 127 };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1311,7 +1368,7 @@ static void GetSetCharStatic(ArtField* f, Thread* self, ArtMethod* referrer,
                              StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   uint16_t values[] = { 0, 1, 2, 255, 32768, 0xFFFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1341,7 +1398,7 @@ static void GetSetShortStatic(ArtField* f, Thread* self,
                               ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   int16_t values[] = { -0x7FFF, -32768, 0, 255, 32767, 0x7FFE };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1372,7 +1429,7 @@ static void GetSetCharInstance(Handle<mirror::Object>* obj, ArtField* f,
                                Thread* self, ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   uint16_t values[] = { 0, 1, 2, 255, 32768, 0xFFFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1406,7 +1463,7 @@ static void GetSetShortInstance(Handle<mirror::Object>* obj, ArtField* f,
                              Thread* self, ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   int16_t values[] = { -0x7FFF, -32768, 0, 255, 32767, 0x7FFE };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1441,7 +1498,7 @@ static void GetSet32Static(ArtField* f, Thread* self, ArtMethod* referrer,
                            StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   uint32_t values[] = { 0, 1, 2, 255, 32768, 1000000, 0xFFFFFFFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1458,7 +1515,7 @@ static void GetSet32Static(ArtField* f, Thread* self, ArtMethod* referrer,
                                            self,
                                            referrer);
 
-#if defined(__mips__) && defined(__LP64__)
+#if (defined(__mips__) && defined(__LP64__)) ||  defined(__riscv)
     EXPECT_EQ(static_cast<uint32_t>(res), values[i]) << "Iteration " << i;
 #else
     EXPECT_EQ(res, values[i]) << "Iteration " << i;
@@ -1477,7 +1534,7 @@ static void GetSet32Instance(Handle<mirror::Object>* obj, ArtField* f,
                              Thread* self, ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   uint32_t values[] = { 0, 1, 2, 255, 32768, 1000000, 0xFFFFFFFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1512,7 +1569,7 @@ static void GetSet32Instance(Handle<mirror::Object>* obj, ArtField* f,
 
 
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
 
 static void set_and_check_static(uint32_t f_idx,
                                  ObjPtr<mirror::Object> val,
@@ -1544,7 +1601,7 @@ static void GetSetObjStatic(ArtField* f, Thread* self, ArtMethod* referrer,
                             StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   set_and_check_static(f->GetDexFieldIndex(), nullptr, self, referrer, test);
 
   // Allocate a string object for simplicity.
@@ -1562,7 +1619,7 @@ static void GetSetObjStatic(ArtField* f, Thread* self, ArtMethod* referrer,
 
 
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
 static void set_and_check_instance(ArtField* f,
                                    ObjPtr<mirror::Object> trg,
                                    ObjPtr<mirror::Object> val,
@@ -1597,7 +1654,7 @@ static void GetSetObjInstance(Handle<mirror::Object>* obj, ArtField* f,
                               Thread* self, ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   set_and_check_instance(f, obj->Get(), nullptr, self, referrer, test);
 
   // Allocate a string object for simplicity.
@@ -1620,7 +1677,7 @@ static void GetSet64Static(ArtField* f, Thread* self, ArtMethod* referrer,
                            StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if (defined(__x86_64__) && !defined(__APPLE__)) || (defined(__mips__) && defined(__LP64__)) \
-    || defined(__aarch64__)
+    || defined(__aarch64__) || defined(__riscv)
   uint64_t values[] = { 0, 1, 2, 255, 32768, 1000000, 0xFFFFFFFF, 0xFFFFFFFFFFFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1653,7 +1710,7 @@ static void GetSet64Instance(Handle<mirror::Object>* obj, ArtField* f,
                              Thread* self, ArtMethod* referrer, StubTest* test)
     REQUIRES_SHARED(Locks::mutator_lock_) {
 #if (defined(__x86_64__) && !defined(__APPLE__)) || (defined(__mips__) && defined(__LP64__)) || \
-    defined(__aarch64__)
+    defined(__aarch64__) || defined(__riscv)
   uint64_t values[] = { 0, 1, 2, 255, 32768, 1000000, 0xFFFFFFFF, 0xFFFFFFFFFFFF };
 
   for (size_t i = 0; i < arraysize(values); ++i) {
@@ -1843,7 +1900,7 @@ TEST_F(StubTest, Fields64) {
 // the bridge and uses that to check for inlined frames, crashing in the process.
 TEST_F(StubTest, DISABLED_IMT) {
 #if defined(__i386__) || defined(__arm__) || defined(__aarch64__) || defined(__mips__) || \
-    (defined(__x86_64__) && !defined(__APPLE__))
+    (defined(__x86_64__) && !defined(__APPLE__)) || defined(__riscv)
   Thread* self = Thread::Current();
 
   ScopedObjectAccess soa(self);
@@ -1981,7 +2038,7 @@ TEST_F(StubTest, DISABLED_IMT) {
 }
 
 TEST_F(StubTest, StringIndexOf) {
-#if defined(__arm__) || defined(__aarch64__) || defined(__mips__)
+#if defined(__arm__) || defined(__aarch64__) || defined(__mips__) || defined(__riscv)
   Thread* self = Thread::Current();
   ScopedObjectAccess soa(self);
   // garbage is created during ClassLinker::Init
@@ -2058,7 +2115,7 @@ TEST_F(StubTest, StringIndexOf) {
 
 TEST_F(StubTest, ReadBarrier) {
 #if defined(ART_USE_READ_BARRIER) && (defined(__i386__) || defined(__arm__) || \
-      defined(__aarch64__) || defined(__mips__) || (defined(__x86_64__) && !defined(__APPLE__)))
+      defined(__aarch64__) || defined(__mips__) || (defined(__x86_64__) && !defined(__APPLE__))) || defined(__riscv)
   Thread* self = Thread::Current();
 
   const uintptr_t readBarrierSlow = StubTest::GetEntrypoint(self, kQuickReadBarrierSlow);
@@ -2094,7 +2151,7 @@ TEST_F(StubTest, ReadBarrier) {
 
 TEST_F(StubTest, ReadBarrierForRoot) {
 #if defined(ART_USE_READ_BARRIER) && (defined(__i386__) || defined(__arm__) || \
-      defined(__aarch64__) || defined(__mips__) || (defined(__x86_64__) && !defined(__APPLE__)))
+      defined(__aarch64__) || defined(__mips__) || (defined(__x86_64__) && !defined(__APPLE__))) || defined(__riscv)
   Thread* self = Thread::Current();
 
   const uintptr_t readBarrierForRootSlow =
diff --git a/runtime/art_method.cc b/runtime/art_method.cc
index 0890da8c83..3761a54041 100644
--- a/runtime/art_method.cc
+++ b/runtime/art_method.cc
@@ -392,6 +392,12 @@ const void* ArtMethod::RegisterNative(const void* native_method) {
                                                                   native_method,
                                                                   /*out*/&new_native_method);
   SetEntryPointFromJni(new_native_method);
+  // FIXME: T-HEAD, hacked here for Native Method: TBD
+  if (IsNative() && (GetEntryPointFromQuickCompiledCode() == nullptr)) {
+      // quick compile it
+      SetEntryPointFromQuickCompiledCode(GetQuickGenericJniStub());
+  }
+
   return new_native_method;
 }
 
diff --git a/runtime/common_runtime_test.cc b/runtime/common_runtime_test.cc
index f4cc1617a5..5f1914c34d 100644
--- a/runtime/common_runtime_test.cc
+++ b/runtime/common_runtime_test.cc
@@ -98,6 +98,10 @@ std::string CommonRuntimeTestImpl::GetAndroidTargetToolsDir(InstructionSet isa)
       return GetAndroidToolsDir("prebuilts/gcc/linux-x86/mips",
                                 "mips64el-linux-android",
                                 "mips64el-linux-android");
+    case InstructionSet::kRiscv64:
+      return GetAndroidToolsDir("prebuilts/gcc/linux-x86/riscv64",
+                                "riscv64-linux-android",
+                                "riscv64-linux-android");
     case InstructionSet::kNone:
       break;
   }
diff --git a/runtime/common_runtime_test.h b/runtime/common_runtime_test.h
index fb3eae75dc..54ebe8036c 100644
--- a/runtime/common_runtime_test.h
+++ b/runtime/common_runtime_test.h
@@ -219,6 +219,12 @@ class CheckJniAbortCatcher {
     return; \
   }
 
+#define TEST_DISABLED_FOR_RISCV64() \
+  if (kRuntimeISA == InstructionSet::kRiscv64) { \
+    printf("WARNING: TEST DISABLED FOR RISCV64\n"); \
+    return; \
+  }
+
 #define TEST_DISABLED_FOR_X86() \
   if (kRuntimeISA == InstructionSet::kX86) { \
     printf("WARNING: TEST DISABLED FOR X86\n"); \
diff --git a/runtime/compiler_filter.cc b/runtime/compiler_filter.cc
index c0864901cf..18e78afe81 100644
--- a/runtime/compiler_filter.cc
+++ b/runtime/compiler_filter.cc
@@ -237,6 +237,10 @@ bool CompilerFilter::ParseCompilerFilter(const char* option, Filter* filter) {
   } else {
     return false;
   }
+
+  // FIXE: T-HEAD, zhengxing: Force use quicken to disable AOT temporally.
+  if ((*filter != kAssumeVerified) && (*filter != kExtract) && (*filter != kVerify) && (*filter != kQuicken))
+    *filter = kQuicken;
   return true;
 }
 
diff --git a/runtime/elf_file.cc b/runtime/elf_file.cc
index 12c33dead8..06692fa049 100644
--- a/runtime/elf_file.cc
+++ b/runtime/elf_file.cc
@@ -1095,6 +1095,8 @@ static InstructionSet GetInstructionSetFromELF(uint16_t e_machine, uint32_t e_fl
       }
       break;
     }
+    case EM_RISCV:
+      return InstructionSet::kRiscv64;
   }
   return InstructionSet::kNone;
 }
diff --git a/runtime/entrypoints/quick/callee_save_frame.h b/runtime/entrypoints/quick/callee_save_frame.h
index e555d68a27..66261f6697 100644
--- a/runtime/entrypoints/quick/callee_save_frame.h
+++ b/runtime/entrypoints/quick/callee_save_frame.h
@@ -32,6 +32,7 @@
 #include "arch/mips64/callee_save_frame_mips64.h"
 #include "arch/x86/callee_save_frame_x86.h"
 #include "arch/x86_64/callee_save_frame_x86_64.h"
+#include "arch/riscv64/callee_save_frame_riscv64.h"
 
 namespace art {
 class ArtMethod;
@@ -86,6 +87,8 @@ template <>
 struct CSFSelector<InstructionSet::kX86> { using type = x86::X86CalleeSaveFrame; };
 template <>
 struct CSFSelector<InstructionSet::kX86_64> { using type = x86_64::X86_64CalleeSaveFrame; };
+template <>
+struct CSFSelector<InstructionSet::kRiscv64> { using type = riscv64::Riscv64CalleeSaveFrame; };
 
 }  // namespace detail
 
diff --git a/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc b/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
index a2420afb69..f599f8898d 100644
--- a/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
+++ b/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
@@ -300,6 +300,46 @@ class QuickArgumentVisitor {
       UNREACHABLE();
     }
   }
+#elif defined(__riscv) && (__riscv_xlen == 64)
+  // The callee save frame is pointed to by SP.
+  // | argN       |  |
+  // | ...        |  |
+  // | arg4       |  |
+  // | arg3 spill |  |  Caller's frame
+  // | arg2 spill |  |
+  // | arg1 spill |  |
+  // | Method*    | ---
+  // | RA         |
+  // | ...        |    callee saves
+  // | A7         |    arg7
+  // | A6         |    arg6
+  // | A5         |    arg5
+  // | A4         |    arg4
+  // | A3         |    arg3
+  // | A2         |    arg2
+  // | A1         |    arg1
+  // | F17        |    f_arg7
+  // | F16        |    f_arg6
+  // | F15        |    f_arg5
+  // | F14        |    f_arg4
+  // | F13        |    f_arg3
+  // | F12        |    f_arg2
+  // | F11        |    f_arg1
+  // | F10        |    f_arg0
+  // |            |    padding
+  // | A0/Method* |  <- sp
+  static constexpr bool kSplitPairAcrossRegisterAndStack = false;
+  static constexpr bool kAlignPairRegister = false;
+  static constexpr bool kQuickSoftFloatAbi = false;
+  static constexpr bool kQuickDoubleRegAlignedFloatBackFilled = false;
+  static constexpr bool kQuickSkipOddFpRegisters = false;
+  static constexpr size_t kNumQuickGprArgs = 7;  // 7 arguments passed in GPRs.
+  static constexpr size_t kNumQuickFprArgs = 8;  // 7 arguments passed in FPRs.
+  static constexpr bool kGprFprLockstep = false;
+
+  static size_t GprIndexToGprOffset(uint32_t gpr_index) {
+    return gpr_index * GetBytesPerGprSpillLocation(kRuntimeISA);
+  }
 #else
 #error "Unsupported architecture"
 #endif
@@ -394,6 +434,25 @@ class QuickArgumentVisitor {
   }
 
   uint8_t* GetParamAddress() const {
+    // FIXME: T-HEAD, Riscv64 get the param value from stack directly.
+    #if defined(__riscv) && (__riscv_xlen == 64)
+    Primitive::Type type = GetParamPrimitiveType();
+    if (UNLIKELY((type == Primitive::kPrimDouble) || (type == Primitive::kPrimFloat))) {
+      if (fpr_index_ + 1 < kNumQuickFprArgs + 1) {
+        return fpr_args_ + (fpr_index_ * GetBytesPerFprSpillLocation(kRuntimeISA));
+      }
+
+      // [Workaround]:
+      // FIXME: T-HEAD, The optimizing compiler and runtime code can guarantee the >8 float/double values
+      //            stored on its stack slot. it's safe to get them on stack.
+      return stack_args_ + (stack_index_ * kBytesStackArgLocation);
+    }
+
+    if (gpr_index_ < kNumQuickGprArgs) {
+      return gpr_args_ + GprIndexToGprOffset(gpr_index_);
+    }
+    return stack_args_ + (stack_index_ * kBytesStackArgLocation);
+    #else
     if (!kQuickSoftFloatAbi) {
       Primitive::Type type = GetParamPrimitiveType();
       if (UNLIKELY((type == Primitive::kPrimDouble) || (type == Primitive::kPrimFloat))) {
@@ -411,6 +470,7 @@ class QuickArgumentVisitor {
       return gpr_args_ + GprIndexToGprOffset(gpr_index_);
     }
     return stack_args_ + (stack_index_ * kBytesStackArgLocation);
+    #endif
   }
 
   bool IsSplitLongOrDouble() const {
@@ -508,7 +568,14 @@ class QuickArgumentVisitor {
               } else if (kQuickSkipOddFpRegisters) {
                 IncFprIndex();
               }
+            #if defined(__riscv)
+            // FIXME: T-HEAD, Riscv64 will try GPR when out of FPR. Need update Gpr index here.
+            } else if (gpr_index_ < kNumQuickGprArgs) {
+              IncGprIndex();
+            }
+            #else
             }
+            #endif
           }
           break;
         case Primitive::kPrimDouble:
@@ -573,7 +640,14 @@ class QuickArgumentVisitor {
                   IncFprIndex();
                 }
               }
+            #if defined(__riscv)
+            // FIXME: T-HEAD, Riscv64 will try GPR when out of FPR. Need update Gpr index here.
+            } else if (gpr_index_ < kNumQuickGprArgs) {
+              IncGprIndex();
+            }
+            #else
             }
+            #endif
           }
           break;
         default:
@@ -1562,6 +1636,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
   static constexpr size_t kRegistersNeededForDouble = 2;
   static constexpr bool kMultiRegistersAligned = true;
   static constexpr bool kMultiFPRegistersWidened = false;
+  static constexpr bool kFPRegisterIsNANBoxing = false;
   static constexpr bool kMultiGPRegistersWidened = false;
   static constexpr bool kAlignLongOnStack = true;
   static constexpr bool kAlignDoubleOnStack = true;
@@ -1574,6 +1649,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
   static constexpr size_t kRegistersNeededForDouble = 1;
   static constexpr bool kMultiRegistersAligned = false;
   static constexpr bool kMultiFPRegistersWidened = false;
+  static constexpr bool kFPRegisterIsNANBoxing = false;
   static constexpr bool kMultiGPRegistersWidened = false;
   static constexpr bool kAlignLongOnStack = false;
   static constexpr bool kAlignDoubleOnStack = false;
@@ -1586,6 +1662,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
   static constexpr size_t kRegistersNeededForDouble = 2;
   static constexpr bool kMultiRegistersAligned = true;
   static constexpr bool kMultiFPRegistersWidened = true;
+  static constexpr bool kFPRegisterIsNANBoxing = false;
   static constexpr bool kMultiGPRegistersWidened = false;
   static constexpr bool kAlignLongOnStack = true;
   static constexpr bool kAlignDoubleOnStack = true;
@@ -1599,6 +1676,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
   static constexpr size_t kRegistersNeededForDouble = 1;
   static constexpr bool kMultiRegistersAligned = false;
   static constexpr bool kMultiFPRegistersWidened = false;
+  static constexpr bool kFPRegisterIsNANBoxing = false;
   static constexpr bool kMultiGPRegistersWidened = true;
   static constexpr bool kAlignLongOnStack = false;
   static constexpr bool kAlignDoubleOnStack = false;
@@ -1612,6 +1690,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
   static constexpr size_t kRegistersNeededForDouble = 2;
   static constexpr bool kMultiRegistersAligned = false;  // x86 not using regs, anyways
   static constexpr bool kMultiFPRegistersWidened = false;
+  static constexpr bool kFPRegisterIsNANBoxing = false;
   static constexpr bool kMultiGPRegistersWidened = false;
   static constexpr bool kAlignLongOnStack = false;
   static constexpr bool kAlignDoubleOnStack = false;
@@ -1624,6 +1703,20 @@ template<class T> class BuildNativeCallFrameStateMachine {
   static constexpr size_t kRegistersNeededForDouble = 1;
   static constexpr bool kMultiRegistersAligned = false;
   static constexpr bool kMultiFPRegistersWidened = false;
+  static constexpr bool kFPRegisterIsNANBoxing = false;
+  static constexpr bool kMultiGPRegistersWidened = false;
+  static constexpr bool kAlignLongOnStack = false;
+  static constexpr bool kAlignDoubleOnStack = false;
+#elif defined(__riscv) && (__riscv_xlen == 64)
+  static constexpr bool kNativeSoftFloatAbi = false;
+  static constexpr size_t kNumNativeGprArgs = 8;
+  static constexpr size_t kNumNativeFprArgs = 8;
+
+  static constexpr size_t kRegistersNeededForLong = 1;
+  static constexpr size_t kRegistersNeededForDouble = 1;
+  static constexpr bool kMultiRegistersAligned = false;
+  static constexpr bool kMultiFPRegistersWidened = false;
+  static constexpr bool kFPRegisterIsNANBoxing = 1;
   static constexpr bool kMultiGPRegistersWidened = false;
   static constexpr bool kAlignLongOnStack = false;
   static constexpr bool kAlignDoubleOnStack = false;
@@ -1760,6 +1853,9 @@ template<class T> class BuildNativeCallFrameStateMachine {
         if (kRegistersNeededForDouble == 1) {
           if (kMultiFPRegistersWidened) {
             PushFpr8(bit_cast<uint64_t, double>(val));
+          } else if (kFPRegisterIsNANBoxing) {
+            PushFpr8(static_cast<uint64_t>((bit_cast<uint32_t, float>(val))
+                                           | (0xffffffffUL << 32)));
           } else {
             // No widening, just use the bits.
             PushFpr8(static_cast<uint64_t>(bit_cast<uint32_t, float>(val)));
@@ -1768,6 +1864,10 @@ template<class T> class BuildNativeCallFrameStateMachine {
           PushFpr4(val);
         }
       } else {
+        // FIXME: T-HEAD, Riscv64 will try GPR, AdvanceInt may not suitable.
+        #if defined(__riscv) && (__riscv_xlen == 64)
+          AdvanceInt(bit_cast<uint32_t, float>(val));
+        #else
         stack_entries_++;
         if (kRegistersNeededForDouble == 1 && kMultiFPRegistersWidened) {
           // Need to widen before storing: Note the "double" in the template instantiation.
@@ -1777,6 +1877,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
         } else {
           PushStack(static_cast<uintptr_t>(bit_cast<uint32_t, float>(val)));
         }
+        #endif
         fpr_index_ = 0;
       }
     }
@@ -1810,6 +1911,10 @@ template<class T> class BuildNativeCallFrameStateMachine {
         PushFpr8(val);
         fpr_index_ -= kRegistersNeededForDouble;
       } else {
+        // FIXME: T-HEAD, Riscv64 will try GPR
+        #if defined(__riscv) && (__riscv_xlen == 64)
+          AdvanceLong(val);
+        #else
         if (DoubleStackNeedsPadding()) {
           PushStack(0);
           stack_entries_++;
@@ -1822,6 +1927,7 @@ template<class T> class BuildNativeCallFrameStateMachine {
           PushStack(static_cast<uintptr_t>((val >> 32) & 0xFFFFFFFF));
           stack_entries_ += 2;
         }
+        #endif
         fpr_index_ = 0;
       }
     }
diff --git a/runtime/hidden_api_test.cc b/runtime/hidden_api_test.cc
index 70fafe6587..6ef87d48b7 100644
--- a/runtime/hidden_api_test.cc
+++ b/runtime/hidden_api_test.cc
@@ -481,7 +481,7 @@ TEST_F(HiddenApiTest, DexDomain_DataDir) {
   ASSERT_EQ(0, remove(data_location_path.c_str()));
 }
 
-TEST_F(HiddenApiTest, DexDomain_SystemDir) {
+TEST_F(HiddenApiTest, DISABLED_DexDomain_SystemDir) {
   // Load file from a system, non-framework directory and check that it is not flagged as framework.
   std::string system_location_path = GetAndroidRoot() + "/foo.jar";
   ASSERT_FALSE(LocationIsOnSystemFramework(system_location_path.c_str()));
@@ -504,7 +504,7 @@ TEST_F(HiddenApiTest, DexDomain_SystemDir) {
   ASSERT_EQ(0, remove(system_location_path.c_str()));
 }
 
-TEST_F(HiddenApiTest, DexDomain_SystemFrameworkDir) {
+TEST_F(HiddenApiTest, DISABLED_DexDomain_SystemFrameworkDir) {
   // Load file from a system/framework directory and check that it is flagged as a framework dex.
   std::string system_framework_location_path = GetAndroidRoot() + "/framework/foo.jar";
   ASSERT_TRUE(LocationIsOnSystemFramework(system_framework_location_path.c_str()));
@@ -555,7 +555,7 @@ TEST_F(HiddenApiTest, DexDomain_DataDir_MultiDex) {
   ASSERT_EQ(0, remove(data_multi_location_path.c_str()));
 }
 
-TEST_F(HiddenApiTest, DexDomain_SystemDir_MultiDex) {
+TEST_F(HiddenApiTest, DISABLED_DexDomain_SystemDir_MultiDex) {
   // Load multidex file from a system, non-framework directory and check that it is not flagged
   // as framework.
   std::string system_multi_location_path = GetAndroidRoot() + "/multifoo.jar";
@@ -580,7 +580,7 @@ TEST_F(HiddenApiTest, DexDomain_SystemDir_MultiDex) {
   ASSERT_EQ(0, remove(system_multi_location_path.c_str()));
 }
 
-TEST_F(HiddenApiTest, DexDomain_SystemFrameworkDir_MultiDex) {
+TEST_F(HiddenApiTest, DISABLED_DexDomain_SystemFrameworkDir_MultiDex) {
   // Load multidex file from a system/framework directory and check that it is flagged as a
   // framework dex.
   std::string system_framework_multi_location_path = GetAndroidRoot() + "/framework/multifoo.jar";
diff --git a/runtime/interpreter/mterp/mterp.cc b/runtime/interpreter/mterp/mterp.cc
index 6a8f864901..79f2fb0f58 100644
--- a/runtime/interpreter/mterp/mterp.cc
+++ b/runtime/interpreter/mterp/mterp.cc
@@ -27,6 +27,8 @@
 #include "interpreter/shadow_frame-inl.h"
 #include "mirror/string-alloc-inl.h"
 
+#include <fstream>
+
 namespace art {
 namespace interpreter {
 /*
@@ -146,6 +148,21 @@ extern "C" ssize_t MterpDoPackedSwitch(const uint16_t* switchData, int32_t testV
 bool CanUseMterp()
     REQUIRES_SHARED(Locks::mutator_lock_) {
   const Runtime* const runtime = Runtime::Current();
+  #if defined(__riscv)
+  // FIXME: T-HEAD, hacking for running mterp without jit support.
+  return
+      runtime->IsStarted() &&
+      !Dbg::IsDebuggerActive() &&
+      !runtime->GetInstrumentation()->IsActive() &&
+      // mterp only knows how to deal with the normal exits. It cannot handle any of the
+      // non-standard force-returns.
+      !runtime->AreNonStandardExitsEnabled() &&
+      // An async exception has been thrown. We need to go to the switch interpreter. MTerp doesn't
+      // know how to deal with these so we could end up never dealing with it if we are in an
+      // infinite loop.
+      !runtime->AreAsyncExceptionsThrown() &&
+      (runtime->GetJit() == nullptr || !runtime->GetJit()->JitAtFirstUse());
+  #else
   return
       runtime->IsStarted() &&
       !runtime->IsAotCompiler() &&
@@ -159,6 +176,7 @@ bool CanUseMterp()
       // infinite loop.
       !runtime->AreAsyncExceptionsThrown() &&
       (runtime->GetJit() == nullptr || !runtime->GetJit()->JitAtFirstUse());
+  #endif
 }
 
 
diff --git a/runtime/interpreter/mterp/mterp.h b/runtime/interpreter/mterp/mterp.h
index af52758bbc..59f61d7083 100644
--- a/runtime/interpreter/mterp/mterp.h
+++ b/runtime/interpreter/mterp/mterp.h
@@ -43,7 +43,12 @@ constexpr uintptr_t kExportPCPoison = 0xdead00ff;
 // Set true to enable poison testing of ExportPC.  Uses Alt interpreter.
 constexpr bool kTestExportPC = false;
 
+#if defined(__riscv)
+// Riscv MterpHandlerSize exceeds 128 bytes.
+constexpr size_t kMterpHandlerSize = 256;
+#else
 constexpr size_t kMterpHandlerSize = 128;
+#endif
 
 }  // namespace interpreter
 }  // namespace art
diff --git a/runtime/interpreter/mterp/riscv64/arithmetic.S b/runtime/interpreter/mterp/riscv64/arithmetic.S
new file mode 100644
index 0000000000..26e1f2ca83
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/arithmetic.S
@@ -0,0 +1,464 @@
+%def binop(preinstr="", result="a0", chkzero="0", instr=""):
+    /*
+     * Generic 32-bit binary operation.  Provide an "instr" line that
+     * specifies an instruction that performs "result = a0 op a1".
+     * This could be a MIPS instruction or a function call.  (If the result
+     * comes back in a register other than a0, you can override "result".)
+     *
+     * If "chkzero" is set to 1, we perform a divide-by-zero check on
+     * vCC (a1).  Useful for integer division and modulus.  Note that we
+     * *don't* check for (INT_MIN / -1) here, because the CPU handles it
+     * correctly.
+     *
+     * For: add-int, sub-int, mul-int, div-int, rem-int, and-int, or-int,
+     *      xor-int, shl-int, shr-int, ushr-int
+     */
+    /* binop vAA, vBB, vCC */
+    srli    a4, rINST, 8                # a4 <- AA
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG a0, a2                     # a0 <- vBB
+    GET_VREG a1, a3                     # a1 <- vCC
+    .if $chkzero
+    beqz    a1, common_errDivideByZero  # is second operand zero?
+    .endif
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    $preinstr                           # optional op
+    $instr                              # $result <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG $result, a4                # vAA <- $result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def binop2addr(preinstr="", result="a0", chkzero="0", instr=""):
+    /*
+     * Generic 32-bit "/2addr" binary operation.  Provide an "instr" line
+     * that specifies an instruction that performs "result = a0 op a1".
+     * This could be a MIPS instruction or a function call.  (If the result
+     * comes back in a register other than a0, you can override "result".)
+     *
+     * If "chkzero" is set to 1, we perform a divide-by-zero check on
+     * vB (a1).  Useful for integer division and modulus.  Note that we
+     * *don't* check for (INT_MIN / -1) here, because the CPU handles it
+     * correctly.
+     *
+     * For: add-int/2addr, sub-int/2addr, mul-int/2addr, div-int/2addr,
+     *      rem-int/2addr, and-int/2addr, or-int/2addr, xor-int/2addr,
+     *      shl-int/2addr, shr-int/2addr, ushr-int/2addr
+     */
+    /* binop/2addr vA, vB */
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG a0, a2                     # a0 <- vA
+    GET_VREG a1, a3                     # a1 <- vB
+    .if $chkzero
+    beqz    a1, common_errDivideByZero  # is second operand zero?
+    .endif
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    $preinstr                           # optional op
+    $instr                              # $result <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG $result, a2                # vA <- $result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def binopLit16(preinstr="", result="a0", chkzero="0", instr=""):
+    /*
+     * Generic 32-bit "lit16" binary operation.  Provide an "instr" line
+     * that specifies an instruction that performs "result = a0 op a1".
+     * This could be an MIPS instruction or a function call.  (If the result
+     * comes back in a register other than a0, you can override "result".)
+     *
+     * If "chkzero" is set to 1, we perform a divide-by-zero check on
+     * CCCC (a1).  Useful for integer division and modulus.
+     *
+     * For: add-int/lit16, rsub-int, mul-int/lit16, div-int/lit16,
+     *      rem-int/lit16, and-int/lit16, or-int/lit16, xor-int/lit16
+     */
+    /* binop/lit16 vA, vB, #+CCCC */
+    lh      a1, 2(rPC)                  # a1 <- sign-extended CCCC
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG a0, a3                     # a0 <- vB
+    .if $chkzero
+    beqz    a1, common_errDivideByZero  # is second operand zero?
+    .endif
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    $preinstr                           # optional op
+    $instr                              # $result <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG $result, a2                # vA <- $result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+
+%def binopLit8(preinstr="", result="a0", chkzero="0", instr=""):
+    /*
+     * Generic 32-bit "lit8" binary operation.  Provide an "instr" line
+     * that specifies an instruction that performs "result = a0 op a1".
+     * This could be an MIPS instruction or a function call.  (If the result
+     * comes back in a register other than a0, you can override "result".)
+     *
+     * If "chkzero" is set to 1, we perform a divide-by-zero check on
+     * CC (a1).  Useful for integer division and modulus.
+     *
+     * For: add-int/lit8, rsub-int/lit8, mul-int/lit8, div-int/lit8,
+     *      rem-int/lit8, and-int/lit8, or-int/lit8, xor-int/lit8,
+     *      shl-int/lit8, shr-int/lit8, ushr-int/lit8
+     */
+    /* binop/lit8 vAA, vBB, #+CC */
+    lbu     a3, 2(rPC)                  # a3 <- BB
+    lb      a1, 3(rPC)                  # a1 <- sign-extended CC
+    srli     a2, rINST, 8                # a2 <- AA
+    GET_VREG a0, a3                     # a0 <- vBB
+    .if $chkzero
+    beqz    a1, common_errDivideByZero  # is second operand zero?
+    .endif
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    $preinstr                           # optional op
+    $instr                              # $result <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG $result, a2                # vAA <- $result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+
+%def binopWide(preinstr="", result="a0", chkzero="0", instr=""):
+    /*
+     * Generic 64-bit binary operation.  Provide an "instr" line that
+     * specifies an instruction that performs "result = a0 op a1".
+     * This could be a MIPS instruction or a function call.  (If the result
+     * comes back in a register other than a0, you can override "result".)
+     *
+     * If "chkzero" is set to 1, we perform a divide-by-zero check on
+     * vCC (a1).  Useful for integer division and modulus.  Note that we
+     * *don't* check for (LONG_MIN / -1) here, because the CPU handles it
+     * correctly.
+     *
+     * For: add-long, sub-long, mul-long, div-long, rem-long, and-long, or-long,
+     *      xor-long, shl-long, shr-long, ushr-long
+     */
+    /* binop vAA, vBB, vCC */
+    srli    a4, rINST, 8                # a4 <- AA
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_WIDE a0, a2                # a0 <- vBB
+    GET_VREG_WIDE a1, a3                # a1 <- vCC
+    .if $chkzero
+    beqz    a1, common_errDivideByZero  # is second operand zero?
+    .endif
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    $preinstr                           # optional op
+    $instr                              # $result <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE $result, a4           # vAA <- $result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def binopWide2addr(preinstr="", result="a0", chkzero="0", instr=""):
+    /*
+     * Generic 64-bit "/2addr" binary operation.  Provide an "instr" line
+     * that specifies an instruction that performs "result = a0 op a1".
+     * This could be a MIPS instruction or a function call.  (If the result
+     * comes back in a register other than a0, you can override "result".)
+     *
+     * If "chkzero" is set to 1, we perform a divide-by-zero check on
+     * vB (a1).  Useful for integer division and modulus.  Note that we
+     * *don't* check for (LONG_MIN / -1) here, because the CPU handles it
+     * correctly.
+     *
+     * For: add-long/2addr, sub-long/2addr, mul-long/2addr, div-long/2addr,
+     *      rem-long/2addr, and-long/2addr, or-long/2addr, xor-long/2addr,
+     *      shl-long/2addr, shr-long/2addr, ushr-long/2addr
+     */
+    /* binop/2addr vA, vB */
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG_WIDE a0, a2                # a0 <- vA
+    GET_VREG_WIDE a1, a3                # a1 <- vB
+    .if $chkzero
+    beqz    a1, common_errDivideByZero  # is second operand zero?
+    .endif
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    $preinstr                           # optional op
+    $instr                              # $result <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE $result, a2           # vA <- $result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def unop(preinstr="", instr=""):
+    /*
+     * Generic 32-bit unary operation.  Provide an "instr" line that
+     * specifies an instruction that performs "a0 = op a0".
+     *
+     * for: int-to-byte, int-to-char, int-to-short,
+     *      not-int, neg-int
+     */
+    /* unop vA, vB */
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG a0, a3                     # a0 <- vB
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    $preinstr                           # optional op
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    $instr                              # a0 <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a2                     # vA <- a0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def unopWide(preinstr="", instr=""):
+    /*
+     * Generic 64-bit unary operation.  Provide an "instr" line that
+     * specifies an instruction that performs "a0 = op a0".
+     *
+     * For: not-long, neg-long
+     */
+    /* unop vA, vB */
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG_WIDE a0, a3                # a0 <- vB
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    $preinstr                           # optional op
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    $instr                              # a0 <- op, a0-a3 changed
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vA <- a0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_add_int():
+%  binop(instr="add a0, a0, a1")
+
+%def op_add_int_2addr():
+%  binop2addr(instr="add a0, a0, a1")
+
+%def op_add_int_lit16():
+%  binopLit16(instr="add a0, a0, a1")
+
+%def op_add_int_lit8():
+%  binopLit8(instr="add a0, a0, a1")
+
+%def op_add_long():
+%  binopWide(instr="add a0, a0, a1")
+
+%def op_add_long_2addr():
+%  binopWide2addr(instr="add a0, a0, a1")
+
+%def op_and_int():
+%  binop(instr="and a0, a0, a1")
+
+%def op_and_int_2addr():
+%  binop2addr(instr="and a0, a0, a1")
+
+%def op_and_int_lit16():
+%  binopLit16(instr="and a0, a0, a1")
+
+%def op_and_int_lit8():
+%  binopLit8(instr="and a0, a0, a1")
+
+%def op_and_long():
+%  binopWide(instr="and a0, a0, a1")
+
+%def op_and_long_2addr():
+%  binopWide2addr(instr="and a0, a0, a1")
+
+%def op_cmp_long():
+    /* cmp-long vAA, vBB, vCC */
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    srli    a4, rINST, 8                # a4 <- AA
+    GET_VREG_WIDE a0, a2                # a0 <- vBB
+    GET_VREG_WIDE a1, a3                # a1 <- vCC
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    slt     a2, a0, a1
+    slt     a0, a1, a0
+    sub     a0, a0, a2
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a4                     # vAA <- result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_div_int():
+%  binop(instr="div a0, a0, a1", chkzero="1")
+
+%def op_div_int_2addr():
+%  binop2addr(instr="div a0, a0, a1", chkzero="1")
+
+%def op_div_int_lit16():
+%  binopLit16(instr="div a0, a0, a1", chkzero="1")
+
+%def op_div_int_lit8():
+%  binopLit8(instr="div a0, a0, a1", chkzero="1")
+
+%def op_div_long():
+%  binopWide(instr="div a0, a0, a1", chkzero="1")
+
+%def op_div_long_2addr():
+%  binopWide2addr(instr="div a0, a0, a1", chkzero="1")
+
+%def op_int_to_byte():
+/* sign-extend lowest 8-bit in a0*/
+%  unop(instr="slli   a0, a0, 56 \n\t srai   a0, a0, 56")
+
+%def op_int_to_char():
+/* unop(instr="andi a0, a0, 0xffff")*/
+%  unop(instr="li t5, 0xffff \n and a0, a0, t5")
+
+%def op_int_to_long():
+    /* int-to-long vA, vB */
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG a0, a3                     # a0 <- vB (sign-extended to 64 bits)
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vA <- vB
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_int_to_short():
+/* sign-extend lowest 16-bit in a0*/
+%  unop(instr="slli   a0, a0, 48 \n\t srai   a0, a0, 48")
+
+%def op_long_to_int():
+/* we ignore the high word, making this equivalent to a 32-bit reg move */
+%  op_move()
+
+%def op_mul_int():
+%  binop(instr="mul a0, a0, a1")
+
+%def op_mul_int_2addr():
+%  binop2addr(instr="mul a0, a0, a1")
+
+%def op_mul_int_lit16():
+%  binopLit16(instr="mul a0, a0, a1")
+
+%def op_mul_int_lit8():
+%  binopLit8(instr="mul a0, a0, a1")
+
+%def op_mul_long():
+%  binopWide(instr="mul a0, a0, a1")
+
+%def op_mul_long_2addr():
+%  binopWide2addr(instr="mul a0, a0, a1")
+
+%def op_neg_int():
+%  unop(instr="sub    a0, zero, a0")
+
+%def op_neg_long():
+%  unopWide(instr="sub   a0, zero, a0")
+
+%def op_not_int():
+%  unop(instr="or     a0, zero, a0 \n\t not    a0, a0")
+
+%def op_not_long():
+%  unopWide(instr="or     a0, zero, a0 \n\t not    a0, a0")
+
+%def op_or_int():
+%  binop(instr="or a0, a0, a1")
+
+%def op_or_int_2addr():
+%  binop2addr(instr="or a0, a0, a1")
+
+%def op_or_int_lit16():
+%  binopLit16(instr="or a0, a0, a1")
+
+%def op_or_int_lit8():
+%  binopLit8(instr="or a0, a0, a1")
+
+%def op_or_long():
+%  binopWide(instr="or a0, a0, a1")
+
+%def op_or_long_2addr():
+%  binopWide2addr(instr="or a0, a0, a1")
+
+%def op_rem_int():
+%  binop(instr="rem a0, a0, a1", chkzero="1")
+
+%def op_rem_int_2addr():
+%  binop2addr(instr="rem a0, a0, a1", chkzero="1")
+
+%def op_rem_int_lit16():
+%  binopLit16(instr="rem a0, a0, a1", chkzero="1")
+
+%def op_rem_int_lit8():
+%  binopLit8(instr="rem a0, a0, a1", chkzero="1")
+
+%def op_rem_long():
+%  binopWide(instr="rem a0, a0, a1", chkzero="1")
+
+%def op_rem_long_2addr():
+%  binopWide2addr(instr="rem a0, a0, a1", chkzero="1")
+
+%def op_rsub_int():
+%  binopLit16(instr="sub a0, a1, a0")
+
+%def op_rsub_int_lit8():
+%  binopLit8(instr="sub a0, a1, a0")
+
+%def op_shl_int():
+%  binop(instr="sllw a0, a0, a1")
+
+%def op_shl_int_2addr():
+%  binop2addr(instr="sllw a0, a0, a1")
+
+%def op_shl_int_lit8():
+%  binopLit8(instr="sllw a0, a0, a1")
+
+%def op_shl_long():
+%  binopWide(instr="sll a0, a0, a1")
+
+%def op_shl_long_2addr():
+%  binopWide2addr(instr="sll a0, a0, a1")
+
+%def op_shr_int():
+%  binop(instr="sraw a0, a0, a1")
+
+%def op_shr_int_2addr():
+%  binop2addr(instr="sraw a0, a0, a1")
+
+%def op_shr_int_lit8():
+%  binopLit8(instr="sraw a0, a0, a1")
+
+%def op_shr_long():
+%  binopWide(instr="sra a0, a0, a1")
+
+%def op_shr_long_2addr():
+%  binopWide2addr(instr="sra a0, a0, a1")
+
+%def op_sub_int():
+%  binop(instr="sub a0, a0, a1")
+
+%def op_sub_int_2addr():
+%  binop2addr(instr="sub a0, a0, a1")
+
+%def op_sub_long():
+%  binopWide(instr="sub a0, a0, a1")
+
+%def op_sub_long_2addr():
+%  binopWide2addr(instr="sub a0, a0, a1")
+
+%def op_ushr_int():
+# For 32bit ushr, clear high 32-bit for the padding 0 can be shift in the result(in low 32-bit)
+%  binop(instr="srlw a0, a0, a1")
+
+%def op_ushr_int_2addr():
+# For 32bit ushr, clear high 32-bit for the padding 0 can be shift in the result(in low 32-bit)
+%  binop2addr(instr="srlw a0, a0, a1")
+
+%def op_ushr_int_lit8():
+# For 32bit ushr, clear high 32-bit for the padding 0 can be shift in the result(in low 32-bit)
+%  binopLit8(instr="srlw a0, a0, a1")
+
+%def op_ushr_long():
+%  binopWide(instr="srl a0, a0, a1")
+
+%def op_ushr_long_2addr():
+%  binopWide2addr(instr="srl a0, a0, a1")
+
+%def op_xor_int():
+%  binop(instr="xor a0, a0, a1")
+
+%def op_xor_int_2addr():
+%  binop2addr(instr="xor a0, a0, a1")
+
+%def op_xor_int_lit16():
+%  binopLit16(instr="xor a0, a0, a1")
+
+%def op_xor_int_lit8():
+%  binopLit8(instr="xor a0, a0, a1")
+
+%def op_xor_long():
+%  binopWide(instr="xor a0, a0, a1")
+
+%def op_xor_long_2addr():
+%  binopWide2addr(instr="xor a0, a0, a1")
diff --git a/runtime/interpreter/mterp/riscv64/array.S b/runtime/interpreter/mterp/riscv64/array.S
new file mode 100644
index 0000000000..84ff9b71c1
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/array.S
@@ -0,0 +1,241 @@
+%def op_aget(load="lw", shift="2", data_offset="MIRROR_INT_ARRAY_DATA_OFFSET"):
+    /*
+     * Array get, 32 bits or less.  vAA <- vBB[vCC].
+     *
+     * for: aget, aget-boolean, aget-byte, aget-char, aget-short
+     *
+     * NOTE: assumes data offset for arrays is the same for all non-wide types.
+     * If this changes, specialize.
+     */
+    /* op vAA, vBB, vCC */
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    srli     a4, rINST, 8                # a4 <- AA
+    GET_VREG a0, a2                     # a0 <- vBB (array object)
+    GET_VREG a1, a3                     # a1 <- vCC (requested index)
+    beqz    a0, common_errNullObject    # bail if null array object
+    lw      a3, MIRROR_ARRAY_LENGTH_OFFSET(a0)  # a3 <- arrayObj->length
+    .if $shift
+    # [d]lsa does not support shift count of 0.
+    DLSA    a0, a1, a0, $shift          # a0 <- arrayObj + index*width
+    .else
+    add   a0, a1, a0                  # a0 <- arrayObj + index*width
+    .endif
+    bgeu    a1, a3, common_errArrayIndex  # unsigned compare: index >= length, bail
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    $load   a2, $data_offset(a0)        # a2 <- vBB[vCC]
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a2, a4                     # vAA <- a2
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_aget_boolean():
+%  op_aget(load="lbu", shift="0", data_offset="MIRROR_BOOLEAN_ARRAY_DATA_OFFSET")
+
+%def op_aget_byte():
+%  op_aget(load="lb", shift="0", data_offset="MIRROR_BYTE_ARRAY_DATA_OFFSET")
+
+%def op_aget_char():
+%  op_aget(load="lhu", shift="1", data_offset="MIRROR_CHAR_ARRAY_DATA_OFFSET")
+
+%def op_aget_object():
+    /*
+     * Array object get.  vAA <- vBB[vCC].
+     *
+     * for: aget-object
+     */
+    /* op vAA, vBB, vCC */
+    .extern artAGetObjectFromMterp
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    EXPORT_PC
+    GET_VREG a0, a2                   # a0 <- vBB (array object)
+    GET_VREG a1, a3                     # a1 <- vCC (requested index)
+    jal     artAGetObjectFromMterp      # (array, index)
+
+    ld      a1, THREAD_EXCEPTION_OFFSET(rSELF)
+    srli     a4, rINST, 8                # a4 <- AA
+    PREFETCH_INST 2
+    bnez    a1, MterpException
+    SET_VREG_OBJECT a0, a4              # vAA <- v0
+    ADVANCE 2
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_aget_short():
+%  op_aget(load="lh", shift="1", data_offset="MIRROR_SHORT_ARRAY_DATA_OFFSET")
+
+%def op_aget_wide():
+    /*
+     * Array get, 64 bits.  vAA <- vBB[vCC].
+     *
+     */
+    /* aget-wide vAA, vBB, vCC */
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    srli     a4, rINST, 8                # a4 <- AA
+    GET_VREG_U a0, a2                   # a0 <- vBB (array object)
+    GET_VREG a1, a3                     # a1 <- vCC (requested index)
+    beqz    a0, common_errNullObject    # bail if null array object
+    lw      a3, MIRROR_ARRAY_LENGTH_OFFSET(a0)  # a3 <- arrayObj->length
+    DLSA    a0, a1, a0, 3               # a0 <- arrayObj + index*width
+    bgeu    a1, a3, common_errArrayIndex  # unsigned compare: index >= length, bail
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    lw      a2, MIRROR_WIDE_ARRAY_DATA_OFFSET(a0)
+    lw      a3, (MIRROR_WIDE_ARRAY_DATA_OFFSET+4)(a0)
+    DINSU   a2, a3, 32, 32              # a2 <- vBB[vCC]
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a2, a4                # vAA <- a2
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_aput(store="sw", shift="2", data_offset="MIRROR_INT_ARRAY_DATA_OFFSET"):
+    /*
+     * Array put, 32 bits or less.  vBB[vCC] <- vAA.
+     *
+     * for: aput, aput-boolean, aput-byte, aput-char, aput-short
+     *
+     * NOTE: this assumes data offset for arrays is the same for all non-wide types.
+     * If this changes, specialize.
+     */
+    /* op vAA, vBB, vCC */
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    srli     a4, rINST, 8                # a4 <- AA
+    GET_VREG a0, a2                   # a0 <- vBB (array object)
+    GET_VREG a1, a3                     # a1 <- vCC (requested index)
+    beqz    a0, common_errNullObject    # bail if null array object
+    lw      a3, MIRROR_ARRAY_LENGTH_OFFSET(a0)  # a3 <- arrayObj->length
+    .if $shift
+    # [d]lsa does not support shift count of 0.
+    DLSA    a0, a1, a0, $shift          # a0 <- arrayObj + index*width
+    .else
+    add     a0, a1, a0                  # a0 <- arrayObj + index*width
+    .endif
+    bgeu    a1, a3, common_errArrayIndex  # unsigned compare: index >= length, bail
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_VREG a2, a4                     # a2 <- vAA
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    $store  a2, $data_offset(a0)        # vBB[vCC] <- a2
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_aput_boolean():
+%  op_aput(store="sb", shift="0", data_offset="MIRROR_BOOLEAN_ARRAY_DATA_OFFSET")
+
+%def op_aput_byte():
+%  op_aput(store="sb", shift="0", data_offset="MIRROR_BYTE_ARRAY_DATA_OFFSET")
+
+%def op_aput_char():
+%  op_aput(store="sh", shift="1", data_offset="MIRROR_CHAR_ARRAY_DATA_OFFSET")
+
+%def op_aput_object():
+    /*
+     * Store an object into an array.  vBB[vCC] <- vAA.
+     */
+    /* op vAA, vBB, vCC */
+    .extern MterpAputObject
+    EXPORT_PC
+    addi     a0, rFP, OFF_FP_SHADOWFRAME
+    move    a1, rPC
+    move    a2, rINST
+    jal     MterpAputObject
+    beqz    a0, MterpPossibleException
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_aput_short():
+%  op_aput(store="sh", shift="1", data_offset="MIRROR_SHORT_ARRAY_DATA_OFFSET")
+
+%def op_aput_wide():
+    /*
+     * Array put, 64 bits.  vBB[vCC] <- vAA.
+     *
+     */
+    /* aput-wide vAA, vBB, vCC */
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    srli    a4, rINST, 8                # a4 <- AA
+    GET_VREG a0, a2                   # a0 <- vBB (array object)
+    GET_VREG a1, a3                     # a1 <- vCC (requested index)
+    beqz    a0, common_errNullObject    # bail if null array object
+    lw      a3, MIRROR_ARRAY_LENGTH_OFFSET(a0)  # a3 <- arrayObj->length
+    DLSA    a0, a1, a0, 3               # a0 <- arrayObj + index*width
+    bgeu    a1, a3, common_errArrayIndex  # unsigned compare: index >= length, bail
+    GET_VREG_WIDE a2, a4                # a2 <- vAA
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    sd      a2, MIRROR_WIDE_ARRAY_DATA_OFFSET(a0) # vBB[vCC] <- a2
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_array_length():
+    /*
+     * Return the length of an array.
+     */
+    srli    a1, rINST, 12               # a1 <- B
+    GET_VREG a0, a1                   # a0 <- vB (object ref)
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    beqz    a0, common_errNullObject    # yup, fail
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    lw      a3, MIRROR_ARRAY_LENGTH_OFFSET(a0)  # a3 <- array length
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a3, a2                     # vB <- length
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_fill_array_data():
+    /* fill-array-data vAA, +BBBBBBBB */
+    .extern MterpFillArrayData
+    EXPORT_PC
+    lhu     a1, 2(rPC)                  # a1 <- 000000000000bbbb (lo)
+    lh      a0, 4(rPC)                  # a0 <- ssssssssssssBBBB (hi)
+    srli    a3, rINST, 8                # a3 <- AA
+    slli    a0, a0, 16
+    or      a1, a0, a1                  # a1 <- ssssssssBBBBbbbb
+    GET_VREG a0, a3                    # a0 <- vAA (array object)
+    DLSA    a1, a1, rPC, 1              # a1 <- PC + BBBBbbbb*2 (array data off.)
+    jal     MterpFillArrayData          # (obj, payload)
+    beqz    a0, MterpPossibleException  # exception?
+    FETCH_ADVANCE_INST 3                # advance rPC, load rINST
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_filled_new_array(helper="MterpFilledNewArray"):
+    /*
+     * Create a new array with elements filled from registers.
+     *
+     * for: filled-new-array, filled-new-array/range
+     */
+    /* op vB, {vD, vE, vF, vG, vA}, class//CCCC */
+    /* op {vCCCC..v(CCCC+AA-1)}, type//BBBB */
+    .extern $helper
+    EXPORT_PC
+    addi    a0, rFP, OFF_FP_SHADOWFRAME
+    move    a1, rPC
+    move    a2, rSELF
+    jal     $helper
+    beqz    a0, MterpPossibleException
+    FETCH_ADVANCE_INST 3                # advance rPC, load rINST
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_filled_new_array_range():
+%  op_filled_new_array(helper="MterpFilledNewArrayRange")
+
+%def op_new_array():
+    /*
+     * Allocate an array of objects, specified with the array class
+     * and a count.
+     *
+     * The verifier guarantees that this is an array class, so we don't
+     * check for it here.
+     */
+    /* new-array vA, vB, class//CCCC */
+    .extern MterpNewArray
+    EXPORT_PC
+    addi   a0, rFP, OFF_FP_SHADOWFRAME
+    move    a1, rPC
+    move    a2, rINST
+    move    a3, rSELF
+    jal     MterpNewArray
+    beqz    a0, MterpPossibleException
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
diff --git a/runtime/interpreter/mterp/riscv64/control_flow.S b/runtime/interpreter/mterp/riscv64/control_flow.S
new file mode 100644
index 0000000000..410012d891
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/control_flow.S
@@ -0,0 +1,223 @@
+%def bincmp(condition=""):
+    /*
+     * Generic two-operand compare-and-branch operation.  Provide a "condition"
+     * fragment that specifies the comparison to perform, e.g. for
+     * "if-le" you would use "le".
+     *
+     * For: if-eq, if-ne, if-lt, if-ge, if-gt, if-le
+     */
+    /* if-cmp vA, vB, +CCCC */
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    lh      rINST, 2(rPC)               # rINST <- offset (sign-extended CCCC)
+    GET_VREG a0, a2                     # a0 <- vA
+    GET_VREG a1, a3                     # a1 <- vB
+    b${condition} a0, a1, MterpCommonTakenBranchNoFlags
+    li      t4, JIT_CHECK_OSR           # possible OSR re-entry?
+    beq    rPROFILE, t4, .L_check_not_taken_osr
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def zcmp(condition=""):
+    /*
+     * Generic one-operand compare-and-branch operation.  Provide a "condition"
+     * fragment that specifies the comparison to perform, e.g. for
+     * "if-lez" you would use "le".
+     *
+     * For: if-eqz, if-nez, if-ltz, if-gez, if-gtz, if-lez
+     */
+    /* if-cmp vAA, +BBBB */
+    srli     a2, rINST, 8                # a2 <- AA
+    lh       rINST, 2(rPC)               # rINST <- offset (sign-extended BBBB)
+    GET_VREG a0, a2                     # a0 <- vAA
+    b${condition}z a0, MterpCommonTakenBranchNoFlags
+    li       t4, JIT_CHECK_OSR           # possible OSR re-entry?
+    beq      rPROFILE, t4, .L_check_not_taken_osr
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_goto():
+    /*
+     * Unconditional branch, 8-bit offset.
+     *
+     * The branch distance is a signed code-unit offset, which we need to
+     * double to get a byte offset.
+     */
+     /* goto +AA */
+    lb      rINST, 1(rPC)                 # sign extended load 8-bit offset into rINST
+
+    j       MterpCommonTakenBranchNoFlags
+
+%def op_goto_16():
+    /*
+     * Unconditional branch, 16-bit offset.
+     *
+     * The branch distance is a signed code-unit offset, which we need to
+     * double to get a byte offset.
+     */
+    /* goto/16 +AAAA */
+    lh      rINST, 2(rPC)               # rINST <- offset (sign-extended AAAA)
+    j       MterpCommonTakenBranchNoFlags
+
+%def op_goto_32():
+    /*
+     * Unconditional branch, 32-bit offset.
+     *
+     * The branch distance is a signed code-unit offset, which we need to
+     * double to get a byte offset.
+     *
+     * Unlike most opcodes, this one is allowed to branch to itself, so
+     * our "backward branch" test must be "<=0" instead of "<0".
+     */
+    /* goto/32 +AAAAAAAA */
+    lhu      rINST, 2(rPC)               # rINST <- aaaa (low)
+    lhu      a1, 4(rPC)                  # a1 <- AAAA (high)
+    slli     a1, a1, 16                  # rINST <- offset (sign-extended AAAAaaaa)
+    or       rINST, rINST, a1
+    j       MterpCommonTakenBranchNoFlags
+
+%def op_if_eq():
+%  bincmp(condition="eq")
+
+%def op_if_eqz():
+%  zcmp(condition="eq")
+
+%def op_if_ge():
+%  bincmp(condition="ge")
+
+%def op_if_gez():
+%  zcmp(condition="ge")
+
+%def op_if_gt():
+%  bincmp(condition="gt")
+
+%def op_if_gtz():
+%  zcmp(condition="gt")
+
+%def op_if_le():
+%  bincmp(condition="le")
+
+%def op_if_lez():
+%  zcmp(condition="le")
+
+%def op_if_lt():
+%  bincmp(condition="lt")
+
+%def op_if_ltz():
+%  zcmp(condition="lt")
+
+%def op_if_ne():
+%  bincmp(condition="ne")
+
+%def op_if_nez():
+%  zcmp(condition="ne")
+
+%def op_packed_switch(func="MterpDoPackedSwitch"):
+    /*
+     * Handle a packed-switch or sparse-switch instruction.  In both cases
+     * we decode it and hand it off to a helper function.
+     *
+     * We don't really expect backward branches in a switch statement, but
+     * they're perfectly legal, so we check for them here.
+     *
+     * for: packed-switch, sparse-switch
+     */
+    /* op vAA, +BBBBBBBB */
+    .extern $func
+    lhu     a0, 2(rPC)                  # a0 <- 000000000000bbbb (lo)
+    lh      a1, 4(rPC)                  # a1 <- ssssssssssssBBBB (hi)
+    srli    a3, rINST, 8                # a3 <- AA
+    slli    a1, a1, 16
+    or      a0, a1, a0                  # a0<- ssssssssBBBBbbbb
+    GET_VREG a1, a3                     # a1 <- vAA
+    DLSA    a0, a0, rPC, 1              # a0 <- PC + ssssssssBBBBbbbb*2
+    jal     $func                       # v0 <- code-unit branch offset
+    move    rINST, a0
+    j       MterpCommonTakenBranchNoFlags
+
+%def op_return(instr="GET_VREG"):
+    /*
+     * Return a 32-bit value.
+     *
+     * for: return (sign-extend), return-object (zero-extend)
+     */
+    /* op vAA */
+    .extern MterpThreadFenceForConstructor
+    .extern MterpSuspendCheck
+    jal     MterpThreadFenceForConstructor
+
+    lw      ra, THREAD_FLAGS_OFFSET(rSELF)
+    move    a0, rSELF
+    andi    ra, ra, THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
+    beqz    ra, 1f
+    jal     MterpSuspendCheck           # (self)
+1:
+    srli    a2, rINST, 8                # a2 <- AA
+    $instr  a0, a2                      # a0 <- vAA
+    j       MterpReturn
+
+%def op_return_object():
+%  op_return(instr="GET_VREG_U")
+
+%def op_return_void():
+    .extern MterpThreadFenceForConstructor
+    .extern MterpSuspendCheck
+    jal     MterpThreadFenceForConstructor
+
+    lw      ra, THREAD_FLAGS_OFFSET(rSELF)
+    move    a0, rSELF
+    andi    ra, ra, THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
+    beqz    ra, 1f
+    jal     MterpSuspendCheck           # (self)
+1:
+    li      a0, 0
+    j       MterpReturn
+
+%def op_return_void_no_barrier():
+    .extern MterpSuspendCheck
+    lw      ra, THREAD_FLAGS_OFFSET(rSELF)
+    move    a0, rSELF
+    andi    ra, ra, THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
+    beqz    ra, 1f
+    jal     MterpSuspendCheck           # (self)
+1:
+    li      a0, 0
+    j       MterpReturn
+
+%def op_return_wide():
+    /*
+     * Return a 64-bit value.
+     */
+    /* return-wide vAA */
+    /* op vAA */
+    .extern MterpThreadFenceForConstructor
+    .extern MterpSuspendCheck
+    lw      ra, THREAD_FLAGS_OFFSET(rSELF)
+    jal     MterpThreadFenceForConstructor
+
+    move    a0, rSELF
+    andi    ra, ra, THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
+    beqz    ra, 1f
+    jal     MterpSuspendCheck           # (self)
+1:
+    srli    a2, rINST, 8                # a2 <- AA
+    GET_VREG_WIDE a0, a2                # a0 <- vAA
+    j       MterpReturn
+
+%def op_sparse_switch():
+%  op_packed_switch(func="MterpDoSparseSwitch")
+
+%def op_throw():
+    /*
+     * Throw an exception object in the current thread.
+     */
+    /* throw vAA */
+    EXPORT_PC
+    srli     a2, rINST, 8                # a2 <- AA
+    GET_VREG_U a0, a2                   # a0 <- vAA (exception object)
+    sd      a0, THREAD_EXCEPTION_OFFSET(rSELF)  # thread->exception <- obj
+    beqz    a0, common_errNullObject
+
+    j       MterpException
diff --git a/runtime/interpreter/mterp/riscv64/floating_point.S b/runtime/interpreter/mterp/riscv64/floating_point.S
new file mode 100644
index 0000000000..dd36d58499
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/floating_point.S
@@ -0,0 +1,400 @@
+%def fbinop(instr=""):
+    /*:
+     * Generic 32-bit floating-point operation.
+     *
+     * For: add-float, sub-float, mul-float, div-float.
+     * form: <op> f0, f0, f1
+     */
+    /* binop vAA, vBB, vCC */
+    srli    a4, rINST, 8                # a4 <- AA
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_FLOAT f0, a2               # f0 <- vBB
+    GET_VREG_FLOAT f1, a3               # f1 <- vCC
+    $instr                              # f0 <- f0 op f1
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_FLOAT f0, a4               # vAA <- f0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fbinop2addr(instr=""):
+    /*:
+     * Generic 32-bit "/2addr" floating-point operation.
+     *
+     * For: add-float/2addr, sub-float/2addr, mul-float/2addr, div-float/2addr.
+     * form: <op> f0, f0, f1
+     */
+    /* binop/2addr vA, vB */
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG_FLOAT f0, a2               # f0 <- vA
+    GET_VREG_FLOAT f1, a3               # f1 <- vB
+    $instr                              # f0 <- f0 op f1
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_FLOAT f0, a2               # vA <- f0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fbinopWide(instr=""):
+    /*:
+     * Generic 64-bit floating-point operation.
+     *
+     * For: add-double, sub-double, mul-double, div-double.
+     * form: <op> f0, f0, f1
+     */
+    /* binop vAA, vBB, vCC */
+    srli    a4, rINST, 8                # a4 <- AA
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_DOUBLE f0, a2              # f0 <- vBB
+    GET_VREG_DOUBLE f1, a3              # f1 <- vCC
+    $instr                              # f0 <- f0 op f1
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_DOUBLE f0, a4              # vAA <- f0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fbinopWide2addr(instr=""):
+    /*:
+     * Generic 64-bit "/2addr" floating-point operation.
+     *
+     * For: add-double/2addr, sub-double/2addr, mul-double/2addr, div-double/2addr.
+     * form: <op> f0, f0, f1
+     */
+    /* binop/2addr vA, vB */
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG_DOUBLE f0, a2              # f0 <- vA
+    GET_VREG_DOUBLE f1, a3              # f1 <- vB
+    $instr                              # f0 <- f0 op f1
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_DOUBLE f0, a2              # vA <- f0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fcmp(gt_bias=""):
+    /*
+     * Compare two floating-point values.  Puts 0, 1, or -1 into the
+     * destination register based on the results of the comparison.
+     *
+     * For: cmpl-float, cmpg-float
+     */
+    /* op vAA, vBB, vCC */
+    srli     a4, rINST, 8                # a4 <- AA
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_FLOAT f0, a2               # f0 <- vBB
+    GET_VREG_FLOAT f1, a3               # f1 <- vCC
+    feq.s t4, f0, f1
+    li      a0, 0
+    bnez    t4, 1f                      # done if vBB == vCC (ordered)
+
+    .if $gt_bias
+    flt.s t4, f0, f1
+    li      a0, -1
+    bnez    t4, 1f                      # done if vBB < vCC (ordered)
+    li      a0, 1                       # vBB > vCC or unordered
+    .else
+    flt.s t4, f1, f0
+    li      a0, 1
+    bnez  t4, 1f                        # done if vBB > vCC (ordered)
+    li      a0, -1                      # vBB < vCC or unordered
+    .endif
+1:
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a4                     # vAA <- a0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fcmpWide(gt_bias=""):
+    /*
+     * Compare two floating-point values.  Puts 0, 1, or -1 into the
+     * destination register based on the results of the comparison.
+     *
+     * For: cmpl-double, cmpg-double
+     */
+    /* op vAA, vBB, vCC */
+    srli    a4, rINST, 8                # a4 <- AA
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_DOUBLE f0, a2              # f0 <- vBB
+    GET_VREG_DOUBLE f1, a3              # f1 <- vCC
+    feq.d   t4, f0, f1
+    li      a0, 0
+    bnez    t4, 1f                      # done if vBB == vCC (ordered)
+
+    .if $gt_bias
+    flt.d   t4, f0, f1
+    li      a0, -1
+    bnez    t4, 1f                      # done if vBB < vCC (ordered)
+    li      a0, 1                       # vBB > vCC or unordered
+    .else
+    flt.d   t4, f1, f0
+    li      a0, 1
+    bnez    t4, 1f                      # done if vBB > vCC (ordered)
+    li      a0, -1                      # vBB < vCC or unordered
+    .endif
+1:
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a4                     # vAA <- a0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fcvtFooter(suffix="", valreg=""):
+    /*
+     * Stores a specified register containing the result of conversion
+     * from or to a floating-point type and jumps to the next instruction.
+     *
+     * Expects a1 to contain the destination Dalvik register number.
+     * a1 is set up by fcvtHeader.S.
+     *
+     * For: int-to-float, int-to-double, long-to-float, long-to-double,
+     *      float-to-int, float-to-long, float-to-double, double-to-int,
+     *      double-to-long, double-to-float, neg-float, neg-double.
+     *
+     * Note that this file can't be included after a break in other files
+     * and in those files its contents appear as a copy.
+     * See: float-to-int, float-to-long, double-to-int, double-to-long.
+     */
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG$suffix $valreg, a1
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def fcvtHeader(suffix="", valreg=""):
+    /*
+     * Loads a specified register from vB. Used primarily for conversions
+     * from or to a floating-point type.
+     *
+     * Sets up a1 = A and a2 = B. a2 is later used by fcvtFooter.S to
+     * store the result in vA and jump to the next instruction.
+     *
+     * For: int-to-float, int-to-double, long-to-float, long-to-double,
+     *      float-to-int, float-to-long, float-to-double, double-to-int,
+     *      double-to-long, double-to-float, neg-float, neg-double.
+     */
+    EXT     a1, rINST, 8, 4             # a1 <- A
+    srli     a2, rINST, 12               # a2 <- B
+    GET_VREG$suffix $valreg, a2
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+
+%def op_add_double():
+%  fbinopWide(instr="fadd.d f0, f0, f1")
+
+%def op_add_double_2addr():
+%  fbinopWide2addr(instr="fadd.d f0, f0, f1")
+
+%def op_add_float():
+%  fbinop(instr="fadd.s f0, f0, f1")
+
+%def op_add_float_2addr():
+%  fbinop2addr(instr="fadd.s f0, f0, f1")
+
+%def op_cmpg_double():
+%  fcmpWide(gt_bias="1")
+
+%def op_cmpg_float():
+%  fcmp(gt_bias="1")
+
+%def op_cmpl_double():
+%  fcmpWide(gt_bias="0")
+
+%def op_cmpl_float():
+%  fcmp(gt_bias="0")
+
+%def op_div_double():
+%  fbinopWide(instr="fdiv.d f0, f0, f1")
+
+%def op_div_double_2addr():
+%  fbinopWide2addr(instr="fdiv.d f0, f0, f1")
+
+%def op_div_float():
+%  fbinop(instr="fdiv.s f0, f0, f1")
+
+%def op_div_float_2addr():
+%  fbinop2addr(instr="fdiv.s f0, f0, f1")
+
+%def op_double_to_float():
+    /*
+     * Conversion from or to floating-point happens in a floating-point register.
+     * Therefore we load the input and store the output into or from a
+     * floating-point register irrespective of the type.
+     */
+%  fcvtHeader(suffix="_DOUBLE", valreg="f0")
+   fcvt.s.d f0, f0
+%  fcvtFooter(suffix="_FLOAT", valreg="f0")
+
+%def op_double_to_int():
+%  fcvtHeader(suffix="_DOUBLE", valreg="f0")
+   xor     a0, a0, a0
+   feq.d   t4, f0, f0        # check if fp(B) is NaN
+   beqz    t4, 1f            # return 0 if fp(B) is NaN
+   fcvt.w.d a0, f0, rtz
+1:
+%  fcvtFooter(suffix="", valreg="a0")
+
+%def op_double_to_long():
+%  fcvtHeader(suffix="_DOUBLE", valreg="f0")
+   xor     a0, a0, a0
+   feq.d   t4, f0, f0        # check if fp(B) is NaN
+   beqz    t4, 1f            # return 0 if fp(B) is NaN
+   fcvt.l.d a0, f0, rtz
+1:
+%  fcvtFooter(suffix="_WIDE", valreg="a0")
+
+%def op_float_to_double():
+    /*
+     * Conversion from or to floating-point happens in a floating-point register.
+     * Therefore we load the input and store the output into or from a
+     * floating-point register irrespective of the type.
+     */
+%  fcvtHeader(suffix="_FLOAT", valreg="f0")
+    fcvt.d.s f0, f0
+%  fcvtFooter(suffix="_DOUBLE", valreg="f0")
+
+%def op_float_to_int():
+%  fcvtHeader(suffix="_FLOAT", valreg="f0")
+   xor     a0, a0, a0
+   feq.s   t4, f0, f0        # check if fp(B) is NaN
+   beqz    t4, 1f            # return 0 if fp(B) is NaN
+   fcvt.w.s a0, f0, rtz
+1:
+%  fcvtFooter(suffix="", valreg="a0")
+
+%def op_float_to_long():
+%  fcvtHeader(suffix="_FLOAT", valreg="f0")
+   xor     a0, a0, a0
+   feq.s   t4, f0, f0        # check if fp(B) is NaN
+   beqz    t4, 1f            # return 0 if fp(B) is NaN
+   fcvt.l.s a0, f0, rtz
+1:
+%  fcvtFooter(suffix="_WIDE", valreg="a0")
+
+%def op_int_to_double():
+    /*
+     * Conversion from or to floating-point happens in a floating-point register.
+     * Therefore we load the input and store the output into or from a
+     * floating-point register irrespective of the type.
+     */
+%  fcvtHeader(suffix="", valreg="a0")
+   fcvt.d.w f0, a0
+%  fcvtFooter(suffix="_DOUBLE", valreg="f0")
+
+%def op_int_to_float():
+    /*
+     * Conversion from or to floating-point happens in a floating-point register.
+     * Therefore we load the input and store the output into or from a
+     * floating-point register irrespective of the type.
+     */
+%  fcvtHeader(suffix="", valreg="a0")
+   fcvt.s.w f0, a0
+%  fcvtFooter(suffix="_FLOAT", valreg="f0")
+
+%def op_long_to_double():
+    /*
+     * Conversion from or to floating-point happens in a floating-point register.
+     * Therefore we load the input and store the output into or from a
+     * floating-point register irrespective of the type.
+     */
+%  fcvtHeader(suffix="_WIDE", valreg="a0")
+   fcvt.d.l f0, a0
+%  fcvtFooter(suffix="_DOUBLE", valreg="f0")
+
+%def op_long_to_float():
+    /*
+     * Conversion from or to floating-point happens in a floating-point register.
+     * Therefore we load the input and store the output into or from a
+     * floating-point register irrespective of the type.
+     */
+%  fcvtHeader(suffix="_WIDE", valreg="a0")
+   fcvt.s.l f0, a0
+%  fcvtFooter(suffix="_FLOAT", valreg="f0")
+
+%def op_mul_double():
+%  fbinopWide(instr="fmul.d f0, f0, f1")
+
+%def op_mul_double_2addr():
+%  fbinopWide2addr(instr="fmul.d f0, f0, f1")
+
+%def op_mul_float():
+%  fbinop(instr="fmul.s f0, f0, f1")
+
+%def op_mul_float_2addr():
+%  fbinop2addr(instr="fmul.s f0, f0, f1")
+
+%def op_neg_double():
+%  fcvtHeader(suffix="_DOUBLE", valreg="f0")
+    fneg.d   f0, f0
+%  fcvtFooter(suffix="_DOUBLE", valreg="f0")
+
+%def op_neg_float():
+%  fcvtHeader(suffix="_FLOAT", valreg="f0")
+    fneg.s   f0, f0
+%  fcvtFooter(suffix="_FLOAT", valreg="f0")
+
+%def op_rem_double():
+    /* rem-double vAA, vBB, vCC */
+    .extern fmod
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_DOUBLE f10, a2             # fa0 <- vBB
+    GET_VREG_DOUBLE f11, a3             # fa1 <- vCC
+    call     fmod@plt                    # f0 <- f12 op f13
+    srli     a4, rINST, 8                # a4 <- AA
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_DOUBLE f10, a4              # vAA <- fa0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_rem_double_2addr():
+    /* rem-double/2addr vA, vB */
+    .extern fmod
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG_DOUBLE f10, a2             # fa0 <- vA
+    GET_VREG_DOUBLE f11, a3             # fa1 <- vB
+    call     fmod@plt                   # fa0 <- fa0 op fa1
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_DOUBLE f10, a2              # vA <- fa0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_rem_float():
+    /* rem-float vAA, vBB, vCC */
+    .extern fmodf
+    lbu     a2, 2(rPC)                  # a2 <- BB
+    lbu     a3, 3(rPC)                  # a3 <- CC
+    GET_VREG_FLOAT f10, a2              # fa0 <- vBB
+    GET_VREG_FLOAT f11, a3              # fa1 <- vCC
+    call     fmodf@plt                   # fa0 <- fa0 op fa1
+    srli     a4, rINST, 8                # a4 <- AA
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_FLOAT f10, a4               # vAA <- fa0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_rem_float_2addr():
+    /* rem-float/2addr vA, vB */
+    .extern fmodf
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    GET_VREG_FLOAT f10, a2              # fa0 <- vA
+    GET_VREG_FLOAT f11, a3              # fa1 <- vB
+    call     fmodf@plt                  # fa0 <- f12 op f13
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_FLOAT f10, a2               # vA <- fa0
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_sub_double():
+%  fbinopWide(instr="fsub.d f0, f0, f1")
+
+%def op_sub_double_2addr():
+%  fbinopWide2addr(instr="fsub.d f0, f0, f1")
+
+%def op_sub_float():
+%  fbinop(instr="fsub.s f0, f0, f1")
+
+%def op_sub_float_2addr():
+%  fbinop2addr(instr="fsub.s f0, f0, f1")
diff --git a/runtime/interpreter/mterp/riscv64/invoke.S b/runtime/interpreter/mterp/riscv64/invoke.S
new file mode 100644
index 0000000000..85417049b4
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/invoke.S
@@ -0,0 +1,109 @@
+%def invoke(helper="UndefinedInvokeHandler"):
+    /*
+     * Generic invoke handler wrapper.
+     */
+    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
+    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
+    .extern $helper
+    .extern MterpShouldSwitchInterpreters
+    EXPORT_PC
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rPC
+    move    a3, rINST
+    jal     $helper
+    beqz   a0, MterpException
+    FETCH_ADVANCE_INST 3
+    ld      a0, THREAD_USE_MTERP_OFFSET(rSELF)
+    beqz    a0, MterpFallback
+    GET_INST_OPCODE a0
+    GOTO_OPCODE a0
+
+%def invoke_polymorphic(helper="UndefinedInvokeHandler"):
+    /*
+     * invoke-polymorphic handler wrapper.
+     */
+    /* op {vC, vD, vE, vF, vG}, meth@BBBB, proto@HHHH */
+    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB, proto@HHHH */
+    .extern $helper
+    EXPORT_PC
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rPC
+    move    a3, rINST
+    jal     $helper
+    beqz    a0, MterpException
+    FETCH_ADVANCE_INST 4
+    ld      a0, THREAD_USE_MTERP_OFFSET(rSELF)
+    beqz    a0, MterpFallback
+    GET_INST_OPCODE a0
+    GOTO_OPCODE a0
+
+%def op_invoke_custom():
+%  invoke(helper="MterpInvokeCustom")
+
+%def op_invoke_custom_range():
+%  invoke(helper="MterpInvokeCustomRange")
+
+%def op_invoke_direct():
+%  invoke(helper="MterpInvokeDirect")
+
+%def op_invoke_direct_range():
+%  invoke(helper="MterpInvokeDirectRange")
+
+%def op_invoke_interface():
+%  invoke(helper="MterpInvokeInterface")
+    /*
+     * Handle an interface method call.
+     *
+     * for: invoke-interface, invoke-interface/range
+     */
+    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
+    /* op {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
+
+%def op_invoke_interface_range():
+%  invoke(helper="MterpInvokeInterfaceRange")
+
+%def op_invoke_polymorphic():
+%  invoke_polymorphic(helper="MterpInvokePolymorphic")
+
+%def op_invoke_polymorphic_range():
+%  invoke_polymorphic(helper="MterpInvokePolymorphicRange")
+
+%def op_invoke_static():
+%  invoke(helper="MterpInvokeStatic")
+
+%def op_invoke_static_range():
+%  invoke(helper="MterpInvokeStaticRange")
+
+%def op_invoke_super():
+%  invoke(helper="MterpInvokeSuper")
+    /*
+     * Handle a "super" method call.
+     *
+     * for: invoke-super, invoke-super/range
+     */
+    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
+    /* op vAA, {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
+
+%def op_invoke_super_range():
+%  invoke(helper="MterpInvokeSuperRange")
+
+%def op_invoke_virtual():
+%  invoke(helper="MterpInvokeVirtual")
+    /*
+     * Handle a virtual method call.
+     *
+     * for: invoke-virtual, invoke-virtual/range
+     */
+    /* op vB, {vD, vE, vF, vG, vA}, class@CCCC */
+    /* op vAA, {vCCCC..v(CCCC+AA-1)}, meth@BBBB */
+
+%def op_invoke_virtual_quick():
+%  invoke(helper="MterpInvokeVirtualQuick")
+
+%def op_invoke_virtual_range():
+%  invoke(helper="MterpInvokeVirtualRange")
+
+%def op_invoke_virtual_range_quick():
+%  invoke(helper="MterpInvokeVirtualQuickRange")
diff --git a/runtime/interpreter/mterp/riscv64/main.S b/runtime/interpreter/mterp/riscv64/main.S
new file mode 100644
index 0000000000..61df5b5655
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/main.S
@@ -0,0 +1,799 @@
+%def header():
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#define AT   t6
+
+/*
+ * It looks like the GNU assembler currently does not support the blec and bgtc
+ * idioms, which should translate into bgec and bltc respectively with swapped
+ * left and right register operands.
+ * TODO: remove these macros when the assembler is fixed.
+ */
+/*.macro blec lreg, rreg, target
+    bgec    \rreg, \lreg, \target
+.endm
+.macro bgtc lreg, rreg, target
+    bltc    \rreg, \lreg, \target
+.endm
+*/
+/*
+Mterp and MIPS64 notes:
+
+The following registers have fixed assignments:
+
+  reg nick      purpose
+  s7  rPC       interpreted program counter, used for fetching instructions
+  s1  rFP       interpreted frame pointer, used for accessing locals and args
+  s2  rSELF     self (Thread) pointer
+  s3  rINST     first 16-bit code unit of current instruction
+  s4  rIBASE    interpreted instruction base pointer, used for computed goto
+  s5  rREFS     base of object references in shadow frame  (ideally, we'll get rid of this later).
+  s6  rPROFILE  jit profile hotness countdown
+*/
+
+/* During bringup, we'll use the shadow frame model instead of rFP */
+/* single-purpose registers, given names for clarity */
+#define rPC      s7
+#define CFI_DEX  23  // DWARF register number of the register holding dex-pc (s7).
+#define CFI_TMP  10   // DWARF register number of the first argument register (a0).
+#define rFP      s1
+#define rSELF    s2
+#define rINST    s3
+#define rIBASE   s4
+#define rREFS    s5
+#define rPROFILE s6
+
+/*
+ * This is a #include, not a %include, because we want the C pre-processor
+ * to expand the macros into assembler assignment statements.
+ */
+#include "asm_support.h"
+#include "interpreter/cfi_asm_support.h"
+
+/*
+ * Instead of holding a pointer to the shadow frame, we keep rFP at the base of the vregs.  So,
+ * to access other shadow frame fields, we need to use a backwards offset.  Define those here.
+ */
+#define OFF_FP(a) (a - SHADOWFRAME_VREGS_OFFSET)
+#define OFF_FP_NUMBER_OF_VREGS OFF_FP(SHADOWFRAME_NUMBER_OF_VREGS_OFFSET)
+#define OFF_FP_DEX_PC OFF_FP(SHADOWFRAME_DEX_PC_OFFSET)
+#define OFF_FP_LINK OFF_FP(SHADOWFRAME_LINK_OFFSET)
+#define OFF_FP_METHOD OFF_FP(SHADOWFRAME_METHOD_OFFSET)
+#define OFF_FP_RESULT_REGISTER OFF_FP(SHADOWFRAME_RESULT_REGISTER_OFFSET)
+#define OFF_FP_DEX_PC_PTR OFF_FP(SHADOWFRAME_DEX_PC_PTR_OFFSET)
+#define OFF_FP_DEX_INSTRUCTIONS OFF_FP(SHADOWFRAME_DEX_INSTRUCTIONS_OFFSET)
+#define OFF_FP_SHADOWFRAME OFF_FP(0)
+
+#define MTERP_PROFILE_BRANCHES 1
+#define MTERP_LOGGING 1
+
+/*
+ * "export" the PC to dex_pc field in the shadow frame, f/b/o future exception objects.  Must
+ * be done *before* something throws.
+ *
+ * It's okay to do this more than once.
+ *
+ * NOTE: the fast interpreter keeps track of dex pc as a direct pointer to the mapped
+ * dex byte codes.  However, the rest of the runtime expects dex pc to be an instruction
+ * offset into the code_items_[] array.  For effiency, we will "export" the
+ * current dex pc as a direct pointer using the EXPORT_PC macro, and rely on GetDexPC
+ * to convert to a dex pc when needed.
+ */
+
+.macro EXPORT_PC
+    sd      rPC, OFF_FP_DEX_PC_PTR(rFP)
+.endm
+
+/*
+ * Refresh handler table.
+ */
+.macro REFRESH_IBASE
+    ld      rIBASE, THREAD_CURRENT_IBASE_OFFSET(rSELF)
+.endm
+
+/*
+ * Fetch the next instruction from rPC into rINST.  Does not advance rPC.
+ */
+.macro FETCH_INST
+    lhu     rINST, 0(rPC)
+.endm
+
+/* Advance rPC by some number of code units. */
+.macro ADVANCE count
+    addi  rPC, rPC, (\count) * 2
+.endm
+
+/*
+ * Fetch the next instruction from an offset specified by _reg and advance xPC.
+ * xPC to point to the next instruction.  "_reg" must specify the distance
+ * in bytes, *not* 16-bit code units, and may be a signed value.  Must not set flags.
+ *
+ */
+.macro FETCH_ADVANCE_INST_RB reg
+    add   rPC, rPC, \reg
+    FETCH_INST
+.endm
+
+/*
+ * Fetch the next instruction from the specified offset.  Advances rPC
+ * to point to the next instruction.
+ *
+ * This must come AFTER anything that can throw an exception, or the
+ * exception catch may miss.  (This also implies that it must come after
+ * EXPORT_PC.)
+ */
+.macro FETCH_ADVANCE_INST count
+    ADVANCE \count
+    FETCH_INST
+.endm
+
+/*
+ * Similar to FETCH_ADVANCE_INST, but does not update rPC.  Used to load
+ * rINST ahead of possible exception point.  Be sure to manually advance rPC
+ * later.
+ */
+.macro PREFETCH_INST count
+    lhu     rINST, ((\count) * 2)(rPC)
+.endm
+
+/*
+ * Put the instruction's opcode field into the specified register.
+ */
+.macro GET_INST_OPCODE reg
+    andi     \reg, rINST, 255
+.endm
+
+/*
+ * Begin executing the opcode in _reg.
+ */
+.macro  GOTO_OPCODE reg
+    slli    AT, \reg, ${handler_size_bits}
+    add     AT, rIBASE, AT
+    jr      AT
+    # .set at
+.endm
+
+
+/*
+ * MIPS64 dlsa rd,rs,rt,sa
+ * GPR[rd] <- (GPR[rs] << (sa+1)) + GPR[rt]
+ */
+ // FIXME: T-HEAD, For riscv, Needn't add 1 to 'sa' as it's included in input 'sa' when invoking DLSA.
+.macro DLSA reg, sreg, treg, sa
+    slli  AT, \sreg, \sa
+    add   \reg, AT, \treg
+.endm
+
+/*
+ * for ext rd,rs,pose,size
+ */
+.macro EXT reg, sreg, pose, size
+    srli    t6, \sreg, \pose
+    li      t5, -1
+    slli    t5, t5, \size
+    not     t5, t5
+    and     t6, t6, t5
+    addi    \reg, t6, 0
+    # addiw   reg, t6, 0
+.endm
+
+/*
+ * for ins rd,rs,pose,size
+ */
+.macro INS reg, sreg, pose, size
+    li      t5, -1
+    slli    t5, t5, \size
+    not     t5, t5
+    and     t0, \sreg, t5
+    slli    t0, t0, \pose
+
+    slli    t5, t5,  \pose
+    not     t5, t5
+    and     \reg, \reg, t5
+
+    or      \reg, \reg, t0
+    sext.w  \reg, \reg
+.endm
+
+/*
+ * for dins rd,rs,pose,size
+ */
+.macro DINS reg, sreg, pose, size
+    li      t5, -1
+    slli    t5, t5, \size
+    not     t5, t5
+    and     \sreg, \sreg, t5
+    slli    \sreg, \sreg, \pose
+
+    slli    t5, t5,  \pose
+    not     t5, t5
+    and     \reg, \reg, t5
+
+    or      \reg, \reg, \sreg
+    # sext.w  \reg, \reg
+.endm
+
+/*
+ * for dinsu rd,rs,pose,size
+ */
+.macro DINSU reg, sreg, pose, size
+    li      t5, -1
+    slli    t5, t5, \size
+    not     t5, t5
+    and     t0, \sreg, t5
+    slli    t0, t0, \pose
+
+    slli    t5, t5,  \pose
+    not     t5, t5
+    and     \reg, \reg, t5
+
+    or      \reg, \reg, t0
+    # sext.w  \reg, \reg
+.endm
+
+/*
+ * Get/set the 32-bit value from a Dalvik register.
+ * Note, GET_VREG does sign extension to 64 bits while
+ * GET_VREG_U does zero extension to 64 bits.
+ * One is useful for arithmetic while the other is
+ * useful for storing the result value as 64-bit.
+ */
+.macro GET_VREG reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    lw      \reg, 0(AT)
+.endm
+.macro GET_VREG_U reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    lwu     \reg, 0(AT)
+.endm
+.macro GET_VREG_FLOAT reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    flw     \reg, 0(AT)
+.endm
+.macro SET_VREG reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    sw      \reg, 0(AT)
+    DLSA    AT, \vreg, rREFS, 2
+    sw      zero, 0(AT)
+.endm
+.macro SET_VREG_OBJECT reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    sw      \reg, 0(AT)
+    DLSA    AT, \vreg, rREFS, 2
+    sw      \reg, 0(AT)
+.endm
+.macro SET_VREG_FLOAT reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    fsw    \reg, 0(AT)
+    DLSA    AT, \vreg, rREFS, 2
+    sw      zero, 0(AT)
+.endm
+
+/*
+ * Get/set the 64-bit value from a Dalvik register.
+ * Avoid unaligned memory accesses.
+ * Note, SET_VREG_WIDE clobbers the register containing the value being stored.
+ * Note, SET_VREG_DOUBLE clobbers the register containing the Dalvik register number.
+ */
+.macro GET_VREG_WIDE reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    ld      \reg, 0(AT)
+.endm
+.macro GET_VREG_DOUBLE reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    fld    \reg, 0(AT)
+.endm
+.macro SET_VREG_WIDE reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    sd      \reg, 0(AT)
+
+    DLSA    AT, \vreg, rREFS, 2
+    sd      zero, 0(AT)
+.endm
+.macro SET_VREG_DOUBLE reg, vreg
+    DLSA    AT, \vreg, rFP, 2
+    fsd    \reg, 0(AT)
+
+    DLSA    AT, \vreg, rREFS, 2
+    sd      zero, 0(AT)
+.endm
+
+/*
+ * On-stack offsets for spilling/unspilling callee-saved registers
+ * and the frame size.
+ */
+#define STACK_OFFSET_RA 0
+// #define STACK_OFFSET_GP 8
+#define STACK_OFFSET_S0 16
+#define STACK_OFFSET_S1 24
+#define STACK_OFFSET_S2 32
+#define STACK_OFFSET_S3 40
+#define STACK_OFFSET_S4 48
+#define STACK_OFFSET_S5 56
+#define STACK_OFFSET_S6 64
+#define STACK_OFFSET_S7 72
+#define STACK_OFFSET_S8 80
+#define STACK_OFFSET_S9 88
+#define STACK_OFFSET_S10 96
+#define STACK_OFFSET_S11 104
+#define STACK_SIZE       112    /* needs 16 byte alignment */
+
+/* Constants for float/double_to_int/long conversions */
+#define INT_MIN             0x80000000
+#define INT_MIN_AS_FLOAT    0xCF000000
+#define INT_MIN_AS_DOUBLE   0xC1E0000000000000
+#define LONG_MIN            0x8000000000000000
+#define LONG_MIN_AS_FLOAT   0xDF000000
+#define LONG_MIN_AS_DOUBLE  0xC3E0000000000000
+
+
+/*
+ * function support macros.
+ */
+.macro ENTRY name
+    .type \name, %function
+    .hidden \name  // Hide this as a global symbol, so we do not incur plt calls.
+    .global \name
+    /* Cache alignment for function entry */
+    .balign 16
+\name:
+.endm
+
+.macro END name
+    .size \name, .-\name
+.endm
+
+
+%def entry():
+/*
+ * Copyright (C) 2016 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Interpreter entry point.
+ */
+
+     # .set    reorder
+
+    .text
+    .global ExecuteMterpImpl
+    .type   ExecuteMterpImpl, %function
+    .balign 16
+/*
+ * On entry:
+ *  a0  Thread* self
+ *  a1  dex_instructions
+ *  a2  ShadowFrame
+ *  a3  JValue* result_register
+ *
+ */
+
+ENTRY ExecuteMterpImpl
+    .cfi_startproc
+
+    .cfi_def_cfa sp, 0
+    addi  sp, sp, -STACK_SIZE
+    .cfi_adjust_cfa_offset STACK_SIZE
+
+    sd      ra, STACK_OFFSET_RA(sp)
+    .cfi_rel_offset 31, STACK_OFFSET_RA
+
+    sd      s0, STACK_OFFSET_S0(sp)
+    .cfi_rel_offset 16, STACK_OFFSET_S0
+    sd      s1, STACK_OFFSET_S1(sp)
+    .cfi_rel_offset 17, STACK_OFFSET_S1
+    sd      s2, STACK_OFFSET_S2(sp)
+    .cfi_rel_offset 18, STACK_OFFSET_S2
+    sd      s3, STACK_OFFSET_S3(sp)
+    .cfi_rel_offset 19, STACK_OFFSET_S3
+    sd      s4, STACK_OFFSET_S4(sp)
+    .cfi_rel_offset 20, STACK_OFFSET_S4
+    sd      s5, STACK_OFFSET_S5(sp)
+    .cfi_rel_offset 21, STACK_OFFSET_S5
+    sd      s6, STACK_OFFSET_S6(sp)
+    .cfi_rel_offset 22, STACK_OFFSET_S6
+    sd      s7, STACK_OFFSET_S7(sp)
+    .cfi_rel_offset 23, STACK_OFFSET_S7
+    sd      s8, STACK_OFFSET_S8(sp)
+    .cfi_rel_offset 24, STACK_OFFSET_S8
+    sd      s9, STACK_OFFSET_S9(sp)
+    .cfi_rel_offset 25, STACK_OFFSET_S9
+    sd      s10, STACK_OFFSET_S10(sp)
+    .cfi_rel_offset 26, STACK_OFFSET_S10
+    sd      s11, STACK_OFFSET_S11(sp)
+    .cfi_rel_offset 27, STACK_OFFSET_S11
+
+    /* Remember the return register */
+    sd      a3, SHADOWFRAME_RESULT_REGISTER_OFFSET(a2)
+
+    /* Remember the dex instruction pointer */
+    sd      a1, SHADOWFRAME_DEX_INSTRUCTIONS_OFFSET(a2)
+
+    /* set up "named" registers */
+    move    rSELF, a0
+    addi    rFP, a2, SHADOWFRAME_VREGS_OFFSET
+    lw      t4, SHADOWFRAME_NUMBER_OF_VREGS_OFFSET(a2)
+    DLSA    rREFS, t4, rFP, 2
+    lw      t4, SHADOWFRAME_DEX_PC_OFFSET(a2)
+    DLSA    rPC, t4, a1, 1
+    CFI_DEFINE_DEX_PC_WITH_OFFSET(CFI_TMP, CFI_DEX, 0)
+    EXPORT_PC
+
+    /* Starting ibase */
+    REFRESH_IBASE
+
+    /* Set up for backwards branches & osr profiling */
+    ld      a0, OFF_FP_METHOD(rFP)
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rSELF
+    jal     MterpSetUpHotnessCountdown
+    move    rPROFILE, a0                # Starting hotness countdown to rPROFILE
+
+    /* start executing the instruction at rPC */
+    FETCH_INST
+    GET_INST_OPCODE t4
+    GOTO_OPCODE t4
+
+    /* NOTE: no fallthrough */
+    END ExecuteMterpImpl
+
+%def dchecks_before_helper():
+    // Call C++ to do debug checks and return to the handler using tail call.
+    .extern MterpCheckBefore
+    la      t6, MterpCheckBefore
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rPC
+    jalr    zero, t6                            # (self, shadow_frame, dex_pc_ptr) Note: tail call.
+
+%def opcode_pre():
+%  add_helper(dchecks_before_helper, "mterp_dchecks_before_helper")
+    #if !defined(NDEBUG)
+    jal    mterp_dchecks_before_helper
+    #endif
+
+%def footer():
+    # .cfi_endproc
+    END MterpHelpers
+
+%def fallback():
+/* Transfer stub to alternate interpreter */
+    j       MterpFallback
+
+%def helpers():
+    ENTRY MterpHelpers
+
+/*
+ * We've detected a condition that will result in an exception, but the exception
+ * has not yet been thrown.  Just bail out to the reference interpreter to deal with it.
+ * TUNING: for consistency, we may want to just go ahead and handle these here.
+ */
+
+    .extern MterpLogDivideByZeroException
+common_errDivideByZero:
+    EXPORT_PC
+#if MTERP_LOGGING
+    move    a0, rSELF
+    addi   a1, rFP, OFF_FP_SHADOWFRAME
+    jal     MterpLogDivideByZeroException
+#endif
+    j       MterpCommonFallback
+
+    .extern MterpLogArrayIndexException
+common_errArrayIndex:
+    EXPORT_PC
+#if MTERP_LOGGING
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    jal     MterpLogArrayIndexException
+#endif
+    j       MterpCommonFallback
+
+    .extern MterpLogNullObjectException
+common_errNullObject:
+    EXPORT_PC
+#if MTERP_LOGGING
+    move    a0, rSELF
+    addi     a1, rFP, OFF_FP_SHADOWFRAME
+    jal     MterpLogNullObjectException
+#endif
+    j       MterpCommonFallback
+
+/*
+ * If we're here, something is out of the ordinary.  If there is a pending
+ * exception, handle it.  Otherwise, roll back and retry with the reference
+ * interpreter.
+ */
+MterpPossibleException:
+    ld      a0, THREAD_EXCEPTION_OFFSET(rSELF)
+    beqz    a0, MterpFallback                       # If not, fall back to reference interpreter.
+    /* intentional fallthrough - handle pending exception. */
+/*
+ * On return from a runtime helper routine, we've found a pending exception.
+ * Can we handle it here - or need to bail out to caller?
+ *
+ */
+    .extern MterpHandleException
+MterpException:
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    jal     MterpHandleException                    # (self, shadow_frame)
+    beqz    a0, MterpExceptionReturn                # no local catch, back to caller.
+    ld      a0, OFF_FP_DEX_INSTRUCTIONS(rFP)
+    lwu     a1, OFF_FP_DEX_PC(rFP)
+    REFRESH_IBASE
+    DLSA    rPC, a1, a0, 1                          # generate new dex_pc_ptr
+    /* Do we need to switch interpreters? */
+    ld      a0, THREAD_USE_MTERP_OFFSET(rSELF)
+    beqz    a0, MterpFallback
+    /* resume execution at catch block */
+    EXPORT_PC
+    FETCH_INST
+    GET_INST_OPCODE a0
+    GOTO_OPCODE a0
+    /* NOTE: no fallthrough */
+
+/*
+ * Common handling for branches with support for Jit profiling.
+ * On entry:
+ *    rINST          <= signed offset
+ *    rPROFILE       <= signed hotness countdown (expanded to 64 bits)
+ *
+ * We have quite a few different cases for branch profiling, OSR detection and
+ * suspend check support here.
+ *
+ * Taken backward branches:
+ *    If profiling active, do hotness countdown and report if we hit zero.
+ *    If in osr check mode, see if our target is a compiled loop header entry and do OSR if so.
+ *    Is there a pending suspend request?  If so, suspend.
+ *
+ * Taken forward branches and not-taken backward branches:
+ *    If in osr check mode, see if our target is a compiled loop header entry and do OSR if so.
+ *
+ * Our most common case is expected to be a taken backward branch with active jit profiling,
+ * but no full OSR check and no pending suspend request.
+ * Next most common case is not-taken branch with no full OSR check.
+ *
+ */
+MterpCommonTakenBranchNoFlags:
+    bgtz    rINST, .L_forward_branch    # don't add forward branches to hotness
+/*
+ * We need to subtract 1 from positive values and we should not see 0 here,
+ * so we may use the result of the comparison with -1.
+ */
+    li      t4, JIT_CHECK_OSR
+    beq     rPROFILE, t4, .L_osr_check
+    blt     rPROFILE, t4, .L_resume_backward_branch
+    addi    rPROFILE, rPROFILE, -1
+    beqz    rPROFILE, .L_add_batch      # counted down to zero - report
+.L_resume_backward_branch:
+    lw      ra, THREAD_FLAGS_OFFSET(rSELF)
+    REFRESH_IBASE
+    add     a2, rINST, rINST            # a2<- byte offset
+    FETCH_ADVANCE_INST_RB a2            # update rPC, load rINST
+    andi    ra, ra, THREAD_SUSPEND_OR_CHECKPOINT_REQUEST
+    bnez    ra, .L_suspend_request_pending
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+.L_suspend_request_pending:
+    EXPORT_PC
+    move    a0, rSELF
+    jal     MterpSuspendCheck           # (self)
+    bnez    a0, MterpFallback
+    REFRESH_IBASE                       # might have changed during suspend
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+.L_no_count_backwards:
+    li      t4, JIT_CHECK_OSR           # check for possible OSR re-entry
+    bne     rPROFILE, t4, .L_resume_backward_branch
+.L_osr_check:
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rINST
+    EXPORT_PC
+    jal MterpMaybeDoOnStackReplacement  # (self, shadow_frame, offset)
+    bnez    a0, MterpOnStackReplacement
+    j       .L_resume_backward_branch
+
+.L_forward_branch:
+    li      t4, JIT_CHECK_OSR           # check for possible OSR re-entry
+    beq     rPROFILE, t4, .L_check_osr_forward
+.L_resume_forward_branch:
+    add     a2, rINST, rINST            # a2<- byte offset
+    FETCH_ADVANCE_INST_RB a2            # update rPC, load rINST
+    GET_INST_OPCODE a0                # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+.L_check_osr_forward:
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rINST
+    EXPORT_PC
+    jal     MterpMaybeDoOnStackReplacement # (self, shadow_frame, offset)
+    bnez    a0, MterpOnStackReplacement
+    j       .L_resume_forward_branch
+
+.L_add_batch:
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    sh      rPROFILE, SHADOWFRAME_HOTNESS_COUNTDOWN_OFFSET(a1)
+    ld      a0, OFF_FP_METHOD(rFP)
+    move    a2, rSELF
+    jal     MterpAddHotnessBatch        # (method, shadow_frame, self)
+    move    rPROFILE, a0                # restore new hotness countdown to rPROFILE
+    j       .L_no_count_backwards
+
+/*
+ * Entered from the conditional branch handlers when OSR check request active on
+ * not-taken path.  All Dalvik not-taken conditional branch offsets are 2.
+ */
+.L_check_not_taken_osr:
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    li      a2, 2
+    EXPORT_PC
+    jal     MterpMaybeDoOnStackReplacement # (self, shadow_frame, offset)
+    bnez    a0, MterpOnStackReplacement
+    FETCH_ADVANCE_INST 2
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+/*
+ * On-stack replacement has happened, and now we've returned from the compiled method.
+ */
+MterpOnStackReplacement:
+#if MTERP_LOGGING
+    move    a0, rSELF
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rINST                               # rINST contains offset
+    jal     MterpLogOSR
+#endif
+    li      a0, 1                                   # Signal normal return
+    j       MterpDone
+
+/*
+ * Bail out to reference interpreter.
+ */
+    .extern MterpLogFallback
+MterpFallback:
+    EXPORT_PC
+#if MTERP_LOGGING
+    move    a0, rSELF
+    addi a1, rFP, OFF_FP_SHADOWFRAME
+    jal     MterpLogFallback
+#endif
+MterpCommonFallback:
+    li      a0, 0                                   # signal retry with reference interpreter.
+    j       MterpDone
+
+/*
+ * We pushed some registers on the stack in ExecuteMterpImpl, then saved
+ * SP and RA.  Here we restore SP, restore the registers, and then restore
+ * RA to PC.
+ *
+ * On entry:
+ *  uint32_t* rFP  (should still be live, pointer to base of vregs)
+ */
+MterpExceptionReturn:
+    li      a0, 1                                   # signal return to caller.
+    j       MterpDone
+/*
+ * Returned value is expected in a0 and if it's not 64-bit, the 32 most
+ * significant bits of a0 must be zero-extended or sign-extended
+ * depending on the return type.
+ */
+MterpReturn:
+    ld      a2, OFF_FP_RESULT_REGISTER(rFP)
+    sd      a0, 0(a2)
+    li      a0, 1                                   # signal return to caller.
+MterpDone:
+/*
+ * At this point, we expect rPROFILE to be non-zero.  If negative, hotness is disabled or we're
+ * checking for OSR.  If greater than zero, we might have unreported hotness to register
+ * (the difference between the ending rPROFILE and the cached hotness counter).  rPROFILE
+ * should only reach zero immediately after a hotness decrement, and is then reset to either
+ * a negative special state or the new non-zero countdown value.
+ */
+    blez    rPROFILE, .L_pop_and_return # if > 0, we may have some counts to report.
+
+MterpProfileActive:
+    move    rINST, a0                   # stash return value
+    /* Report cached hotness counts */
+    ld      a0, OFF_FP_METHOD(rFP)
+    addi    a1, rFP, OFF_FP_SHADOWFRAME
+    move    a2, rSELF
+    sh      rPROFILE, SHADOWFRAME_HOTNESS_COUNTDOWN_OFFSET(a1)
+    jal     MterpAddHotnessBatch        # (method, shadow_frame, self)
+    move    a0, rINST                   # restore return value
+
+.L_pop_and_return:
+    ld      s11, STACK_OFFSET_S11(sp)
+    .cfi_restore 27
+    ld      s10, STACK_OFFSET_S10(sp)
+    .cfi_restore 26
+    ld      s9, STACK_OFFSET_S9(sp)
+    .cfi_restore 25
+    ld      s8, STACK_OFFSET_S8(sp)
+    .cfi_restore 24
+    ld      s7, STACK_OFFSET_S7(sp)
+    .cfi_restore 23
+    ld      s6, STACK_OFFSET_S6(sp)
+    .cfi_restore 22
+    ld      s5, STACK_OFFSET_S5(sp)
+    .cfi_restore 21
+    ld      s4, STACK_OFFSET_S4(sp)
+    .cfi_restore 20
+    ld      s3, STACK_OFFSET_S3(sp)
+    .cfi_restore 19
+    ld      s2, STACK_OFFSET_S2(sp)
+    .cfi_restore 18
+    ld      s1, STACK_OFFSET_S1(sp)
+    .cfi_restore 17
+    ld      s0, STACK_OFFSET_S0(sp)
+    .cfi_restore 16
+
+    ld      ra, STACK_OFFSET_RA(sp)
+    .cfi_restore 31
+
+    # ld      t8, STACK_OFFSET_GP(sp)
+    # .cpreturn
+    # .cfi_restore 28
+
+    # .set    noreorder
+    addi  sp, sp, STACK_SIZE
+    .cfi_adjust_cfa_offset -STACK_SIZE
+    jr      ra
+
+    .cfi_endproc
+    # .set    reorder
+    # .size ExecuteMterpImpl, .-ExecuteMterpImpl
+
+%def instruction_end():
+
+    .hidden artMterpAsmInstructionEnd
+    .global artMterpAsmInstructionEnd
+artMterpAsmInstructionEnd:
+
+%def instruction_start():
+
+    .hidden artMterpAsmInstructionStart
+    .global artMterpAsmInstructionStart
+artMterpAsmInstructionStart = .L_op_nop
+    .text
+
+%def opcode_start():
+    ENTRY mterp_${opcode}
+%def opcode_end():
+    END mterp_${opcode}
+%def helper_start(name):
+    ENTRY ${name}
+%def helper_end(name):
+    END ${name}
diff --git a/runtime/interpreter/mterp/riscv64/object.S b/runtime/interpreter/mterp/riscv64/object.S
new file mode 100644
index 0000000000..be04a2bf2f
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/object.S
@@ -0,0 +1,270 @@
+%def field(helper=""):
+ .extern $helper
+    move     a0, rPC                       // arg0: Instruction* inst
+    move     a1, rINST                     // arg1: uint16_t inst_data
+    addi     a2, rFP, OFF_FP_SHADOWFRAME  // arg2: ShadowFrame* sf
+    move     a3, rSELF                     // arg3: Thread* self
+    PREFETCH_INST 2                        // prefetch next opcode
+    jal      $helper
+    beqz     a0, MterpPossibleException
+    ADVANCE 2
+    GET_INST_OPCODE a0                     // extract opcode from rINST
+    GOTO_OPCODE a0                         // jump to next in
+
+%def op_check_cast():
+    /*
+     * Check to see if a cast from one class to another is allowed.
+     */
+    /* check-cast vAA, class//BBBB */
+    .extern MterpCheckCast
+    EXPORT_PC
+    lhu     a0, 2(rPC)                  # a0 <- BBBB
+    srli     a1, rINST, 8                # a1 <- AA
+    DLSA    a1, a1, rFP, 2              # a1 <- &object
+    ld      a2, OFF_FP_METHOD(rFP)      # a2 <- method
+    move    a3, rSELF                   # a3 <- self
+    jal     MterpCheckCast              # (index, &obj, method, self)
+    PREFETCH_INST 2
+    bnez    a0, MterpPossibleException
+    ADVANCE 2
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_iget(is_object="0", helper="MterpIGetU32"):
+%  field(helper=helper)
+
+%def op_iget_boolean():
+%  op_iget(helper="MterpIGetU8")
+
+%def op_iget_boolean_quick():
+%  op_iget_quick(load="lbu")
+
+%def op_iget_byte():
+%  op_iget(helper="MterpIGetI8")
+
+%def op_iget_byte_quick():
+%  op_iget_quick(load="lb")
+
+%def op_iget_char():
+%  op_iget(helper="MterpIGetU16")
+
+%def op_iget_char_quick():
+%  op_iget_quick(load="lhu")
+
+%def op_iget_object():
+%  op_iget(is_object="1", helper="MterpIGetObj")
+
+%def op_iget_object_quick():
+    /* For: iget-object-quick */
+    /* op vA, vB, offset//CCCC */
+    .extern artIGetObjectFromMterp
+    srli     a2, rINST, 12               # a2 <- B
+    lhu     a1, 2(rPC)                  # a1 <- field byte offset
+    EXPORT_PC
+    GET_VREG a0, a2                   # a0 <- object we're operating on
+    jal     artIGetObjectFromMterp      # (obj, offset)
+
+    ld      a3, THREAD_EXCEPTION_OFFSET(rSELF)
+    EXT   a2, rINST, 8, 4             # a2 <- A
+    PREFETCH_INST 2
+    bnez    a3, MterpPossibleException  # bail out
+    SET_VREG_OBJECT a0, a2              # fp[A] <- v0
+    ADVANCE 2                           # advance rPC
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_iget_quick(load="lw"):
+    /* For: iget-quick, iget-boolean-quick, iget-byte-quick, iget-char-quick, iget-short-quick */
+    /* op vA, vB, offset//CCCC */
+    srli     a2, rINST, 12               # a2 <- B
+    lhu     a1, 2(rPC)                  # a1 <- field byte offset
+    GET_VREG a3, a2                     # a3 <- object we're operating on
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    beqz    a3, common_errNullObject    # object was null
+    add     a1, a1, a3
+    $load   a0, 0(a1)                   # a0 <- obj.field
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    SET_VREG a0, a2                     # fp[A] <- a0
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_iget_short():
+%  op_iget(helper="MterpIGetI16")
+
+%def op_iget_short_quick():
+%  op_iget_quick(load="lh")
+
+%def op_iget_wide():
+%  op_iget(helper="MterpIGetU64")
+
+%def op_iget_wide_quick():
+    /* iget-wide-quick vA, vB, offset//CCCC */
+    srli     a2, rINST, 12               # a2 <- B
+    lhu     a4, 2(rPC)                  # a4 <- field byte offset
+    GET_VREG a3, a2                   # a3 <- object we're operating on
+    EXT   a2, rINST, 8, 4               # a2 <- A
+    beqz    a3, common_errNullObject    # object was null
+    add     a4, a3, a4                  # create direct pointer
+    ld      a0, 0(a4)
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    SET_VREG_WIDE a0, a2
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_instance_of():
+    /*
+     * Check to see if an object reference is an instance of a class.
+     *
+     * Most common situation is a non-null object, being compared against
+     * an already-resolved class.
+     */
+    /* instance-of vA, vB, class//CCCC */
+    .extern MterpInstanceOf
+    EXPORT_PC
+    lhu     a0, 2(rPC)                  # a0 <- CCCC
+    srli     a1, rINST, 12               # a1 <- B
+    DLSA    a1, a1, rFP, 2              # a1 <- &object
+    ld      a2, OFF_FP_METHOD(rFP)      # a2 <- method
+    move    a3, rSELF                   # a3 <- self
+    jal     MterpInstanceOf             # (index, &obj, method, self)
+
+    ld      a1, THREAD_EXCEPTION_OFFSET(rSELF)
+    EXT   a2, rINST, 8, 4             # a2 <- A
+    PREFETCH_INST 2
+    bnez    a1, MterpException
+    ADVANCE 2                           # advance rPC
+    SET_VREG a0, a2                     # vA <- v0
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_iput(is_object="0", helper="MterpIPutU32"):
+%  field(helper=helper)
+
+%def op_iput_boolean():
+%  op_iput(helper="MterpIPutU8")
+
+%def op_iput_boolean_quick():
+%  op_iput_quick(store="sb")
+
+%def op_iput_byte():
+%  op_iput(helper="MterpIPutI8")
+
+%def op_iput_byte_quick():
+%  op_iput_quick(store="sb")
+
+%def op_iput_char():
+%  op_iput(helper="MterpIPutU16")
+
+%def op_iput_char_quick():
+%  op_iput_quick(store="sh")
+
+%def op_iput_object():
+%  op_iput(is_object="1", helper="MterpIPutObj")
+
+%def op_iput_object_quick():
+    .extern MterpIputObjectQuick
+    EXPORT_PC
+    addi    a0, rFP, OFF_FP_SHADOWFRAME
+    move    a1, rPC
+    move    a2, rINST
+    jal     MterpIputObjectQuick
+    beqz    a0, MterpException
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_iput_quick(store="sw"):
+    /* For: iput-quick, iput-boolean-quick, iput-byte-quick, iput-char-quick, iput-short-quick */
+    /* op vA, vB, offset//CCCC */
+    srli     a2, rINST, 12               # a2 <- B
+    lhu     a1, 2(rPC)                  # a1 <- field byte offset
+    GET_VREG a3, a2                   # a3 <- fp[B], the object pointer
+    EXT   a2, rINST, 8, 4               # a2 <- A
+    beqz    a3, common_errNullObject    # object was null
+    GET_VREG a0, a2                     # a0 <- fp[A]
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    add     a1, a1, a3
+    $store  a0, 0(a1)                   # obj.field <- a0
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_iput_short():
+%  op_iput(helper="MterpIPutI16")
+
+%def op_iput_short_quick():
+%  op_iput_quick(store="sh")
+
+%def op_iput_wide():
+%  op_iput(helper="MterpIPutU64")
+
+%def op_iput_wide_quick():
+    /* iput-wide-quick vA, vB, offset//CCCC */
+    srli     a2, rINST, 12               # a2 <- B
+    lhu     a3, 2(rPC)                  # a3 <- field byte offset
+    GET_VREG a2, a2                   # a2 <- fp[B], the object pointer
+    EXT     a0, rINST, 8, 4             # a0 <- A
+    beqz    a2, common_errNullObject    # object was null
+    GET_VREG_WIDE a0, a0                # a0 <- fp[A]
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    add     a1, a2, a3                  # create a direct pointer
+    sd      a0, 0(a1)
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_new_instance():
+    /*
+     * Create a new instance of a class.
+     */
+    /* new-instance vAA, class//BBBB */
+    .extern MterpNewInstance
+    EXPORT_PC
+    addi    a0, rFP, OFF_FP_SHADOWFRAME
+    move    a1, rSELF
+    move    a2, rINST
+    jal     MterpNewInstance            # (shadow_frame, self, inst_data)
+    beqz   a0, MterpPossibleException
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def op_sget(is_object="0", helper="MterpSGetU32"):
+%  field(helper=helper)
+
+%def op_sget_boolean():
+%  op_sget(helper="MterpSGetU8")
+
+%def op_sget_byte():
+%  op_sget(helper="MterpSGetI8")
+
+%def op_sget_char():
+%  op_sget(helper="MterpSGetU16")
+
+%def op_sget_object():
+%  op_sget(is_object="1", helper="MterpSGetObj")
+
+%def op_sget_short():
+%  op_sget(helper="MterpSGetI16")
+
+%def op_sget_wide():
+%  op_sget(helper="MterpSGetU64")
+
+%def op_sput(is_object="0", helper="MterpSPutU32"):
+%  field(helper=helper)
+
+%def op_sput_boolean():
+%  op_sput(helper="MterpSPutU8")
+
+%def op_sput_byte():
+%  op_sput(helper="MterpSPutI8")
+
+%def op_sput_char():
+%  op_sput(helper="MterpSPutU16")
+
+%def op_sput_object():
+%  op_sput(is_object="1", helper="MterpSPutObj")
+
+%def op_sput_short():
+%  op_sput(helper="MterpSPutI16")
+
+%def op_sput_wide():
+%  op_sput(helper="MterpSPutU64")
diff --git a/runtime/interpreter/mterp/riscv64/other.S b/runtime/interpreter/mterp/riscv64/other.S
new file mode 100644
index 0000000000..024d812266
--- /dev/null
+++ b/runtime/interpreter/mterp/riscv64/other.S
@@ -0,0 +1,368 @@
+%def const(helper="UndefinedConstHandler"):
+    /* const/class vAA, type@BBBB */
+    /* const/method-handle vAA, method_handle@BBBB */
+    /* const/method-type vAA, proto@BBBB */
+    /* const/string vAA, string@@BBBB */
+    .extern $helper
+    EXPORT_PC
+    lhu     a0, 2(rPC)                  # a0 <- BBBB
+    srli    a1, rINST, 8                # a1 <- AA
+    addi     a2, rFP, OFF_FP_SHADOWFRAME
+    move    a3, rSELF
+    PREFETCH_INST 2                     # load rINST
+    jal     $helper                     # (index, tgt_reg, shadow_frame, self)
+
+    bnez    a0, MterpPossibleException  # let reference interpreter deal with it.
+    ADVANCE 2                           # advance rPC
+    GET_INST_OPCODE a0                  # extract opcode from rINST
+    GOTO_OPCODE a0                      # jump to next instruction
+
+%def unused():
+/*
+ * Bail to reference interpreter to throw.
+ */
+    j       MterpFallback
+
+%def op_const():
+    /* const vAA, #+BBBBbbbb */
+    srli    a2, rINST, 8                # a2 <- AA
+    lhu     a0, 2(rPC)                  # a0 <- bbbb (low)
+    lhu     a1, 4(rPC)                  # a1 <- BBBB (high)
+    FETCH_ADVANCE_INST 3                # advance rPC, load rINST
+    slli    a1, a1, 16                  # a0 = BBBBbbbb
+    or     a0, a0, a1
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a2                     # vAA <- +BBBBbbbb
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_16():
+    /* const/16 vAA, #+BBBB */
+    srli    a2, rINST, 8                # a2 <- AA
+    lh      a0, 2(rPC)                  # a0 <- sign-extended BBBB
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a2                     # vAA <- +BBBB
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_4():
+    /* const/4 vA, #+B */
+    # EXT  rINST[11:8]      --> a2      # a2 <- A
+    li     t6, 0x0F00
+    and    a2, rINST, t6
+    srli   a2, a2, 8
+
+    # Sign EXT rINST[15:12] --> a0      # a0 <- B
+    lh       a0, 0(rPC)                 # sign extend load A and B in a0
+    srai     a0, a0, 12                 # shift B into its final position
+
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a2                     # vA <- +B
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_class():
+%  const(helper="MterpConstClass")
+
+%def op_const_high16():
+    /* const/high16 vAA, #+BBBB0000 */
+    srli    a2, rINST, 8                # a2 <- AA
+    lh      a0, 2(rPC)                  # a0 <- BBBB
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    sll     a0, a0, 16                  # a0 <- BBBB0000
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG a0, a2                     # vAA <- +BBBB0000
+    GOTO_OPCODE t4                     # jump to next instruction
+
+%def op_const_method_handle():
+%  const(helper="MterpConstMethodHandle")
+
+%def op_const_method_type():
+%  const(helper="MterpConstMethodType")
+
+%def op_const_string():
+%  const(helper="MterpConstString")
+
+%def op_const_string_jumbo():
+    /* const/string vAA, String//BBBBBBBB */
+    .extern MterpConstString
+    EXPORT_PC
+    lh      a0, 2(rPC)                  # a0 <- bbbb (low)
+    lh      a4, 4(rPC)                  # a4 <- BBBB (high)
+    srli     a1, rINST, 8                # a1 <- AA
+    INS     a0, a4, 16, 16              # a0 <- BBBBbbbb
+    addi    a2, rFP, OFF_FP_SHADOWFRAME
+    move    a3, rSELF
+    PREFETCH_INST 3                     # load rINST
+    jal     MterpConstString            # (index, tgt_reg, shadow_frame, self)
+
+    bnez    a0, MterpPossibleException  # let reference interpreter deal with it.
+    ADVANCE 3                           # advance rPC
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_wide():
+    /* const-wide vAA, #+HHHHhhhhBBBBbbbb */
+    srli    a4, rINST, 8                # a4 <- AA
+    lh      a0, 2(rPC)                  # a0 <- bbbb (low)
+    lh      a1, 4(rPC)                  # a1 <- BBBB (low middle)
+    lh      a2, 6(rPC)                  # a2 <- hhhh (high middle)
+    lh      a3, 8(rPC)                  # a3 <- HHHH (high)
+    FETCH_ADVANCE_INST 5                # advance rPC, load rINST
+    INS     a0, a1, 16, 16              # a0 = BBBBbbbb
+    INS     a2, a3, 16, 16              # a2 = HHHHhhhh
+    DINSU   a0, a2, 32, 32              # a0 = HHHHhhhhBBBBbbbb
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a4                # vAA <- +HHHHhhhhBBBBbbbb
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_wide_16():
+    /* const-wide/16 vAA, #+BBBB */
+    srli     a2, rINST, 8                # a2 <- AA
+    lh      a0, 2(rPC)                  # a0 <- sign-extended BBBB
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vAA <- +BBBB
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_wide_32():
+    /* const-wide/32 vAA, #+BBBBbbbb */
+    srli     a2, rINST, 8                # a2 <- AA
+    lh      a0, 2(rPC)                  # a0 <- bbbb (low)
+    lh      a1, 4(rPC)                  # a1 <- BBBB (high)
+    FETCH_ADVANCE_INST 3                # advance rPC, load rINST
+    INS     a0, a1, 16, 16              # a0 = BBBBbbbb
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vAA <- +BBBBbbbb
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_const_wide_high16():
+    /* const-wide/high16 vAA, #+BBBB000000000000 */
+    srli     a2, rINST, 8                # a2 <- AA
+    lh      a0, 2(rPC)                  # a0 <- BBBB
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    slli    a0, a0, 48                  # dsll32  a0, a0, 16                  # a0 <- BBBB000000000000
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vAA <- +BBBB000000000000
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_monitor_enter():
+    /*
+     * Synchronize on an object.
+     */
+    /* monitor-enter vAA */
+    .extern artLockObjectFromCode
+    EXPORT_PC
+    srli     a2, rINST, 8                # a2 <- AA
+    GET_VREG_U a0, a2                   # a0 <- vAA (object)
+    move    a1, rSELF                   # a1 <- self
+    jal     artLockObjectFromCode
+    bnez    a0, MterpException
+    FETCH_ADVANCE_INST 1
+    ld      a0, THREAD_USE_MTERP_OFFSET(rSELF)
+    beqz    a0, MterpFallback
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_monitor_exit():
+    /*
+     * Unlock an object.
+     *
+     * Exceptions that occur when unlocking a monitor need to appear as
+     * if they happened at the following instruction.  See the Dalvik
+     * instruction spec.
+     */
+    /* monitor-exit vAA */
+    .extern artUnlockObjectFromCode
+    EXPORT_PC
+    srli     a2, rINST, 8                # a2 <- AA
+    GET_VREG_U a0, a2                   # a0 <- vAA (object)
+    move    a1, rSELF                   # a1 <- self
+    jal     artUnlockObjectFromCode     # v0 <- success for unlock(self, obj)
+    bnez    a0, MterpException
+    FETCH_ADVANCE_INST 1                # before throw: advance rPC, load rINST
+    ld      a0, THREAD_USE_MTERP_OFFSET(rSELF)
+    beqz    a0, MterpFallback
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move(is_object="0"):
+    /* for move, move-object, long-to-int */
+    /* op vA, vB */
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_VREG a0, a3                     # a0 <- vB
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    .if $is_object
+    SET_VREG_OBJECT a0, a2              # vA <- vB
+    .else
+    SET_VREG a0, a2                     # vA <- vB
+    .endif
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_16(is_object="0"):
+    /* for: move/16, move-object/16 */
+    /* op vAAAA, vBBBB */
+    lhu     a3, 4(rPC)                  # a3 <- BBBB
+    lhu     a2, 2(rPC)                  # a2 <- AAAA
+    FETCH_ADVANCE_INST 3                # advance rPC, load rINST
+    GET_VREG a0, a3                     # a0 <- vBBBB
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    .if $is_object
+    SET_VREG_OBJECT a0, a2              # vAAAA <- vBBBB
+    .else
+    SET_VREG a0, a2                     # vAAAA <- vBBBB
+    .endif
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_exception():
+    /* move-exception vAA */
+    srli    a2, rINST, 8                # a2 <- AA
+    ld      a0, THREAD_EXCEPTION_OFFSET(rSELF)  # load exception obj
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    SET_VREG_OBJECT a0, a2              # vAA <- exception obj
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    sd      zero, THREAD_EXCEPTION_OFFSET(rSELF)  # clear exception
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_from16(is_object="0"):
+    /* for: move/from16, move-object/from16 */
+    /* op vAA, vBBBB */
+    lhu     a3, 2(rPC)                  # a3 <- BBBB
+    srli     a2, rINST, 8                # a2 <- AA
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_VREG a0, a3                     # a0 <- vBBBB
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    .if $is_object
+    SET_VREG_OBJECT a0, a2              # vAA <- vBBBB
+    .else
+    SET_VREG a0, a2                     # vAA <- vBBBB
+    .endif
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_object():
+%  op_move(is_object="1")
+
+%def op_move_object_16():
+%  op_move_16(is_object="1")
+
+%def op_move_object_from16():
+%  op_move_from16(is_object="1")
+
+%def op_move_result(is_object="0"):
+    /* for: move-result, move-result-object */
+    /* op vAA */
+    srli     a2, rINST, 8                # a2 <- AA
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    ld      a0, OFF_FP_RESULT_REGISTER(rFP)  # get pointer to result JType
+    lw      a0, 0(a0)                   # a0 <- result.i
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    .if $is_object
+    SET_VREG_OBJECT a0, a2              # vAA <- result
+    .else
+    SET_VREG a0, a2                     # vAA <- result
+    .endif
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_result_object():
+%  op_move_result(is_object="1")
+
+%def op_move_result_wide():
+    /* for: move-result-wide */
+    /* op vAA */
+    srli     a2, rINST, 8                # a2 <- AA
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    ld      a0, OFF_FP_RESULT_REGISTER(rFP)  # get pointer to result JType
+    ld      a0, 0(a0)                   # a0 <- result.j
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vAA <- result
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_wide():
+    /* move-wide vA, vB */
+    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
+    EXT     a3, rINST, 12, 4            # a3 <- B
+    EXT     a2, rINST, 8, 4             # a2 <- A
+    GET_VREG_WIDE a0, a3                # a0 <- vB
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vA <- vB
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_wide_16():
+    /* move-wide/16 vAAAA, vBBBB */
+    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
+    lhu     a3, 4(rPC)                  # a3 <- BBBB
+    lhu     a2, 2(rPC)                  # a2 <- AAAA
+    GET_VREG_WIDE a0, a3                # a0 <- vBBBB
+    FETCH_ADVANCE_INST 3                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vAAAA <- vBBBB
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_move_wide_from16():
+    /* move-wide/from16 vAA, vBBBB */
+    /* NOTE: regs can overlap, e.g. "move v6,v7" or "move v7,v6" */
+    lhu     a3, 2(rPC)                  # a3 <- BBBB
+    srli     a2, rINST, 8                # a2 <- AA
+    GET_VREG_WIDE a0, a3                # a0 <- vBBBB
+    FETCH_ADVANCE_INST 2                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    SET_VREG_WIDE a0, a2                # vAA <- vBBBB
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_nop():
+    FETCH_ADVANCE_INST 1                # advance rPC, load rINST
+    GET_INST_OPCODE t4                  # extract opcode from rINST
+    GOTO_OPCODE t4                      # jump to next instruction
+
+%def op_unused_3e():
+%  unused()
+
+%def op_unused_3f():
+%  unused()
+
+%def op_unused_40():
+%  unused()
+
+%def op_unused_41():
+%  unused()
+
+%def op_unused_42():
+%  unused()
+
+%def op_unused_43():
+%  unused()
+
+%def op_unused_79():
+%  unused()
+
+%def op_unused_7a():
+%  unused()
+
+%def op_unused_f3():
+%  unused()
+
+%def op_unused_f4():
+%  unused()
+
+%def op_unused_f5():
+%  unused()
+
+%def op_unused_f6():
+%  unused()
+
+%def op_unused_f7():
+%  unused()
+
+%def op_unused_f8():
+%  unused()
+
+%def op_unused_f9():
+%  unused()
+
+%def op_unused_fc():
+%  unused()
+
+%def op_unused_fd():
+%  unused()
diff --git a/runtime/jit/jit.cc b/runtime/jit/jit.cc
index b828aafcb0..190db16048 100644
--- a/runtime/jit/jit.cc
+++ b/runtime/jit/jit.cc
@@ -48,6 +48,8 @@
 #include "thread-inl.h"
 #include "thread_list.h"
 
+#include <fstream>
+
 namespace art {
 namespace jit {
 
diff --git a/runtime/mirror/object-readbarrier-inl.h b/runtime/mirror/object-readbarrier-inl.h
index ee84997fe6..d2c6517825 100644
--- a/runtime/mirror/object-readbarrier-inl.h
+++ b/runtime/mirror/object-readbarrier-inl.h
@@ -107,7 +107,7 @@ inline uint32_t Object::GetReadBarrierState(uintptr_t* fake_address_dependency)
   LockWord lw(static_cast<uint32_t>(result));
   uint32_t rb_state = lw.ReadBarrierState();
   return rb_state;
-#elif defined(__i386__) || defined(__x86_64__)
+#elif defined(__i386__) || defined(__x86_64__) || defined(__riscv)
   LockWord lw = GetLockWord(false);
   // i386/x86_64 don't need fake address dependency. Use a compiler fence to avoid compiler
   // reordering.
diff --git a/runtime/native_stack_dump.cc b/runtime/native_stack_dump.cc
index 150fa7823c..7781e4a194 100644
--- a/runtime/native_stack_dump.cc
+++ b/runtime/native_stack_dump.cc
@@ -328,6 +328,9 @@ void DumpNativeStack(std::ostream& os,
   }
   std::unique_ptr<Backtrace> backtrace(Backtrace::Create(BACKTRACE_CURRENT_PROCESS, tid, map));
   backtrace->SetSkipFrames(skip_frames);
+
+  // FIXME: T-HEAD, skip unwind for rv64, enable in the future.
+#if 0
   if (!backtrace->Unwind(0, reinterpret_cast<ucontext*>(ucontext_ptr))) {
     os << prefix << "(backtrace::Unwind failed for thread " << tid
        << ": " <<  backtrace->GetErrorString(backtrace->GetError()) << ")" << std::endl;
@@ -336,6 +339,7 @@ void DumpNativeStack(std::ostream& os,
     os << prefix << "(no native stack frames for thread " << tid << ")" << std::endl;
     return;
   }
+#endif
 
   // Check whether we have and should use addr2line.
   bool use_addr2line;
diff --git a/runtime/parsed_options_test.cc b/runtime/parsed_options_test.cc
index ca2a4ea32e..edc1c19dff 100644
--- a/runtime/parsed_options_test.cc
+++ b/runtime/parsed_options_test.cc
@@ -160,13 +160,14 @@ TEST_F(ParsedOptionsTest, ParsedOptionsInstructionSet) {
     EXPECT_EQ(kRuntimeISA, isa);
   }
 
-  const char* isa_strings[] = { "arm", "arm64", "x86", "x86_64", "mips", "mips64" };
+  const char* isa_strings[] = { "arm", "arm64", "x86", "x86_64", "mips", "mips64", "riscv64" };
   InstructionSet ISAs[] = { InstructionSet::kArm,
                             InstructionSet::kArm64,
                             InstructionSet::kX86,
                             InstructionSet::kX86_64,
                             InstructionSet::kMips,
-                            InstructionSet::kMips64 };
+                            InstructionSet::kMips64,
+                            InstructionSet::kRiscv64 };
   static_assert(arraysize(isa_strings) == arraysize(ISAs), "Need same amount.");
 
   for (size_t i = 0; i < arraysize(isa_strings); ++i) {
diff --git a/runtime/runtime.cc b/runtime/runtime.cc
index 51a40e78c6..491353eb31 100644
--- a/runtime/runtime.cc
+++ b/runtime/runtime.cc
@@ -1436,6 +1436,7 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
     case InstructionSet::kX86_64:
     case InstructionSet::kMips:
     case InstructionSet::kMips64:
+    case InstructionSet::kRiscv64:
       implicit_null_checks_ = true;
       // Historical note: Installing stack protection was not playing well with Valgrind.
       implicit_so_checks_ = true;
@@ -2294,6 +2295,7 @@ void Runtime::SetInstructionSet(InstructionSet instruction_set) {
     case InstructionSet::kMips64:
     case InstructionSet::kX86:
     case InstructionSet::kX86_64:
+    case InstructionSet::kRiscv64:
       break;
     default:
       UNIMPLEMENTED(FATAL) << instruction_set_;
diff --git a/runtime/runtime_callbacks_test.cc b/runtime/runtime_callbacks_test.cc
index 4a60c045cc..76e5c64061 100644
--- a/runtime/runtime_callbacks_test.cc
+++ b/runtime/runtime_callbacks_test.cc
@@ -150,6 +150,8 @@ TEST_F(ThreadLifecycleCallbackRuntimeCallbacksTest, ThreadLifecycleCallbackJava)
   // Make sure the workers are done starting so we don't get callbacks for them.
   runtime_->WaitForThreadPoolWorkersToStart();
 
+  // FIXME: T-HEAD, [workaround] wait runtime ready
+  sleep(5);
   cb_.state = CallbackState::kBase;  // Ignore main thread attach.
 
   {
diff --git a/test/Android.bp b/test/Android.bp
index 9cf56067d8..c8d70a9778 100644
--- a/test/Android.bp
+++ b/test/Android.bp
@@ -36,6 +36,9 @@ art_cc_defaults {
         android_x86_64: {
             relative_install_path: "art/x86_64",
         },
+		android_riscv64: {
+            relative_install_path: "art/riscv64",
+		},
         darwin: {
             enabled: false,
         },
diff --git a/test/etc/run-test-jar b/test/etc/run-test-jar
index bcd35e6e35..e9d62ea98a 100755
--- a/test/etc/run-test-jar
+++ b/test/etc/run-test-jar
@@ -12,7 +12,7 @@ ANDROID_ROOT="/system"
 ANDROID_RUNTIME_ROOT="/apex/com.android.runtime"
 ANDROID_TZDATA_ROOT="/apex/com.android.tzdata"
 ARCHITECTURES_32="(arm|x86|mips|none)"
-ARCHITECTURES_64="(arm64|x86_64|mips64|none)"
+ARCHITECTURES_64="(arm64|x86_64|mips64|riscv64|none)"
 ARCHITECTURES_PATTERN="${ARCHITECTURES_32}"
 BOOT_IMAGE=""
 CHROOT=
@@ -1021,7 +1021,7 @@ if [ "$HOST" = "n" ]; then
              export ANDROID_LOG_TAGS=$ANDROID_LOG_TAGS && \
              rm -rf ${DEX_LOCATION}/dalvik-cache/ && \
              mkdir -p ${mkdir_locations} && \
-             export LD_LIBRARY_PATH=$LD_LIBRARY_PATH && \
+             export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/system/lib64/:/system/apex/com.android.runtime.debug/lib64/ && \
              export PATH=$BIN_DIR:$PATH && \
              $profman_cmdline && \
              $dex2oat_cmdline && \
@@ -1031,7 +1031,8 @@ if [ "$HOST" = "n" ]; then
              $sync_cmdline && \
              $dalvikvm_cmdline"
 
-    cmdfile=$(tempfile -p "cmd-" -s "-$TEST_NAME")
+    #cmdfile=$(tempfile -p "cmd-" -s "-$TEST_NAME")
+    cmdfile=$(mktemp  "cmd-$-TEST_NAME-XXXXXX")
     echo "$cmdline" > $cmdfile
 
     if [ "$DEV_MODE" = "y" ]; then
diff --git a/test/run-test b/test/run-test
index a2d180ee28..3b20014db1 100755
--- a/test/run-test
+++ b/test/run-test
@@ -597,7 +597,7 @@ function guess_target_arch_name() {
         fi
     else
         grep32bit=`ls ${ANDROID_PRODUCT_OUT}/data/art-test | grep -E '^(arm|x86|mips)$'`
-        grep64bit=`ls ${ANDROID_PRODUCT_OUT}/data/art-test | grep -E '^(arm64|x86_64|mips64)$'`
+        grep64bit=`ls ${ANDROID_PRODUCT_OUT}/data/art-test | grep -E '^(arm64|x86_64|mips64|riscv64)$'`
         if [ "x${suffix64}" = "x64" ]; then
             target_arch_name=${grep64bit}
         else
diff --git a/tools/checker/common/archs.py b/tools/checker/common/archs.py
index 178e0b5bc5..5222512f56 100644
--- a/tools/checker/common/archs.py
+++ b/tools/checker/common/archs.py
@@ -12,4 +12,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-archs_list = ['ARM', 'ARM64', 'MIPS', 'MIPS64', 'X86', 'X86_64']
+archs_list = ['ARM', 'ARM64', 'MIPS', 'MIPS64', 'X86', 'X86_64', "RISCV64"]
diff --git a/tools/cpp-define-generator/make_header.py b/tools/cpp-define-generator/make_header.py
index 1b13923b50..f3657b1db1 100755
--- a/tools/cpp-define-generator/make_header.py
+++ b/tools/cpp-define-generator/make_header.py
@@ -31,7 +31,7 @@ import sys
 def convert(input):
   """Find all defines in the compiler generated assembly and convert them to #define pragmas"""
 
-  asm_define_re = re.compile(r'">>(\w+) (?:\$|#)([-0-9]+) (?:\$|#)(0|1)<<"')
+  asm_define_re = re.compile(r'">>(\w+) (?:\$|#)?([-0-9]+) (?:\$|#)?(0|1)<<"')
   asm_defines = asm_define_re.findall(input)
   if not asm_defines:
     raise RuntimeError("Failed to find any asm defines in the input")
-- 
2.18.4

